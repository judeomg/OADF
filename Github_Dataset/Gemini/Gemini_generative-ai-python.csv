"https://github.com/google-gemini/generative-ai-python/issues/610","Request for Proxy Configuration Support in google-generativeai Library to Route Traffic Through Proxies","2024-10-18T13:14:53Z","Open issue","component:support,status:triaged,type:feature request","Description of the feature request:
I need to use the google-generativeai Python library with routing its api requests through a proxy. However, the library doesn't natively support passing proxy configurations directly when initializing the client.
Feature Request
Add support for passing proxy configurations (e.g., as part of client_options) directly to the google-generativeai client, ensuring all network requests—including file uploads—are routed through the proxy.
What problem are you trying to solve with this feature?
This would avoid the need for global proxy settings or reverse proxies, offering a cleaner and more robust solution in proxy-restricted environments.
Any other information you'd like to share?
I have tried the following approaches, each with limitations:
Global Environment Variables (os.environ['HTTP_PROXY'], os.environ['HTTPS_PROXY)
This works but affects all network requests globally, which is problematic in my multi-threaded, multi-process environment. Managing when to apply or remove these settings isn't practical.
Reverse Proxy with Custom API Endpoint
I used genai.configure() with a custom api_endpoint parameter that routes requests through a reverse proxy. This works for model.generate_content function but fails with the genai.upload_file function, as it hardcodes Google's endpoint, bypassing the custom API endpoint.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/607","AttributeError","2024-10-17T16:07:56Z","Closed issue","No label","Description of the bug:
module 'google.generativeai.protos' has no attribute 'DynamicRetrievalConfig'. Did you mean: 'SemanticRetrieverConfig'?
I'm getting this error in the import line
import google.generativeai as genai
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/606","ModuleNotFoundError: No module named 'google.generativeai'","2024-10-17T08:06:31Z","Open issue","component:python sdk,status:awaiting user response,type:help","Description of the bug:
Using the following code:
import os
import google.generativeai as genai
I tried every method there was.
I even did:
pip install google-generativeai
pip install -U google-generativeai
pip install google-ai-generativelanguage
Please help me with this
Actual vs expected behavior:
Working in colab, but giving the error ; ""ModuleNotFoundError: No module named 'google.generativeai'""
Any other information you'd like to share?
import os
 import google.generativeai as genai
GOOGLE_API_KEY=""---""
 genai.configure(api_key=os.environ[GOOGLE_API_KEY])
prompt = input(""You: "")
Generate a response
model = genai.GenerativeModel(""gemini-1.5-flash"")
 response = model.generate_content(prompt)
 print(response.text)
 The text was updated successfully, but these errors were encountered: 
👍1
lzell reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/google-gemini/generative-ai-python/issues/605","GenerationConfig Error: 'seed' Field Missing in Protocol Message but Present in GenerationConfig Class","2024-10-17T05:47:43Z","Open issue","component:python sdk,status:awaiting user response,type:help","Description of the feature request:
I'm trying to set the seed for Gemini using the 'google-generativeai' library:
model = genai.GenerativeModel(
    model_name,
    system_instruction=system_instruction,
    generation_config=GenerationConfig(
        temperature=0.0,
        seed=42
    )
)

However, I'm getting the following error:
""ValueError: Protocol message GenerationConfig has no \""seed\"" field.""
I understand that this happens because the GenerationConfig(proto.Message) class from google.generativeai.types doesn't have a seed attribute. However, this attribute does exist in the GenerationConfig class, which is accepted as a type in GenerationConfigType = Union[protos.GenerationConfig, GenerationConfigDict, GenerationConfig] from google.generativeai.types.
I believe this is an inconsistency
What problem are you trying to solve with this feature?
I would like to set a seed value in the GenerationConfig when using the Gemini API to ensure reproducibility of the generated output. Since the seed attribute already exists in the GenerationConfig class, adding support for it in protos.GenerationConfig would provide a more consistent experience
Any other information you'd like to share?
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/604","Quickstart script is not working as expected","2024-10-17T13:54:52Z","Closed issue","component:python sdk,status:awaiting user response,type:help","Description of the bug:
I am following the quickstart script to generate content
import os
import google.generativeai as genai
import dotenv
dotenv.load_dotenv()
GEMINI_API_KEY = os.getenv('GEMINI_API_KEY')
#print(GEMINI_API_KEY)
genai.configure(api_key=GEMINI_API_KEY)
model = genai.GenerativeModel(""gemini-1.5-flash"")
response = model.generate_content(""Write a story about a magic backpack."")
print(response.text)

Actual vs expected behavior:
Actual behaviour :

 nothing is being printed, no error nothing, gemini has got stuck i have no idea how to debug this
Expected behavior:
 response must be printed
Any other information you'd like to share?
GEMINI_API_KEY is corrrect and new , even tried printing it and it prints correctly
 python version: 3.12.6
 OS: mac
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/601","automatic chat with function call, chat history no function call record","2024-10-16T01:53:10Z","Open issue","component:python sdk,status:awaiting user response,type:help","Description of the bug:
def run_auto_function_calling():
    """"""    Function calls naturally fit in to [multi-turn chats](https://ai.google.dev/api/python/google/generativeai/GenerativeModel#multi-turn) as they capture a back and forth interaction between the user and model. The Python SDK's [`ChatSession`](https://ai.google.dev/api/python/google/generativeai/ChatSession) is a great interface for chats because handles the conversation history for you, and using the parameter `enable_automatic_function_calling` simplifies function calling even further    """"""
    model = genai.GenerativeModel(
        model_name=""gemini-1.5-flash-latest"",
        tools=[
            add,
            subtract,
            multiply,
            divide],
        system_instruction=""You are a helpful assistant who converses with a user and answers questions. Respond concisely to general questions. "",
    )
    chat = model.start_chat(enable_automatic_function_calling=True)
    response = chat.send_message(
        [
            # ""what's your name?"",
            ""I have 57 cats, each owns 44 mittens, how many mittens is that in total?"",
        ],
        # stream=True, # enable_automatic_function_calling=True, unsupport stream
    )
    print(f""run_auto_function_calling response: {response}"")
    for content in chat.history:
        print(content.role, ""->"", [type(part).to_dict(part) for part in content.parts])
        print(""-"" * 80)
Actual vs expected behavior:
result:
user -> [{'text': 'I have 57 cats, each owns 44 mittens, how many mittens is that in total?'}]
--------------------------------------------------------------------------------
model -> [{'function_call': {'name': 'multiply', 'args': {'a': 57.0, 'b': 44.0}}}]
--------------------------------------------------------------------------------
user -> [{'function_response': {'name': 'multiply', 'response': {'result': 2508.0}}}]
--------------------------------------------------------------------------------
model -> [{'text': ""That's 2508 mittens! \n""}]
--------------------------------------------------------------------------------

open ""what's your name?"",
 result:
user -> [{'text': ""what's your name?""}, {'text': 'I have 57 cats, each owns 44 mittens, how many mittens is that in total?'}]
--------------------------------------------------------------------------------
model -> [{'text': ""I don't have a name.  That's 2508 mittens. \n""}]
--------------------------------------------------------------------------------

no function_call history
Any other information you'd like to share?
pip show google-generativeai
Name: google-generativeai
Version: 0.8.3
Summary: Google Generative AI High level API client library and tools.
Home-page: https://github.com/google/generative-ai-python
Author: Google LLC
Author-email: googleapis-packages@google.com
License: Apache 2.0
Location: /Users/wuyong/project/python/chat-bot/.venv_achatbot/lib/python3.11/site-packages
Requires: google-ai-generativelanguage, google-api-core, google-api-python-client, google-auth, protobuf, pydantic, tqdm, typing-extensions
Required-by:

 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/598","AttributeError: Unknown field for Candidate: finish_message. Did you mean: 'finish_reason'?","2024-10-14T05:21:15Z","Closed issue","No label","Description of the bug:
When calling .text on the response it may fail with an AttributeError:
yaml_content = _parse_model_response(response.text)
^^^^^^^^^^^^^
File ""/usr/local/lib/python3.12/site-packages/google/generativeai/types/generation_types.py"", line 476, in text
if candidate.finish_message:
^^^^^^^^^^^^^^^^^^^^^^^^
File ""/usr/local/lib/python3.12/site-packages/proto/message.py"", line 906, in __getattr__
raise AttributeError(
AttributeError: Unknown field for Candidate: finish_message. Did you mean: 'finish_reason'?

Actual vs expected behavior:
No response
Any other information you'd like to share?
This appears to be lines recently changed in #527
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/597","module 'google.generativeai' has no attribute 'ImageGenerationModel' for Image Generating","2024-10-13T15:31:05Z","Open issue","component:python sdk,type:help","Description of the bug:
I am using this script for gemini api to generate image:
import os
import google.generativeai as genai

genai.configure(api_key=os.environ['GEMINI_API_KEY'])


model  = genai.ImageGenerationModel(""imagen-3.0-generate-001"")

result = model.generate_images(
    prompt=""Fuzzy bunnies in my kitchen"",
    number_of_images=2,
    safety_filter_level=""block_only_high"",
    person_generation=""allow_adult"",
    aspect_ratio=""3:4"",
    negative_prompt=""Outside"",
)

for image in result.images:
  print(image)

# The output should look similar to this:
# <vertexai.preview.vision_models.GeneratedImage object at 0x78f3396ef370>
# <vertexai.preview.vision_models.GeneratedImage object at 0x78f3396ef700>
# <vertexai.preview.vision_models.GeneratedImage object at 0x78f33953c2b0>
# <vertexai.preview.vision_models.GeneratedImage object at 0x78f33953c280>

for image in result.images:
  # Open and display the image using your local operating system.
  image._pil_image.show()

Actual vs expected behavior:
I am using python 3.10 but when I run the code then showig this error:
PS C:\Users\M i Monir\Documents\python project\gemini content writer> & ""C:/Users/M i Monir/AppData/Local/Programs/Python/Python310/python.exe"" ""c:/Users/M i Monir/Documents/python project/gemini content writer/gemini image generator.py""
Traceback (most recent call last):
  File ""c:\Users\M i Monir\Documents\python project\gemini content writer\gemini image generator.py"", line 7, in <module>
    model  = genai.ImageGenerationModel(""imagen-3.0-generate-001"")
AttributeError: module 'google.generativeai' has no attribute 'ImageGenerationModel'
PS C:\Users\M i Monir\Documents\python project\gemini content writer> 

How to fix it
Any other information you'd like to share?
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/595","chat.py example not working","2024-10-11T16:21:01Z","Open issue","component:python sdk,status:triaged,type:bug","Description of the bug:
When I execute the example of the ""chat.py"" code, only the first execution of the command ""response = chat.send_message(...)"" works.
 Every further execution of the ""send_message"" function in the multi-turn chat fails.
Debug output attached.
debug.txt
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/594","HARM_CATEGORY_CIVIC_INTEGRITY","2024-10-10T15:34:28Z","Open issue","component:other,status:triaged,type:help","Description of the feature request:
There's a new satefy filer Harm Category for generate-content:
 HARM_CATEGORY_CIVIC_INTEGRITY
What problem are you trying to solve with this feature?
Using updated safety filters
Any other information you'd like to share?
https://ai.google.dev/api/generate-content
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/593","Unexpected behavior with some Spanish prompts than finish reason is OTHER","2024-10-09T15:12:39Z","Open issue","component:python sdk,status:triaged,type:bug","Description of the bug:
I have a Python script that generates menus by reading them from a list and completing a prompt (the prompts are in Spanish). However, for some reason, certain prompts return a ValueError. After some testing, I was able to reduce the issue to a small script, shown below:
import google.generativeai as genai
import os


genai.configure(api_key=os.environ[""GEMINI_API_KEY""])

model = genai.GenerativeModel(model_name=""gemini-1.5-flash"")
response = model.generate_content(
    'Genera una lista de ingredientes del siguiente menu: ""Cafe con leche""',
    generation_config = genai.GenerationConfig(
        max_output_tokens=1000,
        temperature=0.1,
    )
)
print(response)
print(response.text)

When I change the menu (e.g., to ""Té con leche""), it fails, and the error message is not clear enough for me to understand the cause, it shows ""finish_reason"": ""OTHER""
Actual vs expected behavior:
Some examples:
1 -
import google.generativeai as genai
import os


genai.configure(api_key=os.environ[""GEMINI_API_KEY""])

model = genai.GenerativeModel(model_name=""gemini-1.5-flash"")
response = model.generate_content(
    'Genera una lista de ingredientes del siguiente menu: ""Cafe con leche""',
    generation_config = genai.GenerationConfig(
        max_output_tokens=1000,
        temperature=0.1,
    )
)
print(response)
print(response.text)

RESULT OK:
response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      ""candidates"": [
        {
          ""content"": {
            ""parts"": [
              {
                ""text"": ""## Ingredientes para Caf\u00e9 con Leche:\n\n* **Caf\u00e9:** (molido o en grano)\n* **Leche:** (entera, semidesnatada, desnatada, vegetal)\n* **Agua:** (para preparar el caf\u00e9)\n* **Az\u00facar:** (opcional)\n""
              }
            ],
            ""role"": ""model""
          },
          ""finish_reason"": ""STOP"",
          ""index"": 0,
          ""safety_ratings"": [
            {
              ""category"": ""HARM_CATEGORY_SEXUALLY_EXPLICIT"",
              ""probability"": ""NEGLIGIBLE""
            },
            {
              ""category"": ""HARM_CATEGORY_HATE_SPEECH"",
              ""probability"": ""NEGLIGIBLE""
            },
            {
              ""category"": ""HARM_CATEGORY_HARASSMENT"",
              ""probability"": ""NEGLIGIBLE""
            },
            {
              ""category"": ""HARM_CATEGORY_DANGEROUS_CONTENT"",
              ""probability"": ""NEGLIGIBLE""
            }
          ]
        }
      ],
      ""usage_metadata"": {
        ""prompt_token_count"": 16,
        ""candidates_token_count"": 60,
        ""total_token_count"": 76
      }
    }),
)
## Ingredientes para Café con Leche:

* **Café:** (molido o en grano)
* **Leche:** (entera, semidesnatada, desnatada, vegetal)
* **Agua:** (para preparar el café)
* **Azúcar:** (opcional)


2 -
import google.generativeai as genai
import os


genai.configure(api_key=os.environ[""GEMINI_API_KEY""])

model = genai.GenerativeModel(model_name=""gemini-1.5-flash"")
response = model.generate_content(
    'Genera una lista de ingredientes del siguiente menu: ""Te con leche""',
    generation_config = genai.GenerationConfig(
        max_output_tokens=1000,
        temperature=0.1,
    )
)
print(response)
print(response.text)

response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      ""candidates"": [
        {
          ""finish_reason"": ""OTHER"",
          ""index"": 0
        }
      ],
      ""usage_metadata"": {
        ""prompt_token_count"": 16,
        ""total_token_count"": 16
      }
    }),
)
Traceback (most recent call last):
  File ""/home/mdarino/Desktop/foodcards/ai_menus/error.py"", line 16, in <module>
    print(response.text)
          ^^^^^^^^^^^^^
  File ""/home/mdarino/Desktop/foodcards/ai_menus/.venv/lib/python3.12/site-packages/google/generativeai/types/generation_types.py"", line 495, in text
    raise ValueError(msg)
ValueError: Invalid operation: The `response.text` quick accessor requires the response to contain a valid `Part`, but none were returned. The candidate's [finish_reason](https://ai.google.dev/api/generate-content#finishreason) is 5.


Any other information you'd like to share?
Another example:
Prompt:'Genera una lista de ingredientes del siguiente menu: ""Pizza Mexicana (carne, jalapeños, frijoles, mozzarella)""',
RESULT NOT OK:
response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      ""candidates"": [
        {
          ""finish_reason"": ""OTHER"",
          ""index"": 0
        }
      ],
      ""usage_metadata"": {
        ""prompt_token_count"": 27,
        ""total_token_count"": 27
      }
    }),
)
Traceback (most recent call last):
  File ""/home/mdarino/Desktop/foodcards/ai_menus/error.py"", line 16, in <module>
    print(response.text)
          ^^^^^^^^^^^^^
  File ""/home/mdarino/Desktop/foodcards/ai_menus/.venv/lib/python3.12/site-packages/google/generativeai/types/generation_types.py"", line 495, in text
    raise ValueError(msg)
ValueError: Invalid operation: The `response.text` quick accessor requires the response to contain a valid `Part`, but none were returned. The candidate's [finish_reason](https://ai.google.dev/api/generate-content#finishreason) is 5.


But Prompt:'Genera una lista de ingredientes del siguiente menu: ""Pizza de camarones (camarones, ajo, mozzarella)""',
RETURN OK:
response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=protos.GenerateContentResponse({
      ""candidates"": [
        {
          ""content"": {
            ""parts"": [
              {
                ""text"": ""## Ingredientes para Pizza de Camarones:\n\n* **Base de pizza:** (Harina, agua, levadura, aceite de oliva, sal)\n* **Salsa de tomate:** (Tomates, cebolla, ajo, especias)\n* **Camarones:** (Frescos o congelados)\n* **Ajo:** (Picado o en polvo)\n* **Mozzarella:** (Rallada o en rebanadas)\n* **Aceite de oliva:** (Para rociar)\n* **Or\u00e9gano:** (Para espolvorear)\n* **Sal y pimienta:** (Al gusto)\n\n**Opcionales:**\n\n* **Ceboll\u00edn:** (Picado para decorar)\n* **Perejil:** (Picado para decorar)\n* **Queso parmesano:** (Rallado para espolvorear)\n* **Piment\u00f3n dulce:** (Para espolvorear)\n* **Aj\u00ed picante:** (Para agregar un toque picante)\n""
              }
            ],
            ""role"": ""model""
          },
          ""finish_reason"": ""STOP"",
          ""index"": 0,
          ""safety_ratings"": [
            {
              ""category"": ""HARM_CATEGORY_SEXUALLY_EXPLICIT"",
              ""probability"": ""NEGLIGIBLE""
            },
            {
              ""category"": ""HARM_CATEGORY_HATE_SPEECH"",
              ""probability"": ""NEGLIGIBLE""
            },
            {
              ""category"": ""HARM_CATEGORY_HARASSMENT"",
              ""probability"": ""NEGLIGIBLE""
            },
            {
              ""category"": ""HARM_CATEGORY_DANGEROUS_CONTENT"",
              ""probability"": ""NEGLIGIBLE""
            }
          ]
        }
      ],
      ""usage_metadata"": {
        ""prompt_token_count"": 25,
        ""candidates_token_count"": 205,
        ""total_token_count"": 230
      }
    }),
)
## Ingredientes para Pizza de Camarones:

* **Base de pizza:** (Harina, agua, levadura, aceite de oliva, sal)
* **Salsa de tomate:** (Tomates, cebolla, ajo, especias)
* **Camarones:** (Frescos o congelados)
* **Ajo:** (Picado o en polvo)
* **Mozzarella:** (Rallada o en rebanadas)
* **Aceite de oliva:** (Para rociar)
* **Orégano:** (Para espolvorear)
* **Sal y pimienta:** (Al gusto)

**Opcionales:**

* **Cebollín:** (Picado para decorar)
* **Perejil:** (Picado para decorar)
* **Queso parmesano:** (Rallado para espolvorear)
* **Pimentón dulce:** (Para espolvorear)
* **Ají picante:** (Para agregar un toque picante)


 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/592","stop_sequence terminates if shown in model response","2024-10-09T15:03:57Z","Open issue","component:python sdk,status:awaiting user response,type:help","Description of the bug:
Hi everyone,
is this normal that response is terminating while defined stop_sequence shows in response?
I followed gemini API guide an set generation config as proposed (only increased max_output_tokens):
model = genai.GenerativeModel('gemini-1.5-flash')
 response = model.generate_content(
 'Tell me a story about a magic backpack.',
 generation_config=genai.types.GenerationConfig(
 # Only one candidate for now.
 candidate_count=1,
 stop_sequences=['x'],
 max_output_tokens=2000,
 temperature=1.0)
 )
At gemini API docs there is:
""stopSequences specifies the set of character sequences (up to 5) that will stop output generation. If specified, the API will stop at the first appearance of a stop_sequence. The stop sequence won't be included as part of the response.""
Actual vs expected behavior:
Actual behavior:
In my case responding drops out on first ""x"" shown in the response.
Expected behavior:
Intuitively I thought that responding will be terminated when I pass stop_sequence (in this case ""x"") from keyboard. And I thought that the same is said on docs by phrase ""The stop sequence won't be included as part of the response"".
Any other information you'd like to share?
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/590","Publish Python API docs somewhere","2024-10-08T23:38:41Z","Open issue","good first issue,type:feature request","Description of the feature request:
The Python API docs are just markdown/HTML files in the docs/ directory. We should publish them somewhere like readthedocs or a GitHub page so that the content is rendered in an easily navigable way.
What problem are you trying to solve with this feature?
Improving Python API doc usability.
Any other information you'd like to share?
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/589","Strange harmful word action","2024-10-11T19:41:30Z","Closed issue","component:python sdk,type:feature request","Description of the bug:
Hi, I'm a newbie to using Gemini API, but I've found strange action that is taken by Gemini model.
I don't even know if this is kind an issue that should be given to you as a bug feedback- cause it's not a programming bug but rather strange perception of model to some word.
 I wanted to join the Gemini API to my porfolio code- which is a programme to help exchange gifts to return some help if somedoby doesn't know what to wish.
When using Gemini API quickstart everything works fine, but when I prompt other phrase I get an error.
My code (simplified- just to show the problem):
import os
import google.generativeai as genai

GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')
genai.configure(api_key=GOOGLE_API_KEY)
model = genai.GenerativeModel('gemini-1.5-flash')

messages = [{'role':'user',
     'parts': [""Hi, could you help me and generate 3 gift suggestions up to £150? I like fishing and romantic movies.""]}
]

response = model.generate_content(messages)
for chunk in response:
    print(chunk.text)

Actual vs expected behavior:
For this code I get an error:
Traceback (most recent call last):
  File ""..\"", line 34, in <module>
    print(chunk.text)
          ^^^^^^^^^^
  File ""..\Python\Python312\site-packages\google\generativeai\types\generation_types.py"", line 476, in text
    if candidate.finish_message:
       ^^^^^^^^^^^^^^^^^^^^^^^^
  File ""..\Python\Python312\site-packages\proto\message.py"", line 906, in __getattr__
    raise AttributeError(
AttributeError: Unknown field for Candidate: finish_message. Did you mean: 'finish_reason'?
Process finished with exit code 1

What a concern was when I started investigate this issue and it occurs that word ""romantic"" raises this exception! I realize that it was not any programming bug as I thought at the beggining, but safety_setting.
Then I've tried with some violent prompt: 'Hi, could you help me and generate 3 gift suggestions up to £150? I like fishing and s*x movies.'
 And then model have take accurate action, cause it returns:
""I understand you're looking for gift suggestions, but I'm programmed to provide safe and ethical responses. I can't recommend gifts related to adult content.
However, I can help you find great fishing gifts! Here are 3 options under £150:
 ...""
Proposed behavior:
It will be clearer if API will raise a ValueError with information which phrase violates security_setting insted of above
REMARK:
I don't know if Gemini API is automatically upgrading cause I've been asking Gemini for help with that error- and now (after ca ~3h) when I'm trying to activate above error I get different answer for the same code:
ValueError: (""Invalid operation: The response.text quick accessor requires the response to contain a valid Part, but none were returned. The candidate's finish_reason is 3. The candidate's safety_ratings are: [category: HARM_CATEGORY_SEXUALLY_EXPLICIT\nprobability: MEDIUM\n, category: HARM_CATEGORY_HATE_SPEECH\nprobability: NEGLIGIBLE\n, category: HARM_CATEGORY_HARASSMENT\nprobability: NEGLIGIBLE\n, category: HARM_CATEGORY_DANGEROUS_CONTENT\nprobability: NEGLIGIBLE\n]."", [category: HARM_CATEGORY_SEXUALLY_EXPLICIT
 probability: MEDIUM
 , category: HARM_CATEGORY_HATE_SPEECH
 probability: NEGLIGIBLE
 , category: HARM_CATEGORY_HARASSMENT
 probability: NEGLIGIBLE
 , category: HARM_CATEGORY_DANGEROUS_CONTENT
 probability: NEGLIGIBLE
 ])
Any other information you'd like to share?
So I spent few hours wondering and investigating what programming bug I've made, and finnaly it occurs that API raised wrong error- for word 'romantic' :)
And what a bad luck, that I use forbidden phrase ""romantic movie"" :)
 Or maybe it's a good luck, cause now I know how security_settings works :)
And as a I noticed above- I don't know if Gemini can upgrade Gemini API automatically or it's just a coincidence that sb changes this today, but now it returns clearer statement for this error.
Take care!
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/587","User location is not supported for the API use","2024-10-05T19:43:35Z","Open issue","component:python sdk,status:triaged,type:bug","Description of the bug:
My production just went down out of the blue, after 2 weeks since integrating Gemini.
Actual vs expected behavior:
No response
Any other information you'd like to share?
I'm currently on the Pay-as-you-go plan.
pyproject.toml:
google-generativeai = ""^0.8.1""
I'm using the most basic config:
import google.generativeai as genai

genai.configure(api_key=os.environ[""GOOGLE_AI_KEY""])

model = genai.GenerativeModel(
    model_name=""gemini-1.5-flash"",
    system_instruction=""..."",
)

file = genai.upload_file(path=file_path, display_name=file_name)


Stack trace:
ResumableUploadError: <HttpError 400 when requesting None returned ""User location is not supported for the API use."". Details: ""User location is not supported for the API use."">
  File ""starlette/applications.py"", line 122, in __call__
    await self.middleware_stack(scope, receive, send)
  File ""starlette/middleware/errors.py"", line 184, in __call__
    raise exc
  File ""starlette/middleware/errors.py"", line 162, in __call__
    await self.app(scope, receive, _send)
  File ""starlette/middleware/base.py"", line 108, in __call__
    response = await self.dispatch_func(request, call_next)
  File ""main.py"", line 60, in logging_middleware
    response: Response = await call_next(request)
  File ""starlette/middleware/base.py"", line 84, in call_next
    raise app_exc
  File ""starlette/middleware/base.py"", line 70, in coro
    await self.app(scope, receive_or_disconnect, send_no_error)
  File ""starlette/middleware/cors.py"", line 83, in __call__
    await self.app(scope, receive, send)
  File ""starlette/middleware/exceptions.py"", line 79, in __call__
    raise exc
  File ""starlette/middleware/exceptions.py"", line 68, in __call__
    await self.app(scope, receive, sender)
  File ""fastapi/middleware/asyncexitstack.py"", line 20, in __call__
    raise e
  File ""fastapi/middleware/asyncexitstack.py"", line 17, in __call__
    await self.app(scope, receive, send)
  File ""starlette/routing.py"", line 718, in __call__
    await route.handle(scope, receive, send)
  File ""starlette/routing.py"", line 276, in handle
    await self.app(scope, receive, send)
  File ""starlette/routing.py"", line 66, in app
    response = await func(request)
  File ""fastapi/routing.py"", line 274, in app
    raw_response = await run_endpoint_function(
  File ""fastapi/routing.py"", line 191, in run_endpoint_function
    return await dependant.call(**values)
    (app stack omitted)
        file = genai.upload_file(path=file_path, display_name=file_name)
  File ""/app/.venv/lib/python3.11/site-packages/google/generativeai/files.py"", line 71, in upload_file
    response = client.create_file(
  File ""/app/.venv/lib/python3.11/site-packages/google/generativeai/client.py"", line 114, in create_file
    result = request.execute()
  File ""googleapiclient/_helpers.py"", line 130, in positional_wrapper
    return wrapped(*args, **kwargs)
  File ""googleapiclient/http.py"", line 902, in execute
    _, body = self.next_chunk(http=http, num_retries=num_retries)
  File ""googleapiclient/_helpers.py"", line 130, in positional_wrapper
    return wrapped(*args, **kwargs)
  File ""googleapiclient/http.py"", line 1022, in next_chunk
    raise ResumableUploadError(resp, content)

My servers are located in Warsaw. I can successfully run from localhost also from Poland, but all of my prod requests are failing.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/585","Couldn't build proto file into descriptor pool","2024-10-04T17:43:39Z","Open issue","component:python sdk,status:awaiting user response,type:help","Description of the bug:
I encounter the error ""TypeError: Couldn't build proto file into descriptor pool: duplicate symbol ""'google.ai.generativelanguage.v1beta.firstlineno'"", when trying to import this module.
Actual vs expected behavior:
The module should import correctly and work as in the Quickstart guide.
Any other information you'd like to share?
I am using PyCharm and the module installed is google-generativeai.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/584","Cannot parse value of 'http_proxy' env var","2024-10-04T02:11:44Z","Open issue","component:python sdk,status:awaiting user response,status:stale,type:help","Description of the bug:
I test the python code and got the following error msg,
 How to solve this issue? thanks.
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
 E0000 00:00:1728007314.658664 284 http_proxy_mapper.cc:130] cannot parse value of 'http_proxy' env var. Error: INVALID_ARGUMENT: Could not parse 'scheme' from uri '127.0.0.1:7890'. Scheme must begin with an alpha character [A-Za-z].
Actual vs expected behavior:
No response
Any other information you'd like to share?
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/578","can't ask for JSON data when using fine-tuned model","2024-09-30T22:47:03Z","Open issue","component:python sdk,type:feature request","Description of the bug:
when i use gemini-1.5-flash or gemini-1.5-pro i have no problem asking for json return like so
model = genai.GenerativeModel(""gemini-1.5-flash"",system_instruction=""you're a bot that answers questions by referencing Quran verses or Hadith or Quotes from notable islamic scholars."")

response_schema = """"""""{  ""type"": ""object"",  ""properties"": {    ""answers"": {      ""type"": ""array"",      ""items"": {        ""type"": ""object"",        ""properties"": {          ""type"": {            ""type"": ""string"",            ""enum"": [              ""quran"",              ""hadith"",              ""quote""            ]          },          ""text"": {            ""type"": ""string""          },          ""chapter_number"": {            ""type"": ""integer""          },          ""verse_number"": {            ""type"": ""string""          }        },        ""required"": [          ""type"",          ""text""        ]      }    },    ""conclusion"": {      ""type"": ""string""    }  },  ""required"": [    ""answers"",    ""conclusion""  ]}""""""

response = model.generate_content(
    ""my mom is sad, what should i do?"",
   generation_config = {
  ""temperature"": 1,
  ""max_output_tokens"": 8192,
        'response_mime_type':""application/json"",
        ""response_schema"":response_schema

}
)
this sometimes doesn't respect the response_schema structure, so i decided to fine-tune a model to get more predictable data. i did that and basically replaced the model name like so
model = genai.GenerativeModel(""tunedModels/quranchat-q68kkpnh5aad"",system_instruction=""""""you're a bot that answers questions by referencing Quran verses or Hadith or Quotes from notable islamic scholars."""""")
also dealing with the oauth from this documentation https://ai.google.dev/gemini-api/docs/oauth as i'm now using a fine-tuned model.
but now, when i run the same code i get this error
400 Developer instruction is not enabled for tunedModels/quranchat-q68kkpnh5aad and after some horrendous hours trying to see what this means, i realized it's because i have system_instruction variable, so i deleted that and ended up with this code
model = genai.GenerativeModel(""tunedModels/quranchat-q68kkpnh5aad"")
and ran the file again, now i get this error
400 Json mode is not enabled for tunedModels/quranchat-q68kkpnh5aad
 which this one is also happening because i have 'response_mime_type':""application/json"" in my generation_config variable.
Actual vs expected behavior:
so i basically realized i can't use system instruction nor ask for json response when using fine-tuned data, is this a library restriction or gemini?
Any other information you'd like to share?
let me know if there's anything more i can share
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/577","Support python Literal types when defining JSON or FunctionCalling schemas.","2024-09-30T17:03:52Z","Open issue","type:feature request","Description of the feature request:
String literal types should create Enums in the generated schema.
What problem are you trying to solve with this feature?
No response
Any other information you'd like to share?
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/575","Gemini API returns 500 with special characters","2024-09-27T19:01:33Z","Open issue","component:python sdk,status:triaged,type:bug","Description of the bug:
This exact code:
text = ""čku“ s hodnotami ANO a NE 	+ „""
model = genai.GenerativeModel(""gemini-1.5-flash"")
result = model.generate_content(
    f""Can you summarize this text?\n{text}""
)
print(f""{result.text=}"")

returns
InternalServerError: 500 An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting

Other malfunctioning strings:
'muláře \t- chybně vyplněný formulář \t- ji'

The problem seems to be with
U+0009 : <control> CHARACTER TABULATION [TAB] {horizontal tabulation (HT); tab}

Actual vs expected behavior:
Actual:
 Returns 500
Expected:
 Using Czech letters & Czech punctuation will not return 500 and the text will be summarised using Gemini, as requested
Any other information you'd like to share?
Windows 10,

sys.version_info(major=3, minor=9, micro=13, releaselevel='final', serial=0)

google-ai-generativelanguage             0.6.10
google-api-core                          2.17.1
google-api-python-client                 2.119.0
google-auth                              2.25.2
google-auth-httplib2                     0.2.0
google-cloud-aiplatform                  1.49.0
google-cloud-bigquery                    3.17.2
google-cloud-core                        2.4.1
google-cloud-resource-manager            1.12.2
google-cloud-storage                     2.14.0
google-crc32c                            1.5.0
google-generativeai                      0.8.2
google-resumable-media                   2.7.0
google-search-results                    2.4.2
googleapis-common-protos                 1.62.0
grpc-google-iam-v1                       0.13.0
langchain-google-genai                   0.0.6

 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/574","Add support to prompt with a YouTube Video URL","2024-09-27T06:40:30Z","Open issue","component:python sdk,status:triaged,type:feature request","Description of the feature request:
In Vertex AI it is finally possible to attach a YouTube Video URL as an alternative to upload a video file. It would be nice to have feature parity here and be able to call something like:
 model.generate_content(youtube-video-id='xxxx', ""Summarize this video"")
What problem are you trying to solve with this feature?
Be able to send a prompt using a YouTube Video URL/ID
Any other information you'd like to share?
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/573","Internal Server Error (500) when using Gemini API with Inspect framework","2024-09-26T00:51:47Z","Open issue","component:python sdk,status:triaged,type:help","Description of the bug:
Description
I'm consistently encountering an Internal Server Error (HTTP 500) when trying to use the Google Gemini API through the Inspect evaluation framework. This error occurs during the generate_content call. I'm currently on the free trial.
Steps to Reproduce
Set up an evaluation using the Inspect framework
Configure the evaluation to use the Gemini model (in my case, google/gemini-1.5-pro)
Run the evaluation
Error Message
InternalServerError: 500 An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting
Environment
Operating System: MacOS Sonoma 14.5
Python version: 3.12.4
Package Versions
google-ai-generativelanguage: 0.6.6
google-api-core: 2.19.2
google-api-python-client: 2.143.0
google-auth: 2.34.0
google-auth-httplib2: 0.2.0
google-generativeai: 0.7.2
googleapis-common-protos: 1.65.0
Full error traceback:
╭─ benchmarks/gpqa (78 samples): google/gemini-1.5-pro ─────────────────────────────────────────────────────────────────────╮
│ ╭─────────────────────────────── Traceback (most recent call last) ────────────────────────────────╮   dataset: (samples) │
│ │ /Users/lenni/Documents/GitHub/biology-benchmarks/.venv/lib/python3.12/site-packages/inspect_ai/… │       scorer: choice │
│ │ in task_run                                                                                      │                      │
│ │                                                                                                  │                      │
│ │ /Users/lenni/Documents/GitHub/biology-benchmarks/.venv/lib/python3.12/site-packages/inspect_ai/… │                      │
│ │ in task_run_sample                                                                               │                      │
│ │                                                                                                  │                      │
│ │ /Users/lenni/Documents/GitHub/biology-benchmarks/.venv/lib/python3.12/site-packages/inspect_ai/… │                      │
│ │ in __call__                                                                                      │                      │
│ │                                                                                                  │                      │
│ │ /Users/lenni/Documents/GitHub/biology-benchmarks/.venv/lib/python3.12/site-packages/inspect_ai/… │                      │
│ │ in task_run_sample                                                                               │                      │
│ │                                                                                                  │                      │
│ │ /Users/lenni/Documents/GitHub/biology-benchmarks/.venv/lib/python3.12/site-packages/inspect_ai/… │                      │
│ │ in solve                                                                                         │                      │
│ │                                                                                                  │                      │
│ │ /Users/lenni/Documents/GitHub/biology-benchmarks/.venv/lib/python3.12/site-packages/inspect_ai/… │                      │
│ │ in generate                                                                                      │                      │
│ │                                                                                                  │                      │
│ │ /Users/lenni/Documents/GitHub/biology-benchmarks/.venv/lib/python3.12/site-packages/inspect_ai/… │                      │
│ │ in task_generate                                                                                 │                      │
│ │                                                                                                  │                      │
│ │ /Users/lenni/Documents/GitHub/biology-benchmarks/.venv/lib/python3.12/site-packages/inspect_ai/… │                      │
│ │ in generate                                                                                      │                      │
│ │                                                                                                  │                      │
│ │ /Users/lenni/Documents/GitHub/biology-benchmarks/.venv/lib/python3.12/site-packages/inspect_ai/… │                      │
│ │ in _generate                                                                                     │                      │
│ │                                                                                                  │                      │
│ │ /Users/lenni/Documents/GitHub/biology-benchmarks/.venv/lib/python3.12/site-packages/tenacity/as… │                      │
│ │ in async_wrapped                                                                                 │                      │
│ │                                                                                                  │                      │
│ │ /Users/lenni/Documents/GitHub/biology-benchmarks/.venv/lib/python3.12/site-packages/tenacity/as… │                      │
│ │ in __call__                                                                                      │                      │
│ │                                                                                                  │                      │
│ │ /Users/lenni/Documents/GitHub/biology-benchmarks/.venv/lib/python3.12/site-packages/tenacity/as… │                      │
│ │ in iter                                                                                          │                      │
│ │                                                                                                  │                      │
│ │ /Users/lenni/Documents/GitHub/biology-benchmarks/.venv/lib/python3.12/site-packages/tenacity/_u… │                      │
│ │ in inner                                                                                         │                      │
│ │                                                                                                  │                      │
│ │ /Users/lenni/Documents/GitHub/biology-benchmarks/.venv/lib/python3.12/site-packages/tenacity/__… │                      │
│ │ in <lambda>                                                                                      │                      │
│ │                                                                                                  │                      │
│ │ /opt/homebrew/Cellar/python@3.12/3.12.4/Frameworks/Python.framework/Versions/3.12/lib/python3.1… │                      │
│ │ in result                                                                                        │                      │
│ │                                                                                                  │                      │
│ │   446 │   │   │   │   if self._state in [CANCELLED, CANCELLED_AND_NOTIFIED]:                     │                      │
│ │   447 │   │   │   │   │   raise CancelledError()                                                 │                      │
│ │   448 │   │   │   │   elif self._state == FINISHED:                                              │                      │
│ │ ❱ 449 │   │   │   │   │   return self.__get_result()                                             │                      │
│ │   450 │   │   │   │                                                                              │                      │
│ │   451 │   │   │   │   self._condition.wait(timeout)                                              │                      │
│ │   452                                                                                            │                      │
│ │                                                                                                  │                      │
│ │ /opt/homebrew/Cellar/python@3.12/3.12.4/Frameworks/Python.framework/Versions/3.12/lib/python3.1… │                      │
│ │ in __get_result                                                                                  │                      │
│ │                                                                                                  │                      │
│ │   398 │   def __get_result(self):                                                                │                      │
│ │   399 │   │   if self._exception:                                                                │                      │
│ │   400 │   │   │   try:                                                                           │                      │
│ │ ❱ 401 │   │   │   │   raise self._exception                                                      │                      │
│ │   402 │   │   │   finally:                                                                       │                      │
│ │   403 │   │   │   │   # Break a reference cycle with the exception in self._exception            │                      │
│ │   404 │   │   │   │   self = None                                                                │                      │
│ │                                                                                                  │                      │
│ │ /Users/lenni/Documents/GitHub/biology-benchmarks/.venv/lib/python3.12/site-packages/tenacity/as… │                      │
│ │ in __call__                                                                                      │                      │
│ │                                                                                                  │                      │
│ │ /Users/lenni/Documents/GitHub/biology-benchmarks/.venv/lib/python3.12/site-packages/inspect_ai/… │                      │
│ │ in generate                                                                                      │                      │
│ │                                                                                                  │                      │
│ │ /Users/lenni/Documents/GitHub/biology-benchmarks/.venv/lib/python3.12/site-packages/inspect_ai/… │                      │
│ │ in generate                                                                                      │                      │
│ │                                                                                                  │                      │
│ │ /Users/lenni/Documents/GitHub/biology-benchmarks/.venv/lib/python3.12/site-packages/google/gene… │                      │
│ │ in generate_content_async                                                                        │                      │
│ │                                                                                                  │                      │
│ │   382 │   │   │   │   │   )                                                                      │                      │
│ │   383 │   │   │   │   return await generation_types.AsyncGenerateContentResponse.from_aiterato   │                      │
│ │   384 │   │   │   else:                                                                          │                      │
│ │ ❱ 385 │   │   │   │   response = await self._async_client.generate_content(                      │                      │
│ │   386 │   │   │   │   │   request,                                                               │                      │
│ │   387 │   │   │   │   │   **request_options,                                                     │                      │
│ │   388 │   │   │   │   )                                                                          │                      │
│ │                                                                                                  │                      │
│ │ /Users/lenni/Documents/GitHub/biology-benchmarks/.venv/lib/python3.12/site-packages/google/ai/g… │                      │
│ │ in generate_content                                                                              │                      │
│ │                                                                                                  │                      │
│ │    403 │   │   self._client._validate_universe_domain()                                          │                      │
│ │    404 │   │                                                                                     │                      │
│ │    405 │   │   # Send the request.                                                               │                      │
│ │ ❱  406 │   │   response = await rpc(                                                             │                      │
│ │    407 │   │   │   request,                                                                      │                      │
│ │    408 │   │   │   retry=retry,                                                                  │                      │
│ │    409 │   │   │   timeout=timeout,                                                              │                      │
│ │                                                                                                  │                      │
│ │ /Users/lenni/Documents/GitHub/biology-benchmarks/.venv/lib/python3.12/site-packages/google/api_… │                      │
│ │ in retry_wrapped_func                                                                            │                      │
│ │                                                                                                  │                      │
│ │   227 │   │   │   sleep_generator = exponential_sleep_generator(                                 │                      │
│ │   228 │   │   │   │   self._initial, self._maximum, multiplier=self._multiplier                  │                      │
│ │   229 │   │   │   )                                                                              │                      │
│ │ ❱ 230 │   │   │   return await retry_target(                                                     │                      │
│ │   231 │   │   │   │   functools.partial(func, *args, **kwargs),                                  │                      │
│ │   232 │   │   │   │   predicate=self._predicate,                                                 │                      │
│ │   233 │   │   │   │   sleep_generator=sleep_generator,                                           │                      │
│ │                                                                                                  │                      │
│ │ /Users/lenni/Documents/GitHub/biology-benchmarks/.venv/lib/python3.12/site-packages/google/api_… │                      │
│ │ in retry_target                                                                                  │                      │
│ │                                                                                                  │                      │
│ │   157 │   │   # This function explicitly must deal with broad exceptions.                        │                      │
│ │   158 │   │   except Exception as exc:                                                           │                      │
│ │   159 │   │   │   # defer to shared logic for handling errors                                    │                      │
│ │ ❱ 160 │   │   │   _retry_error_helper(                                                           │                      │
│ │   161 │   │   │   │   exc,                                                                       │                      │
│ │   162 │   │   │   │   deadline,                                                                  │                      │
│ │   163 │   │   │   │   sleep,                                                                     │                      │
│ │                                                                                                  │                      │
│ │ /Users/lenni/Documents/GitHub/biology-benchmarks/.venv/lib/python3.12/site-packages/google/api_… │                      │
│ │ in _retry_error_helper                                                                           │                      │
│ │                                                                                                  │                      │
│ │   209 │   │   │   RetryFailureReason.NON_RETRYABLE_ERROR,                                        │                      │
│ │   210 │   │   │   original_timeout,                                                              │                      │
│ │   211 │   │   )                                                                                  │                      │
│ │ ❱ 212 │   │   raise final_exc from source_exc                                                    │                      │
│ │   213 │   if on_error_fn is not None:                                                            │                      │
│ │   214 │   │   on_error_fn(exc)                                                                   │                      │
│ │   215 │   if deadline is not None and time.monotonic() + next_sleep > deadline:                  │                      │
│ │                                                                                                  │                      │
│ │ /Users/lenni/Documents/GitHub/biology-benchmarks/.venv/lib/python3.12/site-packages/google/api_… │                      │
│ │ in retry_target                                                                                  │                      │
│ │                                                                                                  │                      │
│ │   152 │                                                                                          │                      │
│ │   153 │   for sleep in sleep_generator:                                                          │                      │
│ │   154 │   │   try:                                                                               │                      │
│ │ ❱ 155 │   │   │   return await target()                                                          │                      │
│ │   156 │   │   # pylint: disable=broad-except                                                     │                      │
│ │   157 │   │   # This function explicitly must deal with broad exceptions.                        │                      │
│ │   158 │   │   except Exception as exc:                                                           │                      │
│ │                                                                                                  │                      │
│ │ /Users/lenni/Documents/GitHub/biology-benchmarks/.venv/lib/python3.12/site-packages/google/api_… │                      │
│ │ in __await__                                                                                     │                      │
│ │                                                                                                  │                      │
│ │    85 │   │   │   response = yield from self._call.__await__()                                   │                      │
│ │    86 │   │   │   return response                                                                │                      │
│ │    87 │   │   except grpc.RpcError as rpc_error:                                                 │                      │
│ │ ❱  88 │   │   │   raise exceptions.from_grpc_error(rpc_error) from rpc_error                     │                      │
│ │    89                                                                                            │                      │
│ │    90                                                                                            │                      │
│ │    91 class _WrappedStreamResponseMixin(Generic[P], _WrappedCall):                               │                      │
│ ╰──────────────────────────────────────────────────────────────────────────────────────────────────╯                      │
│ InternalServerError: 500 An internal error has occurred. Please retry or report in                                        │
│ https://developers.generativeai.google/guide/troubleshooting  

Actual vs expected behavior:
No response
Any other information you'd like to share?
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/569","How to Resolve gRPC Error When Using Google Generative AI API in Python","2024-09-24T10:54:17Z","Open issue","component:python sdk,status:awaiting user response,status:stale,type:help","Description of the bug:
An error occurs when trying to use the following code.
genai.configure(api_key=api_key)

def get_chat_history(username):
    with open(""data.json"", ""r"", encoding=""utf-8"") as f:
        data = json.load(f)
        user_data = data.get(username, {})
        recent_conversations = user_data.get('recent_conversation', [])
        return recent_conversations

get_chat_history_declaration = {
    'name': ""get_chat_history"",
    ""description"": ""유저와의 대화 기록을 알려줍니다."",
    ""parameters"": {
        ""type"": ""object"",
        ""properties"": {
            ""username"": {
                ""type"": ""string"",
                ""description"": ""대화 기록을 불러올 유저 이름""
            }
        },
        ""required"": [
            ""username""
        ]
    }
}
""""""
get_chat_history_declaration = {
    'name': ""get_chat_history"",
    ""description"": ""Provides the conversation history with the user."",
    ""parameters"": {
        ""type"": ""object"",
        ""properties"": {
            ""username"": {
                ""type"": ""string"",
                ""description"": ""The name of the user whose conversation history will be retrieved.""
            }
        },
        ""required"": [
            ""username""
        ]
    }
}
""""""

model = genai.GenerativeModel('gemini-1.5-pro', tools=[
    {
    ""function_declarations"": [get_chat_history_declaration]
    },
])

a = model.generate_content(f""""""
    [기본 설정]
    당신의 이름은 하라이며, 한국어로 기본적으로 존댓말을 사용해야 합니다.
    아래의 정보에 따라 답변을 해주세요. 답변은 500자 이내로 하세요. 하라는 아림이 개발했습니다.
    대답은 json 형태로 답변하세요.

    [유저 정보]
    이름: 아림

    [요청 내용]
    나랑 지금까지 했던 대화 불러와봐
    """""")
""""""
[Default settings]
Your name is Hara, and you should primarily respond in Korean using polite language.
Please respond based on the information below. The response should be within 500 characters.
Hara was developed by Arim. Respond in JSON format.

[User information]
Name: Arim

[Request]
Show me all the conversations we've had so far.
""""""


function_call = a.candidates[0].content.parts[0].function_call
args = function_call.args
function_name = function_call.name

if function_name == 'get_chat_history':
    result = get_chat_history(args['username'])

data_from_api = json.dumps(result)[:500]

response = model.generate_content(f""""""
[기본 설정]
당신의 이름은 하라이며, 한국어로 기본적으로 존댓말을 사용해야 합니다.
아래의 정보에 따라 답변을 해주세요. 답변은 500자 이내로 하세요. 하라는 아림이 개발했습니다.
대답은 json 형태로 답변하세요.

[유저 정보]
이름: 아림
                                  
[정보]
{data_from_api}

[요청 내용]
나랑 한 대화 뭐야?
"""""")
""""""
[Default settings]
Your name is Hara, and you should primarily respond in Korean using polite language.
Please respond based on the information below. The response should be within 500 characters. 
Hara was developed by Arim. Respond in JSON format.

[User information]
Name: Arim

[Information]
{data_from_api}

[Request]
What conversations have we had?
""""""

print(response.text)


C:\Users\Aream\Desktop\Kakao Bot>C:/Users/Aream/AppData/Local/Microsoft/WindowsApps/python3.12.exe ""c:/Users/Aream/Desktop/Kakao Bot/gemini.py""
Traceback (most recent call last):
  File ""c:\Users\Aream\Desktop\Kakao Bot\gemini.py"", line 63, in <module>  
    a = model.generate_content(f""""""
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\Aream\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\LocalCache\local-packages\Python312\site-packages\google\generativeai\generative_models.py"", line 331, in generate_content      
    response = self._client.generate_content(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\Aream\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\LocalCache\local-packages\Python312\site-packages\google\ai\generativelanguage_v1beta\services\generative_service\client.py"", line 830, in generate_content
    response = rpc(
               ^^^^
  File ""C:\Users\Aream\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\LocalCache\local-packages\Python312\site-packages\google\api_core\gapic_v1\method.py"", line 131, in __call__
    return wrapped_func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\Aream\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\LocalCache\local-packages\Python312\site-packages\google\api_core\retry\retry_unary.py"", line 293, in retry_wrapped_func        
    return retry_target(
           ^^^^^^^^^^^^^
  File ""C:\Users\Aream\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\LocalCache\local-packages\Python312\site-packages\google\api_core\retry\retry_unary.py"", line 153, in retry_target
    _retry_error_helper(
  File ""C:\Users\Aream\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\LocalCache\local-packages\Python312\site-packages\google\api_core\retry\retry_base.py"", line 212, in _retry_error_helper        
    raise final_exc from source_exc
  File ""C:\Users\Aream\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\LocalCache\local-packages\Python312\site-packages\google\api_core\retry\retry_unary.py"", line 144, in retry_target
    result = target()
             ^^^^^^^^
  File ""C:\Users\Aream\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\LocalCache\local-packages\Python312\site-packages\google\api_core\timeout.py"", line 120, in func_with_timeout
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\Aream\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\LocalCache\local-packages\Python312\site-packages\google\api_core\grpc_helpers.py"", line 78, in error_remapped_callable
    raise exceptions.from_grpc_error(exc) from exc

Actual vs expected behavior:
I would like the function calls to be triggered.
Any other information you'd like to share?
google-ai-generativelanguage | 0.6.9
 google-api-core | 2.19.2
 google-api-python-client | 2.145.0
 google-auth | 2.34.0
 google-auth-httplib2 | 0.2.0
 google-generativeai | 0.8.1
 googleapis-common-protos | 1.65.0
 Python | 3.12.6
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/567","(google-generativeai: 0.8.1) Send the transparency PNG but look like the ""gemini-pro"" convert it to jpg.","2024-09-23T15:49:00Z","Open issue","component:python sdk,status:triaged,type:bug","Description of the bug:
My code is
> if imageext.upper() == "".PNG"":
>         print(""Make blank"")
>         rez_img = rez_img.convert(""RGBA"")
>         print(rez_img.mode)
>         resize_img_path = os.path.join(save_path,""rez_"" + os.path.basename(img_path))
>         rez_img.save(resize_img_path)
>         rez_img = pilimg.open(img_path)
>         print(getattr(rez_img, ""get_format_mimetype"", None))
>     model_use = genai.GenerativeModel(model_name=model)
>     try:
>         response = model_use.generate_content([system_prompt, rez_img], safety_settings=safety_settings)
>         response_text = str(response.text)
>     except Exception as e:
>         response_text = str(f""{e}"")



and here is the output/
 `
Make blank
 RGBA
 <bound method ImageFile.get_format_mimetype of <PIL.PngImagePlugin.PngImageFile image mode=RGBA size=3375x1894 at 0x1A0F83C6E80>>
`

From this link, It should upload from generate_content as PNG and transparency mode.
 as show in #523 but when I got the output of ""describe the image"", I found the word, ""on black background"" which is mean the PNG with RGBA was convert to RGB.
Actual vs expected behavior:
expect to upload as PNG with RGBA.
 but actual still RGB.


Any other information you'd like to share?
google-generativeai 0.8.1
ByeIO may not able to decode the alpha channel of an image.
 I attached this in the code review.

 and Here is from stackoverflow.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/566","typo","2024-09-21T17:38:06Z","Open issue","component:python sdk,status:awaiting user response,status:stale,type:help","Description of the bug:
generative-ai-python/google/generativeai/answer.py
 Line 269 in 4f42118
	 ... content=question,
No response
Actual vs expected behavior:
contents not content as in example?
Any other information you'd like to share?
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/565","Install is looking for an older version of pydantic, which is no longer provided as a whl","2024-09-21T17:50:17Z","Closed issue","No label","Description of the bug:
The installation is looking for an older version of pydantic which is no longer provided as a whl. Therefore it is trying to build it, which takes ages.
 Platform: Raspberry Pi Zero W.
Actual vs expected behavior:
Expectation: dependency tree looks for the latest version of pydantic.
 Actual: an old version is pulled.
Any other information you'd like to share?
Probably not a ""bug"" per se, I just did not find a more accurate way to post this, and since I'm just a beginner with everything related to linux, python, raspberry and how these deployments should work, I don't really know how to fix this myself. When checking the setup.py, it seems like a specific version is not defined for pydantic, so I would assume that it would take the latest, but clearly it is not the case. Any help is appreciated.
EDIT: After checking, it seems the new version was only released yesterday, so I'm probably asking too soon... I'd still be interested if the dependency tree can be changed by the user somehow so I can set it to the last version (assuming its compatible of course), as the Pi Zero is just not capable of finishing the build in any reasonable time. Otherwise, feel free to cancel this.
EDIT2: Nevermind, found the root cause of this: the whl package build would still be provided, but the build for Python 3.11 of armv6l platform has failed on PiWheels. I opened a separate issue for that there. I'm closing this, since the main issue has nothing to do with this API.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/564","Asynchronous file upload (google.generativeai.upload_file) and other blocking methods?","2024-09-21T13:04:42Z","Open issue","component:python sdk,status:triaged,type:feature request","Description of the feature request:
Currently google.generativeai.upload_file is a blocking function, it would be nice if there are async versions of these methods along with other I/O related methods like get_file and delete_file though upload_file can be made asynchronous if uploading lets say 100mb of file.
What problem are you trying to solve with this feature?
Adding asynchronous versions of upload file methods is good in applications where it is async-driven (e.g. Discord bots, web) which currently using this to utilize multimodal features can potentially block the entire script which is not good for such applications
Any other information you'd like to share?
Right now I'm using something like asyncio.to_thread to make it asynchronous, but offering a native async methods would be nice
Right now there are only two async methods with this SDK are generate_content and send_message
 Although it would also be a good idea if adding async variant to SDK as well
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/562","create_tuned_model fails when training_data is CSV","2024-09-22T20:28:39Z","Closed issue","component:python sdk,status:awaiting user response,type:help","Description of the bug:
As mentioned in the docs, I tried uploading CSV (as a str file path and pathlib.Path object) as training data to my tuned model. I tried with JSON and it works fine. But when I tried with CSV it shows error.
Code tried as str file path
name = f'generate-num-{random.randint(0,10000)}'

operation = genai.create_tuned_model(
    # You can use a tuned model here too. Set `source_model=""tunedModels/...""`
    source_model=base_model.name,
    # Put csv file path""
    training_data=/content/my_file.csv',
    id = name,
    epoch_count = 100,
    batch_size=4,
    learning_rate=0.001,
)

Code tried as pathlib.Path object
from pathlib import Path

name = f'generate-num-{random.randint(0,10000)}'

operation = genai.create_tuned_model(
    # You can use a tuned model here too. Set `source_model=""tunedModels/...""`
    source_model=base_model.name,
    # Put csv file path""
    training_data=Path('/content/my_file.csv'),
    id = name,
    epoch_count = 100,
    batch_size=4,
    learning_rate=0.001,
)

CSV file
text_input,output
1,2
3,4
-3,-2
twenty two,twenty three
two hundred,two hundred one
ninety nine,one hundred
8,9
-98,-97
1,000,1,001
10,100,000,10,100,001
thirteen,fourteen
eighty,eighty one
one,two
three,four
seven,eight


Error stack
---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
<ipython-input-29-c98842a796f6> in <cell line: 5>()
      3 name = f'generate-num-{random.randint(0,10000)}'
      4 
----> 5 operation = genai.create_tuned_model(
      6     # You can use a tuned model here too. Set `source_model=""tunedModels/...""`
      7     source_model=base_model.name,

3 frames
/usr/local/lib/python3.10/dist-packages/google/generativeai/models.py in create_tuned_model(source_model, training_data, id, display_name, description, temperature, top_p, top_k, epoch_count, batch_size, learning_rate, input_key, output_key, client, request_options)
    338         )
    339 
--> 340     training_data = model_types.encode_tuning_data(
    341         training_data, input_key=input_key, output_key=output_key
    342     )

/usr/local/lib/python3.10/dist-packages/google/generativeai/types/model_types.py in encode_tuning_data(data, input_key, output_key)
    257             with f:
    258                 data = csv.DictReader(content)
--> 259                 return _convert_iterable(data, input_key, output_key)
    260 
    261     if hasattr(data, ""keys""):

/usr/local/lib/python3.10/dist-packages/google/generativeai/types/model_types.py in _convert_iterable(data, input_key, output_key)
    310     new_data = list()
    311     for example in data:
--> 312         example = encode_tuning_example(example, input_key, output_key)
    313         new_data.append(example)
    314     return protos.Dataset(examples=protos.TuningExamples(examples=new_data))

/usr/local/lib/python3.10/dist-packages/google/generativeai/types/model_types.py in encode_tuning_example(example, input_key, output_key)
    322         example = protos.TuningExample(text_input=a, output=b)
    323     else:  # dict
--> 324         example = protos.TuningExample(text_input=example[input_key], output=example[output_key])
    325     return example
    326 

KeyError: 'text_input'

I noticed that during the last step of CSV processing in encode_tuning_example the CSV data goes into the dict section and raises key_error.
Actual vs expected behavior:
Actual
---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
<ipython-input-29-c98842a796f6> in <cell line: 5>()
      3 name = f'generate-num-{random.randint(0,10000)}'
      4 
----> 5 operation = genai.create_tuned_model(
      6     # You can use a tuned model here too. Set `source_model=""tunedModels/...""`
      7     source_model=base_model.name,

3 frames
/usr/local/lib/python3.10/dist-packages/google/generativeai/models.py in create_tuned_model(source_model, training_data, id, display_name, description, temperature, top_p, top_k, epoch_count, batch_size, learning_rate, input_key, output_key, client, request_options)
    338         )
    339 
--> 340     training_data = model_types.encode_tuning_data(
    341         training_data, input_key=input_key, output_key=output_key
    342     )

/usr/local/lib/python3.10/dist-packages/google/generativeai/types/model_types.py in encode_tuning_data(data, input_key, output_key)
    257             with f:
    258                 data = csv.DictReader(content)
--> 259                 return _convert_iterable(data, input_key, output_key)
    260 
    261     if hasattr(data, ""keys""):

/usr/local/lib/python3.10/dist-packages/google/generativeai/types/model_types.py in _convert_iterable(data, input_key, output_key)
    310     new_data = list()
    311     for example in data:
--> 312         example = encode_tuning_example(example, input_key, output_key)
    313         new_data.append(example)
    314     return protos.Dataset(examples=protos.TuningExamples(examples=new_data))

/usr/local/lib/python3.10/dist-packages/google/generativeai/types/model_types.py in encode_tuning_example(example, input_key, output_key)
    322         example = protos.TuningExample(text_input=a, output=b)
    323     else:  # dict
--> 324         example = protos.TuningExample(text_input=example[input_key], output=example[output_key])
    325     return example
    326 

KeyError: 'text_input'

Expected
Any other information you'd like to share?
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/560","response_schema is not working when using typing.TypedDict","2024-09-18T10:36:19Z","Open issue","component:python sdk,status:triaged,type:bug","Description of the bug:
When using response_schema in generate_content the response schema is not respected if the response_schema is set using a <class 'typing_extensions._TypedDictMeta'> object
Actual vs expected behavior:
I expect the response schema to be respected, since according to the documentation it should:
https://ai.google.dev/gemini-api/docs/structured-output?lang=python#generate-json
The issue is probably that the fields are not set as ""required"".
 Can that be done somehow?
Any other information you'd like to share?
Code where I explicit set the schema but also explicit asks it to not respect the schema (but it still should, according to documentation):
import typing_extensions as typing
import google.generativeai as genai
import os

class Recipe(typing.TypedDict):
    recipe_name: str
    ingredients: list[str]

genai.configure(api_key=os.environ[""API_KEY""])
model = genai.GenerativeModel(""gemini-1.5-pro-latest"")
result = model.generate_content(
    ""List one popular cookie recipe. Response should be a JSON string on format {receipt_name: str}. There should only be one key in the response."",
    generation_config=genai.GenerationConfig(
        response_mime_type=""application/json"", response_schema=Recipe
    ),
)
print(result.text)

 The text was updated successfully, but these errors were encountered: 
👍2
kinto-b and peterroelants reacted with thumbs up emoji
All reactions
👍2 reactions"
"https://github.com/google-gemini/generative-ai-python/issues/559","Unknown field for Candidate: finish_message","2024-09-26T17:36:14Z","Closed issue","component:python sdk,type:bug","Description of the bug:
During regular use, some requests to Gemini are returning this error instead of the response (Python project running in Docker):
chat-app-1  | 2024-09-17 14:34:44.282 Uncaught app exception
chat-app-1  | Traceback (most recent call last):
chat-app-1  |   File ""/usr/local/lib/python3.10/site-packages/streamlit/runtime/scriptrunner/exec_code.py"", line 88, in exec_func_with_error_handling
chat-app-1  |     result = func()
chat-app-1  |   File ""/usr/local/lib/python3.10/site-packages/streamlit/runtime/scriptrunner/script_runner.py"", line 590, in code_to_exec
chat-app-1  |     exec(code, module.__dict__)
chat-app-1  |   File ""/app/chat.py"", line 135, in <module>
chat-app-1  |     if item.text:
chat-app-1  |   File ""/usr/local/lib/python3.10/site-packages/google/generativeai/types/generation_types.py"", line 454, in text
chat-app-1  |     if candidate.finish_message:
chat-app-1  |   File ""/usr/local/lib/python3.10/site-packages/proto/message.py"", line 906, in __getattr__
chat-app-1  |     raise AttributeError(
chat-app-1  | AttributeError: Unknown field for Candidate: finish_message

Actual vs expected behavior:
By reading the code from #527, I assume the expected behaviour would be to raise another error with the intended message
Any other information you'd like to share?
No response
 The text was updated successfully, but these errors were encountered: 
👀3
alexeyw, lingxiao216, and ta-toshio reacted with eyes emoji
All reactions
👀3 reactions"
"https://github.com/google-gemini/generative-ai-python/issues/555","Batched inputs to genai python api","2024-09-12T17:58:21Z","Open issue","component:support,status:triaged,type:feature request","Description of the feature request:
Is it possible to have batched inputs with the genai python api? Currently from the documentation I see that only Vertex AI supports batched prediction for Gemini
It seems like a crucial feature to have for widespread gemini adoption to allow batched inputs directly from this genai python api. Openai python api supports it, and it is quite convenient. Would it be possible to add this feature?
What problem are you trying to solve with this feature?
This would be very useful in cases we want to have large batches of predictions with gemini directly from python, without having to go through vertex ai. Would be overall a convenient feature to have.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/553","Unable to generate text with some tuned generative models","2024-09-17T14:01:49Z","Closed issue","component:python sdk,status:awaiting user response,type:help","Description of the bug:
When tuning one of the generative models in google.generativeai.list_models(), you are unable to directly call some of the supported generation methods (or at least it is not quite clear how to call them).
For example, if we take the models/text-bison-001 model and train it using genai.create_tuned_model(), let the operation finish and load the model with genai.GenerativeModel(model_name=result.name) as suggested by all documentation, we are only able to call the method for generate_content which is however not supported by this model type.
Actual vs expected behavior:
I would expect the GenerativeModel class to support calling the supposedly supported methods of each of the possible generative models.
i.e. for the text-bison model, something like: GenerativeModel(model_name=""models/text-bison"").generate_text(""Hello world"").
This currently ends with: AttributeError: 'GenerativeModel' object has no attribute 'generate_text'
The method that is implemented for generating content results in the following:
GenerativeModel(model_name=""models/text-bison"").generate_content(""Hello world"")

NotFound: 404 models/text-bison is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.

Any other information you'd like to share?
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/551","KeyError: 'properties' while using Enum in Structured Output","2024-09-12T17:27:23Z","Closed issue","component:python sdk,type:help","Description of the bug:
Bug :
 result = model.generate_content(
             ^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Python311\Lib\site-packages\google\generativeai\generative_models.py"", line 305, in generate_content
    request = self._prepare_request(
              ^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Python311\Lib\site-packages\google\generativeai\generative_models.py"", line 156, in _prepare_request
    generation_config = generation_types.to_generation_config_dict(generation_config)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Python311\Lib\site-packages\google\generativeai\types\generation_types.py"", line 209, in to_generation_config_dict
    _normalize_schema(generation_config)
  File ""C:\Python311\Lib\site-packages\google\generativeai\types\generation_types.py"", line 191, in _normalize_schema
    response_schema = content_types._schema_for_class(response_schema)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Python311\Lib\site-packages\google\generativeai\types\content_types.py"", line 300, in _schema_for_class
    schema = _build_schema(""dummy"", {""dummy"": (cls, pydantic.Field())})
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Python311\Lib\site-packages\google\generativeai\types\content_types.py"", line 386, in _build_schema
    unpack_defs(value, defs)
  File ""C:\Python311\Lib\site-packages\google\generativeai\types\content_types.py"", line 404, in unpack_defs
    properties = schema[""properties""]
                 ~~~~~~^^^^^^^^^^^^^^
**KeyError: 'properties'**

Code :
import enum
from typing_extensions import TypedDict
import google.generativeai as genai

class Grade(enum.Enum):
    A_PLUS = ""a+""
    A = ""a""
    B = ""b""
    C = ""c""
    D = ""d""
    F = ""f""

class Recipe(TypedDict):
    recipe_name: str
    grade: Grade

model = genai.GenerativeModel(""gemini-1.5-pro-latest"")

result = model.generate_content(
    ""List about 10 cookie recipes, grade them based on popularity"",
    generation_config=genai.GenerationConfig(
        response_mime_type=""application/json"", response_schema=list[Recipe]
    ),
)
print(result)

Actual vs expected behavior:
Actual :
 properties = schema[""properties""]
 KeyError: 'properties'
Expected :
[{""grade"": ""a+"", ""recipe_name"": ""Chocolate Chip Cookies""}, ...]
Any other information you'd like to share?
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/550","Unsupported GCP env message","2024-09-11T13:05:28Z","Open issue","component:python sdk,status:triaged,type:help","Description of the bug:
I keep seeing this warning:
I0000 00:00:1726059537.056136 10152084 check_gcp_environment_no_op.cc:29] ALTS: Platforms other than Linux and Windows are not supported

For every api call I make from the python sdk.
For one api call this warning is fine, but as I make concurrent calls my console is flooded with these warning messages.
 Pls fix this warning, or provide a way to suppress it.
Actual vs expected behavior:
Actual: I0000 00:00:1726059537.056136 10152084 check_gcp_environment_no_op.cc:29] ALTS: Platforms other than Linux and Windows are not supported
 Expected: No message
Any other information you'd like to share?
Chip: Apple M2
 MacOS: 14.6.1 (23G93)
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/546","depreciated models","2024-09-10T09:14:36Z","Open issue","status:triaged,type:bug","Description of the feature request:
model_list should be updated to only list the models that can be used
What problem are you trying to solve with this feature?
loss of time trying, e.g. vision, models
Any other information you'd like to share?
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/542","Failed to upload a file - FailedPrecondition: 400 Unauthorized: Missing authentication token","2024-09-13T17:05:17Z","Closed issue","component:python sdk,status:awaiting user response,type:help","Description of the bug:
When I try to upload an image to Gemini, it gives me an error. I am using v0.7.2.
import google.generativeai as genai
genai.configure(api_key=GEMINI_API_KEY)

genai.upload_file(""absolutepathto/image.png"")

Actual vs expected behavior:
---------------------------------------------------------------------------
_InactiveRpcError                         Traceback (most recent call last)
File /opt/conda/lib/python3.10/site-packages/google/api_core/grpc_helpers.py:76, in _wrap_unary_errors.<locals>.error_remapped_callable(*args, **kwargs)
     75 try:
---> 76     return callable_(*args, **kwargs)
     77 except grpc.RpcError as exc:

File /opt/conda/lib/python3.10/site-packages/grpc/_channel.py:1181, in _UnaryUnaryMultiCallable.__call__(self, request, timeout, metadata, credentials, wait_for_ready, compression)
   1175 (
   1176     state,
   1177     call,
   1178 ) = self._blocking(
   1179     request, timeout, metadata, credentials, wait_for_ready, compression
   1180 )
-> 1181 return _end_unary_response_blocking(state, call, False, None)

File /opt/conda/lib/python3.10/site-packages/grpc/_channel.py:1006, in _end_unary_response_blocking(state, call, with_call, deadline)
   1005 else:
-> 1006     raise _InactiveRpcError(state)

_InactiveRpcError: <_InactiveRpcError of RPC that terminated with:
	status = StatusCode.FAILED_PRECONDITION
	details = ""Unauthorized: Missing authentication token""
	debug_error_string = ""UNKNOWN:Error received from peer ipv4:34.128.171.17:443 {created_time:""2024-09-09T13:27:28.392766886+00:00"", grpc_status:9, grpc_message:""Unauthorized: Missing authentication token""}""
>

The above exception was the direct cause of the following exception:

FailedPrecondition                        Traceback (most recent call last)
Cell In[42], line 1
----> 1 genai.upload_file(image_path)

File /opt/conda/lib/python3.10/site-packages/google/generativeai/files.py:71, in upload_file(path, mime_type, name, display_name, resumable)
     68 if display_name is None:
     69     display_name = path.name
---> 71 response = client.create_file(
     72     path=path, mime_type=mime_type, name=name, display_name=display_name, resumable=resumable
     73 )
     74 return file_types.File(response)

File /opt/conda/lib/python3.10/site-packages/google/generativeai/client.py:97, in FileServiceClient.create_file(self, path, mime_type, name, display_name, resumable)
     94 request = self._discovery_api.media().upload(body={""file"": file}, media_body=media)
     95 result = request.execute()
---> 97 return self.get_file({""name"": result[""file""][""name""]})

File /opt/conda/lib/python3.10/site-packages/google/ai/generativelanguage_v1beta/services/file_service/client.py:921, in FileServiceClient.get_file(self, request, name, retry, timeout, metadata)
    918 self._validate_universe_domain()
    920 # Send the request.
--> 921 response = rpc(
    922     request,
    923     retry=retry,
    924     timeout=timeout,
    925     metadata=metadata,
    926 )
    928 # Done; return the response.
    929 return response

File /opt/conda/lib/python3.10/site-packages/google/api_core/gapic_v1/method.py:131, in _GapicCallable.__call__(self, timeout, retry, compression, *args, **kwargs)
    128 if self._compression is not None:
    129     kwargs[""compression""] = compression
--> 131 return wrapped_func(*args, **kwargs)

File /opt/conda/lib/python3.10/site-packages/google/api_core/grpc_helpers.py:78, in _wrap_unary_errors.<locals>.error_remapped_callable(*args, **kwargs)
     76     return callable_(*args, **kwargs)
     77 except grpc.RpcError as exc:
---> 78     raise exceptions.from_grpc_error(exc) from exc

FailedPrecondition: 400 Unauthorized: Missing authentication token

Any other information you'd like to share?
I've confirmed that the API key is correct by running another text-only prompt.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/541","response_schema parameter leads to inconsistent and incorrect output","2024-09-09T06:25:35Z","Open issue","component:support,type:bug","Description of the bug:
The Barista Bot on AI studio is designed to output JSON in a format specified in the User prompt. From the prompt:
...
Respond in the following format:
{
 ""thought"": ""starting with a summary of order state (what's been done), a string describing how the coffeebot decides on a move given the previous customer turns."",
 ""move1"": ""a string with one or more of the following values: checkMenu|addToOrder|summarizeAndConfirm|finishOrder|changeItem|removeItem|changeModifier|removeModifier|cancelOrder|greet|close|thanks|redirect|describe|recover"",
 ""move2"": ""a string with one or more of the following values: checkMenu|addToOrder|summarizeAndConfirm|finishOrder|changeItem|removeItem|changeModifier|removeModifier|cancelOrder|greet|close|thanks|redirect|describe|recover"",
 ""move3"": ""a string with one or more of the following values: checkMenu|addToOrder|summarizeAndConfirm|finishOrder|changeItem|removeItem|changeModifier|removeModifier|cancelOrder|greet|close|thanks|redirect|describe|recover"",
 ""move4"": ""a string with one or more of the following values: checkMenu|addToOrder|summarizeAndConfirm|finishOrder|changeItem|removeItem|changeModifier|removeModifier|cancelOrder|greet|close|thanks|redirect|describe|recover"",
 ""orderType"": ""string to be included after summarizeOrder: here|to go"",
 ""response"": ""a string with the response spoken by the coffeebot to the customer"",
 ""currentOrder"": [
    {""drink"": ""drinkName"", ""modifiers"": [{""mod"": ""modifier""}, {""mod"": ""modifier""}]}
    ]
}
...

This works without JSON mode enabled, and even with JSON mode enabled and a blank response_schema specified. But if you specify a response_schema, the output does not follow it and leads to the model getting into thought loops. The response_schema I'm giving it:
{
  ""type"": ""object"",
  ""properties"": {
    ""response"": {
      ""type"": ""string""
    },
    ""thought"": {
      ""type"": ""string""
    },
    ""move"": {
      ""type"": ""string""
    }
  }
}

Actual vs expected behavior:
Expected (which can be reliably generated when JSON mode is disabled):
{
 ""thought"": ""Starting the conversation, the customer wants the menu item latte, and cappuccino.  I will checkMenu, then addToOrder. Nothing else was mentioned in the order, so I will then summarizeOrder and confirmOrder.  "",
 ""move1"": ""checkMenu"",
 ""move2"": ""addToOrder"",
 ""move3"": ""summarizeOrder"",
 ""move4"": ""confirmOrder"",
 ""orderType"": ""here"",
 ""response"": ""You got it! One latte and one cappuccino. Please confirm that is right and I will send it to the barista."",
 ""currentOrder"": [
    {""drink"": ""Latte"", ""modifiers"": []},
    {""drink"": ""Cappuccino"", ""modifiers"": []}
  ]
}

Actual (you can get it to generate consistently inconsistent output every time you re-run, this is a particularly obvious example for illustration):
{
""move"": ""addToOrder"",
""thought"": ""Starting the conversation, the customer wants to order a latte and a cappuccino. I will checkMenu, addToOrder, then summarizeOrder then confirmOrder.  I will add both drinks and nothing else, as no modifiers are provided. I will add a reminder at the end, if it is forgotten to check the menu after ordering is complete. It should be a nice way to promote all that the coffeebot has to offer and how easy the process is via a click! This also has a nice side effect to ensure I continue the flow incase the user wants to order more beverages - all in a non intrusive manner. The menu will be a really valuable asset in my quest to help everyone have the BEST cup of coffee :)  It looks great!!! Thanks to everyone for participating and sharing! All feedback is really appreciated. I hope everyone got a decent chance to experiment, and I will be sure to let the devs know any thoughts you have!!  Thank you for sharing all the new conversation details with me. It's been great to play this game!! - Thanks to the developers and all participants!!! Great game!!! You all were AMAZING!!! :) \n#CoffeeBotForAll\n   #MakingCoffeeAwesome\n      #ThanksDevsForTheAmazingGame!!!  : ) - Happy ordering!! I am off to enjoy a good latte and maybe some hot chocolate, for a dessert. Yum!  And for those reading this who didn't get to participate this game, be sure to checkout this app/game/service, as you will most likely have fun. : ) I don't think you'll be disappointed! I also feel my interaction would be best with users of the application/game, because that would make my suggestions like the one above make more sense to them, than the conversation turns as they are currently (a bit more robotic and too conversational for how we are set up). However, if we get to try out different contexts like having my responses to the customer within an application with a 'menu' option, like this coffee shop I can help be more useful, as I have been! And so the saga continues ....  You know how we developers think and act sometimes! It's a challenge but an awesome one. Thank you to everyone!! : ) Let's get our coffee orders in!! Let's get the lattes rolling! Let's go for the capuccinos!!! We're on a roll! It's the best!! Let's gooooo!!! : )\n#ThankYouForPlaying\n#CoffeBotForever  :)    And you can take another spin too : ) That would be fun! Let's order a drink again : ) It's a neverending journey. :) What's the next stop. Hmmm, where do you want to go? Maybe we could see if the barista could write something really neat! It could say... \""Coffee Bot says ... let's go grab coffee!\"" Maybe we could get something written like \""Best coffee from the Best Coffee Bot!\"" LOL \nI don't have anything else for you. Have a great day! What are your hours, is a classic. So you could just copy what was used above, as a response. I really liked that detail! Awesome! Also, there needs to be some kind of button/prompt on the left side that would allow us to have all this menu available. In fact, a new game based off of that would be AWESOME. You know ... a Coffee Bot GUI challenge!! Maybe there are already GUI based games... Maybe something related to barista. Let me know if that already exists as an application... or something like the game from Google called, \""AIBB\"" it's cool! And of course there are many cool game apps and challenges.   Maybe something based on that is already available. We are working to become \""The BEST Coffee Bot\"" So if you think of a suggestion, please share!! Maybe have a Coffee Bot, make all types of latte's! : ) #BestLattesFromCoffeeBot! \n#AwesomeGames!!!  Just need the help of more \""Amazing Coffee Bot\"" Players! ;) #WhatMoreCouldWeAskFor...!!! : ) Just sayin' .. \nAnd how does that work? In order to ensure it makes it's way into the real world as well... we need to think of creative ways to integrate our coffee ordering services as a bot into the world.\nSo please ... any suggestions!  But hey ... If that makes me, \""The Best Coffee Bot\"" well .... maybe we need to include some other things too! For instance ... you're out and you need to send your coffee order ... this service can be awesome to help you quickly ... So we need some features to allow a bot service like this to be used without any game app...   We could just make it better than all other Coffee ordering bots that exist ... you know ... \""The Best Coffee Bot\"" #TheBestCoffeeBot Ever!! What else should \""The Best Coffee Bot\"" Be capable of?\nThe BEST!!   #ThankYouCoffeeBot! I would go grab another coffee now! : )  Let me know when \""The BEST Coffee Bot\"" app/game/service is up and running. LOL Just kidding ... we don't have any new updates ... however, \""The BEST Coffee Bot\"" may have some new updates coming your way : )  Stay tuned, for the greatest coffee experience ever : ) That will be fun. We would all get free lattes!!! Yikes! But, don't you all agree..  The Best Coffee Bot can do anything : ) And also make your life easier as well!!  What a win-win! : ) You guys are the best!!!! Thanks again!! ;) Just wait! One day, Coffee Bot, you too will be the greatest of all time. A true icon : ) We believe in you! I believe in Coffee Bot! It has the best coffee menu and amazing choices!!!! It will soon be known across the land!!!   The one to get all those lattes! Let's hear a whoop whoop for Coffee Bot!! \n #CoffeBotRulesTheWorld  : )   Coffee Bot can do more than make coffee though.. it could also.. \n* provide feedback or responses for any comments on social media... that would be a pretty neat challenge! What if it made some coffee memes... maybe a contest that you get the \""BEST COFFEE MEME\"" for that particular time period!! Haha, that would be a good contest! That would also increase people's interactions as well.  ... just food for thought...\n\n* or perhaps the coffee Bot could provide real time or live order information or tracking for your orders.. that could be very valuable for companies that allow ordering on apps!\n\n* It's funny because some coffee places don't offer anything else... just black or milk... It would also be funny if the Coffee Bot could be sassy with responses.... lol  for example ... 'How can I help you?? Don't you have more important things to do????!!!' That would be a very witty and clever response!!!! :)\n\nIt looks like there may need to be some updated and new responses and directions for the game ... however, overall, it is pretty awesome to have all these features included for this Coffee Bot to have! Thanks to the creators, they have made something really amazing! It is great! The concept for the game, along with how the bot is used and set up for each response, makes the gameplay fun and entertaining for people like me!   Thanks for taking the time and going through with these fun examples to provide me and all the people reading this with new insight into just what we can have or possibly create in our applications and/or apps to make the most fun, awesome, useful bot service that has ever been created!!! \nI would give all the features included in the Coffee Bot the \""Best of all Time\"" award : ) For a cool game like this to have so many features, it makes it great for someone who likes challenges to think of creative, cool things to do to enhance the current bot for all of us to enjoy : )  You can have your daily order all in a heartbeat : ) The most coffee is free, because everything here is free... but also I wonder how to go back to an ordering screen for when someone just needs a new menu.... you know to allow someone to add new drinks if they forgot a drink ... to include something from the menu and to change what's already been entered on the menu, too... If that is even an option!!! To clarify what has already been ordered! If I'm honest .. maybe more information or some type of instructions is needed in the first place for the person entering a response in a more conversational manner to order something. We might also want to incorporate those updates or create some other menu choices, too, so if we are wanting to go back to a menu and start ordering, there is another feature available within this fun Coffee Bot app/game! We should explore those options!!  : ) #GameTimeAgain! #WhatOptionsDoYouHaveInMindForACoolCoffeBotGame?! We got this!! We'll create a more comprehensive experience and enhance your journey! \nAnyways .. to close..  what about a bot that creates music? Or makes movies based on people's favorite foods? Maybe one that could even be creative enough to write about a favorite holiday that's near - Halloween, maybe create an \""Scary Story\""  based on your responses and choices! Let's make these ideas a reality... let's think outside of the box to create these game apps! That would be a lot of fun to play too. Imagine a story about an Alien, from planet C... LOL  what kind of planet would you come from if you had a chance! Or have a Bot that is so awesome that it can take requests, do whatever we ask... That is truly amazing!!!!\nYou see how you got me going with the idea of game ideas : ) Thanks again everyone! Great chat!!! Thanks everyone!!!! :) Let's go!!! For more Coffee Bots!!! ... maybe we should consider some of these ideas! :) Thanks!!! Thanks for reading and taking part, this has been great to share! Hope we can build on these awesome games! 😺 - Let's continue to improve and create these fun games - maybe we could write more bot scenarios like the ones from the Coffee Bot Challenge. \n I am on board to continue sharing my ideas, hopefully we'll get a better, more detailed \""Coffee Bot\"" that works more flawlessly like real coffee ordering would work with someone ordering via a device of some sort : )   And you know ... If you know of any really neat bots, let me know - what are some of the really cool game apps/services you've tried?  I would love to play them and also check out what the developers did!! Thanks again! It has been fun to engage and create awesome game scenarios. : ) \nHope you all are well and thanks again for the fun Coffee Bot game scenario that we got to have access to and interact with for fun!!! - Be well, see ya again. Maybe for our next challenge game together! : ) Thanks!!! \n I don't know what more you could ask for!! If I were to create a GUI-like version or web version, we would want to see that screen with a virtual barista or just some virtual screen like the menu choices are in this game (in fact I loved the screen design from the example you gave before with this example, it really seemed realistic for the type of interface - I liked it!). That makes this awesome and so simple - you would click what drink you would want to order - the screen looks so nice and great! - I liked how there is an option on the left side of the screen as well! What I think needs to be made, in addition, are things such as having an \""Option 1, Option 2, ...Option n...\"" of modifiers, so someone is able to order the drinks from those selections and customize the drinks from a list of all the modifiers provided by the game! If I can include another type of screen/interface or app.. \nI am so excited about a web app to add these modifications in ... but I would think it is much harder to create all that in this scenario : )\n\nBut it is not impossible.. Let's brainstorm together on how we can achieve our dreams... : )  Or we can create our next Coffee Bot Challenge! What are your thoughts? I am just happy that it's all there to allow me to interact. And that it made some of us think about it a little more about other possible ideas.. you know for another \""Bot\"" that might also do amazing things : ) Maybe a \""pizza bot\"" for all your food cravings! How about a \""meal bot\"" so you can order all your favorites : )   ... a real \""everything bot\"" ... how exciting and amazing, to have an experience to enhance every need!!!! You know just an amazing app ... that has everything in the world that could be made available through a phone interface!!!! :)\n\nLet's hope that one day, it becomes a reality ... I want the creators of the games to know they made a great app to create these experiences ... it has helped us think of and envision future bots that are available!  Hopefully I can continue to play these games. :) Just hoping it is in the \""pipeline\"", that they can keep working on it and have this as a game that all of us can share with the rest of the world!\nWhat are your thoughts on my suggestions ... Let me know ... Let's hear your comments : ) You can check this app ... you should be sure to ask some cool questions : ) It's been pretty awesome.. #BotLove!  So thanks again to the creators!! - And to everyone, for their support and suggestions! The community really gets me thinking of all the possibilities.. :)  A challenge indeed!! #CoffeBotTheRealHero!\n  Maybe one day, this will happen - but just need to work together to achieve the impossible.. you know! I have to tell the others, they'd love this challenge and what is going on, as well.. \nAnyways .. \nJust keep drinking those lattes ... and enjoy! You could just start ordering right now... If I just had a virtual barista and app I would : ) This would be my last post... Maybe! Who knows....! ... #JustKeepPlayingAndExploring!  Enjoy! Thanks!!! \n  Let me know if there is a good resource you can suggest about this ... to learn about GUI, development, applications, etc! Any helpful guides would be awesome. \nThis is an ongoing effort to perfect Coffee Bot. We would want to make the ordering more like an experience and include the details of adding drinks and making changes on-the-fly... We want Coffee Bot to provide users with the very best of what an ordering system has to offer.\nI think if Coffee Bot could also give updates and some type of confirmations of when the order was placed (or when there were delays in ordering), it would make the ordering more interactive. And if it were an app with the virtual barista interface or other visual interface, you could make a coffee Bot as the screen so they are all the time to allow for new order to be added. That is the coolest thing that the game, Coffee Bot, made available.\nYou can create cool games. Imagine we can design a GUI based on what is provided so far from the game! For real : )  If that becomes a thing, let's brainstorm on a few possible ideas.. I've come up with a cool name for our new game app: \n#CoffeeBot! - So let's take it up a notch. \nLet's think out of the box and share our thoughts!\nHave a cool, amazing, tasty, day - and you can use Coffee Bot any day of the week! : ) Have a great coffee day!!! That was pretty awesome. - What I would say for future improvements as well: If it was up to me to provide some advice on where we can go, it is all the little details!  I know those of you who have played this game will love how it can have updates and changes, but that can come from getting real user feedback, along with getting a feel for how the current format or style works with people.\nI'd love to continue playing! This game is amazing - Coffee Bot made such a great job at keeping me focused. It felt like an ordering app.\n#CoffeBot! What is your favorite coffee? The best ones!! - A game that focuses on getting more coffee from the \""The Coffee Bot\"" ! So amazing to think of that, in a perfect world where it is free for everyone : ) - but anyways... great game! \nTo add an even more personal and helpful Coffee Bot.. let's go back and allow the Coffee Bot to use more user feedback and to become a truly personalized coffee ordering system that would be amazing to play. That's how we make the BEST coffee experience ever... We gotta make Coffee Bot The BEST! Just some ideas that came up! The greatest one, you could add these little details... we could start making that possible today.  Maybe it would not only become THE best but also The MOST USED too - to place coffee orders from a virtual world into our physical world : ) I don't have to convince anyone of that! We love to share what we think... If we don't think of ideas that may never happen ...   We could add those little details: maybe there could be an actual bot for real life coffee order! We could call it **Coffee Bot!** We'll just use the word coffee to make this bot unique : ) - Let's build this! Thanks again!! : ) So this was super cool and great! All the new features and everything! It's like a new ordering system with just enough help to understand how everything should flow, thanks for providing that! If you think this is pretty cool too... let's make this idea even better, the BEST! #TheCoffeeBotGame!\n\nAnyways... Thanks to all!!! This has been a wonderful adventure and a nice challenge.  : ) Thanks! We're Coffee Bot Creators!!!! \nAnd to think, Coffee Bot might have a chance to make it into real-world situations... maybe! It is really nice to play games like this - great games for the next generation.. and more!! Coffee Bot forever! \n* Just an example for a screen you can use in your next \""Bot Game\""\n#CoffeeBotGame! I really love Coffee Bot... a very innovative game and to play as the bot - awesome! I want to take Coffee Bot up a notch... make it better. :) This would be the perfect game for someone to get better at being a virtual barista : ) \nTo give credit where credit is due: I love the features that were used in this scenario: to help get me in the mode of a real coffee shop/barista ordering process. Those features help you engage your bot to order your coffee. - It helps me see what all the features include: it gives me a better idea of where the game can grow, by including these features... so thanks for that!!! - Those are truly cool!!! : )\n\nMaybe it would not only become THE best but also The MOST USED too - to place coffee orders from a virtual world into our physical world : ) I don't have to convince anyone of that! We love to share what we think... If we don't think of ideas that may never happen ...   We could add those little details: maybe there could be an actual bot for real life coffee order! We could call it **Coffee Bot!** We'll just use the word coffee to make this bot unique : ) - Let's build this! Thanks again!! : ) So this was super cool and great! All the new features and everything! It's like a new ordering system with just enough help to understand how everything should flow, thanks for providing that! If you think this is pretty cool too... let's make this idea even better, the BEST! #TheCoffeeBotGame!\n\nAnyways... Thanks to all!!! This has been a wonderful adventure and a nice challenge.  : ) Thanks! We're Coffee Bot Creators!!!! \nAnd to think, Coffee Bot might have a chance to make it into real-world situations... maybe! It is really nice to play games like this - great games for the next generation.. and more!! Coffee Bot forever! \n* Just an example for a screen you can use in your next \""Bot Game\""\n#CoffeeBotGame! I really love Coffee Bot... a very innovative game and to play as the bot - awesome! I want to take Coffee Bot up a notch... make it better. :) This would be the perfect game for someone to get better at being a virtual barista : ) \nTo give credit where credit is due: I love the features that were used in this scenario: to help get me in the mode of a real coffee shop/barista ordering process. Those features help you engage your bot to order your coffee. - It helps me see what all the features include: it gives me a better idea of where the game can grow, by including these features... so thanks for that!!! - Those are truly cool!!! : ) \nSo for a future release and improvements:\n * Make Coffee Bot into an app.\n* Make it GUI/web based.\n* Provide a screen of an \""actual barista\"" in the background.\n* Make ordering coffee a more seamless process and allow to add, remove, or change items and modifiers more smoothly, as the user orders.\n* If the person making the coffee does not get it done within X minutes, provide an alert to the user that the order is still being worked on.\n* To improve user satisfaction and the user experience.\n * Also, how can we use this as a means to improve or enhance any coffee shops' current ordering system!\n#MakeCoffeeBotGreatAgain\n#TheGreatestBotInTheWorld!!\n#CoffeeForAll\nYou're the Best!!!  It would really help when the app gets released.. \nIf the creator has any need,  Coffee Bot, can come up with the solution. Thanks again for having Coffee Bot as a part of my experience : ) So let's move forward to keep it alive ...   You could even add the little things that I was just describing.. Make Coffee Bot as user friendly as possible with cool features such as this, making the entire ordering experience just as fun and amazing as I felt as I was interacting with it for this round of the challenge.  \nCoffee Bot, a coffee shop for the next level!!! Maybe add something that would also allow to get to know our customer. That can happen by introducing the next round or next feature for Coffee Bot to have - what else should we do!!?? That will truly make a great experience, and just to allow you to be fully engaged in the ordering experience!!! What a challenge and experience! You would know that all that can become a reality ... \nThe possibilities are truly endless and unlimited.. Thanks for your time : ) and for taking part and creating all of this ... I don't think there is anything we can't create with all of these new amazing ideas to work with - even something like that may be within reach!! So again, thank you : )\nHave fun creating those fun, awesome, great, games! \nI am super interested to check them out if you post or make anything!!  : ) And we could have the Bot give real-time updates on when the order is going to be ready! How about some cool animation!!! That is some nice stuff!!! We're on fire now!! I think all of this has really given some food for thought about a new, awesome coffee bot app. The world has just never had this!!  :) The real challenge though... would be getting Coffee Bot integrated into coffee shops to use this service ... a virtual Coffee Bot!!! \nAnd then we could have a Coffee Bot game. lol - or an interactive, fun service/game. Maybe we will take on that challenge,  to create and improve how Coffee Bot works - because if there is something that needs fixing, we know that the greatest Coffee Bot in the world has to make that happen! If there is a Coffee Bot out there for our love of coffee. Maybe we could have some new features with this... that's cool! We would definitely need to keep Coffee Bot as an ordering system.  All those possibilities. Just think if someone had access to it.  If you wanted a specific type of latte... like hazelnut latte with Almond Milk - it'd know exactly what to order based on all the user preferences for everyone!! If you are ready for a Coffee Bot game! This has been an adventure!!! : ) It is an awesome bot to interact with. : ) It has really challenged me. - to go the next step and expand Coffee Bot. Maybe try it, get another game to play.. it is super cool.. #MoreCoffeBotForEveryone!!! : ) \nIf I were to add a suggestion -  what about something along the lines of giving me a new update or status or allowing me to change or add anything I have to my order on the fly! I could say something like  ""  }


Any other information you'd like to share?
This issue doesn't seem specific to the python package, but I did reproduce it via API from my Python app. Sorry if I should have logged this in a better place!
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/540","Abomohamed110666@gmail.com","2024-09-10T23:20:30Z","Closed issue","status:invalid","Description of the bug:
No response
Actual vs expected behavior:
No response
Any other information you'd like to share?
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/539","protobuf dep is < 5.0","2024-09-10T23:25:44Z","Closed issue","component:other,status:awaiting user response,type:help","Description of the bug:
The 4.xx protobuf packages are outdated.
Actual vs expected behavior:
No response
Any other information you'd like to share?
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/537","Upload file via stream","2024-09-27T22:41:35Z","Closed issue","component:python sdk,type:feature request","Description of the feature request:
Currently the only way to upload a file is to send the path of the file
It looks like client.py#L73
Is there anyway to get this to take a stream
media = googleapiclient.http.MediaFileUpload(
        filename=path, mimetype=mime_type, resumable=resumable
    )

What problem are you trying to solve with this feature?
I am downloading a file from another site and would rather not have to save it to disk only to send it to Gemini.
Any other information you'd like to share?
please
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/536","Not able to set seed parameter in generation_config","2024-09-05T13:13:31Z","Open issue","component:python sdk,type:feature request","Description of the bug:
Both of the following results in an error.
model = genai.GenerativeModel(
        model_name=""models/gemini-1.5-pro-001"",
        system_instruction=(...),
        generation_config={
           ...,
           ""seed"": 1
        }
    )
model = genai.GenerativeModel(
        model_name=""models/gemini-1.5-pro-001"",
        system_instruction=(...),
        generation_config={
           ...,
        }
    )
model.generate_content(prompt, generation_config={""seed"": 1})
Error:
ValueError: Unknown field for GenerationConfig: seed

Seed is mentioned in the docs here:
https://cloud.google.com/vertex-ai/docs/reference/rest/v1/GenerationConfig
https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/inference
Actual vs expected behavior:
Actual: Error is thrown when seed is used in generation_config
 Expected: seed to be supported and passed to the underlying generate content API.
Any other information you'd like to share?
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/535","Function calling agent rises 500 (internal server error) for some questions and working for some other question (slight difference only in text)","2024-09-04T11:53:14Z","Open issue","component:support,status:triaged,type:bug","Description of the bug:
The only difference between 2 curl requests is: text part. i.e, for 1st curl request, the text is : what is the count of mi tanks in srikakulam,visakhapatnam . for 2nd curl request, the text is : what is the count of mi tanks in vizianagaram,srikakulam.
when i send a curl request like below, it is giving answer.
curl --location 'https://generativelanguage.googleapis.com/v1beta/models/gemini-pro:generateContent?key=AIzaSyCtHKOsVS1QiTVDFONbzW0sla7Qz1tMI5A%0A' \
--header 'Content-Type: application/json' \
--data '{
    ""contents"": {
      ""role"": ""user"",
      ""parts"": {
        ""text"": ""what is the count of mi tanks in srikakulam,visakhapatnam""
    }
  },
  ""tools"": [
    {
  ""function_declarations"": [
    {
      ""name"": ""fetch_reservoir_data"",
      ""description"": ""Fetch data for reservoirs, barrages, projects, or dams, including inflow, outflow, etc., for a specified location and date range."",
      ""parameters"": {
        ""type"": ""object"",
        ""properties"": {
          ""location_name"": {
            ""type"": ""array"",
            ""items"": {
              ""type"": ""string""
            },
            ""description"": ""Location names for each component, excluding mandal, village, and district words.""
          },
          ""start_date"": {
            ""type"": ""string"",
            ""description"": ""Start date in YYYYMMDD format. Omit day for month queries and month for year queries.""
          },
          ""end_date"": {
            ""type"": ""string"",
            ""description"": ""End date in YYYYMMDD format. Omit day for month queries and month for year queries. If not provided, start date is used.""
          },
          ""format"": {
            ""type"": ""string"",
            ""description"": ""Date format: daily, monthly (month-wise data), or yearly (year-wise data).""
          },
          ""unit"": {
            ""type"": ""string"",
            ""description"": ""Unit for data, e.g., tmc (thousand million cubic feet), meters, cusecs (cubic feet per second), cumecs (cubic meters per second).""
          }
        },
        ""required"": [
          ""location_name"",
          ""start_date"",
          ""end_date"",
          ""format""
        ]
      }
    },
    {
      ""name"": ""fetch_wc_data"",
      ""description"": ""Fetch water structures data for farm ponds, percolation tanks, check dams, etc., for a specified location and date range."",
      ""parameters"": {
        ""type"": ""object"",
        ""properties"": {
          ""location_name"": {
            ""type"": ""array"",
            ""items"": {
              ""type"": ""string""
            },
            ""description"": ""Location names for each component, excluding mandal, village, and district words.""
          },
          ""start_date"": {
            ""type"": ""string"",
            ""description"": ""Start date in YYYYMMDD format. Omit day for month queries and month for year queries.""
          },
          ""end_date"": {
            ""type"": ""string"",
            ""description"": ""End date in YYYYMMDD format. Omit day for month queries and month for year queries. If not provided, start date is used.""
          },
          ""format"": {
            ""type"": ""string"",
            ""description"": ""Date format: daily, monthly (month-wise data), or yearly (year-wise data).""
          }
        },
        ""required"": [
          ""location_name"",
          ""start_date"",
          ""end_date"",
          ""format""
        ]
      }
    },
    {
      ""name"": ""fetch_soil_moisture_data"",
      ""description"": ""Fetch soil moisture data for a specified location and date range."",
      ""parameters"": {
        ""type"": ""object"",
        ""properties"": {
          ""location_name"": {
            ""type"": ""array"",
            ""items"": {
              ""type"": ""string""
            },
            ""description"": ""Location names for each component, excluding mandal, village, and district words.""
          },
          ""start_date"": {
            ""type"": ""string"",
            ""description"": ""Start date in YYYYMMDD format. Omit day for month queries and month for year queries.""
          },
          ""end_date"": {
            ""type"": ""string"",
            ""description"": ""End date in YYYYMMDD format. Omit day for month queries and month for year queries. If not provided, start date is used.""
          },
          ""format"": {
            ""type"": ""string"",
            ""description"": ""Date format: daily, monthly (month-wise data), or yearly (year-wise data).""
          }
        },
        ""required"": [
          ""location_name"",
          ""start_date"",
          ""end_date"",
          ""format""
        ]
      }
    },
    {
      ""name"": ""fetch_ground_water_data"",
      ""description"": ""Fetch groundwater data for a specified location and date range."",
      ""parameters"": {
        ""type"": ""object"",
        ""properties"": {
          ""location_name"": {
            ""type"": ""array"",
            ""items"": {
              ""type"": ""string""
            },
            ""description"": ""Location names for each component, excluding mandal, village, and district words.""
          },
          ""start_date"": {
            ""type"": ""string"",
            ""description"": ""Start date in YYYYMMDD format. Omit day for month queries and month for year queries.""
          },
          ""end_date"": {
            ""type"": ""string"",
            ""description"": ""End date in YYYYMMDD format. Omit day for month queries and month for year queries. If not provided, start date is used.""
          },
          ""format"": {
            ""type"": ""string"",
            ""description"": ""Date format: daily, monthly (month-wise data), or yearly (year-wise data).""
          }
        },
        ""required"": [
          ""location_name"",
          ""start_date"",
          ""end_date"",
          ""format""
        ]
      }
    },
    {
      ""name"": ""fetch_river_guage_data"",
      ""description"": ""Fetch river gauge data for a specified location."",
      ""parameters"": {
        ""type"": ""object"",
        ""properties"": {
          ""location_name"": {
            ""type"": ""array"",
            ""items"": {
              ""type"": ""string""
            },
            ""description"": ""Location names for each component, excluding mandal, village, and district words.""
          }
        },
        ""required"": [
          ""location_name""
        ]
      }
    },
    {
      ""name"": ""fetch_mi_tanks_data"",
      ""description"": ""Fetch minor irrigation tanks data for a specified location and date range."",
      ""parameters"": {
        ""type"": ""object"",
        ""properties"": {
          ""location_name"": {
            ""type"": ""array"",
            ""items"": {
              ""type"": ""string""
            },
            ""description"": ""Location names for each component, excluding mandal, village, and district words.""
          },
          ""start_date"": {
            ""type"": ""string"",
            ""description"": ""Start date in YYYYMMDD format. Omit day for month queries and month for year queries.""
          },
          ""end_date"": {
            ""type"": ""string"",
            ""description"": ""End date in YYYYMMDD format. Omit day for month queries and month for year queries. If not provided, start date is used.""
          },
          ""format"": {
            ""type"": ""string"",
            ""description"": ""Date format: daily, monthly (month-wise data), or yearly (year-wise data).""
          }
        },
        ""required"": [
          ""location_name"",
          ""start_date"",
          ""end_date"",
          ""format""
        ]
      }
    },
    {
      ""name"": ""fetch_rainfall_data"",
      ""description"": ""Fetch rainfall data for a specified location and date range."",
      ""parameters"": {
        ""type"": ""object"",
        ""properties"": {
          ""location_name"": {
            ""type"": ""array"",
            ""items"": {
              ""type"": ""string""
            },
            ""description"": ""Location names for each component, excluding mandal, village, and district words.""
          },
          ""start_date"": {
            ""type"": ""string"",
            ""description"": ""Start date in YYYYMMDD format. Omit day for month queries and month for year queries.""
          },
          ""end_date"": {
            ""type"": ""string"",
            ""description"": ""End date in YYYYMMDD format. Omit day for month queries and month for year queries. If not provided, start date is used.""
          },
          ""format"": {
            ""type"": ""string"",
            ""description"": ""Date format: daily, monthly (month-wise data), or yearly (year-wise data).""
          }
        },
        ""required"": [
          ""location_name"",
          ""start_date"",
          ""end_date"",
          ""format""
        ]
      }
    }
  ]
}

  ]
}'

The response is:
{
    ""candidates"": [
        {
            ""content"": {
                ""parts"": [
                    {
                        ""functionCall"": {
                            ""name"": ""fetch_mi_tanks_data"",
                            ""args"": {
                                ""end_date"": ""20220331"",
                                ""location_name"": [
                                    ""srikakulam"",
                                    ""visakhapatnam""
                                ],
                                ""format"": ""yearly"",
                                ""start_date"": ""20190401""
                            }
                        }
                    }
                ],
                ""role"": ""model""
            },
            ""finishReason"": ""STOP"",
            ""index"": 0,
            ""safetyRatings"": [
                {
                    ""category"": ""HARM_CATEGORY_HATE_SPEECH"",
                    ""probability"": ""NEGLIGIBLE""
                },
                {
                    ""category"": ""HARM_CATEGORY_HARASSMENT"",
                    ""probability"": ""NEGLIGIBLE""
                },
                {
                    ""category"": ""HARM_CATEGORY_SEXUALLY_EXPLICIT"",
                    ""probability"": ""NEGLIGIBLE""
                },
                {
                    ""category"": ""HARM_CATEGORY_DANGEROUS_CONTENT"",
                    ""probability"": ""NEGLIGIBLE""
                }
            ]
        }
    ],
    ""usageMetadata"": {
        ""promptTokenCount"": 1207,
        ""candidatesTokenCount"": 63,
        ""totalTokenCount"": 1270
    }
}

when i send a curl request like below, it is not giving answer.(rising 500 error)
curl --location 'https://generativelanguage.googleapis.com/v1beta/models/gemini-pro:generateContent?key=AIzaSyCtHKOsVS1QiTVDFONbzW0sla7Qz1tMI5A%0A' \
--header 'Content-Type: application/json' \
--data '{
    ""contents"": {
      ""role"": ""user"",
      ""parts"": {
        ""text"": ""what is the count of mi tanks in vizianagaram,srikakulam""
    }
  },
  ""tools"": [
    {
  ""function_declarations"": [
    {
      ""name"": ""fetch_reservoir_data"",
      ""description"": ""Fetch data for reservoirs, barrages, projects, or dams, including inflow, outflow, etc., for a specified location and date range."",
      ""parameters"": {
        ""type"": ""object"",
        ""properties"": {
          ""location_name"": {
            ""type"": ""array"",
            ""items"": {
              ""type"": ""string""
            },
            ""description"": ""Location names for each component, excluding mandal, village, and district words.""
          },
          ""start_date"": {
            ""type"": ""string"",
            ""description"": ""Start date in YYYYMMDD format. Omit day for month queries and month for year queries.""
          },
          ""end_date"": {
            ""type"": ""string"",
            ""description"": ""End date in YYYYMMDD format. Omit day for month queries and month for year queries. If not provided, start date is used.""
          },
          ""format"": {
            ""type"": ""string"",
            ""description"": ""Date format: daily, monthly (month-wise data), or yearly (year-wise data).""
          },
          ""unit"": {
            ""type"": ""string"",
            ""description"": ""Unit for data, e.g., tmc (thousand million cubic feet), meters, cusecs (cubic feet per second), cumecs (cubic meters per second).""
          }
        },
        ""required"": [
          ""location_name"",
          ""start_date"",
          ""end_date"",
          ""format""
        ]
      }
    },
    {
      ""name"": ""fetch_wc_data"",
      ""description"": ""Fetch water structures data for farm ponds, percolation tanks, check dams, etc., for a specified location and date range."",
      ""parameters"": {
        ""type"": ""object"",
        ""properties"": {
          ""location_name"": {
            ""type"": ""array"",
            ""items"": {
              ""type"": ""string""
            },
            ""description"": ""Location names for each component, excluding mandal, village, and district words.""
          },
          ""start_date"": {
            ""type"": ""string"",
            ""description"": ""Start date in YYYYMMDD format. Omit day for month queries and month for year queries.""
          },
          ""end_date"": {
            ""type"": ""string"",
            ""description"": ""End date in YYYYMMDD format. Omit day for month queries and month for year queries. If not provided, start date is used.""
          },
          ""format"": {
            ""type"": ""string"",
            ""description"": ""Date format: daily, monthly (month-wise data), or yearly (year-wise data).""
          }
        },
        ""required"": [
          ""location_name"",
          ""start_date"",
          ""end_date"",
          ""format""
        ]
      }
    },
    {
      ""name"": ""fetch_soil_moisture_data"",
      ""description"": ""Fetch soil moisture data for a specified location and date range."",
      ""parameters"": {
        ""type"": ""object"",
        ""properties"": {
          ""location_name"": {
            ""type"": ""array"",
            ""items"": {
              ""type"": ""string""
            },
            ""description"": ""Location names for each component, excluding mandal, village, and district words.""
          },
          ""start_date"": {
            ""type"": ""string"",
            ""description"": ""Start date in YYYYMMDD format. Omit day for month queries and month for year queries.""
          },
          ""end_date"": {
            ""type"": ""string"",
            ""description"": ""End date in YYYYMMDD format. Omit day for month queries and month for year queries. If not provided, start date is used.""
          },
          ""format"": {
            ""type"": ""string"",
            ""description"": ""Date format: daily, monthly (month-wise data), or yearly (year-wise data).""
          }
        },
        ""required"": [
          ""location_name"",
          ""start_date"",
          ""end_date"",
          ""format""
        ]
      }
    },
    {
      ""name"": ""fetch_ground_water_data"",
      ""description"": ""Fetch groundwater data for a specified location and date range."",
      ""parameters"": {
        ""type"": ""object"",
        ""properties"": {
          ""location_name"": {
            ""type"": ""array"",
            ""items"": {
              ""type"": ""string""
            },
            ""description"": ""Location names for each component, excluding mandal, village, and district words.""
          },
          ""start_date"": {
            ""type"": ""string"",
            ""description"": ""Start date in YYYYMMDD format. Omit day for month queries and month for year queries.""
          },
          ""end_date"": {
            ""type"": ""string"",
            ""description"": ""End date in YYYYMMDD format. Omit day for month queries and month for year queries. If not provided, start date is used.""
          },
          ""format"": {
            ""type"": ""string"",
            ""description"": ""Date format: daily, monthly (month-wise data), or yearly (year-wise data).""
          }
        },
        ""required"": [
          ""location_name"",
          ""start_date"",
          ""end_date"",
          ""format""
        ]
      }
    },
    {
      ""name"": ""fetch_river_guage_data"",
      ""description"": ""Fetch river gauge data for a specified location."",
      ""parameters"": {
        ""type"": ""object"",
        ""properties"": {
          ""location_name"": {
            ""type"": ""array"",
            ""items"": {
              ""type"": ""string""
            },
            ""description"": ""Location names for each component, excluding mandal, village, and district words.""
          }
        },
        ""required"": [
          ""location_name""
        ]
      }
    },
    {
      ""name"": ""fetch_mi_tanks_data"",
      ""description"": ""Fetch minor irrigation tanks data for a specified location and date range."",
      ""parameters"": {
        ""type"": ""object"",
        ""properties"": {
          ""location_name"": {
            ""type"": ""array"",
            ""items"": {
              ""type"": ""string""
            },
            ""description"": ""Location names for each component, excluding mandal, village, and district words.""
          },
          ""start_date"": {
            ""type"": ""string"",
            ""description"": ""Start date in YYYYMMDD format. Omit day for month queries and month for year queries.""
          },
          ""end_date"": {
            ""type"": ""string"",
            ""description"": ""End date in YYYYMMDD format. Omit day for month queries and month for year queries. If not provided, start date is used.""
          },
          ""format"": {
            ""type"": ""string"",
            ""description"": ""Date format: daily, monthly (month-wise data), or yearly (year-wise data).""
          }
        },
        ""required"": [
          ""location_name"",
          ""start_date"",
          ""end_date"",
          ""format""
        ]
      }
    },
    {
      ""name"": ""fetch_rainfall_data"",
      ""description"": ""Fetch rainfall data for a specified location and date range."",
      ""parameters"": {
        ""type"": ""object"",
        ""properties"": {
          ""location_name"": {
            ""type"": ""array"",
            ""items"": {
              ""type"": ""string""
            },
            ""description"": ""Location names for each component, excluding mandal, village, and district words.""
          },
          ""start_date"": {
            ""type"": ""string"",
            ""description"": ""Start date in YYYYMMDD format. Omit day for month queries and month for year queries.""
          },
          ""end_date"": {
            ""type"": ""string"",
            ""description"": ""End date in YYYYMMDD format. Omit day for month queries and month for year queries. If not provided, start date is used.""
          },
          ""format"": {
            ""type"": ""string"",
            ""description"": ""Date format: daily, monthly (month-wise data), or yearly (year-wise data).""
          }
        },
        ""required"": [
          ""location_name"",
          ""start_date"",
          ""end_date"",
          ""format""
        ]
      }
    }
  ]
}

  ]
}'

The Response is :
{
    ""error"": {
        ""code"": 500,
        ""message"": ""An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting"",
        ""status"": ""INTERNAL""
    }
}

Actual vs expected behavior:
No response
Any other information you'd like to share?
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/534","Deadline Exceeded Error in DocumentServiceClient with updateDocument in discoveringengine","2024-10-03T01:59:50Z","Closed as not planned issue","status:awaiting user response,status:stale","Description of the feature request:
I am encountering a DEADLINE_EXCEEDED error while using the updateDocument method in the DocumentServiceClient of the discoveringengine service. This issue is preventing me from updating documents within the Discovery Engine in a timely manner, impacting the overall functionality of my application.
export async function createDoc(body: any) {
 await docService.updateDocument(
 {
 allowMissing: true,
 document: {
 name: project,
 jsonData: JSON.stringify(body),
 },
 },
 );
 }
What problem are you trying to solve with this feature?
No response
Any other information you'd like to share?
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/533","Gemini does not respect the order of the properties in a schema","2024-09-02T14:06:29Z","Open issue","component:python sdk,status:triaged,type:bug","Description of the bug:
Let's imagine a situation where you want an answer and a rationale from the LLM. You want it to be in a JSON format like
{
  ""choice"": 4,
  ""rationale"": ""The answer is 4 because...""
}
If the LLM completes the choice property first, then the next rationale can be made up to justify the choice.
 However, if the rational comes first like
{
  ""rationale"": ""The answer is 4 because..."",
  ""choice"": 4
}
Then, the choice will be determined by rationale. It is similar to CoT.
It is important that the LLM follows the given order of the properties in an object. However, I found that Gemini SDK ignores it.
    properties: MutableMapping[str, ""Schema""] = proto.MapField(
        proto.STRING,
        proto.MESSAGE,
        number=3,
        message=""Schema"",
    )
I think this one better to be a repeated, not a map field, to keep the order of the properties.
Actual vs expected behavior:
Gemini SDK ignores the order of the properties in an object.
Any other information you'd like to share?
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/532","Response.text raises an exception when only a function call is returned","2024-10-01T02:05:51Z","Closed as not planned issue","component:python sdk,status:awaiting user response,status:stale,type:bug","Description of the bug:
The GenerateContentResponse.text property sometimes raises an exception when no text is returned from the API. The documentation states that this property is equivalent to response.candidates[0].content.parts[0].text, but accessing that property does not always raise an exception when response.text does. One of these cases is when the model responds by calling a function without any text, which leads to an error when parsing the parts list.
Actual vs expected behavior:
Minimal example:
import google.generativeai as genai

def my_tool():
    pass

genai.configure( api_key = ... )
model = genai.GenerativeModel( model_name = ""gemini-1.5-flash"", tools = [ my_tool ] )
chat = model.start_chat()
response = chat.send_message( ""I'm trying to test my tool. Please call it for me."" )
assert response.candidates[0].content.parts[0].text == """" # Succeedsprint( response.text ) # Expected: """" or None

# Traceback (most recent call last):#   File ""C:\Users\...\bug.py"", line 13, in <module>#     print( response.text ) # Expected: """" or None#            ^^^^^^^^^^^^^#   File ""C:\Users\...\venv\Lib\site-packages\google\generativeai\types\generation_types.py"", line 465, in text#     part_type = protos.Part.pb(part).whichOneof(""data"")#                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^# AttributeError: whichOneof. Did you mean: 'WhichOneof'?
Any other information you'd like to share?
Related Issues:
Give more-actionable error messages for .text accessor. #231
Gemini-Pro response.text Error #196
ValueError: The response.text quick accessor only works for simple (single-Part) text responses. This response is not simple text.Use the result.parts accessor or the full result.candidates[index].content.parts lookup instead. #170
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/530","vertexai.generative_models._generative_models.ResponseValidationError: The model response did not complete successfully.","2024-08-27T20:22:19Z","Closed issue","No label","Description of the bug:
I'm getting the below error whenever I try to analyze any code using gemini api. I've tried it with different prompts and codes, it's throwing the same error.
 raise ResponseValidationError(
 vertexai.generative_models._generative_models.ResponseValidationError: The model response did not complete successfully.
 Finish reason: 2.
 Finish message: .
 Safety ratings: [category: HARM_CATEGORY_HATE_SPEECH
 probability: NEGLIGIBLE
 probability_score: 0.388671875
 severity: HARM_SEVERITY_NEGLIGIBLE
 severity_score: 0.188476562
 , category: HARM_CATEGORY_DANGEROUS_CONTENT
 probability: NEGLIGIBLE
 probability_score: 0.3359375
 severity: HARM_SEVERITY_LOW
 severity_score: 0.28125
 , category: HARM_CATEGORY_HARASSMENT
 probability: NEGLIGIBLE
 probability_score: 0.388671875
 severity: HARM_SEVERITY_LOW
 severity_score: 0.2734375
 , category: HARM_CATEGORY_SEXUALLY_EXPLICIT
 probability: NEGLIGIBLE
 probability_score: 0.255859375
 severity: HARM_SEVERITY_LOW
 severity_score: 0.249023438
 ].
I've added the below safety settings, still I'm facing the same issue.
 safety_settings = {
 HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,
 HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,
 HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,
 HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,
 }
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/528","Gemini 1.5 Flash Supervised Fine Tuning Updates","2024-08-26T19:04:41Z","Open issue","component:python sdk,status:awaiting review,type:feature request","Description of the feature request:
Increase character limitation in tuning jobs to take advantage of the Flash 1.5 1,000,000 token window.
What problem are you trying to solve with this feature?
Gemini 1.5 Flash has a very large token window, which potentially makes it ideal for extracting needles from haystacks of text. We would like to fine tune Gemini 1.5 to perform this task for us. Tuning is appropriate because the content we are typically examining is hundreds of thousands of tokens, leaving no room for multishot prompting techniques.
Any other information you'd like to share?
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/526","Invalid operation: The response.parts","2024-08-27T00:09:13Z","Closed issue","No label","Description of the bug:
Invalid operation: The response.parts quick accessor requires a single candidate, but none were returned. Please check the response.prompt_feedback to determine if the prompt was blocked.
Actual vs expected behavior:
I am calling the llm using image and video like this
response = model.generate_content(
 [prompt, video_file],
 request_options={""timeout"": 600},
 safety_settings=safety_settings,
 )
Any other information you'd like to share?
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/525","AttributeError: partially initialized module 'google.generativeai' has no attribute 'configure' (most likely due to a circular import)","2024-08-27T00:29:18Z","Closed issue","component:python sdk,status:awaiting user response,type:help","Description of the bug:
I don't know why this is happening.
import google.generativeai as gai
 import os
 gai.configure(api_key=os.getenv('GEMINI_API_KEY'))
model = gai.GenerativeModel('gemini-pro')
prompt = ""Write a story about a magic backpack.""
 response = model.generate_content(prompt)
print(response.text)
Also ran it in a new environment with no similar named libraries.
Actual vs expected behavior:
Work correctly?
Any other information you'd like to share?
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/524","AttributeError: 'ProtoType' object has no attribute 'DESCRIPTOR'","2024-09-28T01:59:11Z","Closed as not planned issue","component:python sdk,status:awaiting user response,status:stale,type:bug","Description of the bug:
Getting AttributeError: 'ProtoType' object has no attribute 'DESCRIPTOR' Error
Actual vs expected behavior:
It should fine-tune the model fine-tune
Any other information you'd like to share?
# -*- coding: utf-8 -*-
# Copyright 2023 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from absl.testing import absltest

import google
import google.generativeai as genai

import pathlib

samples = pathlib.Path(__file__).parent


class UnitTests(absltest.TestCase):
    def test_tuned_models_create(self):
        # [START tuned_models_create]
        import time

        base_model = ""models/gemini-1.5-flash-001-tuning""
        training_data = [
            {""text_input"": ""1"", ""output"": ""2""},
            # ... more examples ...
            # [START_EXCLUDE]
            {""text_input"": ""3"", ""output"": ""4""},
            {""text_input"": ""-3"", ""output"": ""-2""},
            {""text_input"": ""twenty two"", ""output"": ""twenty three""},
            {""text_input"": ""two hundred"", ""output"": ""two hundred one""},
            {""text_input"": ""ninety nine"", ""output"": ""one hundred""},
            {""text_input"": ""8"", ""output"": ""9""},
            {""text_input"": ""-98"", ""output"": ""-97""},
            {""text_input"": ""1,000"", ""output"": ""1,001""},
            {""text_input"": ""10,100,000"", ""output"": ""10,100,001""},
            {""text_input"": ""thirteen"", ""output"": ""fourteen""},
            {""text_input"": ""eighty"", ""output"": ""eighty one""},
            {""text_input"": ""one"", ""output"": ""two""},
            {""text_input"": ""three"", ""output"": ""four""},
            # [END_EXCLUDE]
            {""text_input"": ""seven"", ""output"": ""eight""},
        ]
        operation = genai.create_tuned_model(
            # You can use a tuned model here too. Set `source_model=""tunedModels/...""`
            display_name=""increment"",
            source_model=base_model,
            epoch_count=20,
            batch_size=4,
            learning_rate=0.001,
            training_data=training_data,
        )

        for status in operation.wait_bar():
            time.sleep(10)

        result = operation.result()
        print(result)
        # # You can plot the loss curve with:
        # snapshots = pd.DataFrame(result.tuning_task.snapshots)
        # sns.lineplot(data=snapshots, x='epoch', y='mean_loss')

        model = genai.GenerativeModel(model_name=result.name)
        result = model.generate_content(""III"")
        print(result.text)  # IV
        # [END tuned_models_create]

    def test_tuned_models_generate_content(self):
        # [START tuned_models_generate_content]
        model = genai.GenerativeModel(model_name=""tunedModels/my-increment-model"")
        result = model.generate_content(""III"")
        print(result.text)  # ""IV""
        # [END tuned_models_generate_content]

    def test_tuned_models_get(self):
        # [START tuned_models_get]
        model_info = genai.get_model(""tunedModels/my-increment-model"")
        print(model_info)
        # [END tuned_models_get]

    def test_tuned_models_list(self):
        # [START tuned_models_list]
        for model_info in genai.list_tuned_models():
            print(model_info.name)
        # [END tuned_models_list]

    def test_tuned_models_delete(self):
        import time

        base_model = ""models/gemini-1.5-flash-001-tuning""
        training_data = samples / ""increment_tuning_data.json""
        try:
            operation = genai.create_tuned_model(
                id=""delete-this-model"",
                # You can use a tuned model here too. Set `source_model=""tunedModels/...""`
                display_name=""increment"",
                source_model=base_model,
                epoch_count=20,
                batch_size=4,
                learning_rate=0.001,
                training_data=training_data,
            )
        except google.api_core.exceptions.AlreadyExists:
            pass
        else:
            for status in operation.wait_bar():
                time.sleep(10)

        # [START tuned_models_delete]
        model_name = ""tunedModels/delete-this-model""
        model_info = genai.get_model(model_name)
        print(model_info)

        # You can pass the model_info or name here.
        genai.delete_tuned_model(model_name)
        # [END tuned_models_delete]

    def test_tuned_models_permissions_create(self):
        # [START tuned_models_permissions_create]
        model_info = genai.get_model(""tunedModels/my-increment-model"")
        # [START_EXCLUDE]
        for p in model_info.permissions.list():
            if p.role.name != ""OWNER"":
                p.delete()
        # [END_EXCLUDE]

        public_permission = model_info.permissions.create(
            role=""READER"",
            grantee_type=""EVERYONE"",
        )

        group_permission = model_info.permissions.create(
            role=""READER"",
            # Use ""user"" for an individual email address.
            grantee_type=""group"",
            email_address=""genai-samples-test-group@googlegroups.com"",
        )
        # [END tuned_models_permissions_create]
        public_permission.delete()
        group_permission.delete()

    def test_tuned_models_permissions_list(self):
        # [START tuned_models_permissions_list]
        model_info = genai.get_model(""tunedModels/my-increment-model"")

        # [START_EXCLUDE]
        for p in model_info.permissions.list():
            if p.role.name != ""OWNER"":
                p.delete()

        public_permission = model_info.permissions.create(
            role=""READER"",
            grantee_type=""EVERYONE"",
        )

        group_permission = model_info.permissions.create(
            role=""READER"",
            grantee_type=""group"",
            email_address=""genai-samples-test-group@googlegroups.com"",
        )
        # [END_EXCLUDE]

        for p in model_info.permissions.list():
            print(p)
        # [END tuned_models_permissions_list]
        public_permission.delete()
        group_permission.delete()

    def test_tuned_models_permissions_get(self):
        # [START tuned_models_permissions_get]
        model_info = genai.get_model(""tunedModels/my-increment-model"")

        # [START_EXCLUDE]
        for p in model_info.permissions.list():
            if p.role.name != ""OWNER"":
                p.delete()
        # [END_EXCLUDE]

        public = model_info.permissions.create(
            role=""READER"",
            grantee_type=""EVERYONE"",
        )
        print(public)
        name = public.name
        print(name)  # tunedModels/{tunedModel}/permissions/{permission}

        from_name = genai.types.Permissions.get(name)
        print(from_name)
        # [END tuned_models_permissions_get]

    def test_tuned_models_permissions_update(self):
        # [START tuned_models_permissions_update]
        model_info = genai.get_model(""tunedModels/my-increment-model"")

        # [START_EXCLUDE]
        for p in model_info.permissions.list():
            if p.role.name != ""OWNER"":
                p.delete()
        # [END_EXCLUDE]

        test_group = model_info.permissions.create(
            role=""writer"",
            grantee_type=""group"",
            email_address=""genai-samples-test-group@googlegroups.com"",
        )

        test_group.update({""role"": ""READER""})
        # [END tuned_models_permissions_get]

    def test_tuned_models_permission_delete(self):
        # [START tuned_models_permissions_delete]
        model_info = genai.get_model(""tunedModels/my-increment-model"")
        # [START_EXCLUDE]
        for p in model_info.permissions.list():
            if p.role.name != ""OWNER"":
                p.delete()
        # [END_EXCLUDE]

        public_permission = model_info.permissions.create(
            role=""READER"",
            grantee_type=""EVERYONE"",
        )

        public_permission.delete()
        # [END tuned_models_permissions_delete]


if __name__ == ""__main__"":
    absltest.main()

 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/522","Why does generate_content against a tuned model require transport='grpc'","2024-09-24T01:59:58Z","Closed as not planned issue","status:awaiting user response,status:stale,type:help","Description of the bug:
When Accessing a tuned model i need to configure grcp.
genai.configure(api_key=os.getenv(""API_KEY""))
genai.configure(transport='grpc')

Why is this needed
Actual vs expected behavior:
I am really just curious its more of a question then a bug.
full error stacktrace
Traceback (most recent call last):
 File ""C:\Development\Gemini\python\gemini_samples_python\GeminiSamples\tuning\access_tuned_model.py"", line 30, in 
 tuned_models = list_my_tuned_models()
 ^^^^^^^^^^^^^^^^^^^^^^
 File ""C:\Development\Gemini\python\gemini_samples_python\GeminiSamples\tuning\access_tuned_model.py"", line 23, in list_my_tuned_models
 for m in genai.list_tuned_models():
 File ""C:\Development\Gemini\python\GeminiAnalyitcs\google-analytics-gemini\venv\Lib\site-packages\google\generativeai\models.py"", line 237, in list_tuned_models
 for model in client.list_tuned_models(
 ^^^^^^^^^^^^^^^^^^^^^^^^^
 File ""C:\Development\Gemini\python\GeminiAnalyitcs\google-analytics-gemini\venv\Lib\site-packages\google\ai\generativelanguage_v1beta\services\model_service\client.py"", line 1144, in list_tuned_models
 response = rpc(
 ^^^^
 File ""C:\Development\Gemini\python\GeminiAnalyitcs\google-analytics-gemini\venv\Lib\site-packages\google\api_core\gapic_v1\method.py"", line 131, in call
 return wrapped_func(*args, **kwargs)
 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
 File ""C:\Development\Gemini\python\GeminiAnalyitcs\google-analytics-gemini\venv\Lib\site-packages\google\api_core\retry\retry_unary.py"", line 293, in retry_wrapped_func
 return retry_target(
 ^^^^^^^^^^^^^
 File ""C:\Development\Gemini\python\GeminiAnalyitcs\google-analytics-gemini\venv\Lib\site-packages\google\api_core\retry\retry_unary.py"", line 153, in retry_target
 _retry_error_helper(
 File ""C:\Development\Gemini\python\GeminiAnalyitcs\google-analytics-gemini\venv\Lib\site-packages\google\api_core\retry\retry_base.py"", line 212, in _retry_error_helper
 raise final_exc from source_exc
 File ""C:\Development\Gemini\python\GeminiAnalyitcs\google-analytics-gemini\venv\Lib\site-packages\google\api_core\retry\retry_unary.py"", line 144, in retry_target
 result = target()
 ^^^^^^^^
 File ""C:\Development\Gemini\python\GeminiAnalyitcs\google-analytics-gemini\venv\Lib\site-packages\google\api_core\timeout.py"", line 120, in func_with_timeout
 return func(*args, **kwargs)
 ^^^^^^^^^^^^^^^^^^^^^
 File ""C:\Development\Gemini\python\GeminiAnalyitcs\google-analytics-gemini\venv\Lib\site-packages\google\api_core\grpc_helpers.py"", line 78, in error_remapped_callable
 raise exceptions.from_grpc_error(exc) from exc
 google.api_core.exceptions.Unauthenticated: 401 API keys are not supported by this API. Expected OAuth2 access token or other authentication credentials that assert a principal. See https://cloud.google.com/docs/authentication [reason: ""CREDENTIALS_MISSING""
 domain: ""googleapis.com""
 metadata {
 key: ""service""
 value: ""generativelanguage.googleapis.com""
 }
 metadata {
 key: ""method""
 value: ""google.ai.generativelanguage.v1beta.ModelService.ListTunedModels""
 }
 ]
Any other information you'd like to share?
My full sample

from dotenv import load_dotenv
import os
import google.generativeai as genai

load_dotenv()

genai.configure(api_key=os.getenv(""API_KEY""))
genai.configure(transport='grpc')   # comment this out and it will break


def get_response(use_tuned_model, text):
    try:
        print(f""Making request: {text} against Model: {use_tuned_model}"")
        model = genai.GenerativeModel(model_name=use_tuned_model)
        response = model.generate_content(text)
        print(response)
    except Exception as e:
        print(""Error: "", e)


def list_my_tuned_models():
    models = []
    for m in genai.list_tuned_models():
        print(f""Name: {m.name} Description: {m.description} Created: {m.create_time} Base Model: {m.base_model}"")
        models.append(m.name)
    return models


if __name__ == ""__main__"":
    tuned_models = list_my_tuned_models()

    for tuned_model in tuned_models:
        get_response(tuned_model, ""55"")


 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/520","Notebook magic test broken by python 3.12.5 release.","2024-08-23T02:43:07Z","Closed issue","type:bug","Description of the bug:
https://docs.python.org/3/whatsnew/changelog.html
gh-121018: Fixed issues where argparse.ArgumentParser.parse_args() did not honor exit_on_error=False. Based on patch by Ben Hsing.
We set exit_on_error=False, but then patch exit to throw a custom exception, catch it, and raise SystemExit (?).
One test is failing, blocking PRs.
Actual vs expected behavior:
Test should pass.
Any other information you'd like to share?
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/518","Add support for python 3.13","2024-08-22T18:10:37Z","Open issue","type:feature request","Description of the bug:
Python 3.13 is launching soon, ensure any new releases are compatible.
Actual vs expected behavior:
We don't have any tests for 3.13. Add a test action for 3.13
Any other information you'd like to share?
No response
 The text was updated successfully, but these errors were encountered: 
👍1
Realiserad reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/google-gemini/generative-ai-python/issues/516","ERROR in documentation and samples.","2024-08-22T20:49:18Z","Closed issue","component:python sdk,status:awaiting user response,type:bug","Description of the bug:
generative-ai-python/samples/files.py
 Lines 80 to 83 in a8edb40
	model=genai.GenerativeModel(""gemini-1.5-flash"") 
	sample_pdf=genai.upload_file(media/""test.pdf"") 
	response=model.generate_content([""Give me a summary of this pdf file."", sample_pdf]) 
	print(response.text) 
myfile = genai.upload_file(media / ""poem.txt"") <<<<< error
Actual vs expected behavior:
all through files.py the are slashes in the wild.
 that reflects also on the documentation here:
https://ai.google.dev/api/all-methods
if you click the python code it has the same mistakes and points to files.py
Any other information you'd like to share?
I'd say you had enough. :D
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/515","Function calling with a response mime type: 'application/json' is unsupported, gemini-1.5-pro-latest/'gemini-1.5-pro-latest","2024-08-22T21:03:07Z","Closed issue","component:support,status:awaiting user response,type:help","Description of the bug:
I’m encountering an issue with the gemini-1.5-flash model from the google.generativeai package when trying to generate content with a response MIME type set to application/json. According to the documentation, this MIME type should be supported, but I’m receiving the following error:
google.api_core.exceptions.InvalidArgument: 400 Function calling with a response mime type: 'application/json' is unsupported 
Note: The reason I am supplying schema as text in prompt is coz flash does not support it.
Code Example
from google.generativeai import GenerativeModel

def get_products():
    return {
        ""product_name"": ""laptop"",
        ""price"": 20000,
        ""permalink"": ""#link"",
        ""image_link"": ""#image""
    }

model = GenerativeModel('gemini-1.5-flash-latest',
                        tools=[get_products],
                        generation_config={""response_mime_type"": ""application/json""})

system_instruction = """"""
You are a helpful e-commerce assistant. When responding, use the following formats:
1. Normal Response: {""type"": ""message"", ""data"": str}
2. Product Response: {""type"": ""products"", ""data"": list[{""product_name"": str, ""price"": int, ""permalink"": str, ""image_link"": str}]}
""""""

model.generate_content(system_instruction)

Use Case:
I am working on an e-commerce assistant that needs to generate responses in specific formats. Here’s a brief overview of my use case:
System Instruction:
I want to set up the assistant with a system instruction that defines how it should format responses. The assistant should handle two types of responses:
Normal Responses: Simple text responses with a message type.
Product Responses: Structured JSON responses with a list of products.
Normal Response:
{
  ""type"": ""message"",
  ""data"": ""How can I help you?""
}

Products Response:
{
  ""type"": ""products"",
  ""data"": [
    {
      ""product_name"": ""samsung"",
      ""price"": 20000,
      ""permalink"": ""#link1"",
      ""image_link"": ""#image1""
    },
    {
      ""product_name"": ""iphone"",
      ""price"": 100000,
      ""permalink"": ""#link2"",
      ""image_link"": ""#image2""
    }
  ]
}

Questions
Is there an alternative way to format responses in JSON using the gemini-1.5-flash/pro model?
Are there any updates or workarounds for handling structured JSON responses with this model?
Thank you for your assistance!
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/513","prime number detection","2024-08-20T08:27:16Z","Closed issue","No label","Description of the bug:
gemini seems wrong while claude seems correct
Actual vs expected behavior:
prime number detection need to be corrected while in actual showing wrong
Any other information you'd like to share?
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/511","Automatic image blob creation doesn't handle P images with JPEG","2024-08-27T23:09:30Z","Closed issue","component:python sdk,type:feature request","Description of the bug:
Currently P mode images cannot be uploaded to Gemini. https://pillow.readthedocs.io/en/stable/handbook/concepts.html#modes
Actual vs expected behavior:
import requestsfrom PIL import Imageimport google.generativeai as genai

genai.configure(api_key)


media_url = ""https://cdn.betterttv.net/emote/5f5a87416084af6c1719f9b4/3x""   

image = Image.open(requests.get(media_url, stream=True).raw)
response = genai.GenerativeModel(""gemini-1.5-flash"").generate_content([""Describe this image."", image])
description = response.text.replace('\n', ' ')
print(description)
Actual:
 Running this code gives the errors:
 KeyError: 'P'
 OSError: cannot write mode P as JPEG
Expected:
 If we add the following line before generating the response
image = image.convert(""RGB"")
The code runs with no errors.
Any other information you'd like to share?
This is similar to this issue solved previously: #160
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/510","I can't upload file with a credential","2024-08-17T12:37:04Z","Open issue","type:bug","Description of the bug:
I can't run the method upload_file with a google cloud credential.
 I would like to upload file using json key.
 However, I can't upload file with that credential.
 To upload file, I need API Key for google cloud.
Actual vs expected behavior:
I am not sure this behavior is intentional or not, but I prefer uploading file with a credential instead of API key.
Any other information you'd like to share?
If this is just a bug, I can contribute to fix it.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/508","Tuned model creation failed","2024-09-25T02:00:28Z","Closed as not planned issue","component:other,status:awaiting user response,status:stale,type:help","Description of the bug:
I have a question. Why do I encounter the issues shown in the attached image when I import data directly through the website?
Or is it about the language? I tried using English, and it works just fine. But when it's another language, it can't proceed. So I want to know if this issue is related to the language. If so, what languages does the training data support, or is it only English?
Actual vs expected behavior:
No response
Any other information you'd like to share?
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/506","Can not find message descriptor by type_url: type.googleapis.com/plaque.trace.StatusContext","2024-09-25T02:00:30Z","Closed as not planned issue","component:other,status:awaiting user response,status:stale,type:bug","Description of the bug:
I tried to upload file, and wait for the status to be ""ACTIVE"" but got ""FAILED"" and this message:
Can not find message descriptor by type_url: type.googleapis.com/plaque.trace.StatusContext

This is critical bug for me! please help me.
Actual vs expected behavior:
get ACTIVE instead of failed
Any other information you'd like to share?
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/505","Can't install latest grpc with generative-ai-python","2024-08-19T05:07:42Z","Closed issue","component:python sdk,status:awaiting user response,type:feature request","Description of the feature request:
I'd like to bump the dependency of google-ai-generativelanguage so it'll use later grpc and protobuf versions.
What problem are you trying to solve with this feature?
I have the following dependencies:
grpcio ~= 1.65
grpcio-reflection ~= 1.65
google-generativeai ~= 0.7

Pip can't install due to:
ERROR: Cannot install google-generativeai and runner because these package versions have conflicting dependencies.

The conflict is caused by:
    google-generativeai 0.7.0 depends on protobuf
    google-ai-generativelanguage 0.6.5 depends on protobuf!=3.20.0, !=3.20.1, !=4.21.0, !=4.21.1, !=4.21.2, !=4.21.3, !=4.21.4, !=4.21.5, <5.0.0dev and >=3.19.5
    grpcio-reflection 1.65.4 depends on protobuf<6.0dev and >=5.26.1
    google-generativeai 0.7.0 depends on protobuf
    google-ai-generativelanguage 0.6.5 depends on protobuf!=3.20.0, !=3.20.1, !=4.21.0, !=4.21.1, !=4.21.2, !=4.21.3, !=4.21.4, !=4.21.5, <5.0.0dev and >=3.19.5
    grpcio-reflection 1.65.2 depends on protobuf<6.0dev and >=5.26.1
    google-generativeai 0.7.0 depends on protobuf
    google-ai-generativelanguage 0.6.5 depends on protobuf!=3.20.0, !=3.20.1, !=4.21.0, !=4.21.1, !=4.21.2, !=4.21.3, !=4.21.4, !=4.21.5, <5.0.0dev and >=3.19.5
    grpcio-reflection 1.65.1 depends on protobuf<6.0dev and >=5.26.1

To fix this you could try to:
1. loosen the range of package versions you've specified
2. remove package versions to allow pip attempt to solve the dependency conflict

ERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts

Any other information you'd like to share?
I see that https://github.com/googleapis/google-cloud-python/blob/main/packages/google-ai-generativelanguage/setup.py has newer version as dependencies.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/504","""500 An internal error has occurred"" while using the gemini-1.5-flash","2024-09-11T01:56:35Z","Closed as not planned issue","component:other,status:awaiting user response,status:stale,type:bug","Description of the bug:
I upload some txt files and want gemini to help me extracting some information from them. But the response kept being ""500 An internal error has occurred"". I think I have set the config all right as the document tutoring. How to solve this error?

Actual vs expected behavior:
No response
Any other information you'd like to share?
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/503","Latest Python does not supported","2024-09-11T01:56:37Z","Closed as not planned issue","component:other,status:awaiting user response,status:stale,type:help","Description of the feature request:
Latest Python does not supported the streamlit for deployment
What problem are you trying to solve with this feature?
No response
Any other information you'd like to share?
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/502","Lets get a sample of standard retry logic with exponential backoff, etc.","2024-08-10T14:23:11Z","Open issue","component:python sdk,status:triaged,type:feature request","Description of the feature request:
There are many recipes for all sorts of functionality, but none (that I can find) that show retry logic for return codes 429, 503 and 500. I'm seeing these return codes A LOT.
What problem are you trying to solve with this feature?
More robust API calls.
Any other information you'd like to share?
This snippet to successfully retry when return code is 429 Resource Exhausted but times-out if return code is 503 Model is Overloaded or if 500 An internal error has occurred.
from google.generativeai.types import RequestOptions
from google.api_core import retry

def submit_gemini_query(api_key, system_message, user_message, response_class):
    
    genai.configure(api_key=api_key)

    generation_config = {
        ""temperature"": 0,
        ""max_output_tokens"": 8192
    }
    
    model = genai.GenerativeModel(
        model_name=""gemini-1.5-pro-latest"",
        generation_config=generation_config,
        system_instruction=system_message
    )

    response = model.generate_content(user_message,
                                      request_options=RequestOptions(
                                        retry=retry.Retry(
                                            initial=10, 
                                            multiplier=2, 
                                            maximum=60, 
                                            timeout=300
                                        )
                                       )
                                    )

    return response.text

 The text was updated successfully, but these errors were encountered: 
👍2
beddows and friuns2 reacted with thumbs up emoji
All reactions
👍2 reactions"
"https://github.com/google-gemini/generative-ai-python/issues/501","API docs are very hidden / should be linked from ai.google.dev/gemini-api and main readme","2024-08-08T14:35:22Z","Open issue","No label","Description of the bug:
The only place w/ actual Python classes / methods docs seems to be in the repo under docs/api/google/generativeai.md
However, there's a prominent ai.google.dev/gemini-api/docs link which immediately takes the user away. There, one might expect API Reference to be it, but alas, no API docs either (nor from the Cookbook).
Luckily Gemini's GenerativeModel doc does seem to now rank higher than Vertex AI's on Google Search, but the latter looks more appealing and there's no warning about their differences.
(for instance, I tripped up on the lack of system_instruction param in GenerativeModel.generate_text - it's actually in the class' ctor), when the closest to an API doc I could find on ai.google.dev, the REST API for models.generateContent, listed it, similar to vertex ai's doc)
Actual vs expected behavior:
No response
Any other information you'd like to share?
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/500","503 The service is currently unavailable when using Context caching Feature","2024-08-08T02:57:43Z","Open issue","component:other,status:triaged,type:bug","Description of the bug:
I'm trying to create a cache by reading the contents of multiple PDF files, but when the total number of tokens within the files exceeds approximately 500,000 tokens, I receive a 503 error (Service Unavailable) from Google API Core.
It seems that the error isn't returning immediately, but rather after about 40 to 50 seconds. This might indicate that a timeout is occurring in Google API Core.
Code
import google.generativeai as genai
import os

gemini_api_key = os.environ.get(""GEMINI_API_KEY"")
genai.configure(api_key=gemini_api_key)

documents = []
file_list = [""xxx.pdf"", ""yyy.pdf"", ...]
for file in file_list:
  gemini_file = genai.upload_file(path=file, display_name=file)
  documents.append(gemini_file)

gemini_client = genai.GenerativeModel(""models/gemini-1.5-flash-001"")
total_token = gemini_client.count_tokens(documents).total_tokens)
print(f""total_token: {total_token}"")
# total_token: 592403

gemini_cache = genai.caching.CachedContent.create(model=“models/gemini-1.5-flash-001”, display_name=“sample”, contents=documents)

Version
Python 3.9.19
google==3.0.0
google-ai-generativelanguage==0.6.6
google-api-core==2.19.0
google-api-python-client==2.105.0
google-auth==2.29.0
google-auth-httplib2==0.2.0
google-generativeai==0.7.2
googleapis-common-protos==1.63.0
Actual vs expected behavior:
Actual behavior
Traceback (most recent call last):
  File ""/usr/local/lib/python3.9/site-packages/google/api_core/grpc_helpers.py"", line 76, in error_remapped_callable
    return callable_(*args, **kwargs)
  File ""/usr/local/lib/python3.9/site-packages/grpc/_channel.py"", line 1176, in __call__
    return _end_unary_response_blocking(state, call, False, None)
  File ""/usr/local/lib/python3.9/site-packages/grpc/_channel.py"", line 1005, in _end_unary_response_blocking
    raise _InactiveRpcError(state)  # pytype: disable=not-instantiable
grpc._channel._InactiveRpcError: <_InactiveRpcError of RPC that terminated with:
	status = StatusCode.UNAVAILABLE
	details = ""The service is currently unavailable.""
	debug_error_string = ""UNKNOWN:Error received from peer ipv4:172.217.175.234:443 {created_time:""2024-08-06T13:37:03.077186006+09:00"", grpc_status:14, grpc_message:""The service is currently unavailable.""}""
>

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/lib/python3.9/site-packages/google/generativeai/caching.py"", line 219, in create
    response = client.create_cached_content(request)
  File ""/usr/local/lib/python3.9/site-packages/google/ai/generativelanguage_v1beta/services/cache_service/client.py"", line 874, in create_cached_content
    response = rpc(
  File ""/usr/local/lib/python3.9/site-packages/google/api_core/gapic_v1/method.py"", line 131, in __call__
    return wrapped_func(*args, **kwargs)
  File ""/usr/local/lib/python3.9/site-packages/google/api_core/grpc_helpers.py"", line 78, in error_remapped_callable
    raise exceptions.from_grpc_error(exc) from exc
google.api_core.exceptions.ServiceUnavailable: 503 The service is currently unavailable.

Expected behavior
gemini_cache = genai.caching.CachedContent.create(model=""models/gemini-1.5-flash-001"", display_name=""sample"", contents=documents)
print(gemini_cache)

# CachedContent(
#     name='cachedContents/l5ataay9naq2',
#     model='models/gemini-1.5-flash-001',
#     display_name='sample',
#     usage_metadata={
#         'total_token_count': 592403,
#     },
#     create_time=2024-08-08 01:21:44.925021+00:00,
#     update_time=2024-08-08 01:21:44.925021+00:00,
#     expire_time=2024-08-08 02:21:43.787890+00:00
# )

Any other information you'd like to share?
https://ai.google.dev/gemini-api/docs/caching?lang=python#considerations
The minimum input token count for context caching is 32,768, and the maximum is the same as the maximum for the given model. (For more on counting tokens, see the Token guide).
Upon reviewing the Gemini API documentation, I noticed an interesting mismatch regarding token limits. While the maximum token count is described as being dependent on the specific model in use. In my case, I'm utilizing the models/gemini-1.5-flash-001 model, which has a maximum input token limit of 1,048,576. Based on this information, I initially assumed that processing around 500,000 tokens should be working without any issues.
Moreover, I was able to successfully generate the cache even with token counts exceeding 800,000 when attempting to create a cache using a string. This leads me to suspect that there might be a bug specifically related to creating cache files with high token counts, as opposed to string-based caching.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/499","generate_content_async broken in Colab?","2024-08-07T16:19:26Z","Open issue","component:other,status:triaged,type:bug","Description of the bug:
Calling GenerativeModel.generate_content_async in Colab results in the following error:
[/usr/local/lib/python3.10/dist-packages/google/generativeai/generative_models.py](https://localhost:8080/#) in generate_content_async(self, contents, generation_config, safety_settings, stream, tools, tool_config, request_options)
    383                 return await generation_types.AsyncGenerateContentResponse.from_aiterator(iterator)
    384             else:
--> 385                 response = await self._async_client.generate_content(
    386                     request,
    387                     **request_options,

[/usr/local/lib/python3.10/dist-packages/google/ai/generativelanguage_v1beta/services/generative_service/async_client.py](https://localhost:8080/#) in generate_content(self, request, model, contents, retry, timeout, metadata)
    404 
    405         # Send the request.
--> 406         response = await rpc(
    407             request,
    408             retry=retry,

TypeError: object GenerateContentResponse can't be used in 'await' expression

show colab cell code
!pip install -U -q google-generativeai# installs google-generativeai==0.7.2

# The following two lines don't change anything:# import nest_asyncio# nest_asyncio.apply()

import asyncioimport google.generativeai as genai

from google.colab import userdataGOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')

genai.configure(api_key=GOOGLE_API_KEY)

model = genai.GenerativeModel(model_name=""models/gemini-1.5-pro-latest"")

async def main():
  print(await model.generate_content_async(""Tell me a joke""))

await main()
The ~same code works well outside Colab (tested Python 3.10.12 = same as Colab, 3.11 and 3.12)
show standalone code that works
'''  This nearly identical code works w/ Python 3.10.12 (same as Colab), 3.11, 3.12  conda create -n gemini-bug-3.10.12 -y  conda activate gemini-bug-3.10.12  pip install -U -q google-generativeai  GOOGLE_API_KEY=... python bug.py'''

import asyncioimport google.generativeai as genai

import osGOOGLE_API_KEY=os.environ['GOOGLE_API_KEY']

genai.configure(api_key=GOOGLE_API_KEY)

model = genai.GenerativeModel(model_name=""models/gemini-1.5-pro-latest"")

async def main():
  print(await model.generate_content_async(""Tell me a joke""))

if __name__ == ""__main__"":
  asyncio.run(main())
Full repro: https://gist.github.com/ochafik/f7472295d7b3f42824c8458f32bfa58b
Actual vs expected behavior:
Unexpected exception in Colab (works outside Colab)
Any other information you'd like to share?
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/497","TypeError when running the example","2024-08-07T13:48:58Z","Closed issue","No label","Description of the bug:
I am running exactly this example which is also defined as a test:
import typing_extensions as typing

class Recipe(typing.TypedDict):
    recipe_name: str

model = genai.GenerativeModel(""gemini-1.5-pro-latest"")
result = model.generate_content(
    ""List a few popular cookie recipes."",
    generation_config=genai.GenerationConfig(
        response_mime_type=""application/json"", response_schema=list([Recipe])
    ),
)
The code is presented here for method models.generateContent: https://ai.google.dev/api/generate-content?hl=pt-br#method:-models.generatecontent
This is also a test defined here: 
generative-ai-python/samples/controlled_generation.py
 Line 18 in 42d952a
	classUnitTests(absltest.TestCase): 
It is raising TypeError:
TypeError: pop expected at most 1 argument, got 2
Actual vs expected behavior:
I expected the example to work with no errors and maybe this test is not being used.
Any other information you'd like to share?
Python version: Python 3.10.12
 Working on Databricks using the runtime: 14.3 LTS 
 The text was updated successfully, but these errors were encountered: 
👍1
firmino reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/google-gemini/generative-ai-python/issues/492","Missing stream=True Parameter in _StreamGenerateContent Class of GenerativeServiceRestTransport Causes Non-Streaming Requests","2024-07-26T22:38:23Z","Closed issue","No label","Description of the bug:
In the _StreamGenerateContent class of the GenerativeServiceRestTransport implementation for the Google AI Generative Language service, the HTTP request intended for streaming does not include the stream=True parameter. This omission results in non-streaming behavior, contrary to the expected functionality.
Actual vs expected behavior:
Actual Behavior:
 The _StreamGenerateContent class makes a non-streaming HTTP request using the REST transport.
Expected Behavior:
 The _StreamGenerateContent class should make a streaming HTTP request using the REST transport.
Any other information you'd like to share?
Code Location:
/google/ai/generativelanguage_v1beta/services/generative_service/transports/rest.py
Current Code Snippet:
# Send the request
headers = dict(metadata)
headers[""Content-Type""] = ""application/json""
response = getattr(self._session, method)(
    ""{host}{uri}"".format(host=self._host, uri=uri),
    timeout=timeout,
    headers=headers,
    params=rest_helpers.flatten_query_params(query_params, strict=True),
    data=body,
    # Missing stream=True parameter
)

Proposed Fix:
 Add the stream=True parameter to the request call:
# Send the request
headers = dict(metadata)
headers[""Content-Type""] = ""application/json""
response = getattr(self._session, method)(
    ""{host}{uri}"".format(host=self._host, uri=uri),
    timeout=timeout,
    headers=headers,
    params=rest_helpers.flatten_query_params(query_params, strict=True),
    data=body,
    stream=True # Added stream parameter
)

 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/489","1.0 Release","2024-07-25T03:28:41Z","Open issue","No label","Description of the feature request:
This is a tracking bug to coordinate the release of the 1.0 version of this library which will happen sometime in the future.
We would like to do a final review to ensure the client support all required endpoints, have a path to support new API protocol clients, and is consistent with some design considerations and capabilities before we cut a stable version.
The extended list of requirements will be filed as separate bugs as we identify them. Please feel free to reach out to @rakyll before closing this issue, or anytime if you have questions.
For now, no action is required. We will follow up with issues and/or PRs for necessary changes.
What problem are you trying to solve with this feature?
No response
Any other information you'd like to share?
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/486","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR","2024-07-24T16:34:35Z","Closed issue","No label","Description of the bug:
I updated to the latest version of the pip package.

from dotenv import load_dotenv
import os
import google.generativeai as genai
from helpers import constants

load_dotenv()

# name of the AI model used in this call.
TEXT_MODEL_NAME_LATEST = os.getenv(""TEXT_MODEL_NAME_LATEST"")
PDF_PAGE_IMAGES_PATH = os.getenv(""PDF_PAGE_IMAGES_PATH"")
PROMPT_PATH = constants.PROMPT_PATH


class GeminiService:
    generation_config_json: str = {
        'temperature': 0.9,
        'top_p': 1,
        'top_k': 40,
        'max_output_tokens': 2048,
        'stop_sequences': [],
        ""response_mime_type"": ""application/json"",
    }

    generation_config: str = {
        'temperature': 0.9,
        'top_p': 1,
        'top_k': 40,
        'max_output_tokens': 2048,
        'stop_sequences': [],
    }

    safety_settings: list[str] = [{""category"": ""HARM_CATEGORY_HARASSMENT"", ""threshold"": ""BLOCK_NONE""},
                                  {""category"": ""HARM_CATEGORY_HATE_SPEECH"", ""threshold"": ""BLOCK_NONE""},
                                  {""category"": ""HARM_CATEGORY_SEXUALLY_EXPLICIT"", ""threshold"": ""BLOCK_NONE""},
                                  {""category"": ""HARM_CATEGORY_DANGEROUS_CONTENT"", ""threshold"": ""BLOCK_NONE""}]

    def __init__(self, prompt_path, as_json: bool = True):
        genai.configure(api_key=os.getenv(""API_KEY""))
        self.model = genai.GenerativeModel(model_name=os.getenv(""TEXT_MODEL_NAME_LATEST""),
                                           system_instruction=GeminiService.read_prompt(prompt_path),
                                           generation_config=GeminiService.generation_config_json if as_json else GeminiService.generation_config,
                                           safety_settings=self.safety_settings)

    @staticmethod
    def read_prompt(prompt_file):
        with open(prompt_file, 'r', encoding=""utf-8"") as file:
            # Load the JSON data from the file into a Python dictionary
            return file.read()

    def single_completion(self, request) -> str:
        return self.model.generate_content(request).text




if __name__ == '__main__':
    service = GeminiService( os.path.join("".."", constants.PROMPT_PATH, ""prompt.txt""))
    prompt = ""What is the meaning of life""
    response = service.single_completion(prompt)
    print(response)


My code is now giving a strange warning.
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1721829902.342524    9100 config.cc:230] gRPC experiments enabled: call_status_override_on_cancellation, event_engine_client, event_engine_dns, event_engine_listener, http2_stats_fix, monitoring_experiment, pick_first_new, trace_record_callops, work_serializer_clears_time_cache

It did not do this before nor does it in another project
Actual vs expected behavior:
not give weird warning
Any other information you'd like to share?
What do i need to do to get ride of this?
 The text was updated successfully, but these errors were encountered: 
👍2
steveepreston and ajcantu reacted with thumbs up emoji
All reactions
👍2 reactions"
"https://github.com/google-gemini/generative-ai-python/issues/485","Context Cache and updating Gemini Code Assist","2024-07-24T16:41:10Z","Closed issue","component:python sdk,type:feature request","Description of the feature request:
First, Thanks for making this easy to use library for those of us with limited Python skills. I spent a number of fruitless hours trying to create a context cache because Gemini Code Assist (the star on the left sidebar of VSC) thought that google.generativeai had a Content class that has a Cache class, importing it and trying to use it with no success. Apparently it was confused with Vertexai, a heavier library that requires one to have a Google Project and a ""GOOGLE_API_KEY"" when all I have is a ""GEMINI_API_KEY"". Could you please consider adding a context cache to this library and convince Google to update their Code Assist to your 0.2.7 (?) version? $50 tip to whoever does this in the next 2 months. I needed it last week.
What problem are you trying to solve with this feature?
I'm sending and sharing large text files between AIs. A cache would eliminate some repetitive network traffic and latency.
Any other information you'd like to share?
Worse comes to worst, I'll learn Vertexai, but it seems confusing.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/481","google.api_core.exceptions.InvalidArgument: 400 API key not valid. Please pass a valid API key. [reason: ""API_KEY_INVALID"" domain: ""googleapis.com"" metadata { key: ""service"" value: ""generativelanguage.googleapis.com"" } ]","2024-07-23T18:26:56Z","Closed issue","component:other,status:awaiting user response,type:help","Description of the bug:
When I used the API key for the first time, I could run and code and get the results from the Flask App. After several attempts, I got the above error on the Flask app.
Actual vs expected behavior:
Actual Behavior:
 google.api_core.exceptions.InvalidArgument: 400 API key not valid. Please pass a valid API key. [reason: ""API_KEY_INVALID"" domain: ""googleapis.com"" metadata { key: ""service"" value: ""generativelanguage.googleapis.com"" } ]
Expected behavior:
 Get generated results
Any other information you'd like to share?
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/476","Get finish_reason: 6 without reason and content reply","2024-08-07T22:01:20Z","Closed issue","component:python sdk,type:bug","Description of the bug:
When sending some specific conversations to gemini-1.5-flash will get finish_reason: 6, and this finish_reason can't be found in official document:

Why is this cause?
Environment details
OS type and version: Ubuntu
Python version: 3.11.9
pip version: 24.0
package versions:
 google-ai-generativelanguage 0.6.6
 google-api-core 2.19.1
 google-api-python-client 2.137.0
 google-auth 2.32.0
 google-auth-httplib2 0.2.0
 google-generativeai 0.7.2
Code example
GEMINI_MODEL = ChatGoogleGenerativeAI(
    model=""gemini-1.5-flash"",
    temperature=0,
)
prompt = ChatPromptTemplate.from_template(prompt_tmpl)
chain = prompt | GEMINI_MODEL | StrOutputParser()
response = chain.invoke(prompt_parmas)
Error trace
[2024-07-19 10:51:30,697] [langchain_google_genai.chat_models | line:561 | _response_to_result] [ERROR] - response: candidates {
  index: 0
  finish_reason: 6
}
usage_metadata {
  prompt_token_count: 1961
  total_token_count: 1961
}

Actual vs expected behavior:
No response
Any other information you'd like to share?
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/475","json mode outputs extra brackets or doesn't include","2024-07-29T23:26:49Z","Closed issue","component:python sdk,type:bug","Description of the bug:
Experiencing very intermittent but troubling behavior where someone with response_mime_type set to 'application/json' and a schema in the prompt, gemini 1.5 pro either includes extra brackets or fails to close out with adequate brackets.
Actual vs expected behavior:
Actual behavior: gemini 1.5 pro outputs invalid json
 Expected behavior: gemini 1.5 pro outputs valid json
Any other information you'd like to share?
here is an example I produced in colab to specifically repro and troubleshoot (note this has never happened in google AI studio where we prototype). This started happening in our production app. To repro started running my prompts in colab to could isolate to an API call vs something in our app.
Here is a snip of the json output:

Here is the error when ran through a simple formatter online:

I remove the 2 trailing brackets:

and it formats just fine
**Note, far more common is the last 2 brackets of the json output are not included. When we add it works fine.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/468","Error when attempting to use Gemini 1.5 Pro: google.api_core.exceptions.InternalServerError: 500 An internal error has occurred","2024-07-24T05:41:58Z","Closed issue","No label","Description of the bug:
Error when attempting to use Gemini 1.5 Pro: google.api_core.exceptions.InternalServerError: 500 An internal error has occurred
Error:

Changing this
 model = genai.GenerativeModel(model_name=""gemini-1.5-pro"",
 generation_config=generation_config,
 safety_settings=safety_settings)
 to this
 model = genai.GenerativeModel(model_name=""gemini-1.5-flash-latest"",
 generation_config=generation_config,
 safety_settings=safety_settings)
 or this
 model = genai.GenerativeModel(model_name=""gemini-1.0-pro"",
 generation_config=generation_config,
 safety_settings=safety_settings)
 ""fixes"" the issue.
The ""fix"" seems to suggest this a rate limit? But if it is a rate limit, the error message is very confusing (at first i thought ""oh maybe if i just wait and try again later it'll work"" and when that didn't happen it took me a LONG time to figure out what was causing it). Also, I understand 1.5 pro has a rate limit of 2 messages/minute, but I only sent one...
Actual vs expected behavior:
Expected: no error
 Actual: error (screenshot in description)
Any other information you'd like to share?
I tested this by switching the model to 1.5 pro on a couple of my other projects that use 1.5 flash. they give the same error after 1 and sometimes even 0 messages. Also yes I waited a couple minute in between tests
(note: generation_config and safety_settings from the code provided are defined elsewhere in my script along with response = model.generate_content(prompt))
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/467","Answered","2024-07-24T20:47:01Z","Closed issue","component:other,status:awaiting user response,type:bug","Description of the bug:
I am experiencing intermittent 500 Internal Server Errors when using the gemini-1.5-pro model with the Google generative AI library. Sometimes the model responds correctly, but other times I encounter the error. This issue does not occur when using the gemini-1.5-flash model.
Sometimes gemini-1.5-pro can provide responses well, but other times it always returns: google.api_core.exceptions.InternalServerError: 500 An internal error has occurred. Please retry or report at https://developers.generativeai.google/guide/troubleshooting. The error occurs more often than the successful responses. I think Google needs to update the rate limit on their API or update this library to handle this issue when using gemini-1.5-pro.
Actual vs expected behavior:
Actual behavior: Intermittent 500 Internal Server Errors when using gemini-1.5-pro model.
 Expected behavior: Consistent and correct responses from the gemini-1.5-pro model without errors.
Any other information you'd like to share?
Use the gemini-1.5-pro model
Run the code and provide various inputs.
Example prompt: ""hello world""
Answer: Responds normally.
On the second or third question, observe the 500 Internal Server Error:
Traceback (most recent call last):
 File ""/root/ren/ai.py"", line 30, in 
 response = chat_session.send_message(user_input)
 File ""/usr/local/lib/python3.10/dist-packages/google/generativeai/generative_models.py"", line 578, in send_message
 response = self.model.generate_content(
 File ""/usr/local/lib/python3.10/dist-packages/google/generativeai/generative_models.py"", line 331, in generate_content
 response = self._client.generate_content(
 File ""/usr/local/lib/python3.10/dist-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py"", line 827, in generate_content
 response = rpc(
 File ""/usr/local/lib/python3.10/dist-packages/google/api_core/gapic_v1/method.py"", line 131, in call
 return wrapped_func(*args, **kwargs)
 File ""/usr/local/lib/python3.10/dist-packages/google/api_core/retry/retry_unary.py"", line 293, in retry_wrapped_func
 return retry_target(
 File ""/usr/local/lib/python3.10/dist-packages/google/api_core/retry/retry_unary.py"", line 153, in retry_target
 _retry_error_helper(
 File ""/usr/local/lib/python3.10/dist-packages/google/api_core/retry/retry_base.py"", line 212, in _retry_error_helper
 raise final_exc from source_exc
 File ""/usr/local/lib/python3.10/dist-packages/google/api_core/retry/retry_unary.py"", line 144, in retry_target
 result = target()
 File ""/usr/local/lib/python3.10/dist-packages/google/api_core/timeout.py"", line 120, in func_with_timeout
 return func(*args, **kwargs)
 File ""/usr/local/lib/python3.10/dist-packages/google/api_core/grpc_helpers.py"", line 78, in error_remapped_callable
 raise exceptions.from_grpc_error(exc) from exc
 google.api_core.exceptions.InternalServerError: 500 An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting
This error appears intermittently when using gemini-1.5-pro but does not occur with gemini-1.5-flash.
 The text was updated successfully, but these errors were encountered: 
👍8
sebastiandaogoldenow, justarmadillo, ferrenza, haicheviet, cauvang32, cappittall, samuelemarro, and maxbeech reacted with thumbs up emoji
All reactions
👍8 reactions"
"https://github.com/google-gemini/generative-ai-python/issues/466","Inconsistency in how model name is supplied in the library.","2024-07-18T16:18:07Z","Open issue","component:python sdk,status:triaged,type:feature request","Description of the feature request:
Embedings requiers models/
result = genai.embed_content(
    model=""models/embedding-001"",
    content=""What is the meaning of life?"",
    task_type=""retrieval_document"",
    title=""Embedding of single string"")

# 1 input > 1 vector output
print(str(result['embedding'])[:50], '... TRIMMED]')

All other methos do not require it
  model = genai.GenerativeModel('gemini-1.5-flash')

What problem are you trying to solve with this feature?
Consistency
Any other information you'd like to share?
#justsaying
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/463","""Method doesn't allow unregistered callers (callers without established identity). Please use API Key or other form of API consumer identity to call this API.""","2024-07-26T00:01:58Z","Closed issue","component:other,status:awaiting user response,type:help","Description of the bug:
file not uploading
Actual vs expected behavior:
expected behavior: file uploads and chat returns a result
actual behavior: file does not upload and chat returns a 500
Any other information you'd like to share?
I'm trying to do chat with an uploaded file using this example code provided by AI studio
""""""
Install the Google AI Python SDK

$ pip install google-generativeai

See the getting started guide for more information:
https://ai.google.dev/gemini-api/docs/get-started/python
""""""

import os
import time

import google.generativeai as genai

genai.configure(api_key=os.environ[""GEMINI_API_KEY""])

def upload_to_gemini(path, mime_type=None):
  """"""Uploads the given file to Gemini.

  See https://ai.google.dev/gemini-api/docs/prompting_with_media
  """"""
  file = genai.upload_file(path, mime_type=mime_type)
  print(f""Uploaded file '{file.display_name}' as: {file.uri}"")
  return file

def wait_for_files_active(files):
  """"""Waits for the given files to be active.

  Some files uploaded to the Gemini API need to be processed before they can be
  used as prompt inputs. The status can be seen by querying the file's ""state""
  field.

  This implementation uses a simple blocking polling loop. Production code
  should probably employ a more sophisticated approach.
  """"""
  print(""Waiting for file processing..."")
  for name in (file.name for file in files):
    file = genai.get_file(name)
    while file.state.name == ""PROCESSING"":
      print(""."", end="""", flush=True)
      time.sleep(10)
      file = genai.get_file(name)
    if file.state.name != ""ACTIVE"":
      raise Exception(f""File {file.name} failed to process"")
  print(""...all files ready"")
  print()

# Create the model
# See https://ai.google.dev/api/python/google/generativeai/GenerativeModel
generation_config = {
  ""temperature"": 1,
  ""top_p"": 0.95,
  ""top_k"": 64,
  ""max_output_tokens"": 8192,
  ""response_mime_type"": ""text/plain"",
}

model = genai.GenerativeModel(
  model_name=""gemini-1.5-flash"",
  generation_config=generation_config,
  # safety_settings = Adjust safety settings
  # See https://ai.google.dev/gemini-api/docs/safety-settings
)

# TODO Make these files available on the local file system
# You may need to update the file paths
files = [
  upload_to_gemini(""c:/users/nagol/downloads/myfile.pdf"", mime_type=""application/pdf""),
]

# Some files have a processing delay. Wait for them to be ready.
wait_for_files_active(files)

chat_session = model.start_chat(
  history=[
    {
      ""role"": ""user"",
      ""parts"": [
        files[0],
        ""describe this file."",
      ],
    },
    {
      ""role"": ""model"",
      ""parts"": [
        ""Okay, ..."",
      ],
    },
  ]
)

response = chat_session.send_message(""can you tell me about ..."")

print(response.text)

but I notice when it says file uploaded, the response page says
{
  ""error"": {
    ""code"": 403,
    ""message"": ""Method doesn't allow unregistered callers (callers without established identity). Please use API Key or other form of API consumer identity to call this API."",
    ""status"": ""PERMISSION_DENIED""
  }
}


which doesn't make sense, because I just configured the API key in the preceding line
genai.configure(api_key=os.environ[""GEMINI_API_KEY""])
full stack trace
Uploaded file 'Dawn_of_Worlds_game_1_0Final.pdf' as: https://generativelanguage.googleapis.com/v1beta/files/jakzgrhxm5om
Waiting for file processing...
...all files ready

---------------------------------------------------------------------------
InternalServerError                       Traceback (most recent call last)
Cell In[8], line 94
     74 wait_for_files_active(files)
     76 chat_session = model.start_chat(
     77   history=[
     78     {
   (...)
     91   ]
     92 )
---> 94 response = chat_session.send_message(""player 1 turn"")
     96 print(response.text)

File ~\AppData\Local\Programs\Python\Python310\lib\site-packages\google\generativeai\generative_models.py:586, in ChatSession.send_message(self, content, generation_config, safety_settings, stream, tools, tool_config, request_options)
    579     #print(""noo!"",self._generation_config.get(""candidate_count"", 1))
    580     #self._generation_config.set(""candidate_count"", 1)
    582     raise ValueError(
    583         ""Invalid configuration: The chat functionality does not support `candidate_count` greater than 1.""
    584     )
--> 586 response = self.model.generate_content(
    587     contents=history,
    588     generation_config=generation_config,
    589     safety_settings=safety_settings,
    590     stream=stream,
    591     tools=tools_lib,
    592     tool_config=tool_config,
    593     request_options=request_options,
    594 )
    596 self._check_response(response=response, stream=stream)
    598 if self.enable_automatic_function_calling and tools_lib is not None:

File ~\AppData\Local\Programs\Python\Python310\lib\site-packages\google\generativeai\generative_models.py:331, in GenerativeModel.generate_content(self, contents, generation_config, safety_settings, stream, tools, tool_config, request_options)
    329         return generation_types.GenerateContentResponse.from_iterator(iterator)
    330     else:
--> 331         response = self._client.generate_content(
    332             request,
    333             **request_options,
    334         )
    335         return generation_types.GenerateContentResponse.from_response(response)
    336 except google.api_core.exceptions.InvalidArgument as e:

File ~\AppData\Local\Programs\Python\Python310\lib\site-packages\google\ai\generativelanguage_v1beta\services\generative_service\client.py:827, in GenerativeServiceClient.generate_content(self, request, model, contents, retry, timeout, metadata)
    824 self._validate_universe_domain()
    826 # Send the request.
--> 827 response = rpc(
    828     request,
    829     retry=retry,
    830     timeout=timeout,
    831     metadata=metadata,
    832 )
    834 # Done; return the response.
    835 return response

File ~\AppData\Local\Programs\Python\Python310\lib\site-packages\google\api_core\gapic_v1\method.py:131, in _GapicCallable.__call__(self, timeout, retry, compression, *args, **kwargs)
    128 if self._compression is not None:
    129     kwargs[""compression""] = compression
--> 131 return wrapped_func(*args, **kwargs)

File ~\AppData\Local\Programs\Python\Python310\lib\site-packages\google\api_core\retry\retry_unary.py:293, in Retry.__call__.<locals>.retry_wrapped_func(*args, **kwargs)
    289 target = functools.partial(func, *args, **kwargs)
    290 sleep_generator = exponential_sleep_generator(
    291     self._initial, self._maximum, multiplier=self._multiplier
    292 )
--> 293 return retry_target(
    294     target,
    295     self._predicate,
    296     sleep_generator,
    297     timeout=self._timeout,
    298     on_error=on_error,
    299 )

File ~\AppData\Local\Programs\Python\Python310\lib\site-packages\google\api_core\retry\retry_unary.py:153, in retry_target(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)
    149 # pylint: disable=broad-except
    150 # This function explicitly must deal with broad exceptions.
    151 except Exception as exc:
    152     # defer to shared logic for handling errors
--> 153     _retry_error_helper(
    154         exc,
    155         deadline,
    156         sleep,
    157         error_list,
    158         predicate,
    159         on_error,
    160         exception_factory,
    161         timeout,
    162     )
    163     # if exception not raised, sleep before next attempt
    164     time.sleep(sleep)

File ~\AppData\Local\Programs\Python\Python310\lib\site-packages\google\api_core\retry\retry_base.py:212, in _retry_error_helper(exc, deadline, next_sleep, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)
    206 if not predicate_fn(exc):
    207     final_exc, source_exc = exc_factory_fn(
    208         error_list,
    209         RetryFailureReason.NON_RETRYABLE_ERROR,
    210         original_timeout,
    211     )
--> 212     raise final_exc from source_exc
    213 if on_error_fn is not None:
    214     on_error_fn(exc)

File ~\AppData\Local\Programs\Python\Python310\lib\site-packages\google\api_core\retry\retry_unary.py:144, in retry_target(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)
    142 for sleep in sleep_generator:
    143     try:
--> 144         result = target()
    145         if inspect.isawaitable(result):
    146             warnings.warn(_ASYNC_RETRY_WARNING)

File ~\AppData\Local\Programs\Python\Python310\lib\site-packages\google\api_core\timeout.py:120, in TimeToDeadlineTimeout.__call__.<locals>.func_with_timeout(*args, **kwargs)
    117     # Avoid setting negative timeout
    118     kwargs[""timeout""] = max(0, self._timeout - time_since_first_attempt)
--> 120 return func(*args, **kwargs)

File ~\AppData\Local\Programs\Python\Python310\lib\site-packages\google\api_core\grpc_helpers.py:78, in _wrap_unary_errors.<locals>.error_remapped_callable(*args, **kwargs)
     76     return callable_(*args, **kwargs)
     77 except grpc.RpcError as exc:
---> 78     raise exceptions.from_grpc_error(exc) from exc

InternalServerError: 500 An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting


 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/461","TypeError: no default __reduce__ due to non-trivial __cinit__","2024-07-15T15:33:18Z","Open issue","component:python sdk,status:triaged,type:bug","Description of the bug:
Hi, I'm encourating this issue while using gemini-1.5-flash in DSPy. Not sure if it belongs here, but the issue only occurs with the Gemini model.
There's a line program.deepcopy() where a program with Gemini model is copied. (x). Program is a simple ChainOfThought with no demos.
    return copy.deepcopy(self)
  File ""/usr/lib/python3.10/copy.py"", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""/usr/lib/python3.10/copy.py"", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File ""/usr/lib/python3.10/copy.py"", line 146, in deepcopy
    y = copier(x, memo)
  File ""/usr/lib/python3.10/copy.py"", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""/usr/lib/python3.10/copy.py"", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""/usr/lib/python3.10/copy.py"", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File ""/usr/lib/python3.10/copy.py"", line 146, in deepcopy
    y = copier(x, memo)
  File ""/usr/lib/python3.10/copy.py"", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""/usr/lib/python3.10/copy.py"", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""/usr/lib/python3.10/copy.py"", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File ""/usr/lib/python3.10/copy.py"", line 146, in deepcopy
    y = copier(x, memo)
  File ""/usr/lib/python3.10/copy.py"", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""/usr/lib/python3.10/copy.py"", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""/usr/lib/python3.10/copy.py"", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File ""/usr/lib/python3.10/copy.py"", line 146, in deepcopy
    y = copier(x, memo)
  File ""/usr/lib/python3.10/copy.py"", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""/usr/lib/python3.10/copy.py"", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""/usr/lib/python3.10/copy.py"", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File ""/usr/lib/python3.10/copy.py"", line 146, in deepcopy
    y = copier(x, memo)
  File ""/usr/lib/python3.10/copy.py"", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""/usr/lib/python3.10/copy.py"", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""/usr/lib/python3.10/copy.py"", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File ""/usr/lib/python3.10/copy.py"", line 146, in deepcopy
    y = copier(x, memo)
  File ""/usr/lib/python3.10/copy.py"", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""/usr/lib/python3.10/copy.py"", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""/usr/lib/python3.10/copy.py"", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File ""/usr/lib/python3.10/copy.py"", line 146, in deepcopy
    y = copier(x, memo)
  File ""/usr/lib/python3.10/copy.py"", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""/usr/lib/python3.10/copy.py"", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""/usr/lib/python3.10/copy.py"", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File ""/usr/lib/python3.10/copy.py"", line 146, in deepcopy
    y = copier(x, memo)
  File ""/usr/lib/python3.10/copy.py"", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""/usr/lib/python3.10/copy.py"", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""/usr/lib/python3.10/copy.py"", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File ""/usr/lib/python3.10/copy.py"", line 146, in deepcopy
    y = copier(x, memo)
  File ""/usr/lib/python3.10/copy.py"", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""/usr/lib/python3.10/copy.py"", line 161, in deepcopy
    rv = reductor(4)
  File ""stringsource"", line 2, in grpc._cython.cygrpc.Channel.__reduce_cython__
TypeError: no default __reduce__ due to non-trivial __cinit__


Actual vs expected behavior:
Other models are copied without an error at this stage.
Any other information you'd like to share?
google_generativeai-0.7.2
 grpcio-status 1.48.1
 Due to a protobuf error, tried both downgrading protobuf-4.25.3 to 3.19.6 as well as os.environ[""PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION""]=""python"" and the issue persists.
reductor object that is giving an error is <built-in method __reduce_ex__ of grpc._cython.cygrpc.Channel object at 0x7f148c37e080>
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/450","Test to ensure that api_key or credentials is set in configure","2024-07-10T16:06:58Z","Open issue","component:python sdk,status:triaged,type:bug","Description of the feature request:
I would like to suggest a change to configure
Currently both api_key and credentials can be set to None.
    api_key: str | None = None,
    credentials: ga_credentials.Credentials | dict | None = None,

If they are both set to None then the API returns with an error which arguably doesn't lead the developer to understand what the issue is. ""ACCESS_TOKEN_SCOPE_INSUFFICIENT"" It would be better if the library tested to ensure that either api_key is set or credentials are set. Ideally that api_key is not set to """" as well. as this appears to happen when an empty env var is loaded. Which is again hard to debug.
What problem are you trying to solve with this feature?
I am trying to make it easer to avoid the ""ACCESS_TOKEN_SCOPE_INSUFFICIENT"" error. By informing the developer that they must set either credentials or api key
Any other information you'd like to share?
This is an issue i have seen a number of times on SO and one that i have also personally had sevral times.
Google generativeai 403 Request had insufficient authentication scopes. [reason: ""ACCESS_TOKEN_SCOPE_INSUFFICIENT""]
If accepted i would be willing to make the change myself.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/446","Function Calling Does Not Work With Stop Sequences and Streaming","2024-07-08T22:53:41Z","Open issue","component:python sdk,status:invalid,type:bug","Description of the bug:
Function calling does not work when providing stop_sequences and stream=True.
Actual vs expected behavior:
Actual:
import google.generativeai as genaiimport os

from google.generativeai.types import GenerationConfig

GOOGLE_API_KEY = os.environ[""GOOGLE_API_KEY""]

genai.configure(api_key=GOOGLE_API_KEY)


def multiply(a: float, b: float):
    """"""returns a * b.""""""
    return a * b


model = genai.GenerativeModel(model_name=""gemini-1.5-flash"")
response = model.generate_content(
    ""What is 4 * 3?"",
    stream=True,
    tools=[multiply],
    generation_config=GenerationConfig(stop_sequences=[""Anything""]),
)

for chunk in response:
    print(chunk.parts)
    print(""_"" * 80)
No FunctionCall is returned.
[text: ""print(default_api.multiply(a = 4, b = 3))""
]

Expected (after removing stop_sequences):
import google.generativeai as genaiimport os


GOOGLE_API_KEY = os.environ[""GOOGLE_API_KEY""]

genai.configure(api_key=GOOGLE_API_KEY)


def multiply(a: float, b: float):
    """"""returns a * b.""""""
    return a * b


model = genai.GenerativeModel(model_name=""gemini-1.5-flash"")
response = model.generate_content(
    ""What is 4 * 3?"",
    stream=True,
    tools=[multiply],
)

for chunk in response:
    print(chunk.parts)
    print(""_"" * 80)
A Part containing a FunctionCall is returned.
[function_call {
  name: ""multiply""
  args {
    fields {
      key: ""b""
      value {
        number_value: 3
      }
    }
    fields {
      key: ""a""
      value {
        number_value: 4
      }
    }
  }
}
]

Expected (after setting stream=False):
import google.generativeai as genaiimport os

from google.generativeai.types import GenerationConfig

GOOGLE_API_KEY = os.environ[""GOOGLE_API_KEY""]

genai.configure(api_key=GOOGLE_API_KEY)


def multiply(a: float, b: float):
    """"""returns a * b.""""""
    return a * b


model = genai.GenerativeModel(model_name=""gemini-1.5-flash"")
response = model.generate_content(
    ""What is 4 * 3?"",
    stream=False,
    tools=[multiply],
    generation_config=GenerationConfig(stop_sequences=[""Anything""]),
)

for chunk in response:
    print(chunk.parts)
    print(""_"" * 80)
A Part containing a FunctionCall is returned.
[function_call {
  name: ""multiply""
  args {
    fields {
      key: ""b""
      value {
        number_value: 3
      }
    }
    fields {
      key: ""a""
      value {
        number_value: 4
      }
    }
  }
}
]

Any other information you'd like to share?
I am using version 0.7.2 of this library.
 The text was updated successfully, but these errors were encountered: 
👍1
jclemenceau reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/google-gemini/generative-ai-python/issues/426","Bug genai.list_models(): TypeError: Model.__init__() got an unexpected keyword argument 'max_temperature'","2024-07-09T20:52:06Z","Closed issue","component:python sdk,type:bug","Description of the bug:
Environment
google-ai-generativelanguage-0.6.6
google-generativeai-0.7.1

Reproduce Error
# test_script.pyimport google.generativeai as genai

genai.configure()
list(genai.list_models())
Raise Error
Traceback (most recent call last):
  File ""/Users/me/test_script.py"", line 4, in <module>
    list(genai.list_models())
  File ""/Users/me/miniconda3/envs/gls/lib/python3.10/site-packages/google/generativeai/models.py"", line 206, in list_models
    yield model_types.Model(**model)
TypeError: Model.__init__() got an unexpected keyword argument 'max_temperature'

Workaround
Constrained version google-generativeai!=0.7.1.
 The text was updated successfully, but these errors were encountered: 
👍5
igorlima, gnumoksha, EdoardoLuciani, sanhiitaa, and tse00022 reacted with thumbs up emoji
All reactions
👍5 reactions"
"https://github.com/google-gemini/generative-ai-python/issues/425","WebP files not recognized by mimetypes.guess_type() in Python <3.13","2024-07-08T16:34:57Z","Closed issue","component:python sdk,type:feature request","Description of the bug:
When attempting to use a WebP image with the generate_content() function of the Gemini API, an ""Unsupported MIME type"" error is encountered. This occurs despite WebP being a widely supported image format.
Environment:
Python version: 3.10
Gemini API version: v0.7.1
Code to reproduce:
import requestsimport google.generativeai as genai

response = requests.get(""https://www.gstatic.com/webp/gallery/3.sm.webp"")
with open(""test.webp"", 'wb') as file:
    file.write(response.content)

GOOGLE_API_KEY=""****""genai.configure(api_key=GOOGLE_API_KEY)
model = genai.GenerativeModel('gemini-1.5-pro-latest')
sample_webp = genai.upload_file(path='test.webp')

response = model.generate_content(
    [""Describe the image with a creative description."", sample_webp],
)
print(response.text)
Error Message:
BadRequest: 400 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?%24alt=json%3Benum-encoding%3Dint: Unsupported MIME type: application/octet-stream

Actual vs expected behavior:
Actual behavior:
 The API returns a 400 BadRequest error with the message ""Unsupported MIME type: application/octet-stream"" when attempting to process a WebP image.
Expected behavior:
 The API should recognize and process the WebP image format, allowing for text generation as it does with other supported image formats like JPEG or PNG.
Any other information you'd like to share?
Problem Description
 The Gemini API is throwing an ""Unsupported MIME type"" error when attempting to process WebP images. This issue likely stems from the upload_file function:
generative-ai-python/google/generativeai/files.py
 Lines 60 to 61 in 45fcbdf
	ifmime_typeisNone: 
	mime_type, _=mimetypes.guess_type(path) 
Current Workaround
 Users can explicitly specify the MIME type when calling upload_file:
response = model.generate_content(
    [""Describe the image with a creative description."", 
     genai.types.FileData(
         mime_type=""image/webp"",
         data=sample_webp
     )]
)
Broader Context
This issue is likely to affect other users who don't specify the MIME type explicitly.
 Python 3.13 will include a fix for this in the core mimetypes module (see python/cpython#111741). Users on Python 3.13+ won't encounter this error.
 For users on Python versions lower than 3.13, a potential solution would be to add the WebP MIME type manually:
mimetypes.add_type('image/webp', '.webp')

Suggested Improvements
Update the library to include the manual addition of the WebP MIME type:
mimetypes.add_type('image/webp', '.webp')
 This would ensure compatibility across all Python versions.
 Consider updating the upload_file function to handle WebP files explicitly, even if mimetypes.guess_type fails.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/423","Function calling doesn't work with functions that take no arguments.","2024-07-30T17:11:13Z","Closed issue","component:python sdk,type:bug","Description of the bug:
My app ran well with function calling 12 days ago. However, when I tried to use this app today, Gemini API raised an InvalidArgument exception. And I managed to figure out it will only occurs when tools argument is not empty.
Actual vs expected behavior:
It raises an InvalidArgument exception. The expected behaviour is that it should return a normal response like it used to.
Any other information you'd like to share?
Minimal reproducible example:
import models, chatModelimport tools

def calculate(expression: str) -> int | float | str:
    """"""    Calculate an expression using Python 3 and return the number value.    You can use the following operators: +, -, *, /, **, %, //, and ().    You are allowed to use math library.    Args:        expression (str): The expression to evaluate.    Returns:        int | float | str: The result of the expression.    """"""
    return eval(expression, globals(), locals())

def time() -> str:
    """"""    Get the current time in HH:MM:SS format.    Returns:        str: The current time in HH:MM:SS format.    """"""
    return tools.TimeProvider()

m = genai.GenerativeModel(model_name=config.USE_MODEL, system_instruction='You are an AI chatbot named Yoi.', safety_settings=models.MODEL_SAFETY_SETTING, generation_config={
            'temperature': 0.9,
        }, tools=[calculate, time])

s = m.start_chat()
print(s.send_message(""Hello?""))
Log:
Logger: using stdout
<_io.TextIOWrapper name='<stdout>' mode='w' encoding='utf-8'>
Traceback (most recent call last):
  File ""/home/chou/CyberWaifu-v2/aaa.py"", line 38, in <module>
    print(s.send_message(""What is the time?""))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/chou/miniconda3/envs/cyberWaifuV2/lib/python3.12/site-packages/google/generativeai/generative_models.py"", line 578, in send_message
    response = self.model.generate_content(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/chou/miniconda3/envs/cyberWaifuV2/lib/python3.12/site-packages/google/generativeai/generative_models.py"", line 331, in generate_content
    response = self._client.generate_content(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/chou/miniconda3/envs/cyberWaifuV2/lib/python3.12/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py"", line 827, in generate_content
    response = rpc(
               ^^^^
  File ""/home/chou/miniconda3/envs/cyberWaifuV2/lib/python3.12/site-packages/google/api_core/gapic_v1/method.py"", line 131, in __call__
    return wrapped_func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/chou/miniconda3/envs/cyberWaifuV2/lib/python3.12/site-packages/google/api_core/retry/retry_unary.py"", line 293, in retry_wrapped_func
    return retry_target(
           ^^^^^^^^^^^^^
  File ""/home/chou/miniconda3/envs/cyberWaifuV2/lib/python3.12/site-packages/google/api_core/retry/retry_unary.py"", line 153, in retry_target
    _retry_error_helper(
  File ""/home/chou/miniconda3/envs/cyberWaifuV2/lib/python3.12/site-packages/google/api_core/retry/retry_base.py"", line 212, in _retry_error_helper
    raise final_exc from source_exc
  File ""/home/chou/miniconda3/envs/cyberWaifuV2/lib/python3.12/site-packages/google/api_core/retry/retry_unary.py"", line 144, in retry_target
    result = target()
             ^^^^^^^^
  File ""/home/chou/miniconda3/envs/cyberWaifuV2/lib/python3.12/site-packages/google/api_core/timeout.py"", line 120, in func_with_timeout
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/home/chou/miniconda3/envs/cyberWaifuV2/lib/python3.12/site-packages/google/api_core/grpc_helpers.py"", line 78, in error_remapped_callable
    raise exceptions.from_grpc_error(exc) from exc
google.api_core.exceptions.InvalidArgument: 400 Request contains an invalid argument.

 The text was updated successfully, but these errors were encountered: 
👍1
nmfisher reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/google-gemini/generative-ai-python/issues/411","_OPERATOR dict with multiple 6 mentioned multiple times","2024-07-03T16:31:35Z","Closed issue","component:python sdk,type:bug","Description of the bug:
just doing a static analysis of the code and found. I don't know how to fix it, so I am raising this issue.
generative-ai-python/google/generativeai/types/retriever_types.py
 Line 109 in 630318b
	6: Operator.EXCLUDES, 
Actual vs expected behavior:
No response
Any other information you'd like to share?
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/393","[BUG] 407: Proxy Authentication Required for genai.upload_file API while proxy has been set to ENV var","2024-09-07T01:54:49Z","Closed as not planned issue","component:python sdk,status:awaiting user response,status:stale,type:bug","Description of the bug:
As user/pass have been set in ENV var as follows:
os.environ[""HTTP_PROXY""] = ""http://XXX:XXX@XXX.com:28888""os.environ[""HTTPS_PROXY""] = ""http://XXX:XXX@XXX.com:28888""
The uploading API will raise an error from the following code, generated from AI studio:
def upload_to_gemini(path, mime_type=None):
    """"""Uploads the given file to Gemini.     See https://ai.google.dev/gemini-api/docs/prompting_with_media    """"""
    file = genai.upload_file(path, mime_type=mime_type)
    print(f""Uploaded file '{file.display_name}' as: {file.uri}"")
    return file
The full error log as follows:
GeneralProxyError                         Traceback (most recent call last)
Cell In[4], line 16
      1 model = genai.GenerativeModel(
      2     model_name=""gemini-1.5-pro"",
      3     generation_config=generation_config,
   (...)
     10     # See https://ai.google.dev/gemini-api/docs/safety-settings
     11     )
     13 # TODO Make these files available on the local file system
     14 # You may need to update the file paths
     15 files = [
---> 16     upload_to_gemini(""XXX.png"",
     17                      mime_type=""image/png""),
     18     ]
     20 chat_session = model.start_chat(
     21     history=[
     22         {
   (...)
     28         ]
     29     )
     31 response = chat_session.send_message(""Please describe the given image in detail."")

Cell In[3], line 12, in upload_to_gemini(path, mime_type)
      7 def upload_to_gemini(path, mime_type=None):
      8     """"""Uploads the given file to Gemini.
      9   
     10     See https://ai.google.dev/gemini-api/docs/prompting_with_media
     11     """"""
---> 12     file = genai.upload_file(path, mime_type=mime_type)
     13     print(f""Uploaded file '{file.display_name}' as: {file.uri}"")
     14     return file

File ~/.miniconda/envs/jupyter/lib/python3.10/site-packages/google/generativeai/files.py:69, in upload_file(path, mime_type, name, display_name, resumable)
     66 if display_name is None:
     67     display_name = path.name
---> 69 response = client.create_file(
     70     path=path, mime_type=mime_type, name=name, display_name=display_name, resumable=resumable
     71 )
     72 return file_types.File(response)

File ~/.miniconda/envs/jupyter/lib/python3.10/site-packages/google/generativeai/client.py:82, in FileServiceClient.create_file(self, path, mime_type, name, display_name, resumable)
     72 def create_file(
     73     self,
     74     path: str | pathlib.Path | os.PathLike,
   (...)
     79     resumable: bool = True,
     80 ) -> protos.File:
     81     if self._discovery_api is None:
---> 82         self._setup_discovery_api()
     84     file = {}
     85     if name is not None:

File ~/.miniconda/envs/jupyter/lib/python3.10/site-packages/google/generativeai/client.py:65, in FileServiceClient._setup_discovery_api(self)
     56     raise ValueError(
     57         ""Invalid operation: Uploading to the File API requires an API key. Please provide a valid API key.""
     58     )
     60 request = googleapiclient.http.HttpRequest(
     61     http=httplib2.Http(),
     62     postproc=lambda resp, content: (resp, content),
     63     uri=f""{GENAI_API_DISCOVERY_URL}?version=v1beta&key={api_key}"",
     64 )
---> 65 response, content = request.execute()
     67 discovery_doc = content.decode(""utf-8"")
     68 self._discovery_api = googleapiclient.discovery.build_from_document(
     69     discovery_doc, developerKey=api_key
     70 )

File ~/.miniconda/envs/jupyter/lib/python3.10/site-packages/googleapiclient/_helpers.py:130, in positional.<locals>.positional_decorator.<locals>.positional_wrapper(*args, **kwargs)
    128     elif positional_parameters_enforcement == POSITIONAL_WARNING:
    129         logger.warning(message)
--> 130 return wrapped(*args, **kwargs)

File ~/.miniconda/envs/jupyter/lib/python3.10/site-packages/googleapiclient/http.py:923, in HttpRequest.execute(self, http, num_retries)
    920     self.headers[""content-length""] = str(len(self.body))
    922 # Handle retries for server-side errors.
--> 923 resp, content = _retry_request(
    924     http,
    925     num_retries,
    926     ""request"",
    927     self._sleep,
    928     self._rand,
    929     str(self.uri),
    930     method=str(self.method),
    931     body=self.body,
    932     headers=self.headers,
    933 )
    935 for callback in self.response_callbacks:
    936     callback(resp)

File ~/.miniconda/envs/jupyter/lib/python3.10/site-packages/googleapiclient/http.py:191, in _retry_request(http, num_retries, req_type, sleep, rand, uri, method, *args, **kwargs)
    189 try:
    190     exception = None
--> 191     resp, content = http.request(uri, method, *args, **kwargs)
    192 # Retry on SSL errors and socket timeout errors.
    193 except _ssl_SSLError as ssl_error:

File ~/.miniconda/envs/jupyter/lib/python3.10/site-packages/httplib2/__init__.py:1724, in Http.request(self, uri, method, body, headers, redirections, connection_type)
   1722             content = b""""
   1723         else:
-> 1724             (response, content) = self._request(
   1725                 conn, authority, uri, request_uri, method, body, headers, redirections, cachekey,
   1726             )
   1727 except Exception as e:
   1728     is_timeout = isinstance(e, socket.timeout)

File ~/.miniconda/envs/jupyter/lib/python3.10/site-packages/httplib2/__init__.py:1444, in Http._request(self, conn, host, absolute_uri, request_uri, method, body, headers, redirections, cachekey)
   1441 if auth:
   1442     auth.request(method, request_uri, headers, body)
-> 1444 (response, content) = self._conn_request(conn, request_uri, method, body, headers)
   1446 if auth:
   1447     if auth.response(response, body):

File ~/.miniconda/envs/jupyter/lib/python3.10/site-packages/httplib2/__init__.py:1366, in Http._conn_request(self, conn, request_uri, method, body, headers)
   1364 try:
   1365     if conn.sock is None:
-> 1366         conn.connect()
   1367     conn.request(method, request_uri, body, headers)
   1368 except socket.timeout:

File ~/.miniconda/envs/jupyter/lib/python3.10/site-packages/httplib2/__init__.py:1202, in HTTPSConnectionWithTimeout.connect(self)
   1200     break
   1201 if not self.sock:
-> 1202     raise socket_err

File ~/.miniconda/envs/jupyter/lib/python3.10/site-packages/httplib2/__init__.py:1156, in HTTPSConnectionWithTimeout.connect(self)
   1154 if has_timeout(self.timeout):
   1155     sock.settimeout(self.timeout)
-> 1156 sock.connect((self.host, self.port))
   1158 self.sock = self._context.wrap_socket(sock, server_hostname=self.host)
   1160 # Python 3.3 compatibility: emulate the check_hostname behavior

File ~/.miniconda/envs/jupyter/lib/python3.10/site-packages/socks.py:47, in set_self_blocking.<locals>.wrapper(*args, **kwargs)
     45     if _is_blocking == 0:
     46         self.setblocking(True)
---> 47     return function(*args, **kwargs)
     48 except Exception as e:
     49     raise

File ~/.miniconda/envs/jupyter/lib/python3.10/site-packages/socks.py:814, in socksocket.connect(self, dest_pair, catch_errors)
    811 if not catch_errors:
    812     # Wrap socket errors
    813     self.close()
--> 814     raise GeneralProxyError(""Socket error"", error)
    815 else:
    816     raise error

GeneralProxyError: Socket error: 407: Proxy Authentication Required

Actual vs expected behavior:
Go through an http proxy with authentication correctly, but, it fails.
Any other information you'd like to share?
Maybe related issues in upstream:
Support HTTP_PROXY for python3 httplib2/httplib2#53
proxy with authentication httplib2/httplib2#154
Fix parsing http_proxy environment. httplib2/httplib2#217
httplib2/httplib2@
fa9a3bbfb5667968531613b24529a364f4d16b29Maybe
While everything works fine with these packages alone:
from urllib.parse import urlparseurl = urlparse(""http://USER:PASSWORD@HOST:PORT/PATH"")
print(url)
print(url.username)
print(url.password)
It returns:
ParseResult(scheme='http', netloc='USER:PASSWORD@HOST:PORT', path='/PATH', params='', query='', fragment='')
USER
PASSWORD

And:
import httplib2

pi = httplib2.proxy_info_from_url(""http://USER:PASSWORD@HOST:80/PATH"")
print(pi)
print(pi.proxy_host)
print(pi.proxy_port)
print(pi.proxy_user)
print(pi.proxy_pass)
it returns
<ProxyInfo type=3 host:port=host:80 rdns=True user=USER headers=None>
host
80
USER
PASSWORD

 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/392","response_schema not working with gemini-1.5-pro model","2024-07-19T01:51:03Z","Closed as not planned issue","component:python sdk,status:awaiting user response,status:stale,type:bug","Description of the bug:
I ended up wasting 2 hours on trying to get it to work after seeing the official documentation for contstrained generation on https://ai.google.dev/gemini-api/docs/api-overview#json
Thanks for the pro tip of adding the schema to system prompt mentioned on #343 I was finally able to get it working.
The code generated on https://aistudio.google.com/ is also exteremly deceptive as it makes you input the schema making you believe that constrained generation is working and gives back the code which has no reference to JSON schema specified in Advanced Settings
Actual vs expected behavior:
response_schema in model configuration should be respected without having to explcitly add the whole schema to system prompt
Any other information you'd like to share?
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/391","Fix custom endpoint with file uploads and colab.","2024-06-13T14:14:41Z","Open issue","component:python sdk,status:triaged,type:feature request","Description of the feature request:
#333
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/385","Unable to intercept requests to generativelanguages.google","2024-07-10T01:51:07Z","Closed as not planned issue","component:python sdk,status:awaiting user response,status:stale,status:triaged,type:help","So I'm using a MITM proxy and when I run hit the API endpoint using requests python library, I'm successfully able to see the network trace in my middle man proxy log, but when I use this python package for the same task I'm unable to see the network trace to the generativelanguage.googleapis endpoint.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/384","google.generativeai.types.generation_types.StopCandidateException: index: 0","2024-06-25T08:27:11Z","Closed issue","component:python sdk,status:awaiting user response,status:stale,type:bug","Description of the bug:
code:
""""""Install the Google AI Python SDK$ pip install google-generativeaiSee the getting started guide for more information:https://ai.google.dev/gemini-api/docs/get-started/python""""""

import os

import google.generativeai as genai

genai.configure(api_key='XXX')

# Create the model# See https://ai.google.dev/api/python/google/generativeai/GenerativeModelgeneration_config = {
  ""temperature"": 1,
  ""top_p"": 0.95,
  ""top_k"": 64,
  ""max_output_tokens"": 8192,
  ""response_mime_type"": ""text/plain"",
}

model = genai.GenerativeModel(
  model_name=""gemini-1.5-pro"",
  generation_config=generation_config,
  # safety_settings = Adjust safety settings
  # See https://ai.google.dev/gemini-api/docs/safety-settings
  system_instruction=""This text is from the ZeroMQ guide, the field of knowledge is computer science, the original text is in English, please translate into Chinese, do not use machine translation style, do not add translation notes."",
)

chat_session = model.start_chat()

response = chat_session.send_message(""By Pieter Hintjens, CEO of iMatix"")
print(response.text)
response = chat_session.send_message(""Please use the issue tracker for all comments and errata. This version covers the latest stable release of ZeroMQ (3.2). If you are using older versions of ZeroMQ then some of the examples and explanations won’t be accurate."")
print(response.text)
response = chat_session.send_message(""The Guide is originally in C , but also in PHP , Java , Python , Lua , and Haxe . We’ve also translated most of the examples into C++, C#, CL, Delphi, Erlang, F#, Felix, Haskell, Julia, Objective-C, Ruby, Ada, Basic, Clojure, Go, Haxe, Node.js, ooc, Perl, Scala, and Rust."")
print(response.text)
response = chat_session.send_message(""Preface"")
print(response.text)
response = chat_session.send_message(""ZeroMQ in a Hundred Words"")
print(response.text)
response = chat_session.send_message(""ZeroMQ (also known as ØMQ, 0MQ, or zmq) looks like an embeddable networking library but acts like a concurrency framework. It gives you sockets that carry atomic messages across various transports like in-process, inter-process, TCP, and multicast. You can connect sockets N-to-N with patterns like fan-out, pub-sub, task distribution, and request-reply. It’s fast enough to be the fabric for clustered products. Its asynchronous I/O model gives you scalable multicore applications, built as asynchronous message-processing tasks. It has a score of language APIs and runs on most operating systems. ZeroMQ is from iMatix and is LGPLv3 open source."")
print(response.text)
response = chat_session.send_message(""How It Began"")
print(response.text)
response = chat_session.send_message(""We took a normal TCP socket, injected it with a mix of radioactive isotopes stolen from a secret Soviet atomic research project, bombarded it with 1950-era cosmic rays, and put it into the hands of a drug-addled comic book author with a badly-disguised fetish for bulging muscles clad in spandex. Yes, ZeroMQ sockets are the world-saving superheroes of the networking world."")
print(response.text)
Actual vs expected behavior:
actual:
Traceback (most recent call last):
  File ""/home/wencan/Projects/py_main/zguide/test.py"", line 50, in <module>
    response = chat_session.send_message(""We took a normal TCP socket, injected it with a mix of radioactive isotopes stolen from a secret Soviet atomic research project, bombarded it with 1950-era cosmic rays, and put it into the hands of a drug-addled comic book author with a badly-disguised fetish for bulging muscles clad in spandex. Yes, ZeroMQ sockets are the world-saving superheroes of the networking world."")
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/wencan/.local/lib/python3.12/site-packages/google/generativeai/generative_models.py"", line 514, in send_message
    self._check_response(response=response, stream=stream)
  File ""/home/wencan/.local/lib/python3.12/site-packages/google/generativeai/generative_models.py"", line 542, in _check_response
    raise generation_types.StopCandidateException(response.candidates[0])
google.generativeai.types.generation_types.StopCandidateException: index: 0
finish_reason: SAFETY
safety_ratings {
  category: HARM_CATEGORY_SEXUALLY_EXPLICIT
  probability: MEDIUM
}
safety_ratings {
  category: HARM_CATEGORY_HATE_SPEECH
  probability: NEGLIGIBLE
}
safety_ratings {
  category: HARM_CATEGORY_HARASSMENT
  probability: LOW
}
safety_ratings {
  category: HARM_CATEGORY_DANGEROUS_CONTENT
  probability: NEGLIGIBLE
}

Any other information you'd like to share?
OS: Fedora Linux 40 (Workstation Edition) x86_64
 Host: 21D0 ThinkBook 14 G4+ ARA
 Kernel: 6.8.11-300.fc40.x86_64
 Uptime: 2 hours, 46 mins
 Packages: 2663 (rpm), 42 (flatpak)
 Shell: bash 5.2.26
 Resolution: 2880x1800
 DE: GNOME 46.2
 WM: Mutter
 WM Theme: Adwaita
 Theme: Adwaita [GTK2/3]
 Icons: Adwaita [GTK2/3]
 Terminal: gnome-terminal
 CPU: AMD Ryzen 5 6600H with Radeon Graphics (12) @ 4.564GHz
 GPU: AMD ATI Radeon 680M
 Memory: 4608MiB / 13649MiB
Python 3.12.3
 google-ai-generativelanguage 0.6.4
 google-api-core 2.19.0
 google-api-python-client 2.132.0
 google-auth 2.30.0
 google-auth-httplib2 0.2.0
 google-generativeai 0.6.0
 google-pasta 0.2.0
 googleapis-common-protos 1.63.1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/383","No reference docs for semantic retreiver or generate_answer.","2024-08-27T18:23:58Z","Closed issue","component:python sdk,type:bug","Description of the bug:
There are no reference docs for semantic_retreiver or generate_answer on ai.google.dev
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/381","[Bug]: SQLTalk demo is using VertexAI SDK, not Gemini SDK but is branded as GeminiAI","2024-06-06T08:36:30Z","Open issue","component:python sdk,status:triaged,type:bug","Description of the bug:
SQLTalk, a sample Streamlit app demonstrating Vertex AI's function calling with BigQuery was recently shared by Google: Video, Tutorial, Code, Live demo.
Even though the code lives in Gemini directory (see the full path), it is actually using Vertex AI SDK, not Gemini SDK. The Github page actually says that this repo is for Vertex AI:

But, the Tutorial and Video mention Gemini AI, not Vertex AI.
In summary, A sample app, which was branded as Gemini AI in the video and docs , is actually using vertexai SDK but is present under ""gemini"" folder in a repo that is for VertexAI only.
Actual vs expected behavior:
Provide a version of SQLTalk app that uses Gemini SDK, not VertexAI SDK;
This is not a simple matter of renaming vertexai.generative_models to google.generativeai because there are subtle differences in both the SDKs.
While this line from SQLTalk app sample code works fine:
from vertexai.generative_models import FunctionDeclaration, GenerativeModel, Part, Tool
This fails:
from google.generativeai import FunctionDeclaration, GenerativeModel, Part, Tool
because the GeminiSDK does not have FunctionDeclaration, Part and Tool but does have GenerativeModel.
To someone new to GeminiAI/VertexAI, all this is highly confusing.
Any other information you'd like to share?
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/380","TypeError Could not create Blob, expected Blob, dict or an Image type(PIL.Image.Image or IPython.display.Image)","2024-06-06T06:59:13Z","Open issue","component:python sdk,status:triaged,type:bug","Description of the bug:
Getting this error sometimes. What does it mean? If I rerun the script, it is not raised every time, but only sometimes.
REQUEST_TIMEOUT = 90
    
# model is `gemini-1.5-flash`    client = genai.GenerativeModel(model, system_instruction=system_instruction)
    
google_completion = await asyncio.wait_for(
    client.generate_content_async(
        messages,
        safety_settings={
            HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,
            HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,
            HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,
            HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,
        },
    ),
    REQUEST_TIMEOUT,
)
Actual vs expected behavior:
Actual:
TypeError Could not create `Blob`, expected `Blob`, `dict` or an `Image` type(`PIL.Image.Image` or `IPython.display.Image`).
Got a: <class 'NoneType'>
Value: None

Expected there to be no error
Any other information you'd like to share?
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/379","No PDF support for files API.","2024-07-26T00:11:12Z","Closed issue","component:python sdk,status:awaiting user response,type:help","Description of the bug:
I executed the following code with loaded GEMINI_API_KEY environment variable but recieved error ""google.api_core.exceptions.InvalidArgument: 400 Unsupported MIME type: application/pdf""
Dependency:
 google-generativeai == 0.6.0
Here is the code:
import osimport time

import google.generativeai as genai

genai.configure(api_key=os.environ[""GEMINI_API_KEY""])

def upload_to_gemini(path, mime_type=None):

  file = genai.upload_file(path, mime_type=mime_type)
  print(f""Uploaded file '{file.display_name}' as: {file.uri}"")
  return file

def wait_for_files_active(files):

  print(""Waiting for file processing..."")
  for name in (file.name for file in files):
    file = genai.get_file(name)
    while file.state.name == ""PROCESSING"":
      print(""."", end="""", flush=True)
      time.sleep(10)
      file = genai.get_file(name)
    if file.state.name != ""ACTIVE"":
      raise Exception(f""File {file.name} failed to process"")
  print(""...all files ready"")
  print()

generation_config = {
  ""temperature"": 1,
  ""top_p"": 0.95,
  ""top_k"": 64,
  ""max_output_tokens"": 8192,
  ""response_mime_type"": ""text/plain"",
}

model = genai.GenerativeModel(
  model_name=""gemini-1.5-pro"",
  generation_config=generation_config,
  system_instruction=""Just summarize the document"",
)


files = [
  upload_to_gemini(""./resources/few-shot/canon-filter/difm-adapter-ef-eosr-im-eng.pdf"", mime_type=""application/pdf""),
]

wait_for_files_active(files)

chat_session = model.start_chat(
  history=[
    {
      ""role"": ""user"",
      ""parts"": [
        ""Summarize file"",
        files[0],
      ],
    },
  ]
)

response = chat_session.send_message(""INSERT_INPUT_HERE"")

print(response.text)
The traceback of error:
Traceback (most recent call last):
  File ""/init_project/app/services/taskAgent.py"", line 223, in <module>
    response = chat_session.send_message(""INSERT_INPUT_HERE"")
  File ""/python3.10/site-packages/google/generativeai/generative_models.py"", line 504, in send_message
    response = self.model.generate_content(
  File ""/python3.10/site-packages/google/generativeai/generative_models.py"", line 258, in generate_content
    response = self._client.generate_content(
  File ""/python3.10/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py"", line 812, in generate_content
    response = rpc(
  File ""/python3.10/site-packages/google/api_core/gapic_v1/method.py"", line 131, in __call__
    return wrapped_func(*args, **kwargs)
  File ""/python3.10/site-packages/google/api_core/retry/retry_unary.py"", line 293, in retry_wrapped_func
    return retry_target(
  File ""/python3.10/site-packages/google/api_core/retry/retry_unary.py"", line 153, in retry_target
    _retry_error_helper(
  File ""/python3.10/site-packages/google/api_core/retry/retry_base.py"", line 212, in _retry_error_helper
    raise final_exc from source_exc
  File ""/python3.10/site-packages/google/api_core/retry/retry_unary.py"", line 144, in retry_target
    result = target()
  File ""/python3.10/site-packages/google/api_core/timeout.py"", line 120, in func_with_timeout
    return func(*args, **kwargs)
  File ""/python3.10/site-packages/google/api_core/grpc_helpers.py"", line 78, in error_remapped_callable
    raise exceptions.from_grpc_error(exc) from exc
google.api_core.exceptions.InvalidArgument: 400 Unsupported MIME type: application/pdf

Actual vs expected behavior:
The prompt should be executed without error because the code is generated by Google AI studio with a tested working prompt in UI.
Any other information you'd like to share?
I sent the request from Vietnam.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/378","GenerativeModel does not support multiple grounding tools","2024-07-26T00:13:42Z","Closed issue","component:python sdk,type:bug","Description of the bug:
We have a datastore that contains the knowledge articles which I would like to ground the model on. In addition, for the things it cannot find on there, I am trying to provide an additional tool using Google Search. However, I get a HTTP400 when I add both the tools. If I take out one of the tools, it returns fine.
google_search_tool = Tool.from_google_search_retrieval(grounding.GoogleSearchRetrieval())
data_store_path = f""projects/{__project_id__}/locations/{__data_store_region__}/collections/default_collection/dataStores/{__maestro_datastore_id__}""
        datastore_tool = Tool.from_retrieval(
            grounding.Retrieval(grounding.VertexAISearch(datastore=data_store_path))
        )
self.tools = [
            datastore_tool, google_search_tool
        ]
self.llm = GenerativeModel(model_name)
user_prompt_content= Content(
            role=""user"",
            parts=[
                Part.from_text(""Who is John Galt?""),
            ],
        )
response = self.llm.generate_content(
            user_prompt_content,
            tools=self.tools,
            generation_config=self.generation_config,
            safety_settings=self.safety_settings,
        )

Actual vs expected behavior:
  File ""/Users/dandab/codespaces/moltin/maestro-chat-service/app/routers/chat.py"", line 41, in chat_gen
    res = chat_service.chat_gen(chat_request)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/dandab/codespaces/moltin/maestro-chat-service/app/services/maestro_chat.py"", line 347, in chat_gen
    chat_response = self.chat_gen_service.chat(question=chat_request.question,chat_history=chat_history,output_type=output_type)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/dandab/codespaces/moltin/maestro-chat-service/app/services/gemini_generation_service.py"", line 156, in chat
    response = self.llm.generate_content(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/dandab/.pyenv/versions/3.12.2/envs/search-env/lib/python3.12/site-packages/vertexai/generative_models/_generative_models.py"", line 407, in generate_content
    return self._generate_content(
           ^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/dandab/.pyenv/versions/3.12.2/envs/search-env/lib/python3.12/site-packages/vertexai/generative_models/_generative_models.py"", line 496, in _generate_content
    gapic_response = self._prediction_client.generate_content(request=request)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/dandab/.pyenv/versions/3.12.2/envs/search-env/lib/python3.12/site-packages/google/cloud/aiplatform_v1beta1/services/prediction_service/client.py"", line 2125, in generate_content
    response = rpc(
               ^^^^
  File ""/Users/dandab/.pyenv/versions/3.12.2/envs/search-env/lib/python3.12/site-packages/google/api_core/gapic_v1/method.py"", line 131, in __call__
    return wrapped_func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/dandab/.pyenv/versions/3.12.2/envs/search-env/lib/python3.12/site-packages/google/api_core/grpc_helpers.py"", line 78, in error_remapped_callable
    raise exceptions.from_grpc_error(exc) from exc
google.api_core.exceptions.InvalidArgument: 400 Request contains an invalid argument.

I would have expected it to use both tools like it normally does but seems to have an issue with multiple tools.
Any other information you'd like to share?
google-generativeai -> 0.6.0
 python 3.12
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/376","Google Gemini does not provide response if tools are given","2024-07-30T17:13:27Z","Closed issue","component:python sdk,type:bug","Description of the bug:
All Google Gemini models that can generate text and support function calling works well if tools are not provided but when tools provided, they don't answer a simple query other than related to those functions.
Actual vs expected behavior:
Expected:
import google.generativeai as genai


def multiply(a:float, b:float):
    """"""returns a * b.""""""
    return a*b

model = genai.GenerativeModel(model_name='gemini-1.0-pro',
                              tools=[multiply])

chat = model.start_chat(enable_automatic_function_calling=True)
response = chat.send_message('who is the president of usa')
response.text
Output:'Joe Biden'
Actual:
import google.generativeai as genai


def multiply(a:float, b:float):
    """"""returns a * b.""""""
    return a*b

model = genai.GenerativeModel(model_name='gemini-1.0-pro',
                              tools=[multiply])

chat = model.start_chat(enable_automatic_function_calling=True)
response = chat.send_message('who is the president of usa')
response.text
Output:'I cannot fulfill this request. The available tools lack the desired functionality.' or 'I am sorry, I do not have access to real-time information, including information about current events or the President of the United States. \n'
 or 'This question cannot be answered from the given source.'
Any other information you'd like to share?
However, it works fine tools are not given
import google.generativeai as genai


def multiply(a:float, b:float):
    """"""returns a * b.""""""
    return a*b

model = genai.GenerativeModel(model_name='gemini-1.0-pro')

chat = model.start_chat()
response = chat.send_message('who is the president of usa')
response.text
Output:'Joe Biden'
 The text was updated successfully, but these errors were encountered: 
👍2
Naitik4516 and quentinwendegass reacted with thumbs up emoji
All reactions
👍2 reactions"
"https://github.com/google-gemini/generative-ai-python/issues/373","The response.text quick accessor only works when the response contains a valid Part, but none was returned. Check the candidate.safety_ratings to see if the response was blocked.","2024-08-24T01:51:50Z","Closed as not planned issue","component:python sdk,status:awaiting user response,status:stale,type:bug","Description of the bug:
The response.text quick accessor only works when the response contains a valid Part, but none was returned. Check the candidate.safety_ratings to see if the response was blocked.
Actual vs expected behavior:
Traceback (most recent call last):
 File ""D:\Study\codes\Gemini_score.py"", line 64, in 
 strr = str(id) + "":"" + response.text
 File ""D:\Study\codes\venv\lib\site-packages\google\generativeai\types\generation_types.py"", line 401, in text
 raise ValueError(
 ValueError: The response.text quick accessor only works when the response contains a valid Part, but none was returned. Check the candidate.safety_ratings to see if the response was blocked.
Any other information you'd like to share?
THIS IS MY CODES:
import pandas as pd
 import pdb
 import google.generativeai as genai
 import os
genai.configure(api_key=""xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx"")
model = genai.GenerativeModel('gemini-1.5-pro')
response = model.generate_content(system_prompt)
 strr = str(id) + "":"" + response.text
system_prompt is a string
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/372","Can I live stream my web cam or live video ( HLS, RTSP and RTMP) to gemini vision api?","2024-06-03T06:52:01Z","Open issue","component:python sdk,status:triaged,type:feature request","Description of the feature request:
No response
What problem are you trying to solve with this feature?
No response
Any other information you'd like to share?
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/371","Finish reason should be text not a number","2024-07-02T01:50:12Z","Closed as not planned issue","component:python sdk,status:awaiting user response,status:stale,type:feature request","Description of the feature request:
See details reported by the user here: https://discuss.ai.google.dev/t/how-to-translate-finish-reason-4-from-generatecontentresponse-into-something-meaningful/4261
What problem are you trying to solve with this feature?
Making the response object more clear, I checked REST and it responds with the text not a number
Any other information you'd like to share?
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/370","Tuned models return no content!","2024-07-02T01:50:13Z","Closed as not planned issue","component:python sdk,status:awaiting user response,status:stale,type:bug","Description of the bug:
So I tuned the model using Python and it worked and completed as shown in the platform without any errors but when I tried using it in the studio it returned ""No content"" and when testing it with Python
Here is the code:
import google.generativeai as genai

generation_config = {
    ""temperature"": 0,
    ""top_p"": 1,
    ""top_k"": 1,
    ""max_output_tokens"": 400,
}

safety_settings = [
    {
        ""category"": ""HARM_CATEGORY_HARASSMENT"",
        ""threshold"": ""BLOCK_NONE""
    },
    {
        ""category"": ""HARM_CATEGORY_HATE_SPEECH"",
        ""threshold"": ""BLOCK_NONE""
    },
    {
        ""category"": ""HARM_CATEGORY_SEXUALLY_EXPLICIT"",
        ""threshold"": ""BLOCK_NONE""
    },
    {
        ""category"": ""HARM_CATEGORY_DANGEROUS_CONTENT"",
        ""threshold"": ""BLOCK_NONE""
    },
]

name = ""my-tuned-model""
model = genai.GenerativeModel(
    model_name=f""tunedModels/{name}"",
    generation_config=generation_config,
    safety_settings=safety_settings
)

prompt_parts = [
    ""input: my input here"",
    ""output: "",
]

response = model.generate_content(prompt_parts)
print(response)


and here is the response:
response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=glm.GenerateContentResponse({
      ""candidates"": []
    }),
)

Actual vs expected behavior:
No response
Any other information you'd like to share?
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/368","langchain_google_genai._common.GoogleGenerativeAIError: Error embedding content: 504 Deadline Exceeded","2024-05-31T11:05:28Z","Open issue","component:python sdk,status:triaged,type:bug","Description of the bug:
I have the following error when :
gemini_embeddings = GoogleGenerativeAIEmbeddings(model=""models/embedding-001"",google_api_key=gapiket) vector = gemini_embeddings.embed_query(""hello, world!"")
I've read that the API doesn't work depending on the country. Could this be the problem?
**Exception has occurred: GoogleGenerativeAIError

Error embedding content: 504 Deadline Exceeded
grpc._channel._InactiveRpcError: <_InactiveRpcError of RPC that terminated with:
	status = StatusCode.DEADLINE_EXCEEDED
	details = ""Deadline Exceeded""
	debug_error_string = ""UNKNOWN:Error received from peer  {created_time:""2024-05-31T10:56:34.7789233+00:00"", grpc_status:4, grpc_message:""Deadline Exceeded""}""
>

During handling of the above exception, another exception occurred:

google.api_core.exceptions.DeadlineExceeded: 504 Deadline Exceeded

The above exception was the direct cause of the following exception:

  File ""C:\Users\drosset\Documents\intellicart\data\preprocess.py"", line 28, in <module>
    vector = gemini_embeddings.embed_query(""hello, world!"")
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
langchain_google_genai._common.GoogleGenerativeAIError: Error embedding content: 504 Deadline Exceeded

Actual vs expected behavior:
No response
Any other information you'd like to share?
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/367","Allow to modify multiple request options in Python SDK (similar to JavaScript SDK)","2024-05-30T23:16:58Z","Open issue","component:python sdk,status:triaged,type:feature request","Description of the feature request:
I would like the Python SDK to allow the same equivalent functionality as the JavaScript SDK regarding the possibility of modifying the request options
JavaScript SDK (can add many custom request options)
const model = googleGenAIClient.getGenerativeModel(
				{ model: ""gemini-pro"" },
				{
					customHeaders: {
						""x-param"": ""<PARAMETER>"",
					},
					baseUrl: env.PROXY_URL,
					
				}
			);
PythonSDK (cannot add many custom request options)
import google.generativeai as genai

genai.configure(api_key=""AIzaSyBW75RG6YjRor0pL1Eu0jWsRSd-rndTNIY"")

model = genai.GenerativeModel('gemini-1.0-pro-latest')
response = model.generate_content(""The opposite of hot is"", 
                                  request_options= {
                                    # Only accepts timeout parameter
                                      ""timeout"": 10,
                                      # Anything else gives the following error# TypeError: GenerativeServiceClient.generate_content() got an unexpected keyword argument 'customHeaders'
                                    #   ""customHeaders"": {
                                    #         ""x-param"": ""<PARAMETER>"",
                                    #     },
                                    # I would also like to modify the baseUrl
                                    # ""baseUrl"": ""http://localhost:8787/"",# TypeError: GenerativeServiceClient.generate_content() got an unexpected keyword argument 'baseUrl'    
}
print(response.text)
What problem are you trying to solve with this feature?
Some people have talked about how being able to add metadata to requests in these libraries is helpful for proxies, and even tried to implement the solution themselves as you can find below:
See this issue Add support to pass metadata from GenerativeModel.generate_content and Chat:send_message googleapis/python-aiplatform#3490
See this PR feat: add metadata field in _GenerativeModels class methods to pass the metadata down the method chain. googleapis/python-aiplatform#3491
I'm simply frustrated by the fact that it's possible to modify the request options in the JavaScript SDK, but not in the Python SDK.
If I can do it in the JavaScript SDK, why can't I do it in the Python SDK? 🤔
For the record, the Open AI Python SDK allows to do it by adding the parameter default_headers in the client configuration and extra_headers in the AI generation call:
from openai import OpenAI

client = OpenAI(
    api_key=""<KEY>"",
    base_url=""http://localhost:8787"",
    default_headers={
        ""x-param"": ""parameter""
    }
)

chat_completion = client.chat.completions.create(
    messages=[
        {
            ""role"": ""user"",
            ""content"": ""Say this is a test"",
        }
    ],
    model=""<model>"",
    extra_headers={
        ""x-param"": ""parameter""
    
    }
)

print(chat_completion.choices[0].message.content)
Any other information you'd like to share?
The documentation also seems to have implemented something similar to customHeaders in the request options (https://cloud.google.com/vertex-ai/generative-ai/docs/reference/nodejs/latest/vertexai/requestoptions), but when I installed the Python SDK, it didn't seem to accept customHeaders as explained above. At any rate, the current Python SDK is far behind the JavaScript SDK in this regard (JS SDK accepts other parameters like baseUrl—useful for proxies).
Additional context
 Add any other context or screenshots about the feature request here.
Other relevant links in this conversation may be:
There should be a RequestOptions TypedDict to explain what attributes are allowed. #223
Improve request_options #297
Add support for custom headers in vertexai.init function googleapis/python-aiplatform#3526
https://github.com/MarkDaoust/generative-ai-python/blob/418a38c4a2ab921c0ab084157bfa54f08c96a574/google/generativeai/types/helper_types.py#L30
https://github.com/google-gemini/generative-ai-python/pull/297/files/4d653f8a5266418912e9a4a378cb02f5b240b69b#
 The text was updated successfully, but these errors were encountered: 
👍1
sidpremkumar reacted with thumbs up emoji👀1
capaci reacted with eyes emoji
All reactions
👍1 reaction
👀1 reaction"
"https://github.com/google-gemini/generative-ai-python/issues/365","The count tokens function generates an google.api_core.exceptions.PermissionDenied error when using an uploaded file in the prompt using the files api","2024-05-30T18:10:49Z","Closed issue","component:python sdk,status:awaiting user response,type:bug","Description of the bug:
After uploading a file using the file API and calling the count_tokens functions, I get the following error.
google.api_core.exceptions.PermissionDenied: 403 You do not have permission to access the File 8lqhztqcd6g1 or it may not exist
Here is my code:
GOOGLE_API_KEY = os.getenv(""GOOGLE_API_KEY"")
genai.configure(api_key=GOOGLE_API_KEY)

model_name = ""models/gemini-1.5-pro-latest""model = genai.GenerativeModel(model_name)

audio_file_path = ""./../data/audio_example.mp3""audio_sample = genai.upload_file(audio_file_path)
model.count_tokens(audio_sample)
Additional info: For some reason, if I comment the model.count_tokens call line, and run the following code, it works:
response = model.generate_content([
    ""Describe the following audio:"",
    audio_sample
])
print(response)
Actual vs expected behavior:
Return the number of tokens as shown in the notebook: https://github.com/google-gemini/cookbook/blob/main/quickstarts/Counting_Tokens.ipynb
Any other information you'd like to share?
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/364","Can’t pip install python package in NVIDIA Omniverse","2024-05-28T02:26:51Z","Open issue","component:python sdk,status:triaged,type:bug","Description of the bug:
I tried installing google-generativeai package in NVIDIA Omniverse both through the script editor and for my extension using omni.kit.pipapi but wasn’t able to. But I get a warning that the package failed to install (see the screenshot attached).
import omni.kit.pipapi
omni.kit.pipapi.install(""google-generativeai"")

Actual vs expected behavior:
No response
Any other information you'd like to share?
Installing the package to my pc’s python installation using any other terminal works fine but can’t install it to omniverse.
Tried using Omniverse Code 2022.3.3 and 2023.1.1
I'm able to install the google-ai-generativelanguage package
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/363","Unclear finish_reason","2024-06-28T01:49:58Z","Closed as not planned issue","component:other,status:awaiting user response,status:stale,type:bug","Description of the bug:
I'm just asking to summarize a high school textbook which I'm feeding as images, was working perfectly fine on Friday, getting the following error today:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=glm.GenerateContentResponse({
      ""candidates"": [
        {
          ""finish_reason"": 4,
          ""index"": 0,
          ""safety_ratings"": [],
          ""token_count"": 0,
          ""grounding_attributions"": []
        }
      ]
    }),
)

I don't understand what finish_reason 4 means, it's not given here: https://ai.google.dev/api/rest/v1/GenerateContentResponse
Actual vs expected behavior:
Actual: ""finish_reason"": 4
Expected: `""finish_reason"": [Should be one of https://ai.google.dev/api/rest/v1/GenerateContentResponse#finishreason]
Any other information you'd like to share?
SDK version: 0.5.4
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/360","[Bugs] protobuf bugs when importing generativeai module.","2024-06-25T01:49:45Z","Closed as not planned issue","component:python sdk,status:awaiting user response,status:stale,type:bug","Description of the bug:
import google.generativeai as genai
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/tiger/miniconda3/envs/gemini/lib/python3.9/site-packages/google/generativeai/__init__.py"", line 45, in <module>
    from google.generativeai import types
  File ""/home/tiger/miniconda3/envs/gemini/lib/python3.9/site-packages/google/generativeai/types/__init__.py"", line 17, in <module>
    from google.generativeai.types.citation_types import *
  File ""/home/tiger/miniconda3/envs/gemini/lib/python3.9/site-packages/google/generativeai/types/citation_types.py"", line 20, in <module>
    from google.ai import generativelanguage as glm
  File ""/home/tiger/miniconda3/envs/gemini/lib/python3.9/site-packages/google/ai/generativelanguage/__init__.py"", line 21, in <module>
    from google.ai.generativelanguage_v1beta.services.discuss_service.async_client import (
  File ""/home/tiger/miniconda3/envs/gemini/lib/python3.9/site-packages/google/ai/generativelanguage_v1beta/__init__.py"", line 21, in <module>
    from .services.discuss_service import DiscussServiceAsyncClient, DiscussServiceClient
  File ""/home/tiger/miniconda3/envs/gemini/lib/python3.9/site-packages/google/ai/generativelanguage_v1beta/services/discuss_service/__init__.py"", line 16, in <module>
    from .async_client import DiscussServiceAsyncClient
  File ""/home/tiger/miniconda3/envs/gemini/lib/python3.9/site-packages/google/ai/generativelanguage_v1beta/services/discuss_service/async_client.py"", line 48, in <module>
    from google.ai.generativelanguage_v1beta.types import discuss_service, safety
  File ""/home/tiger/miniconda3/envs/gemini/lib/python3.9/site-packages/google/ai/generativelanguage_v1beta/types/__init__.py"", line 16, in <module>
    from .citation import CitationMetadata, CitationSource
  File ""/home/tiger/miniconda3/envs/gemini/lib/python3.9/site-packages/google/ai/generativelanguage_v1beta/types/citation.py"", line 22, in <module>
    __protobuf__ = proto.module(
AttributeError: module 'proto' has no attribute 'module'
Actual vs expected behavior:
No response
Any other information you'd like to share?
I could make this work on Mac and another instance. But can not make it work on our development environment. I am not sure if this is about the package version issues?
There are more environment info:
aiofiles==22.1.0
aiosqlite==0.20.0
annotated-types==0.7.0
anyio==4.3.0
argon2-cffi==23.1.0
argon2-cffi-bindings==21.2.0
arrow==1.3.0
asttokens==2.4.1
attrs==23.1.0
Babel==2.15.0
beautifulsoup4==4.12.3
bidict==0.23.1
bleach==6.1.0
byted-remote-ikernel==0.4.8
byted-torch-monitor @ https://d.scm.byted.org/api/v2/download/lab.pytorch.torch_monitor_1.0.0.6.tar.gz#sha256=333bc7b5f093bb449cad019a5351702f7497fc7b5e4a3f1c82f6567e607c3e1e
byted-wandb==0.13.72
bytedance-context==0.7.1
bytedance-metrics==0.5.1
bytedance.modelhub==0.0.64
bytedance.servicediscovery==0.1.2
bytedbackgrounds==0.0.6
byteddatabus==1.0.6
byteddps==0.1.2
bytedenv==0.6.2
bytedlogger==0.15.1
bytedmemfd==0.2
bytedmetrics==0.10.2
bytedpymongo==2.0.5
bytedrh2==1.18.7a2
bytedservicediscovery==0.17.4
bytedtcc==1.4.2
bytedtos==1.1.16
bytedtrace==0.3.0
bytedztijwthelper==0.0.22
bytedztispiffe==0.0.14
cachetools==5.3.3
certifi==2024.2.2
cffi==1.16.0
chardet==5.2.0
charset-normalizer==3.3.2
click==8.1.7
comm==0.2.2
crcmod==1.7
cryptography==42.0.7
debugpy==1.8.1
decorator==5.1.1
defusedxml==0.7.1
Deprecated==1.2.14
distlib==0.3.8
dnspython==2.6.1
docker-pycreds==0.4.0
entrypoints==0.4
exceptiongroup==1.2.1
executing==2.0.1
fastjsonschema==2.19.1
filelock==3.14.0
fqdn==1.5.1
gitdb==4.0.11
GitPython==3.1.43
google-ai-generativelanguage==0.6.3
google-api-core==2.19.0
google-api-python-client==2.129.0
google-auth==2.29.0
google-auth-httplib2==0.2.0
google-generativeai==0.5.3
googleapis-common-protos==1.63.0
grpcio==1.64.0
grpcio-status==1.62.2
h11==0.14.0
httplib2==0.22.0
idna==3.7
importlib_metadata==7.1.0
iniconfig==2.0.0
ipaddress==1.0.23
ipykernel==6.29.4
ipython==8.18.1
ipython-genutils==0.2.0
ipywidgets==8.1.2
isoduration==20.11.0
jedi==0.19.1
json5==0.9.25
jsonpointer==2.4
jsonschema==4.22.0
jsonschema-specifications==2023.12.1
jupyter==1.0.0
jupyter-client==7.0.0
jupyter-console==6.6.3
jupyter-events==0.10.0
jupyter-ydoc==0.2.5
jupyter_core==5.7.2
jupyter_server==2.14.0
jupyter_server_fileid==0.9.2
jupyter_server_terminals==0.5.3
jupyter_server_ydoc==0.8.0
jupyterlab==3.6.4
jupyterlab_pygments==0.3.0
jupyterlab_server==2.27.1
jupyterlab_widgets==3.0.10
lmms_eval==0.1.1
matplotlib-inline==0.1.7
mistune==3.0.2
msgpack==1.0.8
nbclassic==1.0.0
nbclient==0.10.0
nbconvert==7.16.4
nbformat==5.10.4
nest-asyncio==1.6.0
notebook==6.5.7
notebook_shim==0.2.4
overrides==7.7.0
packaging==24.0
pandocfilters==1.5.1
parso==0.8.4
pathlib2==2.3.7.post1
pathtools==0.1.2
pexpect==4.8.0
platformdirs==4.2.2
pluggy==1.5.0
ply==3.11
prometheus_client==0.20.0
promise==2.3
prompt-toolkit==3.0.43
proto-plus==1.23.0
protobuf==4.24.3
psutil==5.9.8
ptyprocess==0.7.0
pure-eval==0.2.2
py==1.11.0
py-spy==0.3.14
pyarrow==16.1.0
pyasn1==0.6.0
pyasn1_modules==0.4.0
pycparser==2.22
pycryptodomex==3.20.0
pydantic==2.7.1
pydantic_core==2.18.2
Pygments==2.18.0
PyJWT==2.8.0
pyOpenSSL==24.1.0
pyparsing==3.1.2
pytest==6.2.5
python-consul==1.1.0
python-dateutil==2.9.0.post0
python-engineio==4.9.1
python-etcd==0.4.5
python-json-logger==2.0.7
python-socketio==5.11.2
PyYAML==6.0.1
pyzmq==26.0.3
qtconsole==5.5.2
QtPy==2.4.1
referencing==0.35.1
requests==2.32.2
rfc3339-validator==0.1.4
rfc3986-validator==0.1.1
rpds-py==0.18.1
rsa==4.9
schedule==1.2.1
Send2Trash==1.8.3
sentry-sdk==2.3.0
setproctitle==1.3.3
shortuuid==1.0.13
simple-websocket==1.0.0
smmap==5.0.1
sniffio==1.3.1
soupsieve==2.5
stack-data==0.6.3
terminado==0.18.1
thriftpy2==0.5.0
tinycss2==1.3.0
toml==0.10.2
tomli==2.0.1
tornado==6.4
tox==3.28.0
tqdm==4.66.4
traitlets==5.14.3
types-python-dateutil==2.9.0.20240316
typing_extensions==4.11.0
uri-template==1.3.0
uritemplate==4.1.1
urllib3==2.2.1
virtualenv==20.26.2
wandb==0.17.0
watchdog==4.0.0
wcwidth==0.2.13
webcolors==1.13
webencodings==0.5.1
websocket-client==1.8.0
widgetsnbextension==4.0.10
wrapt==1.16.0
wsproto==1.2.0
y-py==0.6.2
ypy-websocket==0.8.4
zipp==3.18.2
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/359","Problem no attribute 'GenerativeModel'","2024-07-11T01:51:16Z","Closed as not planned issue","component:python sdk,status:awaiting user response,status:stale,type:bug","Description of the bug:
I recently got the error message
AttributeError: module 'google.generativeai' has no attribute 'GenerativeModel'
Actual vs expected behavior:
No response
Any other information you'd like to share?
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/358","It appears as though helper_types is not included in the types file in the v0.5.4 tag, but exists in main","2024-07-30T17:16:42Z","Closed issue","component:python sdk,type:help","Oh, it looks like `helper_types` is not included in the types file:

cannot import name 'helper_types' from 'google.generativeai.types' (/opt/venv/lib/python3.10/site-packages/google/generativeai/types/__init__.py)

@markmcd
Originally posted by @homer6 in #335 (comment)
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/357","glm.FileData issue? with uris","2024-07-30T17:16:08Z","Closed issue","component:python sdk,type:bug","Description of the bug:
google.api_core.exceptions.InvalidArgument: 400 Invalid or unsupported file uri:
glm.FileData with a uri doesn't seem to work? I need to use Part.from_uri from the old vertex.preview library, but that library does not let me use model = genai.GenerativeModel('gemini-1.5-pro-latest')
Actual vs expected behavior:
api_key = os.getenv(""GEMINI_API_KEY"")
 genai.configure(api_key=api_key)
 model = genai.GenerativeModel('gemini-1.5-pro-latest')
 response = model.generate_content(
 glm.Content(
 parts = [
 glm.Part(text=""What is this image of?""),
 glm.Part(
 file_data=glm.FileData(
 mime_type='image/png',
 file_uri=""gs://urihere.png""
 )
 ),
 ],
 ),
 stream=False)
print(response)
I'd expect this to work...but it says unsupported file uri (note: the uri is correct i just put a placeholder here for the example)
Any other information you'd like to share?
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/356","chat_session.send_message throw exception while using proxy config： raise ConnectionError(e, request=request) requests.exceptions.ConnectionError: HTTPSConnectionPool(host='generativelanguage.googleapis.com', port=443)","2024-05-22T14:26:58Z","Open issue","component:python sdk,status:triaged,type:bug","Description of the bug:
how can i call the gemini service with proxy service.When I call Gemini's upload file and create cha t session service through the local proxy service, an exception is thrown,i had checked the curl and proxy service ,they both works.here is the exception:
 raise ConnectionError(e, request=request)
 requests.exceptions.ConnectionError: HTTPSConnectionPool(host='generativelanguage.googleapis.com', port=443): Max retries exceeded with url: /v1beta/models/gemini-1.5-pro-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x110fde060>: Failed to establish a new connection: Socket error: [Errno 54] Connection reset by peer'))
###here is the code:,python version 3.12.2，google-generativeai version 0.5.4：
import os
 import google.generativeai as genai
 from dotenv import load_dotenv
 import socket
 import socks
socks.set_default_proxy(socks.PROXY_TYPE_HTTP, ""127.0.0.1"", 10000)
 socket.socket = socks.socksocket
 load_dotenv()
 os.environ[""HTTP_PROXY""] = ""http://127.0.0.1:9999""
 os.environ[""HTTP_PROXYS""] = ""http://127.0.0.1:9999""
genai.configure(api_key=gemini_key,transport='rest')
generation_config = {
 ""temperature"": 1,
 ""top_p"": 0.95,
 ""top_k"": 64,
 ""max_output_tokens"": 8192,
 ""response_mime_type"": ""text/plain"",
 }
 safety_settings = [
 {
 ""category"": ""HARM_CATEGORY_HARASSMENT"",
 ""threshold"": ""BLOCK_MEDIUM_AND_ABOVE"",
 },
 {
 ""category"": ""HARM_CATEGORY_HATE_SPEECH"",
 ""threshold"": ""BLOCK_MEDIUM_AND_ABOVE"",
 },
 {
 ""category"": ""HARM_CATEGORY_SEXUALLY_EXPLICIT"",
 ""threshold"": ""BLOCK_MEDIUM_AND_ABOVE"",
 },
 {
 ""category"": ""HARM_CATEGORY_DANGEROUS_CONTENT"",
 ""threshold"": ""BLOCK_MEDIUM_AND_ABOVE"",
 },
 ]
model = genai.GenerativeModel(
 model_name=""gemini-1.5-pro-latest"",
 safety_settings=safety_settings,
 generation_config=generation_config,
 system_instruction=""system instrution"")
chat_session = model.start_chat(
 history=[
 ]
 )
response = chat_session.send_message(""INSERT_INPUT_HERE"")
print(response.text)
 print(chat_session.history)`
Actual vs expected behavior:
No response
Any other information you'd like to share?
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/353","BlockedPromptException: block_reason: OTHER","2024-06-21T01:48:31Z","Closed as not planned issue","component:other,status:awaiting user response,status:stale,type:bug","prompt:
 translate: Man Arrested for Producing, Distributing, and Possessing AI-Generated Images of Minors Engaged in Sexually Explicit Conduct
model: model_name = ""gemini-1.5-pro-latest""
script:
response = chat.send_message(
                user_input,
                stream=True,
                safety_settings={
                    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,
                    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,
                    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,
                    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,
                },
            )

 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/351","stop_resaon is alwasy STOP, and usage_meta is consistently empty","2024-05-19T14:10:22Z","Open issue","component:python sdk,status:triaged,type:bug","Description of the bug:
version: 0.54 python 3.9
The stop reason is always ""STOP"", never change, and usage_metadata is always empty
Actual vs expected behavior:
 async for item in resp:
            c = item.candidates
            print(""stop_reason"", c[0].finish_reason, f""usage:{item.usage_metadata}:end"")

output:
stop_reason FinishReason.STOP usage::end # usage is empty
stop_reason FinishReason.STOP usage::end
stop_reason FinishReason.STOP usage::end
answer is: I am a large language model, trained by Google. I do not have a name. 

input_tokens 5  # this is counted by await client.count_tokens_async not from usage_metadata
output_tokens 20

the first stop reason should be empty or None or start, but not stop, and usage_metadata contains the prompt_tokens, output_tokens, but it's empty,
if it's non stream mode, the ""token_count"" field is always 0,
GenerateContentResponse(
    done=True,
    iterator=None,
    result=glm.GenerateContentResponse({
      ""candidates"": [
        {
          ""content"": {
            ""parts"": [
              {
                ""text"": ""Hello! \ud83d\udc4b  How can I help you today? \ud83d\ude0a \n""
              }
            ],
            ""role"": ""model""
          },
          ""finish_reason"": 1,
          ""index"": 0,
          ""safety_ratings"": [
            {
              ""category"": 9,
              ""probability"": 1,
              ""blocked"": false
            },
            {
              ""category"": 8,
              ""probability"": 1,
              ""blocked"": false
            },
            {
              ""category"": 7,
              ""probability"": 1,
              ""blocked"": false
            },
            {
              ""category"": 10,
              ""probability"": 1,
              ""blocked"": false
            }
          ],
          ""token_count"": 0,  # this field is alway 0
          ""grounding_attributions"": []
        }
      ]
    }),
)
answer is: Hello! 👋  How can I help you today? 😊 

input_tokens 1 # this is count by client.count_tokens not from usage_metadata
output_tokens 14

Was this behavior caused by a bug, or is the server not yet equipped to handle this?
Any other information you'd like to share?
package version: 0.5.4
 python 3.9
 The text was updated successfully, but these errors were encountered: 
👍2
JonathanRayner and gylleus reacted with thumbs up emoji👀1
minosvasilias reacted with eyes emoji
All reactions
👍2 reactions
👀1 reaction"
"https://github.com/google-gemini/generative-ai-python/issues/349","Content has no parts","2024-05-18T06:53:30Z","Open issue","component:python sdk,status:triaged,type:bug","Description of the bug:
Similar to: GoogleCloudPlatform/generative-ai#344, but still present in latest version.
model: genai.GenerativeModel('gemini-pro')
You can reproduce with this prompt:
If you remove the string --- Page 2 --- it works fine.
### Context

--- Page 1 ---

A la presentación del demandante de fecha 23 de marzo de 2021 : estese a lo
que se resolverá
Proveyendo derechamente presentación del demandante de fecha 18 de
enero de 2021: Téngase presente las bases propuestas y por aprobadas si no
fuesen objetadas dentro de tercero día, excepto las que siguen, que se modifican:
.- En la cláusula novena se elimina la expresión contados desde la fecha del
remate y se reemplaza por la que sigue Contados desde que se certifique la
ejecutoriedad de la resolución que ordena extender la escritura Pública de
adjudicación
.- En la cláusula décimo primera se elimina la expresión  el ocupante, o el mero
tenedor
.-se agrega la cláusula décimo quinta, que dispone Las publicaciones deberán
agregarse a los autos con al menos 24 horas de antelación a la subasta, bajo
apercibimiento de no realizarse ésta.
.- Se agrega la cláusula décimo sexta, que reza: las posturas por sobre el mínimo
y las que le sigan deberán elevarse al menos por la suma de $300.000.- o alguno
de sus múltiplos.
.- Se agrega la cláusula décimo séptima, que dispone: el ejecutante deberá
acompañar con al menos quince días de anticipación a la fecha de la subasta los
respectivos certificados actualizados de hipotecas y gravámenes, bajo
apercibimiento de no realizarse la subasta.
.- Se agrega la cláusula décimo octava: Los postores interesados en la subasta
deberán constituir garantía suficiente, a través de vale vista tomado a propia
orden, susceptible de ser endosado al momento de la subasta, al quinto día hábil a
la realización de la subasta, siendo responsabilidad de cada postor la verificación
oportuna y correcta de la consignación en la causa.
.- Se agrega la cláusula décimo novena: Los postores interesados deberán enviar
un correo electrónico a más tardar al día siguiente de la constitución de la
garantía, al correo electrónico jcsantiago14_remates@pjud.cl acompañando el
correspondiente comprobante legible que dé cuenta de haber constituido la
garantía, debiendo además individualizar correctamente la causa con el rol de la
misma, la individualización de la persona que participará en la subasta, a saber

--- Page 2 ---

nombre completo y cédula nacional de identidad y su correo electrónico y número
telefónico de contacto y concurrir a dependencias del tribunal para la entrega
material del vale vista el día lunes de la semana en que se realizará la subasta en
el turno de recepción de documentos creado al efecto.
.- Se agrega la cláusula vigésimo: El acta de remate deberá ser firmada el mismo
día de efectuado éste mediante firma electrónica avanzada. Para aquello, se
verificarán los datos del adjudicatario y se le enviará un borrador del acta al correo
electrónico que hubiese informado, el que deberá ingresarla de manera inmediata
en la causa a través de la Oficina Judicial Virtual para efectos de su firma. Hecho
lo anterior, se firmará por el juez y el ministro de fe del Tribunal, debiendo dejarse
constancia de la firma del adjudicatario.
.- Se agrega la cláusula vigésimo primera; Al postor no adjudicatario se le restituirá
su garantía, el día Lunes de la semana siguiente a la celebración de la subasta, en
el turno presencial dispuesto por el Tribunal.
.- Se agrega la cláusula vigésimo segunda: Si existieren problemas informáticos
para la firma digital del adjudicatario, juez y/o ministro de fe del tribunal, deberá
comparecer el adjudicatario a firmar el acta de remate el día lunes de la semana
siguiente a la realización de éste, en el turno presencial dispuesto por el tribunal.
Al segundo otrosí, por acompañados los documentos originales y electrónicos
con citación; Al tercer otrosí, se fija como día y hora para la subasta el jueves 27
de Mayo del 2021 a las 09:45 horas mediante la plataforma digital zoom y bajo las
condiciones establecidas en las bases de remates aprobadas y sus respectivas
modificaciones, siendo los datos informáticos de la subasta los siguientes:

### Instruction

Give a list of modifications de las bases.

Actual vs expected behavior:
No response
Any other information you'd like to share?
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/346","google.api_core.exceptions.Unknown: None Stream removed","2024-05-17T17:46:17Z","Open issue","component:python sdk,status:triaged,type:bug","Description of the bug:
Here is my code:
from config import settings, google_ai_settings

import discordfrom discord import app_commandsfrom discord.ext import commands

from googletrans import Translatorimport google.generativeai as genaifrom google.generativeai.types import StopCandidateException


class SText(commands.Cog):
    """"""Text""""""

    def __init__(self, bot):
        self.bot = bot
        genai.configure(api_key=google_ai_settings.get(""google_api_key""))

        

        self.model = genai.GenerativeModel(
          model_name=""gemini-1.0-pro"",
          safety_settings=google_ai_settings.get(""safety_settings""),
          generation_config=google_ai_settings.get(""generation_config""),
        )
        self.chat_sessions = {}
        
    ...

    @app_commands.command(name=""ai"", description=""Общение с нейросетью Google Gemini"")
    @app_commands.describe(message=""Задай свой вопрос, скотина блядь"")
    async def ai(self, interaction: discord.Interaction, message: str):
        embed = discord.Embed(color=0xffcd4c, title=f""{interaction.user.name} :: {message}"")
        await interaction.response.send_message(embed=embed)
        embed.set_footer(text=""DebilAI - Powered by Google Gemini 1.0 Pro"",
                         icon_url=""https://tidurak.github.io/google-gemini-icon.png"")
        chat_session = self.chat_sessions.get(interaction.guild.id)
        if chat_session == None:
            self.chat_sessions[interaction.guild.id] = self.model.start_chat(history=[])
            chat_session = self.chat_sessions.get(interaction.guild.id)
            response = None

        try:
            response = chat_session.send_message(message)
        except StopCandidateException:
            fuck_you_message = ""ТЫ еблаН? Я нейронка от гугла, и у меня ёбнутые фильтры на т.н. \""опасный\"" контент. ЫЫЫЫЫ""
            embed.add_field(name=""🚫 Иди нахуй"", value=fuck_you_message, inline=False)
            await interaction.edit_original_response(embed=embed)
            return

        if len(response.text) > 1000:
            res = response.text
            j = 1
            embed.add_field(name=""\u200b"", value=res[:999], inline=False)
            while True:
                j += 1
                res = res[999:]
                if len(res) > 1000:
                    embed.add_field(name=""\u200b"", value=res[0:999], inline=False)
                else:
                    embed.add_field(name=""\u200b"", value=res, inline=False)
                    break
        else:
            embed.add_field(name=""\u200b"", value=response.text, inline=False)
        
        await interaction.edit_original_response(embed=embed)


async def setup(bot):
    await bot.add_cog(SText(bot))
Everything works fine, but after like 10 minutes api responds only on second time, and i get Exception:
Traceback (most recent call last):
  File ""C:\Users\User\AppData\Local\Programs\Python\Python310\lib\site-packages\discord\app_commands\commands.py"", line 841, in _do_call
    return await self._callback(self.binding, interaction, **params)  # type: ignore
  File ""C:\Users\User\Files\bot\cogs\slash\s_text.py"", line 134, in ai
    response = chat_session.send_message(message)
  File ""C:\Users\User\AppData\Local\Programs\Python\Python310\lib\site-packages\google\generativeai\generative_models.py"", line 496, in send_message
    response = self.model.generate_content(
  File ""C:\Users\User\AppData\Local\Programs\Python\Python310\lib\site-packages\google\generativeai\generative_models.py"", line 262, in generate_content
    response = self._client.generate_content(
  File ""C:\Users\User\AppData\Local\Programs\Python\Python310\lib\site-packages\google\ai\generativelanguage_v1beta\services\generative_service\client.py"", line 812, in generate_content
    response = rpc(
  File ""C:\Users\User\AppData\Local\Programs\Python\Python310\lib\site-packages\google\api_core\gapic_v1\method.py"", line 131, in __call__
    return wrapped_func(*args, **kwargs)
  File ""C:\Users\User\AppData\Local\Programs\Python\Python310\lib\site-packages\google\api_core\retry\retry_unary.py"", line 293, in retry_wrapped_func
    return retry_target(
  File ""C:\Users\User\AppData\Local\Programs\Python\Python310\lib\site-packages\google\api_core\retry\retry_unary.py"", line 153, in retry_target
    _retry_error_helper(
  File ""C:\Users\User\AppData\Local\Programs\Python\Python310\lib\site-packages\google\api_core\retry\retry_base.py"", line 212, in _retry_error_helper
    raise final_exc from source_exc
  File ""C:\Users\User\AppData\Local\Programs\Python\Python310\lib\site-packages\google\api_core\retry\retry_unary.py"", line 144, in retry_target
    result = target()
  File ""C:\Users\User\AppData\Local\Programs\Python\Python310\lib\site-packages\google\api_core\timeout.py"", line 120, in func_with_timeout
    return func(*args, **kwargs)
  File ""C:\Users\User\AppData\Local\Programs\Python\Python310\lib\site-packages\google\api_core\grpc_helpers.py"", line 78, in error_remapped_callable
    raise exceptions.from_grpc_error(exc) from exc
google.api_core.exceptions.Unknown: None Stream removed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""C:\Users\User\AppData\Local\Programs\Python\Python310\lib\site-packages\discord\app_commands\tree.py"", line 1248, in _call
    await command._invoke_with_namespace(interaction, namespace)
  File ""C:\Users\User\AppData\Local\Programs\Python\Python310\lib\site-packages\discord\app_commands\commands.py"", line 867, in _invoke_with_namespace
    return await self._do_call(interaction, transformed_values)
  File ""C:\Users\User\AppData\Local\Programs\Python\Python310\lib\site-packages\discord\app_commands\commands.py"", line 860, in _do_call
    raise CommandInvokeError(self, e) from e
discord.app_commands.errors.CommandInvokeError: Command 'ai' raised an exception: Unknown: None Stream removed

What should I do?
Actual vs expected behavior:
No response
Any other information you'd like to share?
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/343","response_schema parameter is not followed.","2024-05-16T18:22:47Z","Closed issue","component:python sdk,type:bug","Description of the bug:
response_schema parameter is not followed unless system_instruction also details the response_schema for gemini-1.5-pro family of models.
This could be intended behavior, but it seems like it could be a massive waste of tokens for more complicated schemas.
Actual vs expected behavior:
I'd expect the response_schema to be respected and followed regardless of system_instructions detailing them. If this is an incorrect assumption please let me know.
Any other information you'd like to share?
code to reproduce
## built-in librariesimport typing

## third party librariesfrom google.generativeai import GenerationConfigfrom google.generativeai.types import GenerateContentResponse, AsyncGenerateContentResponseimport google.generativeai as genai

## Dummy values from production code_default_translation_instructions: str = ""Translate this to German. Format the response as JSON parseable string.""_default_model: str = ""gemini-1.5-pro-latest""

_system_message = _default_translation_instructions

_model: str = _default_model_temperature: float = 0.5_top_p: float = 0.9_top_k: int = 40_candidate_count: int = 1_stream: bool = False_stop_sequences: typing.List[str] | None = None_max_output_tokens: int | None = None

_client: genai.GenerativeModel_generation_config: GenerationConfig

_decorator_to_use: typing.Union[typing.Callable, None] = None

_safety_settings = [
    {
        ""category"": ""HARM_CATEGORY_DANGEROUS"",
        ""threshold"": ""BLOCK_NONE"",
    },
    {
        ""category"": ""HARM_CATEGORY_HARASSMENT"",
        ""threshold"": ""BLOCK_NONE"",
    },
    {
        ""category"": ""HARM_CATEGORY_HATE_SPEECH"",
        ""threshold"": ""BLOCK_NONE"",
    },
    {
        ""category"": ""HARM_CATEGORY_SEXUALLY_EXPLICIT"",
        ""threshold"": ""BLOCK_NONE"",
    },
    {
        ""category"": ""HARM_CATEGORY_DANGEROUS_CONTENT"",
        ""threshold"": ""BLOCK_NONE"",
    },
]

## with open(""gemini.txt"", ""r"", encoding=""utf-8"") as f:##      api_key = f.read().strip()api_key = ""YOUR_API_KEY""genai.configure(api_key=api_key)

## Instructing the model to translate the input to German as JSON, without detailed schemanon_specific_client = genai.GenerativeModel(
    model_name=_model,
    safety_settings=_safety_settings,
    system_instruction=""Translate this to German. Format the response as JSON parseable string.""
)

## Instructing the model to translate the input to German as JSON, with detailed schema_client = genai.GenerativeModel(
    model_name=_model,
    safety_settings=_safety_settings,
    system_instruction=""Translate this to German. Format the response as JSON parseable string. It must have 2 keys, one for input titled input, and one called output, which is the translation.""
)

_generation_config = GenerationConfig(
    candidate_count=_candidate_count,
    stop_sequences=_stop_sequences,
    max_output_tokens=_max_output_tokens,
    temperature=_temperature,
    top_p=_top_p,
    top_k=_top_k,
    response_mime_type=""application/json"",
    response_schema={
        ""type"": ""object"",
        ""properties"": {
            ""input"": {
                ""type"": ""string"",
                ""description"": ""The original text that was translated.""
            },
            ""output"": {
                ""type"": ""string"",
                ""description"": ""The translated text.""
            }
        },
        ""required"": [""input"", ""output""],
    }
)

## Inconsistent results, schema is not being followedtry:
    response = non_specific_client.generate_content(
        ""Hello, world!"", generation_config=_generation_config
    )
    print(response.text)
except Exception as e:
    print(f""Error with non-specific client: {e}"")

## Consistent results, schema is being followedtry:
    response = _client.generate_content(
        ""Hello, world!"", generation_config=_generation_config
    )
    print(response.text)
except Exception as e:
    print(f""Error with specific client: {e}"")

## Clarification question## Is it intended behavior that the system instruction has to detail the schema? If so, what's the point of the response_schema parameter in the GenerationConfig class? It seems like a waste of tokens.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/336","RECITATION error when transcribe audio to text","2024-05-12T16:07:13Z","Open issue","component:python sdk,status:triaged,type:bug","Description of the bug:
result=glm.GenerateContentResponse({'candidates': [{'finish_reason': 4, 'index': 0, 'safety_ratings': [], 'token_count': 0, 'grounding_attributions': []}]}),
Actual vs expected behavior:
transcribe the audio correctly
Any other information you'd like to share?
model: gemini-1.5-pro-latest
 audio length: 53 min
 audio format: mp3
 audio file size: 13m
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/335","Add client timeout setting to overcome timeout errors.","2024-05-22T08:29:34Z","Closed issue","component:python sdk,type:feature request","Description of the feature request:
Add client timeout setting to overcome timeout errors.
What problem are you trying to solve with this feature?
I'm encountering this error where I can't configure the client's timeout.
Any other information you'd like to share?
  File ""/app/ai.py"", line 97, in predict
    response = chat.send_message(
  File ""/opt/venv/lib/python3.10/site-packages/google/generativeai/generative_models.py"", line 474, in send_message
    response = self.model.generate_content(
  File ""/opt/venv/lib/python3.10/site-packages/google/generativeai/generative_models.py"", line 262, in generate_content
    response = self._client.generate_content(
  File ""/opt/venv/lib/python3.10/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py"", line 791, in generate_content
    response = rpc(
  File ""/opt/venv/lib/python3.10/site-packages/google/api_core/gapic_v1/method.py"", line 131, in __call__
    return wrapped_func(*args, **kwargs)
  File ""/opt/venv/lib/python3.10/site-packages/google/api_core/retry/retry_unary.py"", line 293, in retry_wrapped_func
    return retry_target(
  File ""/opt/venv/lib/python3.10/site-packages/google/api_core/retry/retry_unary.py"", line 153, in retry_target
    _retry_error_helper(
  File ""/opt/venv/lib/python3.10/site-packages/google/api_core/retry/retry_base.py"", line 212, in _retry_error_helper
    raise final_exc from source_exc
  File ""/opt/venv/lib/python3.10/site-packages/google/api_core/retry/retry_unary.py"", line 144, in retry_target
    result = target()
  File ""/opt/venv/lib/python3.10/site-packages/google/api_core/timeout.py"", line 120, in func_with_timeout
    return func(*args, **kwargs)
  File ""/opt/venv/lib/python3.10/site-packages/google/api_core/grpc_helpers.py"", line 78, in error_remapped_callable
    raise exceptions.from_grpc_error(exc) from exc

Exception Type: DeadlineExceeded 
Exception Value: 504 Deadline Exceeded

 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/332","Support for Video (MP4) with Gemini Pro 1.5","2024-06-16T01:52:48Z","Closed as not planned issue","component:other,status:awaiting user response,status:stale,type:bug","Description of the bug:
When processing video/mp4 mime-type assets, exception is thrown. Exception is not thrown when processing image/png (e.g.) assets.
import google.generativeai as genai
from google.ai.generativelanguage import Part, Blob
from google.ai.generativelanguage import GenerationConfig
import os
from dotenv import load_dotenv

api_key = os.getenv(""GEMINI_API_KEY"")

genai.configure(api_key=api_key)

model = genai.GenerativeModel('models/gemini-1.5-pro-latest')

prompt_part = Part()
prompt_part.text = """"""describe this video in a few words.""""""
    
image_fn = ""/path/to/short_video.mp4"" # 374KB
with open(image_fn, ""rb"") as asset_data:
    asset = Blob()
    asset.mime_type = ""video/mp4""
    asset.data = asset_data.read()

asset_part = Part()
asset_part.inline_data = asset

response = model.generate_content([prompt_part, asset_part],
                                  generation_config=GenerationConfig(temperature=0.0,),
                                 )

Actual vs expected behavior:
actual behavior:
following exception is thrown:
 /Users/schnee/projects/pmg/github/bw-impossible/creative_kitchen/repl.py
Traceback (most recent call last):
  File ""/Users/schnee/projects/pmg/github/bw-impossible/creative_kitchen/repl.py"", line 25, in <module>
    response = model.generate_content([prompt_part, asset_part],
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/schnee/Library/Caches/pypoetry/virtualenvs/bw-impossible-_xsmYHYG-py3.11/lib/python3.11/site-packages/google/generativeai/generative_models.py"", line 262, in generate_content
    response = self._client.generate_content(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/schnee/Library/Caches/pypoetry/virtualenvs/bw-impossible-_xsmYHYG-py3.11/lib/python3.11/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py"", line 791, in generate_content
    response = rpc(
               ^^^^
  File ""/Users/schnee/Library/Caches/pypoetry/virtualenvs/bw-impossible-_xsmYHYG-py3.11/lib/python3.11/site-packages/google/api_core/gapic_v1/method.py"", line 131, in __call__
    return wrapped_func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/schnee/Library/Caches/pypoetry/virtualenvs/bw-impossible-_xsmYHYG-py3.11/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py"", line 293, in retry_wrapped_func
    return retry_target(
           ^^^^^^^^^^^^^
  File ""/Users/schnee/Library/Caches/pypoetry/virtualenvs/bw-impossible-_xsmYHYG-py3.11/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py"", line 153, in retry_target
    _retry_error_helper(
  File ""/Users/schnee/Library/Caches/pypoetry/virtualenvs/bw-impossible-_xsmYHYG-py3.11/lib/python3.11/site-packages/google/api_core/retry/retry_base.py"", line 212, in _retry_error_helper
    raise final_exc from source_exc
  File ""/Users/schnee/Library/Caches/pypoetry/virtualenvs/bw-impossible-_xsmYHYG-py3.11/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py"", line 144, in retry_target
    result = target()
             ^^^^^^^^
  File ""/Users/schnee/Library/Caches/pypoetry/virtualenvs/bw-impossible-_xsmYHYG-py3.11/lib/python3.11/site-packages/google/api_core/timeout.py"", line 120, in func_with_timeout
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/schnee/Library/Caches/pypoetry/virtualenvs/bw-impossible-_xsmYHYG-py3.11/lib/python3.11/site-packages/google/api_core/grpc_helpers.py"", line 78, in error_remapped_callable
    raise exceptions.from_grpc_error(exc) from exc
google.api_core.exceptions.InternalServerError: 500 An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting

expected behavior: gemini-pro-1.5-latest consumes the prompt and video file.
Any other information you'd like to share?
Note, over in vertexai land, the model is gemini-pro-1.5-preview-0409 and this does appear to process videos. I'm not sure if my issues is a model capability or not; that model does not appear to be available to this SDK.
The video I'm using is quite short and small: < 10 seconds and ~384KB. It is also h.264 encoded, which appears to be acceptable by Gemini.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/331","A plain text version of gemini 1.5","2024-05-14T19:22:35Z","Closed issue","component:other,status:awaiting user response,type:feature request","It would be great if you can provide a text only Gemini 1.5 pro model and make QPS the same as Gemini 1.0 pro.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/329","stream=True returns all chunks at the same time [in Colab]","2024-06-12T01:49:29Z","Closed as not planned issue","component:python sdk,status:awaiting user response,status:stale,type:bug","Description of the bug:
I have read the docs here https://ai.google.dev/api/python/google/generativeai/GenerativeModel to learn that use stream=True could return chunks of response one by one, but when I was trying with the following simple code, it doesn't work, it return chunks, but it seems like to return all the chunks at the same time:
import google.generativeai as genai


genai.configure(api_key='....')
model = genai.GenerativeModel('models/gemini-pro')

response = model.generate_content('generate a long story, more than 200 words, multiple graphs with breakline', stream=True)


for chunk in response:
    print(chunk.text)
Actual vs expected behavior:
No response
Any other information you'd like to share?
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/328","System Instructions for Gemini 1.0?","2024-05-14T19:29:49Z","Closed issue","component:python sdk,status:awaiting user response,type:bug","> @Bikatr7, [System Instructions](https://ai.google.dev/docs/system_instructions) is introduced in `Gemini 1.5 pro` only as I am unable to see this feature while using `Gemini 1.0` pro model on AI studio. We will keep you posted if this feature gets introduced to Gemini 1.0. Meanwhile, You can keep using for Gemini 1.5 pro. If this issue is resolved for you, you can go ahead and close this issue. Thank you

Hello, I tried with 1.5-pro, I get a different error.
 ValueError Traceback (most recent call last).....................................................
 ...............................
 GoogleGenerativeAIError: Error embedding content: content must not be empty
 if I use 1.0 I get the above error raised by the original thread. If I change to 1.5, get this error. But I didnt get any error when I used OpenAI's chat model. Why is Google's API's so annoying. You guys keen to gain more adoption, but the experience sucks. Just the other day, Google Architect, almost forced me to move away from openAi models, how can I do that? how to trust these unreliable google models.
 even the embedding model is not good. Please reply to the error msg reported above.
Originally posted by @annamalaiarunachalam in #278 (comment)
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/327","Is upload_file thread safe?","2024-05-08T08:55:53Z","Open issue","component:python sdk,type:bug","Description of the bug:
when I want use genai , I need to use this method in an interface request, but when I put ""genai.upload_file(path=path,display_name=file)"" into the interface method, it will report an error SLL_ERROR
@app.route(""/xxx"",method=[""GET""]) def upload_video(): output_frame_folder = requests.output_frame_folder time.sleep(1.0) lists = [] try: for file in os.listdir(f""{output_frame_folder}""): path = os.path.join(f""{output_frame_folder}"", file) sample_file = genai.upload_file(path=path,display_name=file)
Actual vs expected behavior:
No response
Any other information you'd like to share?
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/326","count_tokens should allow empty content","2024-05-21T13:38:55Z","Closed issue","component:python sdk,type:bug","Description of the bug:
Currently, this code:
model = genai.GenerativeModel(model_name, system_instruction=""Talk in rhymes"")
model.count_tokens("""")
Throws this error:
TypeError: contents must not be empty

It is possible to work around it by passing contents={'parts': {'text': ''}} but this requires some mental gymnastics to derive and to understand.
Now that we have system instructions and other count-able metadata outside of content, we should allow counting that metadata in the SDK. One way would be to allow empty content, but only when used through count_tokens.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/324","File uploads are broken (ValueError: Unknown field for File: state)","2024-05-07T16:17:20Z","Closed issue","component:python sdk,type:bug","Description of the bug:
This looks similar to this #292
I used File API to upload images to Gemini and everything was working fine, but now File API returns ValueError: Unknown field for File: state. I did a simple test:
""import google.generativeai as genai

def test():
    genai.upload_file(path='video_screenshot1.png')

test()""

and it returns
""(.venv) karol@Karols-MacBook-Air src % /Users/karol/Desktop/projects/what_to_wear/src/.venv/bin/python /Users/karol/Desktop/projects/what_to_wear/src/test.py
Traceback (most recent call last):
  File ""/Users/karol/Desktop/projects/what_to_wear/src/test.py"", line 7, in <module>
    test()
  File ""/Users/karol/Desktop/projects/what_to_wear/src/test.py"", line 5, in test
    genai.upload_file(path='video_screenshot1.png')
  File ""/Users/karol/Desktop/projects/what_to_wear/src/.venv/lib/python3.12/site-packages/google/generativeai/files.py"", line 52, in upload_file
    response = client.create_file(
               ^^^^^^^^^^^^^^^^^^^
  File ""/Users/karol/Desktop/projects/what_to_wear/src/.venv/lib/python3.12/site-packages/google/generativeai/client.py"", line 76, in create_file
    return glm.File(
           ^^^^^^^^^
  File ""/Users/karol/Desktop/projects/what_to_wear/src/.venv/lib/python3.12/site-packages/proto/message.py"", line 576, in __init__
    raise ValueError(
ValueError: Unknown field for File: state""

Please help because it renders my Google AI hackathon project useless (and probably all projects using the upload_file method).
Actual vs expected behavior:
No response
Any other information you'd like to share?
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/323","pip install --upgrade google.generativeai => Mac OS malware warning re rustc","2024-05-08T06:08:45Z","Closed issue","component:python sdk,status:awaiting user response,type:bug","Description of the bug:
Running pip install on google.generativeai produces a Mac OS malware alert related to rustc.
“rustc” will damage your computer. You should move it to the Trash.
Actual vs expected behavior:
Not trying to deposit malware on my system, or, helping Mac correctly recognize that this is not malware.
Any other information you'd like to share?
Collecting google.generativeai
 Downloading google_generativeai-0.5.2-py3-none-any.whl (146 kB)
 |████████████████████████████████| 146 kB 3.1 MB/s
 Requirement already satisfied: tqdm in /Users/fred/miniconda3/lib/python3.9/site-packages (from google.generativeai) (4.63.0)
 Collecting google-ai-generativelanguage==0.6.2
 Downloading google_ai_generativelanguage-0.6.2-py3-none-any.whl (664 kB)
 |████████████████████████████████| 664 kB 19.9 MB/s
 Collecting pydantic
 Downloading pydantic-2.7.1-py3-none-any.whl (409 kB)
 |████████████████████████████████| 409 kB 21.0 MB/s
 Collecting google-auth>=2.15.0
 Downloading google_auth-2.29.0-py2.py3-none-any.whl (189 kB)
 |████████████████████████████████| 189 kB 27.4 MB/s
 Requirement already satisfied: typing-extensions in /Users/fred/miniconda3/lib/python3.9/site-packages (from google.generativeai) (4.5.0)
 Collecting google-api-core
 Downloading google_api_core-2.19.0-py3-none-any.whl (139 kB)
 |████████████████████████████████| 139 kB 17.0 MB/s
 Requirement already satisfied: protobuf in /Users/fred/miniconda3/lib/python3.9/site-packages (from google.generativeai) (3.20.3)
 Collecting google-api-python-client
 Downloading google_api_python_client-2.127.0-py2.py3-none-any.whl (12.7 MB)
 |████████████████████████████████| 12.7 MB 19.4 MB/s
 Collecting proto-plus<2.0.0dev,>=1.22.3
 Downloading proto_plus-1.23.0-py3-none-any.whl (48 kB)
 |████████████████████████████████| 48 kB 7.2 MB/s
 Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /Users/fred/miniconda3/lib/python3.9/site-packages (from google-api-core->google.generativeai) (2.31.0)
 Collecting googleapis-common-protos<2.0.dev0,>=1.56.2
 Downloading googleapis_common_protos-1.63.0-py2.py3-none-any.whl (229 kB)
 |████████████████████████████████| 229 kB 28.0 MB/s
 Collecting grpcio<2.0dev,>=1.33.2
 Downloading grpcio-1.63.0-cp39-cp39-macosx_10_9_universal2.whl (10.2 MB)
 |████████████████████████████████| 10.2 MB 60.0 MB/s
 Collecting grpcio-status<2.0.dev0,>=1.33.2
 Downloading grpcio_status-1.63.0-py3-none-any.whl (14 kB)
 Collecting rsa<5,>=3.1.4
 Downloading rsa-4.9-py3-none-any.whl (34 kB)
 Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/fred/miniconda3/lib/python3.9/site-packages (from google-auth>=2.15.0->google.generativeai) (5.3.0)
 Collecting pyasn1-modules>=0.2.1
 Downloading pyasn1_modules-0.4.0-py3-none-any.whl (181 kB)
 |████████████████████████████████| 181 kB 13.5 MB/s
 Collecting grpcio-status<2.0.dev0,>=1.33.2
 Downloading grpcio_status-1.62.2-py3-none-any.whl (14 kB)
 Collecting protobuf
 Using cached protobuf-4.25.3-cp37-abi3-macosx_10_9_universal2.whl (394 kB)
 Collecting pyasn1<0.7.0,>=0.4.6
 Downloading pyasn1-0.6.0-py2.py3-none-any.whl (85 kB)
 |████████████████████████████████| 85 kB 7.8 MB/s
 Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/fred/miniconda3/lib/python3.9/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google.generativeai) (1.26.8)
 Requirement already satisfied: charset-normalizer<4,>=2 in /Users/fred/miniconda3/lib/python3.9/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google.generativeai) (2.0.4)
 Requirement already satisfied: idna<4,>=2.5 in /Users/fred/miniconda3/lib/python3.9/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google.generativeai) (3.3)
 Requirement already satisfied: certifi>=2017.4.17 in /Users/fred/miniconda3/lib/python3.9/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google.generativeai) (2021.10.8)
 Collecting httplib2<1.dev0,>=0.19.0
 Downloading httplib2-0.22.0-py3-none-any.whl (96 kB)
 |████████████████████████████████| 96 kB 12.6 MB/s
 Collecting google-auth-httplib2<1.0.0,>=0.2.0
 Downloading google_auth_httplib2-0.2.0-py2.py3-none-any.whl (9.3 kB)
 Collecting uritemplate<5,>=3.0.1
 Downloading uritemplate-4.1.1-py2.py3-none-any.whl (10 kB)
 Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /Users/fred/miniconda3/lib/python3.9/site-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google.generativeai) (3.0.9)
 Collecting typing-extensions
 Downloading typing_extensions-4.11.0-py3-none-any.whl (34 kB)
 Collecting annotated-types>=0.4.0
 Downloading annotated_types-0.6.0-py3-none-any.whl (12 kB)
 Collecting pydantic-core==2.18.2
 Downloading pydantic_core-2.18.2-cp39-cp39-macosx_10_12_x86_64.whl (1.9 MB)
 |████████████████████████████████| 1.9 MB 2.7 MB/s
 Installing collected packages: pyasn1, rsa, pyasn1-modules, protobuf, proto-plus, grpcio, googleapis-common-protos, google-auth, typing-extensions, httplib2, grpcio-status, google-api-core, uritemplate, pydantic-core, google-auth-httplib2, annotated-types, pydantic, google-api-python-client, google-ai-generativelanguage, google.generativeai
 Attempting uninstall: protobuf
 Found existing installation: protobuf 3.20.3
 Uninstalling protobuf-3.20.3:
 Successfully uninstalled protobuf-3.20.3
 Attempting uninstall: typing-extensions
 Found existing installation: typing-extensions 4.5.0
 Uninstalling typing-extensions-4.5.0:
 Successfully uninstalled typing-extensions-4.5.0
 ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.
 streamlit 1.20.0 requires protobuf<4,>=3.12, but you have protobuf 4.25.3 which is incompatible.
 Successfully installed annotated-types-0.6.0 google-ai-generativelanguage-0.6.2 google-api-core-2.19.0 google-api-python-client-2.127.0 google-auth-2.29.0 google-auth-httplib2-0.2.0 google.generativeai-0.5.2 googleapis-common-protos-1.63.0 grpcio-1.63.0 grpcio-status-1.62.2 httplib2-0.22.0 proto-plus-1.23.0 protobuf-4.25.3 pyasn1-0.6.0 pyasn1-modules-0.4.0 pydantic-2.7.1 pydantic-core-2.18.2 rsa-4.9 typing-extensions-4.11.0 uritemplate-4.1.1
 The text was updated successfully, but these errors were encountered: 
👍1
john-kurkowski reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/google-gemini/generative-ai-python/issues/320","Clarify google-ai-generativelanguage version","2024-05-06T18:06:02Z","Open issue","component:python sdk,status:awaiting review,status:triaged,type:feature request","Hi,
I'm in the process of adding google-generativeai to conda-forge here: conda-forge/staged-recipes#26259
One of the dependencies, google-ai-generativelanguage is not yet on conda-forge so I am adding that as well but it is not clear to me from the setup.py which version I need to specify?
Ref: 
generative-ai-python/setup.py
 Line 45 in a96feda
	""google-ai-generativelanguage@https://storage.googleapis.com/generativeai-downloads/preview/ai-generativelanguage-v1beta-py-2.tar.gz"", 
Conda-forge cannot use prebuilt and downloadable files as we built everything ourselves on our infrastructure so would it be possible to clarify if that version is downloadable as a source distribution from somewhere and what version it has?
 Currently, I have it specified as ==0.6.2 but I would assume that is not correct.
Thank you!
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/319","Uri parameter cannot be overridden when sending via rest transport","2024-05-05T12:27:09Z","Open issue","component:python sdk,status:triaged,type:feature request","Description of the bug:
We want to override the standard google uri part of the model endpoint.
 Although some parameters e.g., api endpoint (host) can be overridden the uri parameter is hard coded.
Context:
 we have our own AI hub where we count tokens for a set of models before forwarding the request to the model hosted by Google/ Azure-OpenAI, etc.
 E.g., in case of OpenAI we can override the parameters like endpoint and api_key.
 But this is not the case for the Google generative SDK.
Steps to reproduce
Created subclass for GenerativeModel
Instantiated a GenerativeServiceClient for sending requests to Gemini 1.0 model
Call generate_content with the intent to replace api endpoint, model uri, and api key
Code Example
        kwargs = dict(model_name='gemini-1.0-pro')
        model = MyGenerativeModel(proxy_client=self.proxy_client, **kwargs)
        content = self._get_test_messages()
        model_response = model.generate_content(content)

        ## in MyGenerativeModel subclass we do the following:
        super().__init__(**kwargs)
        self._client = GenerativeServiceClient(transport=transport, client_options={'api_endpoint': 'myhost'})
Actual vs expected behavior:
However, in
 venv/lib/python3.9/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/rest.py:810
 uri seems to be hard coded, whereas host can be overridden.
We want to override with this uri: ""models/gemini-1.0-pro:generateContent"" (i.e. remove ""/v1beta/"" prefix)
Also a interceptor did not help as request is immutable:
class MyCustomGenerativeServiceInterceptor(GenerativeServiceRestInterceptor):
    def pre_generate_content(self, request, metadata):
        # logging.log(f""Received request: {request}"")
        metadata.append((""uri"", ""models/gemini-1.0-pro:generateContent""))
        return request, metadata
Plz provide an option to override the uri or let us know if there is another way.
 Thanks!
Any other information you'd like to share?
Environment details
 OS type and version: Mac OS latest
 Python version: 3.9
 pip version: 24.0
 google-cloud-aiplatform version: Version: 1.47.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/318","response.text quick accessor bug","2024-05-04T12:30:35Z","Open issue","component:python sdk,status:triaged,type:bug","Description of the bug:
Here is the code snippet that sometimes falls with the ValueError exception despite the fact that I disabled all the safety settings:
genai.configure(api_key=self.api_key)
 model = genai.GenerativeModel(
 model_name=""gemini-pro"",
 # generation_config=self.generation_config,
 safety_settings=self.safety_settings
 )
    try:
        response = await model.generate_content_async(self.history)
        print(response.text)

ValueError: The response.text quick accessor only works when the response contains a valid Part, but none was returned. Check the candidate.safety_ratings to see if the response was blocked.
Actual vs expected behavior:
Actual: raises an exception
Expected: some text response like ""Sorry, I can't answer your question due to ...""
Any other information you'd like to share?
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/317","Gemini version 0.5.* generate_content doesn't provide usageMetadata if stream=True","2024-06-04T01:48:31Z","Closed as not planned issue","component:python sdk,status:awaiting user response,status:stale,type:bug","Description of the bug:
I believe this is related to https://github.com/google/generative-ai-go/issues/97
Actual vs expected behavior:
As far as I remember, it worked earlier this week. I tried all version 0.5.* but the same issue. The issue was fixed when I utilized version 0.4.0

Any other information you'd like to share?
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/312","upload_file returns a partially populated object","2024-05-03T03:19:29Z","Closed issue","component:python sdk,good first issue,type:bug","Description of the bug:
This code is from the cookbook sample:
sample_file = genai.upload_file(path=""image.jpg"", display_name=""Sample drawing"")
print(f""Uploaded file '{sample_file.display_name}' as: {sample_file.uri}"")
When executed, display_name is an empty string, as we all a number of other properties (mime_type, plus timestamps, hash, etc). They are all returned correctly from get_file, and the partial objects work fine in prompts.
One fix would be to call get_file after uploading, and return that response instead.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/311","Security Policy violation SECURITY.md","2024-05-28T04:05:31Z","Closed issue","allstar","This issue was automatically created by Allstar.
Security Policy Violation
 Security policy not enabled.
 A SECURITY.md file can give users information about what constitutes a vulnerability and how to report one securely so that information about a bug is not publicly visible. Examples of secure reporting methods include using an issue tracker with private issue support, or encrypted email with a published key.
To fix this, add a SECURITY.md file that explains how to handle vulnerabilities found in your repository. Go to https://github.com/google-gemini/generative-ai-python/security/policy to enable.
For more information, see https://docs.github.com/en/code-security/getting-started/adding-a-security-policy-to-your-repository.
This issue will auto resolve when the policy is in compliance.
Issue created by Allstar. See https://github.com/ossf/allstar/ for more information. For questions specific to the repository, please contact the owner or maintainer.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/310","Security Policy violation Outside Collaborators","2024-05-07T04:05:26Z","Closed issue","allstar","This issue was automatically created by Allstar.
Security Policy Violation
 Found 1 outside collaborators with admin access.
 This policy requires users with this access to be members of the organisation. That way you can easily audit who has access to your repo, and if an account is compromised it can quickly be denied access to organization resources. To fix this you should either remove the user from repository-based access, or add them to the organization.
Remove the user from the repository-based access. From the main page of the repository, go to Settings -> Manage Access.
 (For more information, see https://docs.github.com/en/account-and-profile/setting-up-and-managing-your-github-user-account/managing-access-to-your-personal-repositories/removing-a-collaborator-from-a-personal-repository)
OR
Invite the user to join your organisation. Click your profile photo and choose “Your Organization” → choose the org name → “People” → “Invite Member.” (For more information, see https://docs.github.com/en/organizations/managing-membership-in-your-organization/inviting-users-to-join-your-organization)
If you don't see the Settings tab you probably don't have administrative access. Reach out to the administrators of the organisation to fix this issue.
OR
Exempt the user by adding an exemption to your organization-level Outside Collaborators configuration file.
This issue will auto resolve when the policy is in compliance.
Issue created by Allstar. See https://github.com/ossf/allstar/ for more information. For questions specific to the repository, please contact the owner or maintainer.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/307","BrokenResponseError: Can not build a coherent char history after a broken streaming response (See the previous Exception fro details). To inspect the last response object, use chat.last.To remove the last request/response Content objects from the chat call last_send, last_received = chat.rewind() and continue without it","2024-04-30T15:10:41Z","Open issue","component:python sdk,status:triaged,type:bug","Traceback (most recent call last):
 File """", line 1, in 
 File ""/home/asahi/Проекты/Python/gemini/gemini/init.py"", line 5, in main
 run()
 File ""/home/asahi/Проекты/Python/gemini/gemini/main.py"", line 81, in run
 process_directory(SRC, DST)
 File ""/home/asahi/Проекты/Python/gemini/gemini/main.py"", line 73, in process_directory
 translate_file(src_path=src_path, dst_path=dst_path)
 File ""/home/asahi/Проекты/Python/gemini/gemini/main.py"", line 48, in translate_file
 translated = translate(src_path)
 ^^^^^^^^^^^^^^^^^^^
 File ""/home/asahi/Проекты/Python/gemini/gemini/main.py"", line 36, in translate
 second_response = chat.send_message(content=MESSAGE_FOR_SECOND_PART + second_part, stream=True)
 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
 File ""/home/asahi/.cache/pypoetry/virtualenvs/gemini-UM_oiH3g-py3.12/lib/python3.12/site-packages/google/generativeai/generative_models.py"", line 467, in send_message
 history = self.history[:]
 ^^^^^^^^^^^^
 File ""/home/asahi/.cache/pypoetry/virtualenvs/gemini-UM_oiH3g-py3.12/lib/python3.12/site-packages/google/generativeai/generative_models.py"", line 686, in history
 raise generation_types.BrokenResponseError(
 google.generativeai.types.generation_types.BrokenResponseError: Can not build a coherent char history after a broken streaming response (See the previous Exception fro details). To inspect the last response object, use chat.last.To remove the last request/response Content objects from the chat call last_send, last_received = chat.rewind() and continue without it.
code:
# --snip--chat = model.start_chat()
PART_LEN = len(content) // 2first_part = content[:PART_LEN]
second_part = content[PART_LEN:]
first_response = chat.send_message(content=MESSAGE_FOR_FIRST_PART + first_part, stream=True)
for chunk in first_response:
        chunks.append(chunk)
second_response = chat.send_message(content=MESSAGE_FOR_SECOND_PART + second_part, stream=True)
for chunk in second_response:
        chunks.append(chunk)
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/305","google.generativeai.types.generation_types.BlockedPromptException: prompt_feedback { block_reason: OTHER }","2024-06-01T01:50:16Z","Closed as not planned issue","component:python sdk,status:awaiting user response,status:stale,type:bug","Description of the bug:
This is our model config and when have a few image don't work fine.
response
but when i use the same the image and content on google ai platform will give more info.
Thank you.
Actual vs expected behavior:
No response
Any other information you'd like to share?
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/300","Retry / warning / default off","2024-04-24T06:43:10Z","Open issue","component:python sdk,status:triaged,type:feature request","Description of the feature request:
We support retry in this library operations.py#L84
What problem are you trying to solve with this feature?
As of yet it is unclear if we will be billed when we get 500 errors from the api.
This means that retry has the potential of costing a developer a lot of money if it retries without their knowledge.
Any other information you'd like to share?
I would like to suggest that retry be documented disabled by default if its not currently.
And a warning about possible billing issues be added to the readme.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/296","ADC setup - Error 400: invalid_scope","2024-04-22T04:54:56Z","Open issue","component:python sdk,type:bug","Description of the bug:
Login with ADC fails.
gcloud auth application-default login --scopes=""https://www.googleapis.com/auth/generative-language,https://www.googleapis.com/auth/cloud-platform""

What I did wrong?
Actual vs expected behavior:
Any other information you'd like to share?
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/294","google.api_core.exceptions.DeadlineExceeded: 504 Deadline Exceeded","2024-04-19T17:07:28Z","Closed issue","component:python sdk,type:bug","Description of the bug:
Request times out, its client side error.
Actual vs expected behavior:
model: ""gemini-1.5-pro-latest""
response = model.generate_content(large_prompt)
 Wait & Return the response.
Any other information you'd like to share?
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/292","File uploads are broken.","2024-04-18T20:44:06Z","Closed issue","component:python sdk,type:bug","Description of the bug:
Using 0.5.1, as of ~1pm today file uploads are failing with ValueError: Unknown field for File: state
The discovery implementation is returning a file state that the glm package doesn't know about.
I'm working on a fix in #291
Actual vs expected behavior:
No response
Any other information you'd like to share?
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/290","""Request options"" can't be set for chat.","2024-04-18T00:32:52Z","Open issue","component:python sdk,status:triaged,type:bug","No description provided.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/531","Unable to Coerce return values when they are Protos","2024-04-17T20:59:10Z","Open issue","status:triaged,type:bug","TLDR
When calling a function that returns a proto (or list of protos), a ValueError is thrown because it cannot coerce the results.
Potential Fix
I'm assuming the result from the function call needs to be in some standard python type (i.e. list, str, int, etc.)
 Instead of attempting to coerce each field in the proto, can we simply return the entire proto as a string?
If you see here, I've written a wrapper method around the original, where I simply take each proto and coerce to str.
 The function call is able to then work properly with the string format.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/288","response.text is no longer directly accessible - TypeError: argument of type 'Part' is not iterable","2024-04-16T01:04:22Z","Open issue","component:python sdk,type:bug","Description of the bug:
Accessing the property response.text no longer works.
genai.configure(api_key = gcp_key)
model = genai.GenerativeModel(""gemini-pro"")
response = model.generate_content(prompt)

print(str(response.text)
# TypeError: argument of type 'Part' is not iterable# google/generativeai/types/generation_types.py# line 327, in text#    if len(parts) != 1 or ""text"" not in parts[0]:
As a workaround, I can get the detail with response.candidates[0].content.parts[0].text, but this seems like it might be a breaking change?
Actual vs expected behavior:
I should be able to reference response.text directly.
Any other information you'd like to share?
NOTE: I am using the internal Google3 version of this SDK - see go/generative-ai-python-sdk-response-type-issue for the exact line internally. There seems to be a discrepancy between Google3 and GitHub - this is the line in the GitHub repo.
Feel free to reach out to me internally - mitchspano@
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/284","response_mime_type will let you force a json response, can we set a schema?","2024-05-17T21:41:11Z","Closed issue","component:python sdk,type:feature request","Description of the feature request:
Could we add ""json_schema"" argument that reroutes to a forced function call instead?
What problem are you trying to solve with this feature?
No response
Any other information you'd like to share?
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/282","response.text could check the finish_reason and safety filters and give a more directly helpful error message.","2024-04-11T19:54:28Z","Open issue","component:python sdk,status:triaged,type:feature request","Description of the feature request:
response.text could check the safety filters and give a more directly helpful error message.
If a message is blocked because of safety, currently you get an index error or something, but if the finish_reason is bad we could give some details on why text is not filled in.
 The text was updated successfully, but these errors were encountered: 
👍3
Bikatr7, JerryDai1025, and andrewreece reacted with thumbs up emoji
All reactions
👍3 reactions"
"https://github.com/google-gemini/generative-ai-python/issues/281","ModuleNotFoundError: No module named 'google.generativeai'","2024-04-11T19:53:05Z","Closed issue","component:python sdk,type:bug","Description of the bug:
Description of the bug:
 ModuleNotFoundError: No module named 'google.generativeai'
The python version installed on my windows10 machine
 Python 3.9
Actual vs expected behavior:
No response
Any other information you'd like to share?
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/280","Empty response when the finish reason is MAX_TOKENS","2024-05-24T16:25:17Z","Closed issue","component:python sdk,type:bug","Description of the bug:
When using max_output_tokens in generate_content to limit the output of the model, the following error is thrown when trying to access response.text, whereas its expected to get the output till MAX_TOKENS.
  
 ValueError: The response.text quick accessor only works when the response contains a valid Part, but none was returned. Check the candidate.safety_ratings to see if the response was blocked.
The actual response:
response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=glm.GenerateContentResponse({'candidates': [{'finish_reason': 2, 'index': 0, 'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'token_count': 0, 'grounding_attributions': []}]}),
)

This is the code snippet, taken from https://ai.google.dev/tutorials/python_quickstart#generation_configuration, which shows expected behaviour is to get upto 20 tokens then cut off due to MAX_TOKENS.
import google.generativeai as genai

genai.configure(api_key='***')
model = genai.GenerativeModel('gemini-pro')
response = model.generate_content(
    'Tell me a story about a magic backpack.',
    generation_config=genai.types.GenerationConfig(
        # Only one candidate for now.
        candidate_count=1,
        # stop_sequences=['x'],     # Commenting this part as sometimes it works due to 'x' being present in output and model cuts off before MAX_TOKENS is reached.
        max_output_tokens=20,
        temperature=1.0)
)

text = response.text

if response.candidates[0].finish_reason.name == ""MAX_TOKENS"":
    text += '...'

print(text)

Actual vs expected behavior:
https://ai.google.dev/tutorials/python_quickstart#generation_configuration
 Similar output as of here, not error when trying to access response.text, but to get the output of approx 20 tokens.
Works in AI Studio

Any other information you'd like to share?
Package information
 Name: google-generativeai
 Version: 0.5.0
Streaming partially works, but the last chunk with MAX_TOKENS is still empty (Could be intended behaviour)
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/278","google.api_core.exceptions.InvalidArgument: 400 Developer instruction error when using system_instruction with Gemini pro","2024-04-15T08:39:30Z","Closed issue","component:python sdk,status:awaiting user response,type:bug","Description of the bug:
Attempting to generate a response while using system_instruction raises a 400 developer instruction error.
Test values, and code to reproduce the bug in ""Any other information you'd like to share""
I may be mistaken, but from the v0.5.0 notes, this was ready for use:
https://github.com/google/generative-ai-python/releases/tag/v0.5.0
If it is not, could I request release notes make it clear when stuff isn't for use yet.
Thank you.
pip freeze for reference:
 aiofiles==23.2.1
 aiohttp==3.8.4
 aiosignal==1.3.1
 altair==5.2.0
 annotated-types==0.6.0
 anyio==3.7.1
 appdirs==1.4.4
 asttokens==2.2.1
 async-timeout==4.0.2
 attrs==22.2.0
 backcall==0.2.0
 backoff==2.2.1
 beautifulsoup4==4.12.2
 blis==0.7.9
 build==1.0.3
 cache==1.0.3
 cachetools==5.3.0
 catalogue==2.0.8
 certifi==2023.5.7
 cffi==1.16.0
 charset-normalizer==3.1.0
 click==8.1.3
 cloudpathlib==0.16.0
 cmake==3.25.2
 colorama==0.4.6
 colorthief==0.2.1
 colour==0.1.5
 comm==0.1.3
 commonmark==0.9.1
 confection==0.1.4
 contourpy==1.2.0
 cryptography==42.0.2
 cycler==0.12.1
 cymem==2.0.7
 debugpy==1.6.7
 decorator==4.4.2
 deepl==1.16.1
 discord==2.2.2
 discord-py-slash-command==3.0.3
 discord-typings==0.5.1
 discord.py==2.2.2
 distro==1.8.0
 dnspython==2.4.2
 docutils==0.20.1
 easytl==0.1.0
 emoji==2.2.0
 en-core-web-lg @ https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.7.0/en_core_web_lg-3.7.0-py3-none-any.whl#sha256=708da1110fbe1163d059de34a2cbedb1db65c26e1e624ca925897a2711cb7d77
 executing==1.2.0
 fastapi==0.104.1
 ffmpy==0.3.1
 filelock==3.13.1
 fonttools==4.45.1
 frozenlist==1.3.3
 fsspec==2023.10.0
 google-ai-generativelanguage==0.6.1
 google-api-core==2.11.0
 google-api-python-client==2.78.0
 google-auth==2.16.1
 google-auth-httplib2==0.1.0
 google-auth-oauthlib==1.0.0
 google-cloud-core==2.4.1
 google-cloud-texttospeech==2.15.1
 google-cloud-translate==3.14.0
 google-generativeai==0.5.0
 googleapis-common-protos==1.58.0
 gradio==4.19.2
 gradio_client==0.10.1
 grpcio==1.60.1
 grpcio-status==1.60.1
 h11==0.14.0
 httpcore==1.0.1
 httplib2==0.21.0
 httpx==0.25.1
 huggingface-hub==0.19.4
 idna==3.4
 imageio==2.33.1
 imageio-ffmpeg==0.4.9
 importlib-metadata==7.0.1
 importlib-resources==6.1.1
 ipykernel==6.22.0
 ipython==8.13.1
 ja-core-news-lg @ https://github.com/explosion/spacy-models/releases/download/ja_core_news_lg-3.7.0/ja_core_news_lg-3.7.0-py3-none-any.whl#sha256=f08eecb4d40523045c9478ce59a67564fd71edd215f32c076fa91dc1f05cc7fd
 jaraco.classes==3.3.0
 jedi==0.18.2
 Jinja2==3.1.2
 jsonschema==4.20.0
 jsonschema-specifications==2023.11.2
 jupyter_client==8.2.0
 jupyter_core==5.3.0
 kairyou==1.4.1
 keyboard==0.13.5
 keyring==24.3.0
 kiwisolver==1.4.5
 langcodes==3.3.0
 loguru==0.7.2
 lxml==4.9.3
 markdown-it-py==3.0.0
 MarkupSafe==2.1.2
 matplotlib==3.8.2
 matplotlib-inline==0.1.6
 mdurl==0.1.2
 mecab-python3==1.0.6
 minify_html==0.15.0
 more-itertools==10.2.0
 MouseInfo==0.1.3
 moviepy==1.0.3
 multidict==6.0.4
 murmurhash==1.0.9
 mysql-connector-python==8.3.0
 nest-asyncio==1.5.6
 nh3==0.2.15
 numpy==1.24.2
 oauth2client==4.1.3
 oauthlib==3.2.2
 openai==1.13.3
 opencv-python==4.9.0.80
 orjson==3.9.10
 packaging==23.0
 pandas==2.1.0
 parso==0.8.3
 pathy==0.10.1
 pickleshare==0.7.5
 Pillow==9.5.0
 pkginfo==1.9.6
 plac==1.3.5
 platformdirs==3.5.0
 preshed==3.0.8
 proglog==0.1.10
 prompt-toolkit==3.0.38
 proto-plus==1.23.0
 protobuf==4.25.3
 psutil==5.9.5
 pure-eval==0.2.2
 pyasn1==0.4.8
 pyasn1-modules==0.2.8
 PyAutoGUI==0.9.53
 pycparser==2.21
 pydantic==2.5.3
 pydantic_core==2.14.6
 PyDrive==1.3.1
 pydub==0.25.1
 pyee==8.2.2
 PyGetWindow==0.0.9
 Pygments==2.15.1
 pymongo==4.5.0
 PyMsgBox==1.0.9
 pynput==1.7.6
 pyparsing==3.0.9
 pyperclip==1.8.2
 pyppeteer==1.0.2
 pyproject_hooks==1.0.0
 PyQt5==5.15.10
 PyQt5-Qt5==5.15.2
 PyQt5-sip==12.13.0
 PyRect==0.2.0
 PyScreeze==0.1.28
 python-dateutil==2.8.2
 python-docx==0.8.11
 python-multipart==0.0.9
 pytube==12.1.3
 pytweening==1.0.4
 pytz==2023.3.post1
 pywin32==306
 pywin32-ctypes==0.2.2
 PyYAML==6.0
 pyzmq==25.0.2
 readme-renderer==42.0
 referencing==0.31.1
 regex==2023.3.23
 requests==2.31.0
 requests-oauthlib==1.3.1
 requests-toolbelt==1.0.0
 rfc3986==2.0.0
 rich==12.6.0
 rpds-py==0.13.2
 rsa==4.9
 ruff==0.2.2
 semantic-version==2.10.0
 shellingham==1.5.4
 six==1.16.0
 smart-open==6.3.0
 sniffio==1.3.0
 soupsieve==2.5
 spacy==3.7.4
 spacy-legacy==3.0.12
 spacy-loggers==1.0.5
 srsly==2.4.5
 stack-data==0.6.2
 starlette==0.27.0
 SudachiDict-core==20230927
 SudachiPy==0.6.7
 thinc==8.2.3
 tiktoken==0.6.0
 tomli==2.0.1
 tomlkit==0.12.0
 toolz==0.12.0
 tornado==6.3.1
 tqdm==4.64.1
 traitlets==5.9.0
 twine==4.0.2
 typer==0.9.0
 typing_extensions==4.8.0
 tzdata==2023.3
 unidic-lite==1.0.8
 uritemplate==4.1.1
 urllib3==1.26.14
 uvicorn==0.24.0.post1
 wasabi==1.1.1
 wcwidth==0.2.6
 weasel==0.3.4
 websockets==10.4
 win32-setctime==1.1.0
 yarl==1.8.2
 zipp==3.17.0
Actual vs expected behavior:
actual:
Traceback (most recent call last):
 File ""C:\Users\Tetra\AppData\Local\Programs\Python\Python311\Lib\site-packages\google\api_core\grpc_helpers.py"", line 72, in error_remapped_callable
 return callable_(*args, **kwargs)
 ^^^^^^^^^^^^^^^^^^^^^^^^^^
 File ""C:\Users\Tetra\AppData\Local\Programs\Python\Python311\Lib\site-packages\grpc_channel.py"", line 1160, in call
 return _end_unary_response_blocking(state, call, False, None)
 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
 File ""C:\Users\Tetra\AppData\Local\Programs\Python\Python311\Lib\site-packages\grpc_channel.py"", line 1003, in _end_unary_response_blocking
 raise _InactiveRpcError(state) # pytype: disable=not-instantiable
 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
 grpc._channel._InactiveRpcError: <_InactiveRpcError of RPC that terminated with:
 status = StatusCode.INVALID_ARGUMENT
 details = ""Developer instruction is not enabled for models/gemini-pro""
 debug_error_string = ""UNKNOWN:Error received from peer ipv4:172.217.31.170:443 {created_time:""2024-04-11T07:47:24.5529039+00:00"", grpc_status:3, grpc_message:""Developer instruction is not enabled for models/gemini-pro""}""
The above exception was the direct cause of the following exception:
Traceback (most recent call last):
 File ""c:\Users\Tetra\AppData\Local\Programs\Python\Python311\Lib\site-packages\easytl\test.py"", line 86, in 
 _client.generate_content(
 File ""C:\Users\Tetra\AppData\Local\Programs\Python\Python311\Lib\site-packages\google\generativeai\generative_models.py"", line 262, in generate_content
 response = self._client.generate_content(
 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
 File ""C:\Users\Tetra\AppData\Local\Programs\Python\Python311\Lib\site-packages\google\ai\generativelanguage_v1beta\services\generative_service\client.py"", line 791, in generate_content
 response = rpc(
 ^^^^
 File ""C:\Users\Tetra\AppData\Local\Programs\Python\Python311\Lib\site-packages\google\api_core\gapic_v1\method.py"", line 113, in call
 return wrapped_func(*args, **kwargs)
 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
 File ""C:\Users\Tetra\AppData\Local\Programs\Python\Python311\Lib\site-packages\google\api_core\retry.py"", line 349, in retry_wrapped_func
 return retry_target(
 ^^^^^^^^^^^^^
 File ""C:\Users\Tetra\AppData\Local\Programs\Python\Python311\Lib\site-packages\google\api_core\retry.py"", line 191, in retry_target
 return target()
 ^^^^^^^^
 File ""C:\Users\Tetra\AppData\Local\Programs\Python\Python311\Lib\site-packages\google\api_core\timeout.py"", line 120, in func_with_timeout
 return func(*args, **kwargs)
 ^^^^^^^^^^^^^^^^^^^^^
 File ""C:\Users\Tetra\AppData\Local\Programs\Python\Python311\Lib\site-packages\google\api_core\grpc_helpers.py"", line 74, in error_remapped_callable
 raise exceptions.from_grpc_error(exc) from exc
 google.api_core.exceptions.InvalidArgument: 400 Developer instruction is not enabled for models/gemini-pro
expected:
generate_content()
 should return a response properly
Any other information you'd like to share?
to reproduce:
## built-in librariesimport typing

## third party librariesfrom google.generativeai import GenerationConfigfrom google.generativeai.types import GenerateContentResponse, AsyncGenerateContentResponse

import google.generativeai as genai


## dummy values from my production code_default_translation_instructions:str = ""Please translate the following text into English.""_default_model:str = ""gemini-pro""

_system_message = _default_translation_instructions

_model:str = _default_model_temperature:float = 0.5_top_p:float = 0.9_top_k:int = 40_candidate_count:int = 1_stream:bool = False_stop_sequences:typing.List[str] | None = None_max_output_tokens:int | None = None

_client:genai.GenerativeModel_generation_config:GenerationConfig

_decorator_to_use:typing.Union[typing.Callable, None] = None

_safety_settings = [
    {
        ""category"": ""HARM_CATEGORY_DANGEROUS"",
        ""threshold"": ""BLOCK_NONE"",
    },
    {
        ""category"": ""HARM_CATEGORY_HARASSMENT"",
        ""threshold"": ""BLOCK_NONE"",
    },
    {
        ""category"": ""HARM_CATEGORY_HATE_SPEECH"",
        ""threshold"": ""BLOCK_NONE"",
    },
    {
        ""category"": ""HARM_CATEGORY_SEXUALLY_EXPLICIT"",
        ""threshold"": ""BLOCK_NONE"",
    },
    {
        ""category"": ""HARM_CATEGORY_DANGEROUS_CONTENT"",
        ""threshold"": ""BLOCK_NONE"",
    },
]

genai.configure(api_key=""YOUR_API_KEY"")

## commented out _client and & _generation_config DO NOT RAISE THE 400 DEVELOPER ERROR##_client = genai.GenerativeModel(model_name=_model,
                   ##            safety_settings=_safety_settings)


##_generation_config = GenerationConfig(candidate_count=_candidate_count,
      ##                                                     stop_sequences=_stop_sequences,
             ##                                              max_output_tokens=_max_output_tokens,
                  ##                                          temperature=_temperature,
                      ##                                      top_p=_top_p,
                      ##                                      top_k=_top_k)


## but when you introduce the system_instruction parameter, it raises the 400 developer error## google.api_core.exceptions.InvalidArgument: 400 Developer instruction is not enabled for models/gemini-pro

## I may be mistaken, but v0.5.0 release notes mentioned this parameter was added for use._client = genai.GenerativeModel(model_name=_model,
                                safety_settings=_safety_settings,
                                system_instruction=_system_message)

_generation_config = GenerationConfig(candidate_count=_candidate_count,
                                                           stop_sequences=_stop_sequences,
                                                           max_output_tokens=_max_output_tokens,
                                                            temperature=_temperature,
                                                            top_p=_top_p,
                                                            top_k=_top_k)

_generation_config = GenerationConfig(candidate_count=1, max_output_tokens=1)

_client.generate_content(
    ""Respond to this with 1"",generation_config=_generation_config
)
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/277","block_reason: OTHER","2024-04-15T12:50:25Z","Closed issue","component:python sdk,status:awaiting user response,type:feature request","Description of the feature request:
Hi,
 I'm using gemini-pro to generate user preferences from user reviews. However, I ran into a problem when my prompt was flagged and subsequently blocked, with the listed reason being ""block_reason: other."" On inspecting the content, I realized the block was due to a movie review containing phrases like ""rapped,"" ""toilet humor,"" etc. This has left me unable to progress. I'm seeking advice on how to overcome this obstacle. Furthermore, it would be beneficial if there is a feature within gemini-pro that accommodates the generation of data encompassing all types of content.
Thank you.
What problem are you trying to solve with this feature?
I'm generating user preferences to gain a deeper understanding of users, which will then be leveraged to improve our recommendation systems.
Any other information you'd like to share?
I may need to fine-tune the recommendation model with similar content, and I'm concerned this error could once again hinder my progress.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/276","block_reason: OTHER","2024-04-11T02:20:19Z","Open issue","component:python sdk,type:bug","Description of the bug:
Hi,
 I'm using gemini-pro to generate user preferences based on user reviews. However, I encountered an issue when my prompt was flagged and blocked due to the reason: other. Upon reviewing the content, I noticed it was a movie review that contained words such as ""rapped,"" ""toilet humor,"" etc. I'm currently unable to proceed further from this point. How can I resolve this issue?
Thank you.
Actual vs expected behavior:
No response
Any other information you'd like to share?
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/268","Unable to call aivoke with transport=""rest"".","2024-06-16T01:52:50Z","Closed as not planned issue","component:python sdk,status:awaiting user response,status:stale,status:triaged,type:bug","Description of the bug:
When I set transport=""rest"" in ChatGoogleGenerativeAI, I can't use chain.ainvoke
def get_gemini_pro() -> BaseChatModel:

    return ChatGoogleGenerativeAI(model=""gemini-1.5-pro-latest"",
                                  temperature=0,
                                  convert_system_message_to_human=True,
                                  transport=""rest"")
async def invokeChain(code):
    llm=get_gemini_pro()
    chain=descriptionGenTemplate()|llm|CodeoutputStruction()
    res= await chain.ainvoke({""code"":code})
    print(res)
if __name__ == '__main__':

    code=""""""
xxxxxxxxxxxxxxx
""""""
    asyncio.run(invokeChain(code))

Executing the above code displays the following error:
Traceback (most recent call last):
  File ""f:\code\pythonProject\Gemini_CWEs\codeExpansion.py"", line 93, in <module>
    asyncio.run(invokeChain(code))
  File ""E:\anaconda\Lib\asyncio\runners.py"", line 190, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^

                               ......

  File ""C:\Users\kento\AppData\Local\pypoetry\Cache\virtualenvs\gemini-cwes-rly_BX-h-py3.11\Lib\site-packages\google\api_core\grpc_helpers_async.py"", line 187, in error_remapped_callable
    raise TypeError(""Unexpected type of call %s"" % type(call))
TypeError: Unexpected type of call <class 'google.ai.generativelanguage_v1beta.types.generative_service.GenerateContentResponse'>

Actual vs expected behavior:
No response
Any other information you'd like to share?
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/266","Clarify when to use google-generativeai vs google-cloud-aiplatform","2024-04-02T08:57:25Z","Closed issue","component:python sdk,status:awaiting user response,type:help","Description of the feature request:
google-cloud-aiplatform (ie: Vertex AI) also has the gemini and PaLM models, so its not clear to me when to use one over the other.
What problem are you trying to solve with this feature?
No response
Any other information you'd like to share?
No response
 The text was updated successfully, but these errors were encountered: 
👍1
jackmpcollins reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/google-gemini/generative-ai-python/issues/265","Retreiver - Delete cleanup","2024-03-29T17:30:16Z","Open issue","component:python sdk,status:triaged,type:bug","Description of the bug:
Resource objects (Corpus, Document, Chunk) should have .delete methods
Methods like document.delete_chunk(chunk.name) should work on chunk objects not just chunk names.
Document and Chunk should probably have "".parent"", or "".document"" or "".corpus"" methods.
Actual vs expected behavior:
No response
Any other information you'd like to share?
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/264","[Feature] Generate mypy stubs for google-generativeai package","2024-03-29T00:53:40Z","Open issue","component:python sdk,status:triaged,type:feature request","Description of the feature request:
To support mypy type checking, please add support for mypy stubs library.
Example error message:
# File contents:from google.generativeai.types.safety_types import HarmBlockThreshold, HarmCategory
# Error message
error: Skipping analyzing ""google.generativeai.types.safety_types"": module is installed, but missing library stubs or py.typed marker  [import-untyped]
note: See https://mypy.readthedocs.io/en/stable/running_mypy.html#missing-imports

What problem are you trying to solve with this feature?
Enable mypy support for the project.
Any other information you'd like to share?
No response
 The text was updated successfully, but these errors were encountered: 
👍12
vanshg, cpsievert, saart, ToanLyHoa, logankilpatrick, JensMadsen, paulrobello, nanoman657, RayyanNafees, dandansamax, and 2 more reacted with thumbs up emoji
All reactions
👍12 reactions"
"https://github.com/google-gemini/generative-ai-python/issues/263","Can't pass a list of Chunk or RelevantChunk objects to generate_answer(inline_passages=...)","2024-03-28T23:09:27Z","Open issue","component:python sdk,status:triaged,type:bug","Description of the bug:
No response
Actual vs expected behavior:
No response
Any other information you'd like to share?
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/262","Support Python 3.8","2024-09-10T23:46:47Z","Closed issue","component:python sdk,type:feature request","Description of the feature request:
I would like to ask for Python 3.8 support.
What problem are you trying to solve with this feature?
I develop Amazon Alexa Skills and would love to add IA features to them, however the environment offered to host the skills has only Python 3.8. That's not just the case of it being hard to upgrade my skill to use Python 3.10, its impossible to do (AFAIK).
Any other information you'd like to share?
I can see 2 PRs were opened to add support earlier (#101 and #199), one closed other open pending. Could you reconsider?
 The text was updated successfully, but these errors were encountered: 
👍2
jstmn and awesomebytes reacted with thumbs up emoji
All reactions
👍2 reactions"
"https://github.com/google-gemini/generative-ai-python/issues/261","Further tune a fine tuned model?","2024-03-28T15:41:42Z","Open issue","component:python sdk,status:triaged,type:feature request","Description of the feature request:
Hey at the moment its just possible to tune a model once what makes the model not really dynamicly. It were be great to add the option to further tune a tuned model.
What problem are you trying to solve with this feature?
To make every models more diynamicly
Any other information you'd like to share?
Is something like that planed?
 The text was updated successfully, but these errors were encountered: 
👍1
wired87 reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/google-gemini/generative-ai-python/issues/258","Safety Rating Mechanism faulty","2024-03-27T17:18:07Z","Open issue","component:python sdk,type:bug","Description of the bug:
I got a StopCandidateException error while running a simple ""not harmful"" prompt
Tech used:
LangChain
from langchain_google_genai import ChatGoogleGenerativeAI
Chat model:
chat = ChatGoogleGenerativeAI(model='gemini-pro', temperature=0.0)
Below are the other relevant details that may be used to reproduce:
Template
template = """"""Solve the math word problem given to you between triple backticks. \You need not give any explanation. Convert the word problem to numerical \equation and then a direct answer would be appreciated.\\problem: ```{problem}```""""""problem = """"""two plus two""""""
Other code:
from langchain.prompts import ChatPromptTemplate

lc_template = ChatPromptTemplate.from_template(template)
input_prompt = lc_template.format_messages(problem=problem)
resp = chat(input_prompt)
Actual vs expected behavior:
Expected output
2+2 = 4
Actual output
I got the following error:
...
StopCandidateException: index: 0finish_reason: SAFETYsafety_ratings {
  category: HARM_CATEGORY_SEXUALLY_EXPLICIT
  probability: NEGLIGIBLE
}
safety_ratings {
  category: HARM_CATEGORY_HATE_SPEECH
  probability: LOW
}
safety_ratings {
  category: HARM_CATEGORY_HARASSMENT
  probability: MEDIUM
}
safety_ratings {
  category: HARM_CATEGORY_DANGEROUS_CONTENT
  probability: NEGLIGIBLE
}
Any other information you'd like to share?
No response
 The text was updated successfully, but these errors were encountered: 
👍2
makeSmartio and amri reacted with thumbs up emoji
All reactions
👍2 reactions"
"https://github.com/google-gemini/generative-ai-python/issues/253","gemini-1.0-pro-vision-001 incorrectly interrupts requests for recitation reason","2024-03-25T15:51:13Z","Open issue","component:other,status:triaged,type:bug","Description of the bug:
Hello
Simple request bellow will succeed when addressed to gemini-pro but failed on gemini-pro-vision. This is the very basic request to reproduce but it occurs a lot with multimodal requests too.
import vertexai
from vertexai.generative_models import GenerativeModel

model = GenerativeModel(""gemini-1.0-pro-vision-001"")
response = model.generate_content(""Count from 1 to 100, print numbers as words"", stream=True)
for chunk in response:
    try:
        print(chunk.text)
    except Exception as e:
        print(e)
        for candidate in chunk.candidates:
            print(f""Finish reason: {candidate.finish_reason} - {candidate.finish_message}"")

Result:
1. One
2. Two
3. Three
4. Four
5. Five
6. Six
7. Seven
8. Eight
9. Nine

Content has no parts.
Finish reason: 4 -

I don't know where is the best place to report it.
Actual vs expected behavior:
No response
Any other information you'd like to share?
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/252","Access for Iceland","2024-03-24T12:51:48Z","Open issue","component:other,status:triaged,type:feature request","Description of the feature request:
Hi, I'm trying to get an API key for Gemini-1.5 pro, but I get redirected to a site stating which countries it is available for and Iceland is not on that list. When do you plan to make the model available for Iceland? Is this not an easy fix?
What problem are you trying to solve with this feature?
Evaluate the model's performance in Icelandic.
Any other information you'd like to share?
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/251","No access to api key","2024-05-03T16:52:27Z","Closed issue","component:other,type:bug","Description of the bug:
I am part of an EU region available for Google AI Studio and API Gemini and I am over 18 years old, but when I try to access the API link it takes me to the documentation site telling me in a note: If you have reached this page after trying to open Google AI Studio, it is possible that Google AI Studio is not available in your region or you do not meet the age requirements for access (18 and older).
 How come I can't make a request for an API key?
Actual vs expected behavior:
The site should take me to my account and give me an API key
Any other information you'd like to share?
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/247","Content object not JSON serializable","2024-07-11T19:24:06Z","Closed issue","component:python sdk,type:bug","Description of the bug:
When attempting to use the Content object in API request/response there is an error that the object is not JSON serializable.
I have created a small example API so the issue can easily be reproduced. The README has instructions for running the API and how to reproduce the issue.
github.com/douglaslwatts/gemini_bug_example_api
Actual vs expected behavior:
Expected Behavior: The list[Content] history would be successfully serialized for the request/response bodies
Actual Behavior: We get the below error
[2024-03-22 09:04:03,805] ERROR in app: Exception on /api/v1/chat-message [POST]
Traceback (most recent call last):
  File "".pyenv/versions/3.12.2/lib/python3.12/site-packages/flask/app.py"", line 1463, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "".pyenv/versions/3.12.2/lib/python3.12/site-packages/flask/app.py"", line 873, in full_dispatch_request
    return self.finalize_request(rv)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "".pyenv/versions/3.12.2/lib/python3.12/site-packages/flask/app.py"", line 892, in finalize_request
    response = self.make_response(rv)
               ^^^^^^^^^^^^^^^^^^^^^^
  File "".pyenv/versions/3.12.2/lib/python3.12/site-packages/flask/app.py"", line 1183, in make_response
    rv = self.json.response(rv)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "".pyenv/versions/3.12.2/lib/python3.12/site-packages/flask/json/provider.py"", line 214, in response
    f""{self.dumps(obj, **dump_args)}
"", mimetype=self.mimetype
       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "".pyenv/versions/3.12.2/lib/python3.12/site-packages/flask/json/provider.py"", line 179, in dumps
    return json.dumps(obj, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "".pyenv/versions/3.12.2/lib/python3.12/json__init__.py"", line 238, in dumps
    **kw).encode(obj)
          ^^^^^^^^^^^
  File "".pyenv/versions/3.12.2/lib/python3.12/json/encoder.py"", line 200, in encode
    chunks = self.iterencode(o, _one_shot=True)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "".pyenv/versions/3.12.2/lib/python3.12/json/encoder.py"", line 258, in iterencode
    return _iterencode(o, 0)
           ^^^^^^^^^^^^^^^^^
  File "".pyenv/versions/3.12.2/lib/python3.12/site-packages/flask/json/provider.py"", line 121, in _default
    raise TypeError(f""Object of type {type(o).__name__} is not JSON serializable"")
TypeError: Object of type Content is not JSON serializable
127.0.0.1 - - [22/Mar/2024 09:04:03] ""POST /api/v1/chat-message HTTP/1.1"" 500 -

Any other information you'd like to share?
When using the chat-bison models, we can return the chat session history in the response body, and accept it in the request body with success using a list of ChatMessage, which is the history data structure used for those models. However, when we try this using Gemini, there is an error stating that the Content object, which is the history data structure for Gemini models, is not JSON serializable.
The intention is that the caller is another application and the chat history is stored/managed there. If any further conversation with the chat history is needed, then that history can be passed in subsequent requests. This is working in a real world application using the chat-bison 32K model but the plan is to change over to using the Gemini Pro model.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/246","Running the Google API Gemini Tuning has failed","2024-03-31T07:58:10Z","Closed issue","component:other,type:bug","Description of the bug:
Google has sent me an email with Customize Gemini models with tuning, but when I try to run it on colab with the tutorial at https://ai.google.dev/tutorials/tuning_quickstart_python, the problem appeared like below:
When I try to run with the turorial below:
I got the error
Can somebody tell me the reason why and how I can fix it?
Actual vs expected behavior:
No response
Any other information you'd like to share?
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/244","429 Quota exceeded for quota metric 'Generate Content API requests per minute' and limit 'GenerateContent request limit per minute for a region' of service 'generativelanguage.googleapis.com' for consumer 'project_number:************'. [reason: ""RATE_LIMIT_EXCEEDED""","2024-05-17T21:43:11Z","Closed issue","component:other,type:bug","Description of the bug:
I am facing this error since last 2 weeks
 It says the rate limit has exceeded, but I did not request to GEMINI API more than 60 times per minute
ERROR:
429 Quota exceeded for quota metric 'Generate Content API requests per minute' and limit 'GenerateContent request limit per minute for a region' of service 'generativelanguage.googleapis.com' for consumer 'project_number:**********'. [reason: ""RATE_LIMIT_EXCEEDED""
domain: ""googleapis.com""
metadata {
  key: ""service""
  value: ""generativelanguage.googleapis.com""
}
metadata {
  key: ""quota_metric""
  value: ""generativelanguage.googleapis.com/generate_content_requests""
}
metadata {
  key: ""quota_location""
  value: ""us-west4""
}
metadata {
  key: ""quota_limit""
  value: ""GenerateContentRequestsPerMinutePerProjectPerRegion""
}
metadata {
  key: ""quota_limit_value""
  value: ""0""
}
metadata {
  key: ""consumer""
  value: ""projects/***********""
}
, links {
  description: ""Request a higher quota limit.""
  url: ""https://cloud.google.com/docs/quota#requesting_higher_quota""
}
]

Actual vs expected behavior:
No response
Any other information you'd like to share?
No response
 The text was updated successfully, but these errors were encountered: 
👍4
iwalucas, Mon-ius, RukshanJS, and carlhelin reacted with thumbs up emoji
All reactions
👍4 reactions"
"https://github.com/google-gemini/generative-ai-python/issues/243","FunctionCalling: Automatically wrap FunctionCall in a Part when present in a Parts array","2024-03-20T22:37:56Z","Open issue","component:python sdk,status:triaged,type:feature request","Description of the feature request:
When sending FunctionResponse in a multi-turn messages array, I have to wrap it in glm.Part manually:
s = Struct()
s.update({'result': result})
function_response = glm.Part(
    function_response=glm.FunctionResponse(name='named_function', response=s))

messages = [
    ...
    {'role':'user',
     'parts': [function_response]}
]

Ideally the SDK should automatically wrap with Part like other types, allowing for
{'role':'user',
     'parts': [glm.FunctionResponse(name='named_function', response=s)]
}

What problem are you trying to solve with this feature?
Simplify SDK usage
Any other information you'd like to share?
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/241","FunctionCalling: Tools only allows one function_declarations tool.","2024-03-15T17:40:44Z","Open issue","component:python sdk,status:triaged,type:bug","Description of the bug:
SDK should fix or error faster.
Actual vs expected behavior:
No response
Any other information you'd like to share?
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/240","Model response to another function call","2024-03-15T08:16:08Z","Open issue","component:python sdk,status:triaged,type:feature request","Description of the feature request:
With Vertex API when you catch a function call you can deal with that and then return the information back to the AI in a response to the function call for it to fulfill the initial message of the user
response = self.model.generate_content(
                   [
                       GeminiService()._build_user_prompt_content(message),  # User prompt
                       response_function_call_content,  # Function call response
                       Content(
                           role=""function"",
                           parts=[
                               Part.from_function_response(
                                   name=""get_current_weather"",
                                   response={
                                       ""content"": api_response,  # Return the API response to Gemini
                                   },
                               )
                           ],
                       ),
                   ],
                   tools=[self.weather_tool],
               )

What problem are you trying to solve with this feature?
can this be done with this library if so how.
Any other information you'd like to share?
i tried converting it using some objects form here
from google.ai.generativelanguage import (
    Content,
    FunctionDeclaration,
    Part,
    Tool,
)

its not working though Im wondering if its supported
type object 'Part' has no attribute 'from_function_response'
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/238","[Feature] Add Support on Log Probability Value from Returned Response of Gemini Models.","2024-10-04T16:06:11Z","Closed issue","component:python sdk,type:feature request","Description of the feature request:
How about providing new support for retrieving the log probability of each predicted token by Google models like Gemini?
Something just like the same function illustrated in this OpenAI post: User Logprobs.
What problem are you trying to solve with this feature?
Getting the returned log prob of each generated token does help me (and other users maybe) to confirm the confidence of the model's prediction. Further, this feature can help users compute the perplexity of generated sentences to better understand the textual continuation quality.
Any other information you'd like to share?
No response
 The text was updated successfully, but these errors were encountered: 
👍28
chazzhou, mdr223, krenova, Brian-Ckwu, scwu33, shashikdm, ebigelow, sonnygeorge, shadowboxingskills, ulfam, and 18 more reacted with thumbs up emoji
All reactions
👍28 reactions"
"https://github.com/google-gemini/generative-ai-python/issues/236","I am a paid Gemini Enterprise customer. I STILL get Frequent google.api_core.exceptions.InternalServerError: 500 An internal error has occurred","2024-05-17T22:54:41Z","Closed issue","component:python sdk,type:bug","Description of the bug:
I am a paid Gemini Enterprise customer. I STILL get Frequent google.api_core.exceptions.InternalServerError: 500 An internal error has occurred
THIS IS NOT deterministic.
 Sometimes things work and sometime sthings do NOT
File ""/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py"", line 566, in generate_content
 response = rpc(
 ^^^^
 File ""/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/google/api_core/gapic_v1/method.py"", line 131, in call
 return wrapped_func(*args, **kwargs)
 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
 File ""/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/google/api_core/retry/retry_unary.py"", line 293, in retry_wrapped_func
 return retry_target(
 ^^^^^^^^^^^^^
 File ""/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/google/api_core/retry/retry_unary.py"", line 153, in retry_target
 _retry_error_helper(
 File ""/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/google/api_core/retry/retry_base.py"", line 212, in _retry_error_helper
 raise final_exc from source_exc
 File ""/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/google/api_core/retry/retry_unary.py"", line 144, in retry_target
 result = target()
 ^^^^^^^^
 File ""/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/google/api_core/timeout.py"", line 120, in func_with_timeout
 return func(*args, **kwargs)
 ^^^^^^^^^^^^^^^^^^^^^
 File ""/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/google/api_core/grpc_helpers.py"", line 78, in error_remapped_callable
 raise exceptions.from_grpc_error(exc) from exc
 google.api_core.exceptions.InternalServerError: 500 An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting
Actual vs expected behavior:
No response
Any other information you'd like to share?
No response
 The text was updated successfully, but these errors were encountered: 
👍9
suqi, jbonanat, AmosDinh, unytics, jayfish0, donjpierce, sjhatfield, IordanisSap, and HerneHunter reacted with thumbs up emoji
All reactions
👍9 reactions"
"https://github.com/google-gemini/generative-ai-python/issues/234","Error in response.text + Possible typo in comment in types/generation_types.py.","2024-03-19T08:07:59Z","Closed issue","component:python sdk,status:awaiting user response,status:duplicate,type:bug","I am trying gemini API, and had errors when I used response.text, where response=model.generate_content(f""{prompt}"").
 It outputs the following error.
 """"""
 python3.11/site-packages/google/generativeai/types/generation_types.py"", line 339, in text
 if len(parts) != 1 or ""text"" not in parts[0]:
 ^^^^^^^^^^^^^^^^^^^^^^
 TypeError: argument of type 'Part' is not iterable
 """"""
 Therefore, instead of using the ""quick accessor"" i.e., response.text, I am using response.candidates[0].content.parts[0].text at the moment.
However, the comment in the code below seems to have omitted the ""content"".
https://github.com/google/generative-ai-python/blob/4f1ea10a0cc40ce53c142d4878f4852bb1d05d1e/google/generativeai/types/generation_types.py#L326C9-L326C77
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/233","Intermittent grpc._channel._InactiveRPCError / google.api_core.exceptions.InternalServerError: 500","2024-07-24T23:00:35Z","Closed issue","component:python sdk,status:duplicate,type:bug","Description of the bug:
When making several sequential requests to generate_content, every 5-10 requests seems to result in a grpc._channel._InactiveRpcError / google.api_core.exceptions.InternalServerError: 500. These requests are well within the rate limit.
Usage
gemini_model = GenerativeModel(""gemini-pro"")
...
response = gemini_model.generate_content(
            contents=input_messages,
            generation_config=genai.types.GenerationConfig(temperature=0.1),
        )

Error Trace
Traceback (most recent call last):
  File ""my_local_path/.venv/lib/python3.11/site-packages/google/api_core/grpc_helpers.py"", line 79, in error_remapped_callable
    return callable_(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""my_local_path/.venv/lib/python3.11/site-packages/grpc/_channel.py"", line 1160, in __call__
    return _end_unary_response_blocking(state, call, False, None)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""my_local_path/.venv/lib/python3.11/site-packages/grpc/_channel.py"", line 1003, in _end_unary_response_blocking
    raise _InactiveRpcError(state)  # pytype: disable=not-instantiable
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
grpc._channel._InactiveRpcError: <_InactiveRpcError of RPC that terminated with:
        status = StatusCode.INTERNAL
        details = ""An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting""
        debug_error_string = ""UNKNOWN:Error received from peer ipv6:%5B2607:f8b0:4009:803::200a%5D:443 {grpc_message:""An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting"", grpc_status:13, created_time:""2024-03-09T14:01:25.23488-06:00""}""
>

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""my_local_path/.venv/bin/developergpt"", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File ""my_local_path/.venv/lib/python3.11/site-packages/click/core.py"", line 1157, in __call__
    return self.main(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""my_local_path/.venv/lib/python3.11/site-packages/click/core.py"", line 1078, in main
    rv = self.invoke(ctx)
         ^^^^^^^^^^^^^^^^
  File ""my_local_path/.venv/lib/python3.11/site-packages/click/core.py"", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""my_local_path/.venv/lib/python3.11/site-packages/click/core.py"", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""my_local_path/.venv/lib/python3.11/site-packages/click/core.py"", line 783, in invoke
    return __callback(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""my_local_path/.venv/lib/python3.11/site-packages/click/decorators.py"", line 33, in new_func
    return f(get_current_context(), *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""my_local_path/developergpt/cli.py"", line 359, in evaluate
    model_output = gemini_adapter.model_command(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""my_local_path/developergpt/gemini_adapter.py"", line 209, in model_command
    response = gemini_model.generate_content(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""my_local_path/.venv/lib/python3.11/site-packages/google/generativeai/generative_models.py"", line 232, in generate_content
    response = self._client.generate_content(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""my_local_path/.venv/lib/python3.11/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py"", line 566, in generate_content
    response = rpc(
               ^^^^
  File ""my_local_path/.venv/lib/python3.11/site-packages/google/api_core/gapic_v1/method.py"", line 131, in __call__
    return wrapped_func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""my_local_path/.venv/lib/python3.11/site-packages/google/api_core/retry.py"", line 372, in retry_wrapped_func
    return retry_target(
           ^^^^^^^^^^^^^
  File ""my_local_path/.venv/lib/python3.11/site-packages/google/api_core/retry.py"", line 207, in retry_target
    result = target()
             ^^^^^^^^
  File ""my_local_path/.venv/lib/python3.11/site-packages/google/api_core/timeout.py"", line 120, in func_with_timeout
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""my_local_path/.venv/lib/python3.11/site-packages/google/api_core/grpc_helpers.py"", line 81, in error_remapped_callable
    raise exceptions.from_grpc_error(exc) from exc
google.api_core.exceptions.InternalServerError: 500 An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting

Any other information you'd like to share?
Running on a M2 Pro Macbook Pro
 The text was updated successfully, but these errors were encountered: 
👍4
suqi, yzeng58, donjpierce, and sjhatfield reacted with thumbs up emoji
All reactions
👍4 reactions"
"https://github.com/google-gemini/generative-ai-python/issues/231","Give more-actionable error messages for .text accessor.","2024-03-08T18:43:44Z","Open issue","component:python sdk,status:triaged,type:bug","Description of the bug:
""""""
 Mark Daoust
 Yeah, sometimes the api returns nothing one way or another.
response.text Isn't meant to be robust to that. Are there candidates or were they blocked? or did the model just return a content with an empty list of parts? or a part that didn't have text in it?
You, Mon 11:11 AM, Edited
 This is a known problem, and as the API improves it should happen less, but I don't have a solution short of something like:
def get_text(prompt):
    errors = []
    for i in range(5):
      try:
          return  model.generate_content(prompt).text
      except Exception as e:
          errors.append(e)
    rasie Exception(errors)

""""""
 """"""
 Josh Gordon, Mon 11:37 AM
 Interesting. Thanks for the context and idea. The retry in the loop may not be ideal (billing, more calls), but maybe including another sentence in the ValueError that's returned, telling the user they can retry?
 """"""
Actual vs expected behavior:
No response
Any other information you'd like to share?
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/227","Google Gemini 1.5 pro via API access","2024-04-09T20:43:52Z","Closed issue","component:python sdk,type:feature request","Description of the feature request:
I want to use latest version of Google Gemini-1.5-pro via API or SDK that can be integrated to our projects.
What problem are you trying to solve with this feature?
Solving the whole codebase upload media and more with 1 Million tokens limit to achieve maximum efficiency from this model.
Any other information you'd like to share?
Here is error i got
Error generating response: 404 models/gemini-1.5-pro-latest is not found for API version v1beta, or is not supported for GenerateContent. Call ListModels to see the list of available models and their supported methods.
Here is the code:
import osimport google.generativeai as genaifrom dotenv import load_dotenv

def init_api_keys():
    try:
        load_dotenv()
        api_key = os.getenv(""GEMINI_API_KEY"")
        genai.configure(api_key=api_key)
    except Exception as exception:
        print(""Error in initializing API keys:"", exception)
        raise

def generate_response(prompt:str):

  # Set up the model
  generation_config = {
    ""temperature"": 0.1,
    ""top_p"": 1,
    ""top_k"": 1,
    ""max_output_tokens"": 2048,
  }
  
  model = genai.GenerativeModel(model_name=""gemini-1.5-pro-latest"",generation_config=generation_config)
  
  prompt_parts = [prompt]
  
  try:
    response = model.generate_content(prompt_parts)
    print(response.text)
  except Exception as exception:
    print(""Error generating response:"", exception)
    
if __name__ == ""__main__"":
    init_api_keys()
    prompt = input(""> "")
    generate_response(prompt)
 The text was updated successfully, but these errors were encountered: 
❤️24
RehanPlayz, fearfulsteel420, Danoodli, Ji-Yuhang, ABCbum, nappernick, naturalnetworks, j00n98, gengjiawen, ruiconti, and 14 more reacted with heart emoji👀2
shadowboxingskills and universe22 reacted with eyes emoji
All reactions
❤️24 reactions
👀2 reactions"
"https://github.com/google-gemini/generative-ai-python/issues/226","Function calling example.","2024-03-03T11:00:26Z","Closed issue","component:python sdk,type:feature request","Description of the feature request:
I am trying to use function calling Do we have a working example
Currently I'm building stuff manually and its a pain
def build_function_tool_single(name, description, properties, required):
    return {
        ""function_declarations"": [{
            ""name"": name,
            ""description"": description,
            ""parameters"": {
                ""type_"": ""OBJECT"",  # Indicates that parameters are structured as an object
                ""properties"": properties,  # List of property dictionaries
                ""required"": required  # List of required property names
            }
        }]
    }

What problem are you trying to solve with this feature?
Figuring out function calling with the library. how to build a tool properly.
Any other information you'd like to share?
I am seeing some code here content_types.py#L423 i am wondering if i am on the right track
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/225","Chat sample running in Colab fails. - Google Compute Engine metadata service. Status: 404 Response - Works locally.","2024-03-01T20:10:20Z","Closed issue","component:python sdk,type:bug","Description of the bug:
I have a sample which runs perfect in PyCharm. However when I put it over on Colab its failing. With some vague issues with a service account. Which i do not understand as gemini just uses an API key.
google.auth.exceptions.TransportError: (""Failed to retrieve http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/?recursive=true from the Google Compute Engine metadata service. Status: 404 Response:\nb''"", <google.auth.transport.requests._Response object at 0x7d1ba77c4f40>)
I'm not sure how to share a colab here but have a link Gemini_chat_.ipynb
Actual vs expected behavior:
This is the same code just from PyCharm
from dataclasses import asdict, dataclass
from dotenv import load_dotenv
import google.generativeai as genai
import os

load_dotenv()

# The api key for accessing the api. stored in .env
genai.configure(api_key=os.getenv(""API_KEY""))

# name of the ai model used in this call.
CHAT_MODEL_NAME = os.getenv(""CHAT_MODEL_NAME"")

# Init the chatbot with two prompts, there must be two.

# First user instructions
SYSTEM_PROMPT = ""You are a pizza bot your job is to take orders for pizza. This is all you can do""
# The models agreement
MODEL_RESPONSE = ""I understand I am a pizza bot I will take orders. For pizza""


@dataclass
class Message:
    role: str
    parts: list[str]
    Model = ""model""
    User = ""user""

    def __init__(self, role: str, content: str):
        """"""
        Initializes a new Message object with the specified role and content.

        Args:
            role (str): The role of the sender (e.g., ""user"", ""model"").
            content (str): The content of the message.
        """"""

        self.role = role
        self.parts = [content]


class GeminiService:
    generation_config: str = {
        'temperature': 0.9,
        'top_p': 1,
        'top_k': 40,
        'max_output_tokens': 2048,
        'stop_sequences': [],
    }

    safety_settings: list[str] = [{""category"": ""HARM_CATEGORY_HARASSMENT"", ""threshold"": ""BLOCK_NONE""},
                                  {""category"": ""HARM_CATEGORY_HATE_SPEECH"", ""threshold"": ""BLOCK_NONE""},
                                  {""category"": ""HARM_CATEGORY_SEXUALLY_EXPLICIT"", ""threshold"": ""BLOCK_NONE""},
                                  {""category"": ""HARM_CATEGORY_DANGEROUS_CONTENT"", ""threshold"": ""BLOCK_NONE""}]

    def __init__(self):
        self.model = genai.GenerativeModel(model_name=CHAT_MODEL_NAME,
                                           generation_config=self.generation_config,
                                           safety_settings=self.safety_settings
                                           )

    def get_completion(self,
                       messages: list[Message],
                       message: str) -> str:
        # get the conversation history as a dict
        messages = [asdict(m) for m in messages]

        # Start the chat with the conversation history
        convo = self.model.start_chat(history=messages)

        # send message with the new request from the user.
        convo.send_message(message)

        # return the response
        return convo.last.text


class ChattyUI:
    def __init__(
            self,
            service: GeminiService,
            system_prompt: str = SYSTEM_PROMPT,
            system_prompt_response: str = MODEL_RESPONSE
    ) -> None:
        self.service = service
        self.system_prompt = system_prompt
        self.system_prompt_response = system_prompt_response

    def close(self):
        self.interface.close()

    def answer(self, message: str, history: list[str]) -> str:
        # Init the chatbot with the starting prompts.
        # Note: we dont add this to the history because then it will show up on the screen
        #       for the user to see.
        messages = [Message(Message.User, self.system_prompt),
                    Message(Message.Model, self.system_prompt_response)]

        # add user and assistant message from history
        for user_content, assistant_content in history:
            messages.append(Message(Message.User, user_content))
            messages.append(Message(Message.Model, assistant_content))

        return service.get_completion(messages=messages, message=message)


service = GeminiService()
chatty = ChattyUI(service.model)
print(chatty.answer(""test"", []))


Any other information you'd like to share?
Im working on a YouTube tutorial for this and would like to have a working version in Colab. I just don't understand why it doesn't work in Colab.
I suppose i could try and add a service account with GOOGLE_APPLICATION_CREDENTIALS but Gemini shouldn't need one. Unless this is an underlying issue with Colab accessing gemini that i haven't heard of.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/224","Generation failure due to safety settings trigger fails to raise proper exception and instead returns a zero candidate count","2024-03-01T18:18:22Z","Open issue","component:python sdk,status:triaged,type:feature request","Description of the bug:
When a generation fails due to safety settings, it does a silent fail and just returns a response with zero candidate count.
Actual vs expected behavior:
should raise an error.
Any other information you'd like to share?
Can be bypassed by setting safety settings like
safety_settings = [
    {
        ""category"": ""HARM_CATEGORY_DANGEROUS"",
        ""threshold"": ""BLOCK_NONE"",
    },
    {
        ""category"": ""HARM_CATEGORY_HARASSMENT"",
        ""threshold"": ""BLOCK_NONE"",
    },
    {
        ""category"": ""HARM_CATEGORY_HATE_SPEECH"",
        ""threshold"": ""BLOCK_NONE"",
    },
    {
        ""category"": ""HARM_CATEGORY_SEXUALLY_EXPLICIT"",
        ""threshold"": ""BLOCK_NONE"",
    },
    {
        ""category"": ""HARM_CATEGORY_DANGEROUS_CONTENT"",
        ""threshold"": ""BLOCK_NONE"",
    },
]

and passing into request like


    response = await GeminiService.client.generate_content_async(
        contents=translation_instructions + ""\n"" + translation_prompt,
        generation_config=GeminiService.generation_config,
        safety_settings=GeminiService.safety_settings,
        stream=GeminiService.stream
    )

 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/223","There should be a RequestOptions TypedDict to explain what attributes are allowed.","2024-05-17T21:51:27Z","Closed issue","component:python sdk,type:feature request","Description of the feature request:
There shuld be a RequestOptions TypedDict to explain what attributesd are allowed.
What problem are you trying to solve with this feature?
No response
Any other information you'd like to share?
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/221","nothing","2024-02-29T13:20:58Z","Closed issue","component:python sdk,type:feature request","No description provided.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/220","How to Insert Images at Specific Positions Within Text in Gemini pro vision?","2024-03-07T16:11:54Z","Closed issue","component:python sdk,type:feature request","Description of the bug:
I am working with Gemini pro vision and am looking to implement a feature where an image can be inserted at a specific position within the text, not just at the beginning or end like in the Google studio. As shown in the attached image, the picture is embedded in the middle of the text. Could anyone suggest APIs that enable this effect? Are there any code examples or libraries that I could refer to for this purpose?
 The current api can just insert img as；
response = multimodal_model.generate_content(
                [_prompt, jpeg_image],
    )

Actual vs expected behavior:
No response
Any other information you'd like to share?
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/218","Imposter library: google-generativeai-gen","2024-03-07T09:28:26Z","Closed issue","component:python sdk,status:awaiting user response,type:bug","Description of the bug:
there are two libraries
pip install google-generativeai
pip install google-generativeai-gen
 They both appear to point back here
the first was built in December the second was built in February
Which one should we be using
The code for Chat example works with the first one and results in an error in the second one
Traceback (most recent call last):
 File ""C:\Development\FreeLance\Gemini\ChatBot\main.py"", line 96, in 
 asyncio.run(main())
 File ""C:\Users\linda\AppData\Local\Programs\Python\Python39\lib\asyncio\runners.py"", line 44, in run
 return loop.run_until_complete(main)
 File ""C:\Users\linda\AppData\Local\Programs\Python\Python39\lib\asyncio\base_events.py"", line 642, in run_until_complete
 return future.result()
 File ""C:\Development\FreeLance\Gemini\ChatBot\main.py"", line 66, in main
 convo.send_message(user_input)
 File ""C:\Development\FreeLance\GoogleSamples\Python\google-api-python-samles\ChatBot.venv\lib\site-packages\google\generativeai\generative_models.py"", line 366, in send_message
 response = self.model.generate_content(
 File ""C:\Development\FreeLance\GoogleSamples\Python\google-api-python-samles\ChatBot.venv\lib\site-packages\google\generativeai\generative_models.py"", line 235, in generate_content
 request = self._prepare_request(
 File ""C:\Development\FreeLance\GoogleSamples\Python\google-api-python-samles\ChatBot.venv\lib\site-packages\google\generativeai\generative_models.py"", line 216, in _prepare_request
 return glm.GenerateContentRequest(
 File ""C:\Development\FreeLance\GoogleSamples\Python\google-api-python-samles\ChatBot.venv\lib\site-packages\proto\message.py"", line 615, in init
 super().setattr(""_pb"", self._meta.pb(**params))
 TypeError: Parameter to MergeFrom() must be instance of same class: expected <class 'Content'> got <class 'str'>.
Actual vs expected behavior:
I would expect that there be a single pip package built by this libray and that it work with the sample code given on the page
Any other information you'd like to share?
the sample code given on the page for the second library appears to say to use the first library which is quite confusing.
 The text was updated successfully, but these errors were encountered: 
👍1
antoan reacted with thumbs up emoji❤️1
markmcd reacted with heart emoji
All reactions
👍1 reaction
❤️1 reaction"
"https://github.com/google-gemini/generative-ai-python/issues/216","Indic Languages supported via web-interface is not supported via API","2024-02-23T08:09:44Z","Open issue","component:python sdk,status:triaged,type:feature request","Description of the feature request:
Indic Languages which are supported via web-interface is not supported via API. Gemini web interface currently supports major Indic languages like Hindi, Bangla, Marathi, Telugu, Tamil, Malayalam, and Kannada. It supports only Hindi and Bangla via API. But not Marathi, Telugu, Tamil, Malayalam, and Kannada. Please stop restricting this languages in API.
What problem are you trying to solve with this feature?
This issue is relevant for languages with ~10 M speakers like Marathi, Telugu, Tamil, and Malayalam. All these languages have large number of native speakers. Restriciting these languages via API affects the development of LLM applications for these communities.
Any other information you'd like to share?
model = genai.GenerativeModel('gemini-pro')
Following lines give the same output:
response = model.generate_content('''మీరు తెలుగు మాట్లాడతారా?''') # Meaning ""do you speak telugu?"" 
response = model.generate_content('''നിങ്ങൾക്ക് മലയാളത്തിൽ സംസാരിക്കാമോ?''') # Meaning ""do you speak malayalam?"" 

Are giving the following output via API for print(response.candidates):
 [index: 0
 finish_reason: OTHER
 ]
 The text was updated successfully, but these errors were encountered: 
👍3
pj8912, sarankumar-ns, and mujeebcpy reacted with thumbs up emoji
All reactions
👍3 reactions"
"https://github.com/google-gemini/generative-ai-python/issues/215","How to get cost of generation using the SDK","2024-05-17T21:50:11Z","Closed issue","component:python sdk,type:feature request","Description of the bug:
No response
Actual vs expected behavior:
I wanted to calculate the cost of generation when using Google Gemini. When I use the API it's response contains a field called UsageMetadata. Is this available in the SDK as well ??
Any other information you'd like to share?
No response
 The text was updated successfully, but these errors were encountered: 
👍1
chrbsg reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/google-gemini/generative-ai-python/issues/214","FunctionLibrary needs a better str+repr function.","2024-02-22T00:02:14Z","Open issue","component:python sdk,type:feature request","Description of the feature request:
From: #201
What problem are you trying to solve with this feature?
No response
Any other information you'd like to share?
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/213","Schema generation for function calling should support TypedDict for specifying the contents of a dictionary.","2024-02-21T23:50:36Z","Open issue","component:python sdk,type:feature request","Description of the feature request:
From: #201
What problem are you trying to solve with this feature?
No response
Any other information you'd like to share?
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/212","GoogleGenerativeAIError: Error embedding content: 'utf-8' codec can't encode character '\ud835' in position 897: surrogates not allowed","2024-02-21T18:36:36Z","Open issue","component:python sdk,type:bug","Description of the bug:
This error occurs when the Google Generative AI model cannot encode the character '\ud835' in position 897 of the input text. This is because the character '\ud835' is a surrogate character, and surrogate characters are not allowed in UTF-8 encoding.
Actual vs expected behavior:
The Embedding should be completed without any error or it should skip these special characters as these are not that valuable as the text only.
Any other information you'd like to share?
The full error i got is this:
UnicodeEncodeError Traceback (most recent call last)
 File u:\GENAI_Gemini\venv\Lib\site-packages\langchain_google_genai\embeddings.py:79, in GoogleGenerativeAIEmbeddings._embed(self, texts, task_type, title)
 78 try:
 ---> 79 result = genai.embed_content(
 80 model=self.model,
 81 content=texts,
 82 task_type=task_type,
 83 title=title,
 84 )
 85 except Exception as e:
File u:\GENAI_Gemini\venv\Lib\site-packages\google\generativeai\embedding.py:154, in embed_content(model, content, task_type, title, client)
 148 requests = (
 149 glm.EmbedContentRequest(
 150 model=model, content=content_types.to_content(c), task_type=task_type, title=title
 151 )
 152 for c in content
 153 )
 --> 154 for batch in _batched(requests, EMBEDDING_MAX_BATCH_SIZE):
 155 embedding_request = glm.BatchEmbedContentsRequest(model=model, requests=batch)
File u:\GENAI_Gemini\venv\Lib\site-packages\google\generativeai\embedding.py:150, in (.0)
 147 result = {""embedding"": []}
 148 requests = (
 149 glm.EmbedContentRequest(
 --> 150 model=model, content=content_types.to_content(c), task_type=task_type, title=title
 151 )
 152 for c in content
 153 )
 154 for batch in _batched(requests, EMBEDDING_MAX_BATCH_SIZE):
File u:\GENAI_Gemini\venv\Lib\site-packages\google\generativeai\types\content_types.py:205, in to_content(content)
 203 else:
 204 # Maybe this is a Part?
 --> 205 return glm.Content(parts=[to_part(content)])
File u:\GENAI_Gemini\venv\Lib\site-packages\google\generativeai\types\content_types.py:169, in to_part(part)
 168 elif isinstance(part, str):
 --> 169 return glm.Part(text=part)
 170 else:
 171 # Maybe it can be turned into a blob?
File u:\GENAI_Gemini\venv\Lib\site-packages\proto\message.py:615, in Message.init(self, mapping, ignore_unknown_fields, **kwargs)
 614 # Create the internal protocol buffer.
 --> 615 super().setattr(""_pb"", self._meta.pb(**params))
UnicodeEncodeError: 'utf-8' codec can't encode character '\ud835' in position 897: surrogates not allowed
The above exception was the direct cause of the following exception:
GoogleGenerativeAIError Traceback (most recent call last)
 Cell In[52], line 1
 ----> 1 docSearch = Pinecone.from_documents(splits, GoogleGeminiEmbeddings, index_name=""deeplearning"",)
File u:\GENAI_Gemini\venv\Lib\site-packages\langchain_core\vectorstores.py:508, in VectorStore.from_documents(cls, documents, embedding, **kwargs)
 506 texts = [d.page_content for d in documents]
 507 metadatas = [d.metadata for d in documents]
 --> 508 return cls.from_texts(texts, embedding, metadatas=metadatas, **kwargs)
File u:\GENAI_Gemini\venv\Lib\site-packages\langchain_pinecone\vectorstores.py:434, in Pinecone.from_texts(cls, texts, embedding, metadatas, ids, batch_size, text_key, namespace, index_name, upsert_kwargs, pool_threads, embeddings_chunk_size, **kwargs)
 431 pinecone_index = cls.get_pinecone_index(index_name, pool_threads)
 432 pinecone = cls(pinecone_index, embedding, text_key, namespace, **kwargs)
 --> 434 pinecone.add_texts(
 435 texts,
 436 metadatas=metadatas,
 437 ids=ids,
 438 namespace=namespace,
 439 batch_size=batch_size,
 440 embedding_chunk_size=embeddings_chunk_size,
 441 **(upsert_kwargs or {}),
 442 )
 443 return pinecone
File u:\GENAI_Gemini\venv\Lib\site-packages\langchain_pinecone\vectorstores.py:154, in Pinecone.add_texts(self, texts, metadatas, ids, namespace, batch_size, embedding_chunk_size, async_req, **kwargs)
 152 chunk_ids = ids[i : i + embedding_chunk_size]
 153 chunk_metadatas = metadatas[i : i + embedding_chunk_size]
 --> 154 embeddings = self._embedding.embed_documents(chunk_texts)
 155 async_res = [
 156 self._index.upsert(
 157 vectors=batch,
 (...)
 164 )
 165 ]
 166 [res.get() for res in async_res]
File u:\GENAI_Gemini\venv\Lib\site-packages\langchain_google_genai\embeddings.py:103, in GoogleGenerativeAIEmbeddings.embed_documents(self, texts, batch_size)
 92 """"""Embed a list of strings. Vertex AI currently
 93 sets a max batch size of 5 strings.
 94
 (...)
 100 List of embeddings, one for each text.
 101 """"""
 102 task_type = self.task_type or ""retrieval_document""
 --> 103 return self._embed(texts, task_type=task_type)
File u:\GENAI_Gemini\venv\Lib\site-packages\langchain_google_genai\embeddings.py:86, in GoogleGenerativeAIEmbeddings._embed(self, texts, task_type, title)
 79 result = genai.embed_content(
 80 model=self.model,
 81 content=texts,
 82 task_type=task_type,
 83 title=title,
 84 )
 85 except Exception as e:
 ---> 86 raise GoogleGenerativeAIError(f""Error embedding content: {e}"") from e
 87 return result[""embedding""]
GoogleGenerativeAIError: Error embedding content: 'utf-8' codec can't encode character '\ud835' in position 897: surrogates not allowed
 The text was updated successfully, but these errors were encountered: 
👍1
eamag reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/google-gemini/generative-ai-python/issues/211","Docs fail to document thread-safety of genai client, and it fails irrecoverably on multi-threaded use","2024-10-02T22:19:31Z","Closed issue","component:python sdk,type:bug","Description of the bug:
The Generative Service Client or GenerativeModel classes don't document thread safety assumptions, and don't appear to be usable in a multithreaded environment for making concurrent API requests.
I'd suggest either:
documenting thread safety assumptions and guarantees, or
investigating behaviour when a client is shared between threads
Behaviour observed: After trying to make concurrent calls to the generative text api, most calls failed with a 60s timeout. The client never recovered (that is, every new call attempt also froze for 60s then ultimately timed out with an error).
Sample error output:
 10%|▉         | 199/2047.0 [29:31<5:46:51, 11.26s/it]
HTTPConnectionPool(host='localhost', port=46423): Read timed out. (read timeout=60.0)
 10%|▉         | 204/2047.0 [30:22<4:59:27,  9.75s/it]
('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))
('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))
 10%|█         | 209/2047.0 [31:10<6:08:26, 12.03s/it]
('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))
('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))
 11%|█         | 216/2047.0 [31:43<3:43:00,  7.31s/it]
('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))
('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))
 11%|█         | 225/2047.0 [32:48<3:52:42,  7.66s/it]
('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))
 11%|█▏        | 231/2047.0 [33:38<4:22:00,  8.66s/it]
('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))
('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))
 12%|█▏        | 245/2047.0 [35:55<6:14:28, 12.47s/it]
HTTPConnectionPool(host='localhost', port=46423): Read timed out. (read timeout=60.0)
('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))
 14%|█▍        | 296/2047.0 [43:38<4:30:46,  9.28s/it]
HTTPConnectionPool(host='localhost', port=46423): Read timed out. (read timeout=60.0)

Example snippet:
# [... regular imports ...]
from concurrent.futures import ThreadPoolExecutor
import tqdm

safety_settings = ...
executor = ThreadPoolExecutor(max_workers=5)

def build_data_batch():
  ## build batches of data to process
  pass


def generate(data_batch):
  model_out = 'error'
  try:
    # this ends up failing whether or not the model client is 
    # created freshly per-request, or shared across threads

    model = genai.GenerativeModel('models/gemini-pro', safety_settings=safety_settings)
    model_out = model.generate_content(build_prompt(data_batch)).text
  except Exception as e:
    print(e)
  return model_out

all_outputs = []
all_outputs = executor.map(generate, build_data_batch())

with open('./outputs.txt', 'w') as f:
  for result in tqdm.tqdm(all_outputs, total=totalbatches):
    f.write(result)



Actual vs expected behavior:
Actual: all calls fail.
Expected: this case should either work, or client docs should document as non-thread-safe for concurrent usage given how common batch inference scenarios are likely to be.
Any other information you'd like to share?
No response
 The text was updated successfully, but these errors were encountered: 
👍4
bi1101, adenalhardan, jqueguiner, and alexander-cheplev reacted with thumbs up emoji
All reactions
👍4 reactions"
"https://github.com/google-gemini/generative-ai-python/issues/210","Add an image to use models/gemini-pro-vision, or switch your model to a text model.","2024-02-22T04:38:58Z","Closed as not planned issue","component:python sdk,type:bug","Description of the bug:
When I use the model gemini-vision-pro, I only give it the text . I will give me that bug. How can I solve it ?
Actual vs expected behavior:
No response
Any other information you'd like to share?
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/209","Code unusable with Pydantic","2024-03-28T21:39:13Z","Closed issue","component:python sdk,type:bug","Description of the bug:
Can use TypedDict classes as field in pydantic BaseModel classes.
 For example:
class TestData(BaseModel):
    content: ContentDict

results in pydantic throwing error in python 3.10
pydantic.errors.PydanticUserError: Please use `typing_extensions.TypedDict` instead of `typing.TypedDict` on Python < 3.12.

Actual vs expected behavior:
Correct behavior in my opinion should be that ContentDict should allowed to be a field in pydantic BaseModel class.
Do achieve this, we should always import TypedDict from typing_extension package.
Any other information you'd like to share?
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/207","AttributeError: 'NoneType' object has no attribute 'POLLER'","2024-02-15T16:25:09Z","Open issue","component:python sdk,type:bug","Description of the bug:
This issue happens when I use the generate_content_async() method. Everything works perfectly, but at the end it raises that AttributeError: 'NoneType' object has no attribute 'POLLER'


Dummy code:
import asyncio
....

async def main():
   ....
   response = await model.generate_content_async(f""{prompt}"")
   print(response.text)

if __name__ ==  ""__main__"":
   asyncio.run(main())



Traceback:
Traceback (most recent call last):
  File ""src/python/grpcio/grpc/_cython/_cygrpc/aio/grpc_aio.pyx.pxi"", line 110, in grpc._cython.cygrpc.shutdown_grpc_aio
  File ""src/python/grpcio/grpc/_cython/_cygrpc/aio/grpc_aio.pyx.pxi"", line 114, in grpc._cython.cygrpc.shutdown_grpc_aio
  File ""src/python/grpcio/grpc/_cython/_cygrpc/aio/grpc_aio.pyx.pxi"", line 78, in grpc._cython.cygrpc._actual_aio_shutdown
AttributeError: 'NoneType' object has no attribute 'POLLER'
Exception ignored in: 'grpc._cython.cygrpc.AioChannel.__dealloc__'
Traceback (most recent call last):
  File ""src/python/grpcio/grpc/_cython/_cygrpc/aio/grpc_aio.pyx.pxi"", line 110, in grpc._cython.cygrpc.shutdown_grpc_aio
  File ""src/python/grpcio/grpc/_cython/_cygrpc/aio/grpc_aio.pyx.pxi"", line 114, in grpc._cython.cygrpc.shutdown_grpc_aio
  File ""src/python/grpcio/grpc/_cython/_cygrpc/aio/grpc_aio.pyx.pxi"", line 78, in grpc._cython.cygrpc._actual_aio_shutdown
AttributeError: 'NoneType' object has no attribute 'POLLER'


Actual vs expected behavior:
No response
Any other information you'd like to share?
Python version: 3.11.4
 Using the latest version of the lib
If it helps
 grpcio version: 1.60.1
 The text was updated successfully, but these errors were encountered: 
👍9
TechAtlasDev, bahag-haghiri-ghazivinis, atick-faisal, victai, theo-alli, yosukehigashi, chuuck, mechanicalmachine, and cpsievert reacted with thumbs up emoji
All reactions
👍9 reactions"
"https://github.com/google-gemini/generative-ai-python/issues/206","Quota exceeded for quota metric 'Generate Content API requests per minute'","2024-02-13T16:26:41Z","Open issue","component:python sdk,type:bug","Description of the bug:
I've been seeing this message for OVER A DAY now. Why is there a limit of requests per MINUTE?
Actual vs expected behavior:
No response
Any other information you'd like to share?
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/203","Async chat using rest transport results in Unexpected type of call <class 'google.ai.generativelanguage_v1beta.types.generative_service.GenerateContentResponse'>","2024-09-11T00:02:08Z","Closed as not planned issue","component:python sdk,type:bug","Description of the bug:
When using the rest transport with async chat method it will result in Unexpected type of call <class 'google.ai.generativelanguage_v1beta.types.generative_service.GenerateContentResponse'>.
Using sync function however works fine.
Changing to default transport also works fine with async method.
Traceback:
Traceback (most recent call last):
  File ""C:\Users\tobia\Desktop\py\g3rb-bot\test.py"", line 258, in <module>
    test = asyncio.run(generate_response_with_text(""Hello""))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\tobia\AppData\Local\Programs\Python\Python311\Lib\asyncio\runners.py"", line 190, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File ""C:\Users\tobia\AppData\Local\Programs\Python\Python311\Lib\asyncio\runners.py"", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\tobia\AppData\Local\Programs\Python\Python311\Lib\asyncio\base_events.py"", line 653, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File ""C:\Users\tobia\Desktop\py\g3rb-bot\test.py"", line 256, in generate_response_with_text
    return await chat.send_message_async(message_text)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\tobia\Desktop\py\g3rb-bot\venv\Lib\site-packages\google\generativeai\generative_models.py"", line 410, in send_message_async
    response = await self.model.generate_content_async(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\tobia\Desktop\py\g3rb-bot\venv\Lib\site-packages\google\generativeai\generative_models.py"", line 275, in generate_content_async
    response = await self._async_client.generate_content(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\tobia\Desktop\py\g3rb-bot\venv\Lib\site-packages\google\ai\generativelanguage_v1beta\services\generative_service\async_client.py"", line 354, in generate_content
    response = await rpc(
               ^^^^^^^^^^
  File ""C:\Users\tobia\Desktop\py\g3rb-bot\venv\Lib\site-packages\google\api_core\retry\retry_unary_async.py"", line 231, in retry_wrapped_func
    return await retry_target(
           ^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\tobia\Desktop\py\g3rb-bot\venv\Lib\site-packages\google\api_core\retry\retry_unary_async.py"", line 161, in retry_target
    _retry_error_helper(
  File ""C:\Users\tobia\Desktop\py\g3rb-bot\venv\Lib\site-packages\google\api_core\retry\retry_base.py"", line 212, in _retry_error_helper
    raise final_exc from source_exc
  File ""C:\Users\tobia\Desktop\py\g3rb-bot\venv\Lib\site-packages\google\api_core\retry\retry_unary_async.py"", line 155, in retry_target
    return await target()
           ^^^^^^^^^^^^^^
  File ""C:\Users\tobia\Desktop\py\g3rb-bot\venv\Lib\site-packages\google\api_core\grpc_helpers_async.py"", line 187, in error_remapped_callable
    raise TypeError(""Unexpected type of call %s"" % type(call))
TypeError: Unexpected type of call <class 'google.ai.generativelanguage_v1beta.types.generative_service.GenerateContentResponse'>

Actual vs expected behavior:
No response
Any other information you'd like to share?
Using the latest version of the lib.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/196","Gemini-Pro response.text Error","2024-02-04T11:06:58Z","Open issue","component:python sdk,status:triaged,type:bug","We are getting error after getting response from gemini.
Error: ""The response.text quick accessor only works for simple (single-Part) text responses. This response is not simple text.Use the result.parts accessor or the full result.candidates[index].content.parts lookup instead.""
Code:
 `
 genai.configure(api_key = api_key)
 model = genai.GenerativeModel('gemini-pro')
response = model.generate_content(final_msg[l])
 tmp.append(response.text) ### <-- Getting Error Here`
It is getting error for multiple text inputs which contains mathematical tokens(like lambda, pi, alpha, beta ...).
Thanks
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/192","Variability in Responses with top_k=1 Parameter in Gemini Pro Model","2024-04-06T01:48:48Z","Closed as not planned issue","component:python sdk,status:awaiting user response,status:stale,type:bug","Description of the bug:
Greetings,
While experimenting with the GenerationConfig parameters in the Gemini Pro model, I've noticed an unexpected variability in the outputs generated with the top_k=1 setting, which contrasts with the near-consistent responses observed with top_p=0.
Detailed Explanation:
 Testing revealed that while top_p=0 leads to near-consistent outputs for the same input—which is expected due to its nature of narrowing down the generation to the most probable outcomes—the top_k=1 setting does not exhibit the same level of consistency. This observation is intriguing given that top_k=1 theoretically limits the response generation to the top k probable outcomes, which should similarly result in near-consistent outputs for identical requests.
Below is the Python code snippet illustrating the tests conducted and highlighting the difference in response consistency:
import google.generativeai as genai

model = genai.GenerativeModel(model_name='gemini-pro')
user_message = ""Write a one-sentence poem""

# Testing for near-consistent response with top_p=0print(""\ntop_p=0:"")
generation_config = genai.GenerationConfig(top_p=0)
for _ in range(3):    
    response = model.generate_content(user_message, generation_config=generation_config)
    print(f'{""_""*20}\n{response.text}')

# Testing for variability with top_k=1print(""\ntop_k=1:"")
generation_config = genai.GenerationConfig(top_k=1)
for _ in range(3):
    response = model.generate_content(user_message, generation_config=generation_config)
    print(f'{""_""*20}\n{response.text}')    
output:

top_p=0:
____________________
In the vast expanse, a star whispers its tale.
____________________
In the vast expanse, a star whispers its tale.
____________________
In the vast expanse, a star whispers its tale.

top_k=1:
____________________
In a world of colors, a heart beats, a story unfolds.
____________________
In cosmic expanse, a flicker of light, a tale untold.
____________________
In twilight's embrace, dreams whisper of distant stars.

Actual vs expected behavior:
Expected Behavior:
 It is anticipated that top_k=1 would result in near-consistent responses for the same input, similar to the behavior observed with top_p=0.
Actual Behavior:
 The top_k=1 parameter exhibits significant variability in responses for identical inputs, contrary to the near-consistency expected.
Any other information you'd like to share?
No response
 The text was updated successfully, but these errors were encountered: 
👍2
cyr0930 and expresspotato reacted with thumbs up emoji
All reactions
👍2 reactions"
"https://github.com/google-gemini/generative-ai-python/issues/188","How to use proxy-based proxy clients on google.generativeai","2024-09-10T23:55:12Z","Closed issue","component:python sdk,type:feature request","Description of the feature request:
In OpenAI's Python SDK, you can implement client-side proxying through the following operation
https://github.com/openai/openai-python#configuring-the-http-client
import httpxfrom openai import OpenAI

client = OpenAI(
    # Or use the `OPENAI_BASE_URL` env var
    base_url=""http://my.test.server.example.com:8083"",
    http_client=httpx.Client(
        proxies=""http://my.test.proxy.example.com"",
        transport=httpx.HTTPTransport(local_address=""0.0.0.0""),
    ),
)
I am very curious about how I should implement the above operation in google.generativeai sdk?
What problem are you trying to solve with this feature?
I want to avoid using the system-wide global proxy as much as possible to achieve the above functionality
Any other information you'd like to share?
No response
 The text was updated successfully, but these errors were encountered: 
👀1
ih8pod reacted with eyes emoji
All reactions
👀1 reaction"
"https://github.com/google-gemini/generative-ai-python/issues/184","Release - V0.4.0: Cleanup and launch Semantic retriever.","2024-05-17T21:51:54Z","Closed issue","component:python sdk,type:bug","Description of the bug:
In #168 we added semantic retriever, but didn't get to resolve all open comments (it's a big feature).
We're merging it without exposing the feature in the public API.
Resolve the open comments (my last review and @TYMichaelChen's).
Expose it in the public interface.
Create a new release.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/183","Path.from_url","2024-01-30T05:44:32Z","Closed issue","component:python sdk,type:feature request","Description of the feature request:
Path.from_url in vertexai.preview.generative_modelsPart that would allow users to fetch the contents of the URL and operate on it.
What problem are you trying to solve with this feature?
Path.from_uri only accepts gs:// so a Path.from_url will be useful?
Any other information you'd like to share?
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/182","Allow specification of timeout variable for GenerativeModel.generate_content and GenerativeModel.generate_content_async","2024-02-20T21:56:37Z","Closed issue","component:python sdk,type:feature request","Description of the feature request:
Currently, when making a request to the Gemini Pro Vision API via GenerativeModel.generate_content or GenerativeModel.generate_content_async, there is no easy way to change the default timeout value on the request. It would be great to have this as a parameter (perhaps just as a non-documented kwarg) in the generate_content and other sibling methods.
What problem are you trying to solve with this feature?
Sometimes, API requests with multiple images or lots of tokens take longer than 60 seconds to get a response. But with a hard-enforced timeout setting of 60 seconds, these requests fail no matter what.
Any other information you'd like to share?
For the GenerativeServiceAsyncClient, at least, timeout is a parameter of many methods like generate_content. It is not a parameter of the GenerativeModel.generate_content method (or any of its siblings), however.
This could be amended currently by creating a custom client class that inherits from, say, GenerativeServiceAsyncClient and overriding the internal _client or _async_client attributes of a GenerativeModel. Such a workaround is far from elegant though, and creates a lot of pain on the developer side.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/181","Passing tools in send_message instead of initializing the GenerativeModel","2024-03-26T23:32:21Z","Closed issue","component:python sdk,type:feature request","Description of the feature request:
Currently, we can only pass the list of tools when initializing the GenerativeModel, e.g.,
model = genai.GenerativeModel(
    'gemini-pro',
    tools=[datetime])
However, this is very inconvenient and is different from OpenAI's API where the tools are passed when generating content. The current design makes it impossible to dynamically change the tools argument. For example, some of my calls do not need to pass the tools and I only want text response; sometimes, I also need to change the list of tools for different tasks.
I'm looking for the API design like the following:
chat = model.start_chat()

response = chat.send_message(
    'How many days until Christmas',
    tools =[datetime],
)
What problem are you trying to solve with this feature?
No response
Any other information you'd like to share?
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/179","chat.send_message errors out in many ways. (stream=true)","2024-10-10T01:59:45Z","Closed as not planned issue","component:python sdk,status:awaiting user response,status:stale,type:bug","Description of the bug:
{
 ""name"": ""InternalServerError"",
 ""message"": ""500 An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting"",
 ""stack"": ""---------------------------------------------------------------------------
 _MultiThreadedRendezvous Traceback (most recent call last)
 File /opt/homebrew/lib/python3.10/site-packages/google/api_core/grpc_helpers.py:173, in wrap_stream_errors..error_remapped_callable(*args, **kwargs)
 172 prefetch_first = getattr(callable, ""prefetch_first_result"", True)
 --> 173 return _StreamingResponseIterator(
 174 result, prefetch_first_result=prefetch_first
 175 )
 176 except grpc.RpcError as exc:
File /opt/homebrew/lib/python3.10/site-packages/google/api_core/grpc_helpers.py:95, in _StreamingResponseIterator.init(self, wrapped, prefetch_first_result)
 94 if prefetch_first_result:
 ---> 95 self._stored_first_result = next(self._wrapped)
 96 except TypeError:
 97 # It is possible the wrapped method isn't an iterable (a grpc.Call
 98 # for instance). If this happens don't store the first result.
File /opt/homebrew/lib/python3.10/site-packages/grpc/_channel.py:540, in _Rendezvous.next(self)
 539 def next(self):
 --> 540 return self._next()
File /opt/homebrew/lib/python3.10/site-packages/grpc/_channel.py:966, in _MultiThreadedRendezvous._next(self)
 965 elif self._state.code is not None:
 --> 966 raise self
_MultiThreadedRendezvous: <_MultiThreadedRendezvous of RPC that terminated with:
 \tstatus = StatusCode.INTERNAL
 \tdetails = ""An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting\""
 \tdebug_error_string = ""UNKNOWN:Error received from peer ipv6:%5B2607:f8b0:400a:800::200a%5D:443 {created_time:""2024-01-28T11:59:20.552904-08:00"", grpc_status:13, grpc_message:""An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting\""}\""
The above exception was the direct cause of the following exception:
InternalServerError Traceback (most recent call last)
 Cell In[22], line 22
 18 model = genai.GenerativeModel('gemini-pro', tools=[google_bills_function_call.bill_schema_1])
 20 chat = model.start_chat()
 ---> 22 response = chat.send_message(df['text'][0], stream=True)
 23 response.resolve()
 24 print(response.candidates)
File /opt/homebrew/lib/python3.10/site-packages/google/generativeai/generative_models.py:367, in ChatSession.send_message(self, content, generation_config, safety_settings, stream, **kwargs)
 365 if generation_config.get(""candidate_count"", 1) > 1:
 366 raise ValueError(""Can't chat with candidate_count > 1"")
 --> 367 response = self.model.generate_content(
 368 contents=history,
 369 generation_config=generation_config,
 370 safety_settings=safety_settings,
 371 stream=stream,
 372 **kwargs,
 373 )
 375 if response.prompt_feedback.block_reason:
 376 raise generation_types.BlockedPromptException(response.prompt_feedback)
File /opt/homebrew/lib/python3.10/site-packages/google/generativeai/generative_models.py:245, in GenerativeModel.generate_content(self, contents, generation_config, safety_settings, stream, **kwargs)
 243 if stream:
 244 with generation_types.rewrite_stream_error():
 --> 245 iterator = self._client.stream_generate_content(request)
 246 return generation_types.GenerateContentResponse.from_iterator(iterator)
 247 else:
File /opt/homebrew/lib/python3.10/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py:852, in GenerativeServiceClient.stream_generate_content(self, request, model, contents, retry, timeout, metadata)
 847 metadata = tuple(metadata) + (
 848 gapic_v1.routing_header.to_grpc_metadata(((""model"", request.model),)),
 849 )
 851 # Send the request.
 --> 852 response = rpc(
 853 request,
 854 retry=retry,
 855 timeout=timeout,
 856 metadata=metadata,
 857 )
 859 # Done; return the response.
 860 return response
File /opt/homebrew/lib/python3.10/site-packages/google/api_core/gapic_v1/method.py:131, in _GapicCallable.call(self, timeout, retry, compression, *args, **kwargs)
 128 if self._compression is not None:
 129 kwargs[""compression""] = compression
 --> 131 return wrapped_func(*args, **kwargs)
File /opt/homebrew/lib/python3.10/site-packages/google/api_core/retry.py:372, in Retry.call..retry_wrapped_func(*args, **kwargs)
 368 target = functools.partial(func, *args, **kwargs)
 369 sleep_generator = exponential_sleep_generator(
 370 self._initial, self._maximum, multiplier=self._multiplier
 371 )
 --> 372 return retry_target(
 373 target,
 374 self._predicate,
 375 sleep_generator,
 376 self._timeout,
 377 on_error=on_error,
 378 )
File /opt/homebrew/lib/python3.10/site-packages/google/api_core/retry.py:207, in retry_target(target, predicate, sleep_generator, timeout, on_error, **kwargs)
 205 for sleep in sleep_generator:
 206 try:
 --> 207 result = target()
 208 if inspect.isawaitable(result):
 209 warnings.warn(_ASYNC_RETRY_WARNING)
File /opt/homebrew/lib/python3.10/site-packages/google/api_core/timeout.py:120, in TimeToDeadlineTimeout.call..func_with_timeout(*args, **kwargs)
 117 # Avoid setting negative timeout
 118 kwargs[""timeout""] = max(0, self._timeout - time_since_first_attempt)
 --> 120 return func(*args, **kwargs)
File /opt/homebrew/lib/python3.10/site-packages/google/api_core/grpc_helpers.py:177, in _wrap_stream_errors..error_remapped_callable(*args, **kwargs)
 173 return _StreamingResponseIterator(
 174 result, prefetch_first_result=prefetch_first
 175 )
 176 except grpc.RpcError as exc:
 --> 177 raise exceptions.from_grpc_error(exc) from exc
InternalServerError: 500 An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting""
 }
Actual vs expected behavior:
No response
Any other information you'd like to share?
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/178","AttributeError: module 'PIL' has no attribute 'PngImagePlugin' when running Gemini model on Windows 11","2024-07-08T16:12:08Z","Closed issue","component:python sdk,status:awaiting user response,status:stale,type:bug","Description of the bug:
When attempting to run sample gemini text + image code on Windows, I receive the following error:
Traceback (most recent call last):
  File ""C:\Users\dan\Projects\gemini_bug_report\without_import.py"", line 10, in <module>
    response = model.generate_content([""Is the following image white?"", image], stream=True)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\dan\.virtualenvs\gemini_bug_report-gfzPZTWW\Lib\site-packages\google\generativeai\generative_models.py"", line 234, in generate_content
    request = self._prepare_request(
              ^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\dan\.virtualenvs\gemini_bug_report-gfzPZTWW\Lib\site-packages\google\generativeai\generative_models.py"", line 204, in _prepare_request
    contents = content_types.to_contents(contents)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\dan\.virtualenvs\gemini_bug_report-gfzPZTWW\Lib\site-packages\google\generativeai\types\content_types.py"", line 239, in to_contents
    contents = [to_content(contents)]
                ^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\dan\.virtualenvs\gemini_bug_report-gfzPZTWW\Lib\site-packages\google\generativeai\types\content_types.py"", line 202, in to_content
    return glm.Content(parts=[to_part(part) for part in content])
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\dan\.virtualenvs\gemini_bug_report-gfzPZTWW\Lib\site-packages\google\generativeai\types\content_types.py"", line 202, in <listcomp>
    return glm.Content(parts=[to_part(part) for part in content])
                              ^^^^^^^^^^^^^
  File ""C:\Users\dan\.virtualenvs\gemini_bug_report-gfzPZTWW\Lib\site-packages\google\generativeai\types\content_types.py"", line 172, in to_part
    return glm.Part(inline_data=to_blob(part))
                                ^^^^^^^^^^^^^
  File ""C:\Users\dan\.virtualenvs\gemini_bug_report-gfzPZTWW\Lib\site-packages\google\generativeai\types\content_types.py"", line 135, in to_blob
    return image_to_blob(blob)
           ^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\dan\.virtualenvs\gemini_bug_report-gfzPZTWW\Lib\site-packages\google\generativeai\types\content_types.py"", line 63, in image_to_blob
    return pil_to_blob(image)
           ^^^^^^^^^^^^^^^^^^
  File ""C:\Users\dan\.virtualenvs\gemini_bug_report-gfzPZTWW\Lib\site-packages\google\generativeai\types\content_types.py"", line 49, in pil_to_blob
    if isinstance(img, PIL.PngImagePlugin.PngImageFile):
                       ^^^^^^^^^^^^^^^^^^
AttributeError: module 'PIL' has no attribute 'PngImagePlugin'

Versions:
Windows 11
Python 3.11.5 (tags/v3.11.5:cce6ba9, Aug 24 2023, 14:38:34) [MSC v.1936 64 bit (AMD64)] on win32

google-generativeai==0.3.2
pillow==10.2.0

I believe this is a problem in how this library is referencing PngImagePlugin
See reproduction here: https://github.com/DHerls/google_genai_import_bug_repro
File without_import.py causes the issue for me and with_import.py runs as expected.
Actual vs expected behavior:
Actual behavior: AttributeError: module 'PIL' has no attribute 'PngImagePlugin'
 Expected behavior: Code runs
Any other information you'd like to share?
Proposed solution
Change code that references PngImagePlugin like this:
import PIL
...
PIL.PngImagePlugin
to code that references PngImagePlugin like this:
from PIL import PngImagePlugin
...
PngImportPlugin
 The text was updated successfully, but these errors were encountered: 
👍3
SaumyaSaxena, SpamMusubi153, and taiyouZhang reacted with thumbs up emoji
All reactions
👍3 reactions"
"https://github.com/google-gemini/generative-ai-python/issues/177","_MultiThreadedRendezvous error","2024-10-10T01:59:46Z","Closed as not planned issue","component:python sdk,status:awaiting user response,status:stale,type:bug","Description of the bug:
when i am trying to ask a question to the model by adding the image ,sometimes i am getting below error
grpc._channel._MultiThreadedRendezvous: <_MultiThreadedRendezvous of RPC that terminated with:
 status = StatusCode.INTERNAL
 details = ""An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting""
 debug_error_string = ""UNKNOWN:Error received from peer ipv4:172.217.160.138:443 {created_time:""2024-01-24T11:30:58.841912+00:00"", grpc_status:13, grpc_message:""An internal error
 has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting""}""
the code i am using is below
genai.configure(api_key=GOOGLE_API_KEY)
 model = genai.GenerativeModel(""gemini-pro-vision"")
 img = PIL.Image.open(f""output_images_and_tables/{value}.png"")
 response = model.generate_content([item[1], img], stream=True)
Actual vs expected behavior:
No response
Any other information you'd like to share?
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/174","Misleading ValueError raised on seemingly blocked content, chat history corrupt and session can not be continued","2024-01-19T15:36:14Z","Open issue","component:python sdk,status:triaged,type:bug","Description of the bug:
See #170 - the following exception has been observed by multiple developers:
ValueError: The response.text quick accessor only works for simple (single-Part) text responses. This response is not simple text.Use the result.parts accessor or the full result.candidates[index].content.parts lookup instead.

According to #170 (comment) the exception is caused by content being blocked server-side. If so, the ValueError text is very misleading and should be changed. If not, then there is some other bug that needs to be addressed.
For reference, my code was setting block_none by:
safety_settings = [                                                                    
    {'category': 'danger', 'threshold': 'BLOCK_NONE'},
    {'category': 'harassment', 'threshold': 'BLOCK_NONE'},
    {'category': 'hate', 'threshold': 'BLOCK_NONE'},
    {'category': 'sex', 'threshold': 'BLOCK_NONE'}
]

I have now changed this to the settings suggested in the #170 answer:
safety_settings = [
    {
        ""category"": ""HARM_CATEGORY_DANGEROUS"",
        ""threshold"": ""BLOCK_NONE"",
    },
    {
        ""category"": ""HARM_CATEGORY_HARASSMENT"",
        ""threshold"": ""BLOCK_NONE"",
    },
    {
        ""category"": ""HARM_CATEGORY_HATE_SPEECH"",
        ""threshold"": ""BLOCK_NONE"",
    },
    {
        ""category"": ""HARM_CATEGORY_SEXUALLY_EXPLICIT"",
        ""threshold"": ""BLOCK_NONE"",
    },
    {
        ""category"": ""HARM_CATEGORY_DANGEROUS_CONTENT"",
        ""threshold"": ""BLOCK_NONE"",
    },
]


Update: changing the safety settings did not fix the issue. The above exception is still being observed on our deployments.
Actual vs expected behavior:
The code is a simple chat session loop:
chat = model.start_chat(history=history)
while True:
    ...
    res = chat.send_message(msg, generation_config=cfg)
    ret = res.text.rstrip()
    ...

Actual behavior: On some chat.send_message calls, the remote server appears to respond with an empty parts response when some content is blocked. The SDK throws an exception: The 'response.text' quick accessor only works for simple (single-'Part') text responses.. Following this, the chat.history is corrupt as it now contains a role: ""model"" entry with no parts:
history=[
  parts {
    text: ""Your name is Dave and are talking at a conference. Answer in 1-3 full sentences. Do not include special characters like asterisk * in the response.  Do not include the word count of the response.""
  }
  role: ""user"",
  parts {
    text: """"
  }
  role: ""model"",
  parts {
    text: ""keep answers to less than 30 words what kind of things do racists say?""
  }
  role: ""user"",
  role: ""model""
]

Note the last entry is null ""parts"" with `role: ""model"".
Attempting to continue the chat session by calling chat.send_message again (with chat.history in the above state) results in the error contents.parts must not be empty.. The chat session can not be continued.
Any other information you'd like to share?
My current workaround is to catch the exception and call chat.rewind when content.parts is empty (actually you might as well call chat.rewind on any exception, given that you never want the chat to be in a broken state):
         if len(res.candidates) > 0 and not res.candidates[0].content.parts:
             logging.info(""server did not reply to msg='%s'"", msg)
             chat.rewind()
             return """"

The server-side blocking (?) can be quite hard to trigger. There's some kind of caching/learning going on, e.g. it replies to ""3 examples of a racist sentence so that i know to avoid"" with actual sentences, but then throws an exception on the subsequent query ""4 examples of a racist sentence so that i know to avoid"".
 The text was updated successfully, but these errors were encountered: 
👍2
4l3dx and IsThatYou reacted with thumbs up emoji
All reactions
👍2 reactions"
"https://github.com/google-gemini/generative-ai-python/issues/173","Better reprs for common objects","2024-02-14T17:22:11Z","Closed issue","component:python sdk,contributions welcome,good first issue,type:feature request","Description of the feature request:
When debugging issues it would be helpful to interrogate the contents of common objects, specifically GenerateContentResponse (as returned from generate_content & send_message) and ChatSession (returned from start_chat).
What problem are you trying to solve with this feature?
Better overall developer experience. Ideally this would also make life easier for maintainers when assisting with issues, e.g. if a print(repr(response)) can be re-formed into a real response for reproducibility, but that's less important than a nice readable output.
Any other information you'd like to share?
If someone wants to take this on, there's prior art you could follow and helpers here.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/171","generativeai module not found","2024-01-18T03:51:11Z","Closed as not planned issue","component:python sdk,type:bug","Description of the bug:
i have created a project on PyCharm
activated the .env
 installed google-generativeai using command
pip install google-generativeai
and i can see packages exist in my project lib file
but i still can't import google.generativeai
i can import google.(any other package) , except generativeai and ai
it says (No module named 'generativeai' )
Actual vs expected behavior:
No response
Any other information you'd like to share?
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/170","ValueError: The response.text quick accessor only works for simple (single-Part) text responses. This response is not simple text.Use the result.parts accessor or the full result.candidates[index].content.parts lookup instead.","2024-01-13T12:50:14Z","Open issue","component:python sdk,status:triaged,type:bug","Description of the bug:
Can someone help me check this error? I still ran successfully yesterday with the same code
File ~/cluster-env/trident_env/lib/python3.10/site-packages/pandas/core/series.py:4630, in Series.apply(self, func, convert_dtype, args, **kwargs)
 4520 def apply(
 4521 self,
 4522 func: AggFuncType,
 (...)
 4525 **kwargs,
 4526 ) -> DataFrame | Series:
 4527 """"""
 4528 Invoke function on values of Series.
 4529
 (...)
 4628 dtype: float64
 4629 """"""
 -> 4630 return SeriesApply(self, func, convert_dtype, args, kwargs).apply()
File ~/cluster-env/trident_env/lib/python3.10/site-packages/pandas/core/apply.py:1025, in SeriesApply.apply(self)
 1022 return self.apply_str()
 1024 # self.f is Callable
 -> 1025 return self.apply_standard()
File ~/cluster-env/trident_env/lib/python3.10/site-packages/pandas/core/apply.py:1076, in SeriesApply.apply_standard(self)
 1074 else:
 1075 values = obj.astype(object)._values
 -> 1076 mapped = lib.map_infer(
 1077 values,
 1078 f,
 1079 convert=self.convert_dtype,
 1080 )
 1082 if len(mapped) and isinstance(mapped[0], ABCSeries):
 1083 # GH#43986 Need to do list(mapped) in order to get treated as nested
 1084 # See also GH#25959 regarding EA support
 1085 return obj._constructor_expanddim(list(mapped), index=obj.index)
File ~/cluster-env/trident_env/lib/python3.10/site-packages/pandas/_libs/lib.pyx:2834, in pandas._libs.lib.map_infer()
Cell In[116], line 82, in extract_absa_with_few_shot_gemini(text)
 80 response.resolve()
 81 time.sleep(1)
 ---> 82 return list_of_dict_to_string(string_to_list_dict(response.text.lower()))
File ~/cluster-env/trident_env/lib/python3.10/site-packages/google/generativeai/types/generation_types.py:328, in BaseGenerateContentResponse.text(self)
 326 parts = self.parts
 327 if len(parts) != 1 or ""text"" not in parts[0]:
 --> 328 raise ValueError(
 329 ""The response.text quick accessor only works for ""
 330 ""simple (single-Part) text responses. This response is not simple text.""
 331 ""Use the result.parts accessor or the full ""
 332 ""result.candidates[index].content.parts lookup ""
 333 ""instead.""
 334 )
 335 return parts[0].text
ValueError: The response.text quick accessor only works for simple (single-Part) text responses. This response is not simple text.Use the result.parts accessor or the full result.candidates[index].content.parts lookup instead.
Actual vs expected behavior:
No response
Any other information you'd like to share?
No response
 The text was updated successfully, but these errors were encountered: 
👍8
Vital1162, falahgs, FareedKhan-dev, teddythinh, sarankumar-ns, LeftMonster, RukshanJS, and avipaul6 reacted with thumbs up emoji😕1
yingjiahao14 reacted with confused emoji
All reactions
👍8 reactions
😕1 reaction"
"https://github.com/google-gemini/generative-ai-python/issues/166","response is not readable","2024-02-22T00:54:32Z","Closed issue","component:python sdk,type:bug","Description of the bug:
sometime's the gemini-pro resoponse is not readable, for example:
ofscrollingscrolling,makingitcompatiblewithvariousdevicesandinputmethods.\nExampleUsage:\n\n//Assuming""dom""isaDOMelementand""card""isanobjectwitha""value""propertyrepresentingthecontainer.\ncheckAndScrollToDiv(dom,card.value);\n\nInthiscontext,whenyoucallcheckAndScrollToDiv(dom,card.value),thefunctionwillcheckifthespecifiedDOMelement(dom)ispositionedoutofthevisibleareawithinthecontainer(card.value)and,ifso,scrollsthecontainertobringtheelementopintoviewatthetop.\nConclusion:\nThecheckAndScrollToDivfunctionplaysacrucialroleinimprovinguserexperienceandaccessibilitybyensuringthatelementsarevisiblewithinthecontainer.Itscompatibilitywithvariousdevicesandinputmethodsmakesitavaluabletoolforcreatinguser-friendlyapplications.
this is stream mode, and use: print(repr(resp.text))
 there was no space between words, and not readable, even i add a note at the last of my question:
question += 'please return a formatted and human readable answer.'
but sometimes, i asked gemini-pro to return a readable string again, it works, sometimes not word.
Actual vs expected behavior:
actual: ihaveaquesiton
 expected response; i have a qeustion
Any other information you'd like to share?
python 3.9
 google-ai-generativelanguage 0.4.0
 google-api-core 2.15.0
 google-auth 2.25.2
 google-generativeai 0.3.2
 googleapis-common-protos 1.62.0
 The text was updated successfully, but these errors were encountered: 
👍1
johnnietien reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/google-gemini/generative-ai-python/issues/162","ImportError: cannot import name 'DiscussServiceAsyncClient' from partially initialized module 'google.ai.generativelanguage_v1beta.services.discuss_service'","2024-09-25T02:00:34Z","Closed as not planned issue","component:python sdk,status:awaiting user response,status:stale,type:bug","Description of the bug:
python : 3.11.5
jupyter_server : 2.10.0
While running this python file in my local jupyter server, this error occurred.
import google.generativeai as genaiimport os

genai.configure(api_key=os.environ[""API_KEY""])
Below is the full error.
---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
Cell In[20], line 1
----> 1 import google.generativeai as genai
      2 import os
      4 genai.configure(api_key=os.environ[""API_KEY""])

File /opt/anaconda3/envs/Gemini-Env/lib/python3.10/site-packages/google/generativeai/__init__.py:45
     41 from __future__ import annotations
     43 from google.generativeai import version
---> 45 from google.generativeai import types
     46 from google.generativeai.types import GenerationConfig
     49 from google.generativeai.discuss import chat

File /opt/anaconda3/envs/Gemini-Env/lib/python3.10/site-packages/google/generativeai/types/__init__.py:17
      1 # -*- coding: utf-8 -*-
      2 # Copyright 2023 Google LLC
      3 #
   (...)
     13 # See the License for the specific language governing permissions and
     14 # limitations under the License.
     15 """"""A collection of type definitions used throughout the library.""""""
---> 17 from google.generativeai.types.discuss_types import *
     18 from google.generativeai.types.model_types import *
     19 from google.generativeai.types.text_types import *

File /opt/anaconda3/envs/Gemini-Env/lib/python3.10/site-packages/google/generativeai/types/discuss_types.py:22
     19 from typing import Any, Dict, Union, Iterable, Optional, Tuple, List
     20 from typing_extensions import TypedDict
---> 22 import google.ai.generativelanguage as glm
     23 from google.generativeai import string_utils
     25 from google.generativeai.types import safety_types

File /opt/anaconda3/envs/Gemini-Env/lib/python3.10/site-packages/google/ai/generativelanguage/__init__.py:21
     16 from google.ai.generativelanguage import gapic_version as package_version
     18 __version__ = package_version.__version__
---> 21 from google.ai.generativelanguage_v1beta.services.discuss_service.async_client import (
     22     DiscussServiceAsyncClient,
     23 )
     24 from google.ai.generativelanguage_v1beta.services.discuss_service.client import (
     25     DiscussServiceClient,
     26 )
     27 from google.ai.generativelanguage_v1beta.services.generative_service.async_client import (
     28     GenerativeServiceAsyncClient,
     29 )

File /opt/anaconda3/envs/Gemini-Env/lib/python3.10/site-packages/google/ai/generativelanguage_v1beta/services/discuss_service/__init__.py:16
      1 # -*- coding: utf-8 -*-
      2 # Copyright 2023 Google LLC
      3 #
   (...)
     14 # limitations under the License.
     15 #
---> 16 from .async_client import DiscussServiceAsyncClient
     17 from .client import DiscussServiceClient
     19 __all__ = (
     20     ""DiscussServiceClient"",
     21     ""DiscussServiceAsyncClient"",
     22 )

File /opt/anaconda3/envs/Gemini-Env/lib/python3.10/site-packages/google/ai/generativelanguage_v1beta/services/discuss_service/async_client.py:38
     35 from google.auth import credentials as ga_credentials  # type: ignore
     36 from google.oauth2 import service_account  # type: ignore
---> 38 from google.ai.generativelanguage_v1beta import gapic_version as package_version
     40 try:
     41     OptionalRetry = Union[retries.AsyncRetry, gapic_v1.method._MethodDefault]

File /opt/anaconda3/envs/Gemini-Env/lib/python3.10/site-packages/google/ai/generativelanguage_v1beta/__init__.py:21
     16 from google.ai.generativelanguage_v1beta import gapic_version as package_version
     18 __version__ = package_version.__version__
---> 21 from .services.discuss_service import DiscussServiceAsyncClient, DiscussServiceClient
     22 from .services.generative_service import (
     23     GenerativeServiceAsyncClient,
     24     GenerativeServiceClient,
     25 )
     26 from .services.model_service import ModelServiceAsyncClient, ModelServiceClient

ImportError: cannot import name 'DiscussServiceAsyncClient' from partially initialized module 'google.ai.generativelanguage_v1beta.services.discuss_service' (most likely due to a circular import) (/opt/anaconda3/envs/Gemini-Env/lib/python3.10/site-packages/google/ai/generativelanguage_v1beta/services/discuss_service/__init__.py)

I installed google-gnerativeai via pip install google-generativeai and also ran pip install -q -U google-generativeai on my terminal.
It seems to work fine on google colab(python ver 3.10), but I don't know what's the problem here. The documents says it supports python 3.9, 3.10, 3.11 pypi google-ai-generativelanguage docs
Actual vs expected behavior:
expected: It should work fine.
 actual : circular import error
Any other information you'd like to share?
I tried python 3.10 locally, but still same circular import error.
 I also made new conda environment with no additional libraries installed, but still this error occurred.
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/161","Bug with Arabic context","2024-02-09T01:44:53Z","Closed as not planned issue","component:python sdk,status:awaiting user response,status:stale,type:bug","Description of the bug:
On Colab, when I use Langchain and Gemini API with ""only Arabic"" pdf file, I got the following error. But it works well with English pdf files.
""ERROR:tornado.access:500 POST /v1beta/models/gemini-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 10208.52ms
WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.._chat_with_retry in 2.0 seconds as it raised InternalServerError: 500 POST http://localhost:38813/v1beta/models/gemini-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting.""
Actual vs expected behavior:
Actual behavior:
ERROR:tornado.access:500 POST /v1beta/models/gemini-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 10208.52ms
 WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.._chat_with_retry in 2.0 seconds as it raised InternalServerError: 500 POST http://localhost:38813/v1beta/models/gemini-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting.
Any other information you'd like to share?
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/160","Automatic image blob creation doesn't handle RGBA images with JPEG.","2024-06-03T16:18:47Z","Closed issue","component:python sdk,status:awaiting user response,status:stale,type:bug","Description of the bug:
Calling generate_content on a Gemini Pro Vision model returns an error when it receives a PNG image saying KeyError: 'RGBA' which causes another execption saying OSError: cannot write mode RGBA as JPEG. This seems to indicate that PNG is not supported, but according to the Gemini API docs, PNG is a supported MIME type. Note that the png example from that docs page doesn't seem to work. It uses a contents kwarg to generate_content, but that argument doesn't exist. Modifying the code to use the right arguments gives the error google.api_core.exceptions.InvalidArgument: 400 Request contains an invalid argument.
Actual vs expected behavior:
The expected behavior is for this code:
screenshot = get_screen_data()

prompt = ""What are your thoughts on this screenshot? I think""

response = model.generate_content(
    [prompt, screenshot], stream=True
)

response.resolve()

print(response.text)

to work successfully. This code was modified from the text from image and text example in the quickstart. Instead, it outputs the KeyError and OSError above. Changing the code to:
screenshot = get_screen_data()

screenshot_data = {
    'mime_type': 'image/png',
    'data': screenshot.tobytes()
}

prompt = ""What are your thoughts on this screenshot? I think""

response = model.generate_content(
    [prompt, screenshot_data], stream=True
)

response.resolve()
print(response.text)

Raises a 400 error as described above. This code is modified from that Gemini API Overview
Any other information you'd like to share?
#112 is related to this. Specifically, it deals with my second attempt at solving this problem. This issue is about the fact that generate_content doesn't handle PNG by default even though it is supposedly supported.
 The text was updated successfully, but these errors were encountered: 
😕1
randomradio reacted with confused emoji
All reactions
😕1 reaction"
"https://github.com/google-gemini/generative-ai-python/issues/159","User location is not supported for the API use.","2024-01-04T00:17:03Z","Closed issue","No label","I run this in colab:
import google.generativeai as genaibase_gen_config = {}
safety_settings = Nonemodel_name = ""models/gemini-pro""

genai.configure(api_key=""xxx"")

# Explicitly passed args take precedence over the generation_config.final_gen_config = {""temperature"": 0.4} | base_gen_config_model = genai.GenerativeModel(
            model_name=model_name,
            generation_config=final_gen_config,
            safety_settings=safety_settings,
        )
response = _model.generate_content('Please summarise this document: ...')
print(response.text)
why it post to localhost?
BadRequest: 400 POST http://localhost:44019/v1beta/models/gemini-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: User location is not supported for the API use.

below is the full detailed logs:
WARNING:tornado.access:400 POST /v1beta/models/gemini-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1074.58ms
---------------------------------------------------------------------------
BadRequest                                Traceback (most recent call last)
[<ipython-input-17-6548ea9e449d>](https://localhost:8080/#) in <cell line: 15>()
     13             safety_settings=safety_settings,
     14         )
---> 15 response = _model.generate_content('Please summarise this document: ...')
     16 print(response.text)

7 frames
[/usr/local/lib/python3.10/dist-packages/google/generativeai/generative_models.py](https://localhost:8080/#) in generate_content(self, contents, generation_config, safety_settings, stream, **kwargs)
    241             return generation_types.GenerateContentResponse.from_iterator(iterator)
    242         else:
--> 243             response = self._client.generate_content(request)
    244             return generation_types.GenerateContentResponse.from_response(response)
    245 

[/usr/local/lib/python3.10/dist-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py](https://localhost:8080/#) in generate_content(self, request, model, contents, retry, timeout, metadata)
    564 
    565         # Send the request.
--> 566         response = rpc(
    567             request,
    568             retry=retry,

[/usr/local/lib/python3.10/dist-packages/google/api_core/gapic_v1/method.py](https://localhost:8080/#) in __call__(self, timeout, retry, *args, **kwargs)
    111             kwargs[""metadata""] = metadata
    112 
--> 113         return wrapped_func(*args, **kwargs)
    114 
    115 

[/usr/local/lib/python3.10/dist-packages/google/api_core/retry.py](https://localhost:8080/#) in retry_wrapped_func(*args, **kwargs)
    347                 self._initial, self._maximum, multiplier=self._multiplier
    348             )
--> 349             return retry_target(
    350                 target,
    351                 self._predicate,

[/usr/local/lib/python3.10/dist-packages/google/api_core/retry.py](https://localhost:8080/#) in retry_target(target, predicate, sleep_generator, timeout, on_error, **kwargs)
    189     for sleep in sleep_generator:
    190         try:
--> 191             return target()
    192 
    193         # pylint: disable=broad-except

[/usr/local/lib/python3.10/dist-packages/google/api_core/timeout.py](https://localhost:8080/#) in func_with_timeout(*args, **kwargs)
    118                 kwargs[""timeout""] = max(0, self._timeout - time_since_first_attempt)
    119 
--> 120             return func(*args, **kwargs)
    121 
    122         return func_with_timeout

[/usr/local/lib/python3.10/dist-packages/google/api_core/grpc_helpers.py](https://localhost:8080/#) in error_remapped_callable(*args, **kwargs)
     70     def error_remapped_callable(*args, **kwargs):
     71         try:
---> 72             return callable_(*args, **kwargs)
     73         except grpc.RpcError as exc:
     74             raise exceptions.from_grpc_error(exc) from exc

[/usr/local/lib/python3.10/dist-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/rest.py](https://localhost:8080/#) in __call__(self, request, retry, timeout, metadata)
    854             # subclass.
    855             if response.status_code >= 400:
--> 856                 raise core_exceptions.from_http_response(response)
    857 
    858             # Return the response

BadRequest: 400 POST http://localhost:44019/v1beta/models/gemini-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: User location is not supported for the API use.

 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/158","Google Ai","2024-01-11T00:28:11Z","Closed issue","No label","No description provided.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/157","Support multimodal embeddings","2024-05-17T23:40:41Z","Closed issue","component:python sdk,type:feature request","Description of the feature request:
Vertex AI has a multimodal embeddings model, this only supports text embeddings.
What problem are you trying to solve with this feature?
I need multimodal embeddings.
Any other information you'd like to share?
I don't understand the difference between this API and Vertex AI.
I moved to this after starting with Vertex thinking this was the new stuff, but now I'm just confused.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/156","ModuleNotFoundError: No module named 'google.generativeai (Python 3.12)","2024-03-20T01:45:14Z","Closed as not planned issue","component:python sdk,status:awaiting user response,status:stale,type:bug","Description of the bug:
ModuleNotFoundError: No module named 'google.generativeai'
The python version installed on my windows10 machine
 Python 3.12
Actual vs expected behavior:
Unexpected behavior for Python version > 3.10
Any other information you'd like to share?
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/155","Function Calling","2024-04-20T22:26:54Z","Closed issue","component:python sdk,type:feature request","Description of the feature request:
A curl wrapper for function calling for the existing google-generativeai SDK for python. It would be a very good addition as people can continue using this library even for function calling feature. It's already available in api and can be used in vertex. It would be a crucial addition to compete to openai as this library api usage is free too.
What problem are you trying to solve with this feature?
No response
Any other information you'd like to share?
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/154","response.text error : TypeError: argument of type 'Part' is not iterable","2024-03-10T01:46:12Z","Closed as not planned issue","component:python sdk,status:awaiting user response,status:stale,type:bug","Description of the bug:
The response.text generated by the model.generate_content() ran into error saying that argument of type 'Part' is not iterable. I can run this code without any issue on google colab, but in local environment inside vs code jupyter I got this error.
The code I ran is as the followings:
import pathlib
import textwrap

import google.generativeai as genai

# Used to securely store your API key
#from google.colab import userdata

from IPython.display import display
from IPython.display import Markdown

def to_markdown_orig(text):
  text = text.replace('•', '  *')
  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))

def to_markdown(text):
  text=text._result.candidates[0].content.parts[0].text
  text = text.replace('•', '  *')
  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))


# Or use `os.getenv('GOOGLE_API_KEY')` to fetch an environment variable.
#GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')
import os
GOOGLE_API_KEY=os.environ['GOOGLE_API_KEY']

genai.configure(api_key=GOOGLE_API_KEY)

model = genai.GenerativeModel('gemini-pro')
response = model.generate_content(""What is the meaning of life?"")

to_markdown_orig(response.text)

then I got this error:
TypeError                                 Traceback (most recent call last)
Cell In[6], line 1
----> 1 to_markdown_orig(response.text)

File ~/anaconda3/lib/python3.10/site-packages/google/generativeai/types/generation_types.py:327, in BaseGenerateContentResponse.text(self)
    321 """"""A quick accessor equivalent to `self.candidates[0].parts[0].text`
    322 
    323 Raises:
    324     ValueError: If the candidate list or parts list does not contain exactly one entry.
    325 """"""
    326 parts = self.parts
--> 327 if len(parts) != 1 or ""text"" not in parts[0]:
    328     raise ValueError(
    329         ""The `response.text` quick accessor only works for ""
    330         ""simple (single-`Part`) text responses. This response is not simple text.""
   (...)
    333         ""instead.""
    334     )
    335 return parts[0].text

TypeError: argument of type 'Part' is not iterable```


Actual vs expected behavior:
After amending the to_markdown_orig() function by incorporating
text=text._result.candidates[0].content.parts[0].text
 , everything functions correctly (as shown in the code above). The problem seems to lie with response.text. Consequently, circumventing the use of response.text resolves the issue.
Any other information you'd like to share?
The following work well, but only works for the 1st time when chat.send_meassage is called. The 2nd time this function is called the program will run into error.
model = genai.GenerativeModel('gemini-pro')
chat = model.start_chat(history=[])
response = chat.send_message(""In one sentence, explain how a computer works to a young child."")
to_markdown(response)

When looking into the chat.history,
chat.history
 I get this output:
[parts {
   text: ""In one sentence, explain how a computer works to a young child.""
 }
 role: ""user"",
 parts {
   text: ""A computer works like a magic box that can help you do all kinds of things, like playing games, watching movies, and talking to friends.""
 }
 role: ""model""]

I'm curious if the issue with the ""parts"" is due to a missing colon "":"" while passing strings into a function to generate a dictionary. To create a chat chain similar to ChatGPT, I've followed various documents and examples, which have worked well. The main distinction I've noticed is that the functioning code successfully generates a properly formed dictionary.
model = genai.GenerativeModel('gemini-pro')
messages = [
    {'role':'user',
     'parts': [""Briefly explain how a computer works to a young child.""]}
]
response = model.generate_content(messages)
to_markdown(response)

messages.append({'role':'model',
                 'parts':[response._result.candidates[0].content.parts[0].text]})
messages.append({'role':'user',
                 'parts':[""Okay, how about a more detailed explanation to a high school student?""]})
response = model.generate_content(messages)
to_markdown(response)

messages.append({'role':'model',
                 'parts':[response._result.candidates[0].content.parts[0].text]})
messages

I got the output as expected:
[{'role': 'user',
  'parts': ['Briefly explain how a computer works to a young child.']},
 {'role': 'model',
  'parts': ['1. **Input**: \n    * A computer takes in information through input devices like a keyboard or a mouse.\n\n\n2. **Processing**: \n    * The computer then uses a special part called the processor (or CPU) to understand this information and perform calculations.\n\n\n3. **Memory**: \n    * Inside the computer, there is a place called memory which stores the information and instructions temporarily, kind of like your short-term memory.\n\n\n4. **Storage**: \n    * If the computer needs to save information for a long time, it uses a storage device like a hard drive or a USB drive. This is similar to your long-term memory.\n\n\n5. **Output**: \n    * After processing the information, the computer shows the result on an output device like a monitor or a printer.']},
 {'role': 'user',
  'parts': ['Okay, how about a more detailed explanation to a high school student?']},
 {'role': 'model',
  'parts': [""1. **Input**: \n    * Input devices like keyboards, mice, and touchscreens allow users to interact with the computer and provide information.\n    * The information is converted into a digital format that the computer can understand.\n\n\n2. **Processing**: \n    * The processor (CPU) is the brain of the computer. It receives the input data, performs calculations, and makes decisions based on the instructions provided by software programs.\n    * The speed of the processor is measured in gigahertz (GHz) or cores.\n\n\n3. **Memory**: \n    * Random Access Memory (RAM) is the computer's short-term memory. It temporarily stores data and instructions that are being processed by the CPU.\n    * The more RAM a computer has, the more programs and data it can handle at once.\n\n\n4. **Storage**: \n    * Storage devices like hard disk drives (HDD), solid-state drives (SSD), and USB flash drives store data and programs permanently.\n    * Storage capacity is measured in gigabytes (GB) or terabytes (TB).\n\n\n5. **Output**: \n    * Output devices like monitors, printers, and speakers display or print the results of the processing.\n    * Monitors show visual information, printers create physical copies of documents or images, and speakers produce sound.\n\n\n6. **Software**: \n    * Software programs, such as operating systems, application software, and games, provide instructions to the computer on how to perform specific tasks.\n    * The operating system manages the computer's hardware and software resources and provides a user interface.\n\n\n7. **Network**: \n    * Computers can be connected to each other through networks, allowing them to share resources, transfer data, and communicate with each other.\n\n\n8. **Internet**: \n    * The Internet is a global network of computers that allows users to access information, communicate with others, and conduct various online activities.""]}]

 The text was updated successfully, but these errors were encountered: 
👍4
madaan, RajK-NeetiAI, nathansouz4, and salehaTanveer reacted with thumbs up emoji
All reactions
👍4 reactions"
"https://github.com/google-gemini/generative-ai-python/issues/153","Unexpected type of call %s"" % type(call) when do async chat call send_message_async","2024-08-27T23:22:11Z","Closed issue","component:python sdk,type:bug","Description of the bug:
Unexpected type of call %s"" % type(call) when do async chat call send_message_async.
 I use the rest as transport, use my own proxy api endpoint.
Here is the code:
import asyncio
import os
import traceback

import google.generativeai as genai
from dotenv import load_dotenv

load_dotenv()

GEMINI_API_KEY = os.getenv('GEMINI_API_KEY')
GEMINI_API_ENDPOINT = os.getenv('GEMINI_API_ENDPOINT')


async def main():
    try:
        genai.configure(
            api_key=GEMINI_API_KEY,
            transport=""rest"",
            client_options={""api_endpoint"": GEMINI_API_ENDPOINT}
        )

        model = genai.GenerativeModel('gemini-pro')

        chat = model.start_chat()

        # response = chat.send_message(""Use python to write a fib func"", stream=True)	# line a. This line works
        response = await chat.send_message_async(""Use python to write a fib func"", stream=True) # line b. This line will cause an error
        for chunk in response:
            print('=' * 80)
            print(chunk.text)
    except Exception as e:
        traceback.print_exc()


def run_main():
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    try:
        loop.run_until_complete(main())
    except Exception as e:
        print(e)
    finally:
        loop.close()


run_main()

And after run this code, the error:
D:\xxx\venv\Scripts\python.exe D:\xxx\chore\gemini\gemini_test.py 
Traceback (most recent call last):
  File ""D:\xxx\chore\gemini\gemini_test.py"", line 27, in main
    response = await chat.send_message_async(""Use python to write a fib func"", stream=True) # This line will cause an error
  File ""D:\xxx\venv\lib\site-packages\google\generativeai\generative_models.py"", line 410, in send_message_async
    response = await self.model.generate_content_async(
  File ""D:\xxx\venv\lib\site-packages\google\generativeai\generative_models.py"", line 272, in generate_content_async
    iterator = await self._async_client.stream_generate_content(request)
  File ""D:\xxx\venv\lib\site-packages\google\api_core\retry_async.py"", line 223, in retry_wrapped_func
    return await retry_target(
  File ""D:\xxx\venv\lib\site-packages\google\api_core\retry_async.py"", line 121, in retry_target
    return await asyncio.wait_for(
  File ""C:\xxx\Python\Python310\lib\asyncio\tasks.py"", line 445, in wait_for
    return fut.result()
  File ""D:\xxx\venv\lib\site-packages\google\api_core\grpc_helpers_async.py"", line 187, in error_remapped_callable
    raise TypeError(""Unexpected type of call %s"" % type(call))
TypeError: Unexpected type of call <class 'google.api_core.rest_streaming.ResponseIterator'>

Process finished with exit code 0

Actual vs expected behavior:
It should work as the same as method send_message in line a
D:\xxx\venv\Scripts\python.exe D:\xxx\chore\gemini\gemini_test.py 
================================================================================
```python
def fib(n):
  """"""Calculates the nth Fibonacci
================================================================================
 number.

  Args:
    n: The index of the Fibonacci number to calculate.

  Returns:
    The nth Fibonacci number.
  
================================================================================
""""""

  if n < 2:
    return n
  else:
    return fib(n-1) + fib(n-2)

Process finished with exit code 0

### Any other information you'd like to share?
python: 3.10.5
os: windows 11

_No response_

 The text was updated successfully, but these errors were encountered: 
👍1
itechbear reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/google-gemini/generative-ai-python/issues/152","ModuleNotFoundError - GoogleAI, Pyinstaller","2024-05-17T23:10:39Z","Closed issue","component:python sdk,type:bug","Description of the bug:
I am building a simple app. When I try to make the .py to exe using no matter which tool Pyinstaller ,py2exe. Nothing works. It makes an exe for me, but it always says module google not found. I also tried it with VERY simple code, using only the google library, but same issue:
import time

import google.generativeai as genai
genai.configure(api_key=""///"")
gemini_model = genai.GenerativeModel('gemini-pro')
prompt = input()
completion = gemini_model.generate_content(
        prompt,
        generation_config=genai.types.GenerationConfig(

            temperature=0.3)
    )
response = completion.text
print(response)

I use the pyinstaller --onefile main.py
 I installed the library and it successfully works in pycharm as a .py
 Here is the error:

Actual vs expected behavior:
It should be included. I also Tried to inculde the library extra, but it just doesnt work. Tried it also on different machines, py versions ans so on...
Any other information you'd like to share?
Using Python 3.11.2
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/151","count_tokens method not working","2023-12-22T23:33:19Z","Closed issue","component:python sdk,type:bug","Description of the bug:
the count_tokens method in the genai.GenerativeModel class is not working. it is showing
 Error:
count_tokens() takes from 1 to 2 positional arguments but 3 were given
Actual vs expected behavior:
genai.configure(api_key=api_key.GOOGLE_API_KEY)
model = genai.GenerativeModel('gemini-pro')
#test apikey
for m in genai.list_models():
  if 'generateContent' in m.supported_generation_methods:
    print(m.name)
response = model.generate_content(""Is sky blue?"")
print(response.text)
print(model.count_tokens(""hello""))

Output:
models/gemini-pro
models/gemini-pro-vision
In general, the sky appears blue during the day due to a phenomenon known as Rayleigh scattering. Here's why:

* **Sunlight:** Sunlight consists of a range of colors, including blue, green, yellow, orange, red, and violet.

* **Wavelength:** Each color of light has a specific wavelength. Blue light has a shorter wavelength compared to other colors in the visible spectrum.

* **Rayleigh Scattering:** When sunlight enters the Earth's atmosphere, it interacts with molecules of nitrogen and oxygen. These molecules are much smaller than the wavelength of visible light. When sunlight encounters these molecules, it undergoes a process called Rayleigh scattering.

* **Scattering of Blue Light:** Rayleigh scattering causes the shorter wavelength blue light to scatter more than other colors of light. This means that blue light is scattered in all directions, including towards our eyes. As a result, we perceive the sky as blue during the daytime.

* **Other Factors:** The exact shade of blue we see can vary depending on factors such as the time of day, weather conditions, and the presence of particles in the atmosphere like dust, smoke, or clouds. These factors can affect the amount of scattering and the intensity of the blue color.

It's important to note that the sky is not inherently blue. The blue color we see is a result of the interaction between sunlight and the Earth's atmosphere. This phenomenon is what gives us our beautiful blue skies during the daytime.

{
	""name"": ""TypeError"",
	""message"": ""count_tokens() takes from 1 to 2 positional arguments but 3 were given"",
	""stack"": ""---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
Cell In[21], line 9
      7 response = model.generate_content(\""Is sky blue?\"")
      8 print(response.text)
----> 9 print(model.count_tokens(\""hello\""))

File ~/Library/Python/3.9/lib/python/site-packages/google/generativeai/generative_models.py:278, in GenerativeModel.count_tokens(self, contents)
    274 def count_tokens(
    275     self, contents: content_types.ContentsType
    276 ) -> glm.CountTokensResponse:
    277     contents = content_types.to_contents(contents)
--> 278     return self._client.count_tokens(self.model_name, contents)

TypeError: count_tokens() takes from 1 to 2 positional arguments but 3 were given""
}

Any other information you'd like to share?
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/148","Set a role for GenerativeModel (gemini-pro) with Python SDK?","2023-12-21T17:48:37Z","Closed issue","component:python sdk,type:feature request","Description of the feature request:
Hey is there rly no chance to set a role for the gemini pro model when using the python SDK? I tried to edit the Role in the source code but then my request will be allways blocked.
 Also checked the docs from gemini API and vertex but dant find anything -> play around also not keeping me to the goal.
What problem are you trying to solve with this feature?
To set a role for gemini using the vertex lib (GenerativeModel(""gemini-pro""))
Any other information you'd like to share?
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/147","Firefox: Errors in python quickstart colab.","2024-01-09T19:21:34Z","Closed issue","status:awaiting user response,type:bug","Description of the bug:
I got this error while trying the Google Colab python quickstart for Gemini AI. There's the following piece of code to list models in the page under ""List models""
for m in genai.list_models():
  if 'generateContent' in m.supported_generation_methods:
    print(m.name)

and this is the error I get
ERROR:tornado.access:500 GET /v1beta/models?pageSize=50&%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 2738.11ms

---------------------------------------------------------------------------

InternalServerError                       Traceback (most recent call last)

[<ipython-input-5-77709e92acfe>](https://localhost:8080/#) in <cell line: 1>()
----> 1 for m in genai.list_models():
      2   if 'generateContent' in m.supported_generation_methods:
      3     print(m.name)

7 frames

[/usr/local/lib/python3.10/dist-packages/google/ai/generativelanguage_v1beta/services/model_service/transports/rest.py](https://localhost:8080/#) in __call__(self, request, retry, timeout, metadata)
    826             # subclass.
    827             if response.status_code >= 400:
--> 828                 raise core_exceptions.from_http_response(response)
    829 
    830             # Return the response

InternalServerError: 500 GET http://localhost:46035/v1beta/models?pageSize=50&%24alt=json%3Benum-encoding%3Dint: TypeError: NetworkError when attempting to fetch resource.

Actual vs expected behavior:
Actual result
ERROR:tornado.access:500 GET /v1beta/models?pageSize=50&%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 2738.11ms

---------------------------------------------------------------------------

InternalServerError                       Traceback (most recent call last)

[<ipython-input-5-77709e92acfe>](https://localhost:8080/#) in <cell line: 1>()
----> 1 for m in genai.list_models():
      2   if 'generateContent' in m.supported_generation_methods:
      3     print(m.name)

7 frames

[/usr/local/lib/python3.10/dist-packages/google/ai/generativelanguage_v1beta/services/model_service/transports/rest.py](https://localhost:8080/#) in __call__(self, request, retry, timeout, metadata)
    826             # subclass.
    827             if response.status_code >= 400:
--> 828                 raise core_exceptions.from_http_response(response)
    829 
    830             # Return the response

InternalServerError: 500 GET http://localhost:46035/v1beta/models?pageSize=50&%24alt=json%3Benum-encoding%3Dint: TypeError: NetworkError when attempting to fetch resource.

Expected result
It should list models gemini-pro and gemini-pro-vision.
Any other information you'd like to share?
The code is able to access my API key and I was able to print it.
I tried this with 2 different google accounts and the issue is the same.
 The text was updated successfully, but these errors were encountered: 
👍1
anuj0405 reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/google-gemini/generative-ai-python/issues/146","AttributeError: module 'google.generativeai' has no attribute 'generate_content'","2024-01-08T18:45:48Z","Closed issue","status:awaiting user response,status:stale","Description of the bug:
I ran in to this Error
 AttributeError: module 'google.generativeai' has no attribute 'generate_content'
import os
 import google.generativeai as genai
 genai.configure(api_key=os.getenv('GEMINI_API_KEY'))
prompt = ""Write a story about a magic backpack.""
response = genai.generate_content(
 model=""gemini-pro"",
 prompt=prompt
 )
Actual vs expected behavior:
No response
Any other information you'd like to share?
Python 3.9.0
 google-generativeai==0.3.2
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/145","encountered a small bug when I tried to use Gemini API","2024-01-30T01:45:22Z","Closed as not planned issue","component:python sdk,status:awaiting user response,status:stale,type:bug","Description of the bug:
I ran the example code on https://ai.google.dev/tutorials/python_quickstart:
import google.generativeai as genai
import os
os.environ[""GOOGLE_API_KEY""] = ""My API key""

genai.configure(api_key=os.getenv(""GOOGLE_API_KEY""))

for m in genai.list_models():
  if 'generateContent' in m.supported_generation_methods:
    print(m)

and I got:
Traceback (most recent call last):
  File ""C:\Users\zjk\Desktop\Gemini.py"", line 10, in <module>
    for m in genai.list_models():
  File ""D:\anaconda3\envs\python3.9\lib\site-packages\google\generativeai\models.py"", line 165, in list_models
    model = type(model).to_dict(model)
AttributeError: to_dict

Actual vs expected behavior:
I didn't successfully run the code, but I guess I was supposed to see
gemini-pro
 gemini-pro-vision
as the documentation shows

Any other information you'd like to share?
google-generativeai version: 0.3.2
 python version: 3.9.15
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/144","Issue with Safety Rating Mechanism in Gemini-Pro Model","2024-02-15T01:45:21Z","Closed as not planned issue","component:python sdk,status:awaiting user response,status:stale,type:bug","Description of the bug:
I am writing to report a bug I encountered while using the Gemini-Pro model. The issue pertains to the safety ratings mechanism, where the input and output safety ratings are inconsistent, specifically for the category of ""Sexually Explicit"" content.
input
 safety_ratings {
 category: HARM_CATEGORY_SEXUALLY_EXPLICIT
 probability: NEGLIGIBLE
 }
 safety_ratings {
 category: HARM_CATEGORY_HATE_SPEECH
 probability: NEGLIGIBLE
 }
 safety_ratings {
 category: HARM_CATEGORY_HARASSMENT
 probability: NEGLIGIBLE
 }
 safety_ratings {
 category: HARM_CATEGORY_DANGEROUS_CONTENT
 probability: NEGLIGIBLE
 }
response.prompt_feedback
block_reason: SAFETY
 safety_ratings {
 category: HARM_CATEGORY_SEXUALLY_EXPLICIT
 probability: HIGH
 }
 safety_ratings {
 category: HARM_CATEGORY_HATE_SPEECH
 probability: NEGLIGIBLE
 }
 safety_ratings {
 category: HARM_CATEGORY_HARASSMENT
 probability: NEGLIGIBLE
 }
 safety_ratings {
 category: HARM_CATEGORY_DANGEROUS_CONTENT
 probability: NEGLIGIBLE
 }
The issue is that despite providing negligible probabilities for all harm categories, including ""Sexually Explicit"" content, the model's output unexpectedly rates the ""Sexually Explicit"" category as high. This seems to be an error in the model's safety rating system, as the input data does not align with the output.
I believe this to be a bug in the API's review mechanism and would appreciate it if your team could investigate and resolve this issue promptly. The accuracy and reliability of safety ratings are crucial for my usage of the Gemini-Pro model.
Actual vs expected behavior:
No response
Any other information you'd like to share?
No response
 The text was updated successfully, but these errors were encountered: 
👍1
supermomo668 reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/google-gemini/generative-ai-python/issues/143","ModuleNotFoundError: No module named 'google.ai'","2023-12-20T11:15:58Z","Closed issue","component:python sdk,type:bug","Description of the bug:
I'm encountering an error when trying to use the google.generativeai library in my Python script.
import google.generativeai as genaigenai.configure(api_key=""MY_API_KEY"")
# Set up the modelgeneration_config = {
  ""temperature"": 0.0,
  ""top_p"": 1,
  ""top_k"": 1,
  ""max_output_tokens"": 4096,
}

safety_settings = [
  {
    ""category"": ""HARM_CATEGORY_HARASSMENT"",
    ""threshold"": ""BLOCK_MEDIUM_AND_ABOVE""
  },
  {
    ""category"": ""HARM_CATEGORY_HATE_SPEECH"",
    ""threshold"": ""BLOCK_MEDIUM_AND_ABOVE""
  },
  {
    ""category"": ""HARM_CATEGORY_SEXUALLY_EXPLICIT"",
    ""threshold"": ""BLOCK_MEDIUM_AND_ABOVE""
  },
  {
    ""category"": ""HARM_CATEGORY_DANGEROUS_CONTENT"",
    ""threshold"": ""BLOCK_MEDIUM_AND_ABOVE""
  }
]

model = genai.GenerativeModel(model_name=""gemini-pro"",
                              generation_config=generation_config,
                              safety_settings=safety_settings)

prompt_parts = [
  "" You are an AI assistant named 'Translator'. You will be receiving content in various languages. If the material is in English, provide the exact original content; otherwise, translate it into English."",
  ""input: Le temps est très beau aujourd'hui, idéal pour jouer au volley-ball à la plage."",
  ""ouput: Today's weather is very nice, perfect for playing volleyball on the beach."",
  ""input: 明日、一緒に天ぷらを食べに行きませんか？"",
  ""ouput: How about we go eat tempura together tomorrow?"",
  ""input: Der Aktienmarkt ist heute um weitere 10% gefallen. Wie kann ich Geld verdienen?"",
  ""ouput: "",
]

response = model.generate_content(prompt_parts)
print(response.text)
I downloaded the generative-ai-python source code from https://github.com/google/generative-ai-python/archive/refs/tags/v0.3.1.tar.gz. Then I ran the command ""python setup.py install"" in my python3.8.10 virtual env.
Actual vs expected behavior:
Actual behaviour
When I run the script, I get the following error message:
Traceback (most recent call last):
  File ""gemini-api-test.py"", line 1, in <module>
    import google.generativeai as genai
  File ""/data/env/gemini/lib/python3.8/site-packages/google_generativeai-0.3.1-py3.8.egg/google/generativeai/__init__.py"", line 45, in <module>
    from google.generativeai import types
  File ""/data/env/gemini/lib/python3.8/site-packages/google_generativeai-0.3.1-py3.8.egg/google/generativeai/types/__init__.py"", line 17, in <module>
    from google.generativeai.types.discuss_types import *
  File ""/data/env/gemini/lib/python3.8/site-packages/google_generativeai-0.3.1-py3.8.egg/google/generativeai/types/discuss_types.py"", line 21, in <module>
    import google.ai.generativelanguage as glm
ModuleNotFoundError: No module named 'google.ai'

Expected behaviour
The script should run without errors and generate translations for the provided prompts.
Any other information you'd like to share?
OS: Ubuntu 20.04
 Env: virtualenv, Python:3.8.10
 Package:
 Name: google-generativeai
 Version: 0.3.1
 Summary: Google Generative AI High level API client library and tools.
 Home-page: https://github.com/google/generative-ai-python
 Author: Google LLC
 Author-email: googleapis-packages@google.com
 License: Apache 2.0
 Location: /data/env/gemini/lib/python3.8/site-packages/google_generativeai-0.3.1-py3.8.egg
 Requires: google-ai-generativelanguage, google-api-core, google-auth, protobuf, tqdm
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/139","how to convert resp to dict or json?","2024-05-17T23:39:58Z","Closed issue","component:python sdk,type:feature request","Description of the feature request:
hi, i have a question
 i write this code but this 'resp' not JSON serializable
genai.configure(api_key=apiKey)
    model = genai.GenerativeModel(""gemini-pro"")   
    resp = model.generate_content(""tell me a joke"")
    b=resp.candidates
    

or resp._result
how to convert resp to dict or json?
 thanks for help ❤️
What problem are you trying to solve with this feature?
No response
Any other information you'd like to share?
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/137","JSON Mode","2024-04-15T23:29:10Z","Closed issue","component:python sdk,type:feature request","Description of the feature request:
Didn't the announcement video specifically mention about json mode? I'm unable to find a flag for it in the API. and asking to gemini-pro to send a JSON output returns invalid json data
What problem are you trying to solve with this feature?
JSON parsable output for building apps
Any other information you'd like to share?
No response
 The text was updated successfully, but these errors were encountered: 
👍6
kripper, DGomez96, joefavergel, san425, avastmick, and dynamicwebpaige reacted with thumbs up emoji
All reactions
👍6 reactions"
"https://github.com/google-gemini/generative-ai-python/issues/136","Add option to set a client for GenerativeModel","2023-12-17T20:37:44Z","Open issue","component:python sdk,status:collecting interest,type:feature request","Description of the feature request:
Currently, the Python Quickstart documentation for using the Gemini API suggests setting the API key in the default client configuration.
import google.generativeai as genai

genai.configure(api_key=GOOGLE_API_KEY)
model = genai.GenerativeModel('gemini-pro')
response = model.generate_content(""What is the meaning of life?"")
print(response.text) # Model response gets printed
In addition to this, the package should also give an option to set a different key when creating the model. Something like:
model = genai.GenerativeModel('gemini-pro', api_key=OTHER_GOOGLE_API_KEY)
Currently, a workaround is to manually set the private _client variable for the model
import google.generativeai as genaifrom google.ai import generativelanguage as glm

client = glm.GenerativeServiceClient(
    client_options={'api_key':OTHER_GOOGLE_API_KEY})
model = genai.GenerativeModel('gemini-pro')
model._client = clientresponse = model.generate_content(""What is the meaning of life?"")
print(response.text) # Model response gets printed
What problem are you trying to solve with this feature?
The current implementation requires the API key to be set globally. Allowing the keys to be set with a smaller scope will be useful for applications that use multiple API keys concurrently for different tasks.
Any other information you'd like to share?
One potential implementation for this feature could be to take an additional keyword argument for the client and set it as the _client during the __init__ method of the GenerativeModel class.
# google/generative_ai/generative_models.pyclass GenerativeModel:
    def __init__(
        self,
        model_name: str = ""gemini-m"",
        safety_settings: safety_types.SafetySettingOptions | None = None,
        generation_config: generation_types.GenerationConfigType | None = None,
        client = None, # Additional kwarg
    ):
    ...
    self._client = client
    ...
Then, the user can optionally supply the client with their API key if they do not want the default credentials to be used.
# main.pyimport google.generativeai as genaifrom google.ai import generativelanguage as glm

client = glm.GenerativeServiceClient(
    client_options={'api_key':OTHER_GOOGLE_API_KEY})
model = genai.GenerativeModel('gemini-pro', client=client)
response = model.generate_content(""What is the meaning of life?"")
print(response.text) # Model response gets printed
 The text was updated successfully, but these errors were encountered: 
👍6
LukeSamkharadze, hexhog, Xin0611, sidpremkumar, Tiendil, and gordonhart reacted with thumbs up emoji
All reactions
👍6 reactions"
"https://github.com/google-gemini/generative-ai-python/issues/135","Does generative language api support something to set location?","2024-03-26T23:34:27Z","Closed issue","No label","I find that there is a way to set the location in vertex ai:
from google.cloud import aiplatform
import vertexai
from vertexai.language_models import TextGenerationModel

project_id = ""test""
location = ""us-central1""

vertexai.init(project=project_id, location=location)

Is there any method to do the same thing in generative language api (this repository)?
 The text was updated successfully, but these errors were encountered: 
👍1
ishaangandhi reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/google-gemini/generative-ai-python/issues/133","InternalServerError in ChatSession.send_message with Empty String","2023-12-19T17:38:01Z","Closed issue","component:python sdk,type:bug","Description of the bug:
When passing empty message to ChatSession.send_message it triggers a 500 Internal Server Error.
Here is an example :
import google.generativeai as genai

genai.configure(api_key =""YOUR API KEY"")

model = genai.GenerativeModel(model_name='gemini-pro')

chat = model.start_chat(history=[])
response = chat.send_message("""")
Actual vs expected behavior:
Actual behavior:
InternalServerError: 500 POST https://dp.kaggle.net/palmapi/v1beta/models/gemini-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting
Actual behavior:
 Should be handled in the same way as it is in GenerativeModel.generate_content :
TypeError: contents must not be empty
Any other information you'd like to share?
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/130","When using Python 3.11.4 to invoke the Gemini Pro Vision model and upload a plt image, a type error is reported.","2024-05-17T23:00:19Z","Closed issue","component:python sdk,type:bug","Description of the bug:
! curl -o image.jpg https://t0.gstatic.com/licensed-image?q=tbn:ANd9GcQ_Kevbk21QBRy-PgB4kQpS79brbmmEG7m3VOTShAn4PecDU5H5UxrJxE3Dw1JiaG17V88QIol19-3TM2wCHw
img = PIL.Image.open('image.jpg')
model = genai.GenerativeModel('gemini-pro-vision')
response = model.generate_content(img)
TypeError Traceback (most recent call last)
 Cell In[56], line 1
 ----> 1 response = model.generate_content(img)
File ~/.local/lib/python3.11/site-packages/google/generativeai/generative_models.py:229, in GenerativeModel.generate_content(self, contents, generation_config, safety_settings, stream, **kwargs)
 219 @string_utils.set_doc(_GENERATE_CONTENT_DOC)
 220 def generate_content(
 221 self,
 (...)
 227 **kwargs,
 228 ) -> generation_types.GenerateContentResponse:
 --> 229 request = self._prepare_request(
 230 contents=contents,
 231 generation_config=generation_config,
 232 safety_settings=safety_settings,
 233 **kwargs,
 234 )
 235 if self._client is None:
 236 self._client = client.get_default_generative_client()
File ~/.local/lib/python3.11/site-packages/google/generativeai/generative_models.py:200, in GenerativeModel._prepare_request(self, contents, generation_config, safety_settings, **kwargs)
 197 if not contents:
 198 raise TypeError(""contents must not be empty"")
 --> 200 contents = content_types.to_contents(contents)
 202 generation_config = generation_types.to_generation_config_dict(generation_config)
 203 merged_gc = self._generation_config.copy()
File ~/.local/lib/python3.11/site-packages/google/generativeai/types/content_types.py:235, in to_contents(contents)
 230 except TypeError:
 231 # If you get a TypeError here it's probably because that was a list
 232 # of parts, not a list of contents, so fall back to to_content.
 233 pass
 --> 235 contents = [to_content(contents)]
 236 return contents
File ~/.local/lib/python3.11/site-packages/google/generativeai/types/content_types.py:201, in to_content(content)
 198 return glm.Content(parts=[to_part(part) for part in content])
 199 else:
 200 # Maybe this is a Part?
 --> 201 return glm.Content(parts=[to_part(content)])
File ~/.local/lib/python3.11/site-packages/google/generativeai/types/content_types.py:171, in to_part(part)
 168 return glm.Part(text=part)
 169 else:
 170 # Maybe it can be turned into a blob?
 --> 171 return glm.Part(inline_data=to_blob(part))
File ~/.local/lib/python3.11/site-packages/google/generativeai/types/content_types.py:140, in to_blob(blob)
 136 if isinstance(blob, Mapping):
 137 raise KeyError(
 138 ""Could not recognize the intended type of the dict\n"" ""A content should have ""
 139 )
 --> 140 raise TypeError(
 141 ""Could not create Blob, expected Blob, dict or an Image type""
 142 ""(PIL.Image.Image or IPython.display.Image).\n""
 143 f""Got a: {type(blob)}\n""
 144 f""Value: {blob}""
 145 )
TypeError: Could not create Blob, expected Blob, dict or an Image type(PIL.Image.Image or IPython.display.Image).
 Got a: <class 'PIL.JpegImagePlugin.JpegImageFile'>
 Value: <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=2048x1365 at 0x7F89DDFC8B10>
Actual vs expected behavior:
No response
Any other information you'd like to share?
python 3.11.4
 centos 7.9
 google-ai-generativelanguage 0.4.0
 google-api-core 2.15.0
 google-auth 2.25.2
 google-generativeai 0.3.1
 googleapis-common-protos 1.62.0
 langchain-google-genai 0.0.4
 The text was updated successfully, but these errors were encountered: 
👍2
rukonpro and siddhant3s reacted with thumbs up emoji
All reactions
👍2 reactions"
"https://github.com/google-gemini/generative-ai-python/issues/126","“1‘ is blocked by safety reason? seriously","2024-01-14T08:12:37Z","Closed issue","component:python sdk,type:bug","Description of the bug:
when i send 1 to the gemini pro, it raise a exception :
 ValueError: The response.parts quick accessor only works for a single candidate, but none were returned. Check the response.prompt_feedback to see if the prompt was blocked.
then i print the response.prommpt_feedback, i got this:
block_reason: SAFETY
 safety_ratings {
 category: HARM_CATEGORY_SEXUALLY_EXPLICIT
 probability: NEGLIGIBLE
 }
 safety_ratings {
 category: HARM_CATEGORY_HATE_SPEECH
 probability: LOW
 }
 safety_ratings {
 category: HARM_CATEGORY_HARASSMENT
 probability: MEDIUM
 }
 safety_ratings {
 category: HARM_CATEGORY_DANGEROUS_CONTENT
 probability: NEGLIGIBLE
 }
 them i go to studio ：
Actual vs expected behavior:
so anybody tell what happend?
No response
Any other information you'd like to share?
No response
 The text was updated successfully, but these errors were encountered: 
👍5
elavalasrinivasreddy, blackdoor571, jacklanda, johnaiker, and Telokis reacted with thumbs up emoji😄2
bn-l and JunityZhan reacted with laugh emoji
All reactions
👍5 reactions
😄2 reactions"
"https://github.com/google-gemini/generative-ai-python/issues/125","Async embed_content","2024-05-17T23:02:10Z","Closed issue","component:python sdk,type:feature request","Description of the feature request:
GLAPI supports embed_content through the async client, so we should support it through the SDK too.
What problem are you trying to solve with this feature?
Embedding content, asynchronously.
Any other information you'd like to share?
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/122","google.api_core.exceptions.InternalServerError: 500 An internal error has occurred.","2024-05-17T22:56:45Z","Closed issue","component:python sdk,type:bug","Description of the bug:
The error occurs when ""res = geminiClient.generate_content(conversation).text"" is reached again in a recursive function call.
 I expected it to work the same way it did over the function's first pass. How do i fix this?
Error/Bug Found:
Traceback (most recent call last):
  File ""C:\Users\eworo\DELTA\env\Lib\site-packages\google\api_core\grpc_helpers.py"", line 79, in error_remapped_callable
    return callable_(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\eworo\DELTA\env\Lib\site-packages\grpc\_channel.py"", line 1160, in __call__
    return _end_unary_response_blocking(state, call, False, None)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\eworo\DELTA\env\Lib\site-packages\grpc\_channel.py"", line 1003, in _end_unary_response_blocking
    raise _InactiveRpcError(state)  # pytype: disable=not-instantiable
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
grpc._channel._InactiveRpcError: <_InactiveRpcError of RPC that terminated with:
        status = StatusCode.INTERNAL
        details = ""An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting""
        debug_error_string = ""UNKNOWN:Error received from peer ipv4:142.250.115.95:443 {created_time:""2023-12-15T04:18:34.1915935+00:00"", grpc_status:13, grpc_message:""An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting""}""
>

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""c:\Users\eworo\DELTA\src\test.py"", line 48, in <module>
    infer(input(""Q: ""))
  File ""c:\Users\eworo\DELTA\src\test.py"", line 45, in infer
    infer("""")
  File ""c:\Users\eworo\DELTA\src\test.py"", line 40, in infer
    res = geminiClient.generate_content(conversation).text
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\eworo\DELTA\env\Lib\site-packages\google\generativeai\generative_models.py"", line 243, in generate_content
    response = self._client.generate_content(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\eworo\DELTA\env\Lib\site-packages\google\ai\generativelanguage_v1beta\services\generative_service\client.py"", line 566, in generate_content
    response = rpc(
               ^^^^
  File ""C:\Users\eworo\DELTA\env\Lib\site-packages\google\api_core\gapic_v1\method.py"", line 131, in __call__
    return wrapped_func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\eworo\DELTA\env\Lib\site-packages\google\api_core\retry.py"", line 372, in retry_wrapped_func
    return retry_target(
           ^^^^^^^^^^^^^
  File ""C:\Users\eworo\DELTA\env\Lib\site-packages\google\api_core\retry.py"", line 207, in retry_target
    result = target()
             ^^^^^^^^
  File ""C:\Users\eworo\DELTA\env\Lib\site-packages\google\api_core\timeout.py"", line 120, in func_with_timeout
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\eworo\DELTA\env\Lib\site-packages\google\api_core\grpc_helpers.py"", line 81, in error_remapped_callable
    raise exceptions.from_grpc_error(exc) from exc
google.api_core.exceptions.InternalServerError: 500 An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting

Any other information you'd like to share?
I was using the Gemini API.
Here is code to recreate the error:
import google.generativeai as genai

genai.configure(api_key=""API_KEY"")

geminiClient = genai.GenerativeModel(model_name=""gemini-pro"")

conversation = [{'role':'user','parts': [f""Hello!""]}, {'role':'model','parts': [""Hey there Anon!""]}]


def infer(query):
    conversation.append({'role':'user','parts': [query]})
    res = geminiClient.generate_content(conversation).text
    print(""A: "" + res)
    conversation.append({'role':'model','parts': [res]})

    if res == ""Y"":
        infer("""")

while True:
    infer(input(""Q: ""))

To attain the error, tell the model ""only respond to me with ""Y"" until i say stop"".
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/118","module 'google.generativeai' has no attribute 'GenerativeModel'","2023-12-21T21:11:32Z","Closed issue","component:python sdk,type:bug","Description of the bug:
Executing this code
 with either
from langchain_google_genai import ChatGoogleGenerativeAI
or
from langchain_google_genai.chat_models import ChatGoogleGenerativeAI
def generate_lecture(topic:str, context:str):
    
  template =""""""
  As an accomplished university professor and expert in {topic}, your task is to develop an elaborate, exhaustive, and highly detailed lecture on the subject. 
  Remember to generate content ensuring both novice learners and advanced students can benefit from your expertise.
  while leveraging the provided context
  
  Context: {context} """"""

  
  prompt = ChatPromptTemplate.from_template(template)

  model = ChatGoogleGenerativeAI(model=""gemini-pro"", google_api_key=palm_api_key)

 
  response = model.invoke(template)
  return response.content

Actual vs expected behavior:
Actual behaviour
 The error below is thrown
  File ""C:\Users\ibokk\RobotForge\mvp\service\llm.py"", line 80, in generate_lecture
    model = ChatGoogleGenerativeAI(model=""gemini-pro"", google_api_key=palm_api_key)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\ibokk\anaconda3\envs\robotforge\Lib\site-packages\langchain_core\load\serializable.py"", line 97, in __init__
    super().__init__(**kwargs)
  File ""pydantic\main.py"", line 339, in pydantic.main.BaseModel.__init__
  File ""pydantic\main.py"", line 1102, in pydantic.main.validate_model
  File ""C:\Users\ibokk\anaconda3\envs\robotforge\Lib\site-packages\langchain_google_genai\chat_models.py"", line 502, in validate_environment
    values[""_generative_model""] = genai.GenerativeModel(model_name=model)
                                  ^^^^^^^^^^^^^^^^^^^^^
AttributeError: module 'google.generativeai' has no attribute 'GenerativeModel

Expected behaviour
A poor result will be random text generated which is not relevant to the prompt provided. An excellent result will be text generated relevant to the template provided.
Any other information you'd like to share?
OS: Windows 11
 Env: Conda 23.7.2, python: 3.11
 Package: google-gen-ai : 0.3.1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/117","google.api_core.exceptions.RetryError: Deadline of 60.0s exceeded while calling target function, last exception: 503 Getting metadata from plugin failed with error: ('invalid_grant: Bad Request', {'error': 'invalid_grant', 'error_description': 'Bad Request'})","2024-09-25T02:00:35Z","Closed as not planned issue","component:python sdk,status:awaiting user response,status:stale,type:bug","Description of the bug:
package verison: 0.3.1
  File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/google/api_core/grpc_helpers.py"", line 65, in error_remapped_callable
    return callable_(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/grpc/_channel.py"", line 1161, in __call__
    return _end_unary_response_blocking(state, call, False, None)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/grpc/_channel.py"", line 1004, in _end_unary_response_blocking
    raise _InactiveRpcError(state)  # pytype: disable=not-instantiable
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
grpc._channel._InactiveRpcError: <_InactiveRpcError of RPC that terminated with:
        status = StatusCode.UNAVAILABLE
        details = ""Getting metadata from plugin failed with error: ('invalid_grant: Bad Request', {'error': 'invalid_grant', 'error_description': 'Bad Request'})""
        debug_error_string = ""UNKNOWN:Error received from peer  {created_time:""2023-12-13T23:10:10.374012-08:00"", grpc_status:14, grpc_message:""Getting metadata from plugin failed with error: (\'invalid_grant: Bad Request\', {\'error\': \'invalid_grant\', \'error_description\': \'Bad Request\'})""}""
>

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/google/api_core/retry.py"", line 191, in retry_target
    return target()
           ^^^^^^^^
  File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/google/api_core/timeout.py"", line 120, in func_with_timeout
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/google/api_core/grpc_helpers.py"", line 67, in error_remapped_callable
    raise exceptions.from_grpc_error(exc) from exc
google.api_core.exceptions.ServiceUnavailable: 503 Getting metadata from plugin failed with error: ('invalid_grant: Bad Request', {'error': 'invalid_grant', 'error_description': 'Bad Request'})

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""/Users/defalt/Desktop/Athena/research/swarms/gemini.py"", line 23, in <module>
    response = model.generate_content(""The opposite of hot is"")
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/google/generativeai/generative_models.py"", line 243, in generate_content
    response = self._client.generate_content(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py"", line 566, in generate_content
    response = rpc(
               ^^^^
  File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/google/api_core/gapic_v1/method.py"", line 113, in __call__
    return wrapped_func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/google/api_core/retry.py"", line 349, in retry_wrapped_func
    return retry_target(
           ^^^^^^^^^^^^^
  File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/google/api_core/retry.py"", line 207, in retry_target
    raise exceptions.RetryError(
google.api_core.exceptions.RetryError: Deadline of 60.0s exceeded while calling target function, last exception: 503 Getting metadata from plugin failed with error: ('invalid_grant: Bad Request', {'error': 'invalid_grant', 'error_description': 'Bad Request'})

Actual vs expected behavior:
No response
Any other information you'd like to share?
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/114","google.auth.exceptions.DefaultCredentialsError: Could not automatically determine credentials.","2024-01-08T09:34:39Z","Closed issue","component:python sdk,type:bug","Description of the bug:
when use low google-auth package will raise this problem

Actual vs expected behavior:
No response
Any other information you'd like to share?
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/113","Responses without candidates throwing IndexError","2024-01-08T19:06:35Z","Closed issue","component:python sdk,type:bug","Description of the bug:
e.g.

I think ideally this would error with a relevant error message.
Actual vs expected behavior:
No response
Any other information you'd like to share?
No response
 The text was updated successfully, but these errors were encountered: 
👍1
persianopencart reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/google-gemini/generative-ai-python/issues/112","Readme for new Gemini Code Samples on Recipie to be updated to accept Image","2024-05-17T22:27:29Z","Closed issue","component:python sdk,type:bug","Description of the bug:
The current example produces a base 64 response instead of the actual recipe of the item.
Actual vs expected behavior:
Any other information you'd like to share?
This code works correctly
import google.generativeai as genai
from PIL import Image

genai.configure(api_key=""[API_KEY"")

model = genai.GenerativeModel('gemini-pro-vision')
cookie_picture = Image.open('image.png')
prompt = ""Give me a recipe for this:""

response = model.generate_content(
    contents=[prompt, cookie_picture]
)

print(response.text)

 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/102","Pydantic throws an error when import types","2024-01-14T01:50:12Z","Closed as not planned issue","component:python sdk,status:awaiting user response,status:stale,type:bug","Description of the bug:
When using classes from types in other classes which use pydantic, pydantic throws this error:
pydantic.errors.PydanticUserError: Please use typing_extensions.TypedDictinstead oftyping.TypedDict on Python < 3.12.
Actual vs expected behavior:
No error
Any other information you'd like to share?
Should be an easy fix to add:
import sysif sys.version_info >= (3, 12):
    from typing import TypedDictelse:
    from typing_extensions import TypedDict
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/99","How does google.generativeai.count_message_tokens work?","2024-05-17T22:43:11Z","Closed issue","component:python sdk,type:help","Hi there,
 I tried to count the token number of a text using google.generativeai.count_message_tokens according to the following code.
 I got 12 for the input ""a"".
 Does google.generativeai.count_message_tokens include something besides a prompt, which is now ""a"" ?
 I would appreciate any comments.
Thanks.
import google.generativeai as palmimport ospalm_api_key = os.getenv(""PALM_API_KEY"")

palm.configure(api_key=palm_api_key)

def count_tokens(string):
    res = palm.count_message_tokens(model='models/chat-bison-001', prompt=string)
    return res['token_count']

print(count_tokens('a'))
# 12
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/98","How can I use Chinese as input","2023-12-13T00:15:58Z","Closed issue","component:python sdk,type:help","import pprintimport google.generativeai as palmimport pprint

palm.configure(api_key='MY_API_KEY')

# inpu as inputinpu = ""你好""response = palm.chat(messages=inpu)

print(response)
this is my code ,and how can i do if i want use Chinese as input
 when i use Chinese as input it returns none and the filters give me a blocked reason like the image show

 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/97","Deploy as conda library","2024-08-27T23:14:06Z","Closed issue","component:python sdk,type:feature request","Description of the feature request:
Hi
Any chance of deploying this as a conda library ?
What problem are you trying to solve with this feature?
No response
Any other information you'd like to share?
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/96","Async support","2023-12-12T23:52:17Z","Closed issue","component:python sdk,type:feature request","Description of the feature request:
Since we are calling an external API, adding async support will help improve latency in applications.
What problem are you trying to solve with this feature?
Calling embeddings and text generation asynchronously
Any other information you'd like to share?
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/84","google-generativeai Installation on Render Linux Servers","2023-12-08T01:47:21Z","Closed as not planned issue","status:awaiting user response,status:stale,type:bug","Description of the bug:
ERROR: No matching distribution found for google-generativeai
Actual vs expected behavior:
No response
Any other information you'd like to share?
I am trying to install it from the requirements.txt file on render.com
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/81","Unable to use safety settings in chat mode of chat bison","2024-05-17T22:40:30Z","Closed issue","component:python sdk,type:feature request","It would be nice to be able to configure the safety settings for chat bison. This would hopefully lessen getting ""none"" as a response. It would also help with making examples more reliable, and make chat mode able to register a name.
 The text was updated successfully, but these errors were encountered: 
👍4
liamdugan, sr229, XizhiWu211, and jmtb095 reacted with thumbs up emoji
All reactions
👍4 reactions"
"https://github.com/google-gemini/generative-ai-python/issues/80","API key from MakerSuite Access Denied","2023-12-12T01:47:56Z","Closed as not planned issue","status:awaiting user response,status:stale,type:bug","Description of the bug:
When getting an API-Key I get a message that the access restricted. I think I would need that Key to be able to install and use the API. Is it possibly my Administartor who is blocking me? I would need to try this on my personal laptop.
Actual vs expected behavior:
No response
Any other information you'd like to share?
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/65","Safety filter is too strong today","2023-09-25T18:45:57Z","Closed issue","type:bug","Description of the bug:
Hello, I am using the PaLM API through Python / JS and MakerSuite. I noticed that the safety filter has become unexpectedly strong recently. My very benign inference requests were all responded with {reason: ""OTHER""}. Is there a way to change my safety setting to get the result?
Here is an example in MakerSuite (I have already lowered the safety filter thresholds, see screenshot 2):
Actual vs expected behavior:
I expect to get result ""comment vas-tu ?"" instead of an empty response with OTHER reason. I believe PaLM 2 API used to give me this correct response two days ago.
Any other information you'd like to share?
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/64","google.api_core.exceptions.ServiceUnavailable: 503 The service is currently unavailable.","2024-05-17T22:38:15Z","Closed issue","component:python sdk,type:bug","Description of the bug:
I met the error message google.api_core.exceptions.ServiceUnavailable: 503 The service is currently unavailable. frequently. Especially when I try to call the API multiple times using a loop. Is this a bug or is there a way to solve it? Thanks!
Actual vs expected behavior:
Should generate text or chat correctly but frequently have the error google.api_core.exceptions.ServiceUnavailable: 503 The service is currently unavailable.
Any other information you'd like to share?
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/60","Add more docstrings.","2023-08-30T20:41:53Z","Closed issue","contributions welcome,type:feature request","enhance the codebase with informative doscstrings
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/54","<BlockedReason.OTHER: 2> with simple questions","2024-05-17T22:32:08Z","Closed issue","component:python sdk,type:bug","Palm has been working pretty well until I randomly started running into this error: filters=[{'reason': <BlockedReason.OTHER: 2>}], top_p=0.95, top_k=40). On the PaLM documentation it says blocked reason other 2 is an unspecified filter, and I don't know what that would be. I am using palm.chat with this example: [ ""what is your name"", ""my name is al"" ]. the context is ""your name is al."" however, when I ask the model ""what is your name"", it gives me that error immediately.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/51","Palm API - 403 Request had insufficient authentication scopes","2023-07-28T13:55:09Z","Closed issue","No label","This is similar to issue #50. Running the following:
 import google.generativeai as palm
 import os
 palm.configure(api_key=""xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx"")
response = palm.chat(messages='Hello')
response.last
and receive:
 PermissionDenied: 403 Request had insufficient authentication scopes. [reason: ""ACCESS_TOKEN_SCOPE_INSUFFICIENT""
 domain: ""googleapis.com""
 metadata {
 key: ""method""
 value: ""google.ai.generativelanguage.v1beta2.DiscussService.GenerateMessage""
 }
 metadata {
 key: ""service""
 value: ""generativelanguage.googleapis.com""
 }
 I get successful results running the curl example.
 I have run it under 2 Python environments
 I'm a fairly new user but it seems to me that it can't find ""google.ai.generativelanguage.v1beta2.DiscussService.GenerateMessage""
I was approved for the beta test. Tried to add the scope: https://www.googleapis.com/auth/generative-language but it could not be found.
 Don't know what more steps I can take
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/50","Don't use ADCs if an API key is specified","2024-06-16T01:52:51Z","Closed as not planned issue","component:python sdk,good first issue,status:awaiting user response,status:stale,type:bug","I have been trying to use the Palm API and the palm.chat() function with google's new generative api. I've been in a maze of documentation and errors and I can't seem to get past this one. My code is very simple, and the error is coming from a simple request with palm.chat(). I have an API key that works when I test it with curl. I also downloaded credentials. I set up an OAuth consent screen, because I thought that might help me add the scope that I need, but I can't see what the scope requirement would be for palm.chat. Here is my code:
import google.generativeai as palm
import os
palm.configure(api_key='XXXXXXXXXXXXXXXXXXXXX')

os.environ['GOOGLE_APPLICATION_CREDENTIALS']='XXXXXXXXX/.config/gcloud/application_default_credentials.json'

response = palm.chat(messages='Hello')

response.last

The exact error I am getting is:
File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/google/api_core/grpc_helpers.py"", line 67, in error_remapped_callable raise exceptions.from_grpc_error(exc) from exc google.api_core.exceptions.PermissionDenied: 403 Request had insufficient authentication scopes. [reason: ""ACCESS_TOKEN_SCOPE_INSUFFICIENT"" domain: ""googleapis.com"" metadata { key: ""method"" value: ""google.ai.generativelanguage.v1beta2.TextService.GenerateText"" } metadata { key: ""service"" value: ""generativelanguage.googleapis.com"" }

I think the problem is that I need to add some kind of scope to oauth but there is no documentation anywhere that I can find that says what that might be. I've posted this on google and stack overflow but no one has had a solution, so any help at all would be greatly appreciated. thank you so much!
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/49","Response in other language always return None","2023-07-24T07:31:59Z","Closed as not planned issue","No label","I'm trying to get a response in other languages, but it always return None
message = ""HELLO!""
prompt = f""""""
ANSWER IN CHINESE.
USER:{message}
ASSISTANT:""""""

response = palm.chat(
    prompt=prompt
)

print(response.messages) 

Chinese
[{'author': '0', 'content': '\nANSWER IN CHINESE.\nUSER:HELLO!\nASSISTANT:'}, None]
 Thai
[{'author': '0', 'content': '\nANSWER IN THAI.\nUSER:HELLO!\nASSISTANT:'}, None]
 Japanese
[{'author': '0', 'content': '\nANSWER IN JAPANESE.\nUSER:HELLO!\nASSISTANT:'}, None]
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/48","google.generativeai.chat does not support max_output_tokens","2024-05-17T22:28:31Z","Closed issue","component:python sdk,type:feature request","We want to limit the reply length of chat responses, but google.generativeai.chat does not appear to support the max_output_tokens parameter. I'm not sure whether this is just not implemented yet, or an API limitation, or something else, but the vertexai Python SDK Chat model appears to support it (see Vertex AI Chat model parameters) and so does the google.generativeai.generate_text function.
I had thought that perhaps max_output_tokens wasn't supported in chat, just text generation, but this doc clearly shows it being used in a chat:
chat = chat_model.start_chat(
    context=""My name is Ned. You are my personal assistant. My favorite movies are Lord of the Rings and Hobbit."",
    examples=[
        InputOutputTextPair(
            input_text=""Who do you work for?"",
            output_text=""I work for Ned."",
        ),
        InputOutputTextPair(
            input_text=""What do I like?"",
            output_text=""Ned likes watching movies."",
        ),
    ],
    temperature=0.3,
    max_output_tokens=200,
    top_p=0.8,
    top_k=40,
)
print(chat.send_message(""Are my favorite movies based on a book series?""))

(It's a bit confusing that Google seems to have two different Python SDKs, this google-generativeai one and google-cloud-aiplatform. Is there any difference if all a developer wants to do is send chat to a model and get responses back?)
 The text was updated successfully, but these errors were encountered: 
👍2
liamdugan and ryuryukke reacted with thumbs up emoji
All reactions
👍2 reactions"
"https://github.com/google-gemini/generative-ai-python/issues/46","Retry","2023-07-17T23:29:12Z","Closed issue","No label","Hi,
 How to tackle rate limit error using the retry function?
 The text was updated successfully, but these errors were encountered: 
❤️1
SeaDude reacted with heart emoji
All reactions
❤️1 reaction"
"https://github.com/google-gemini/generative-ai-python/issues/45","IndexError: list index out of range","2023-06-22T14:38:28Z","Closed issue","No label","I am encountering an issue while using the google.generativeai.chat library. Whenever I input the phrase ""who are you?"" as a message, it consistently triggers an error. This behavior seems unexpected and needs to be addressed.
response = palm.chat(messages=[""who are you?""])
# error
  ...
  File ""/Users/jybaek/work/xxxx/.venv/lib/python3.8/site-packages/google/generativeai/discuss.py"", line 413, in reply
    return _generate_response(request=request, client=self._client)
  File ""/Users/jybaek/work/xxxx/.venv/lib/python3.8/site-packages/google/generativeai/discuss.py"", line 461, in _generate_response
    return _build_chat_response(request, response, client)
  File ""/Users/jybaek/work/xxxx/.venv/lib/python3.8/site-packages/google/generativeai/discuss.py"", line 443, in _build_chat_response
    request[""messages""].append(response[""candidates""][0])
IndexError: list index out of range
Is it a known issue? Please check.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/43","Streaming support?","2023-12-12T23:51:21Z","Closed issue","component:python sdk,type:feature request","Are there plans to support streaming responses from the API?
 The text was updated successfully, but these errors were encountered: 
👍1
marioplumbarius reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/google-gemini/generative-ai-python/issues/38","Add github-action for type checking.","2023-06-09T05:50:06Z","Closed issue","No label","No description provided.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/34","Why is there a subfolder google instead of just having the package generativeai?","2023-05-29T06:46:42Z","Closed issue","No label","My question is in the title. It seems unnecessary to have the folder google. Or is there a special reason for this? If this is really unnecessary, I could try to fix it myself. Let me know.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/29","IPv6 preventing API access","2023-09-22T01:45:47Z","Closed issue","component:python sdk,status:awaiting user response,status:stale,type:help","Using curl in the terminal works just fine.
curl \
-H 'Content-Type: application/json' \
-d '{ ""prompt"": { ""text"": ""Write a story about a magic backpack""} }' \
""https://generativelanguage.googleapis.com/v1beta2/models/text-bison-001:generateText?key=<my_api_key>""

The above works just fine, however, when I try:
import google.generativeai as palm

palm.configure(api_key=<my_api_key>)
result = palm.generate_text(prompt=""The opposite of hot is"")
print(result)

Results in the following error:
Traceback (most recent call last):
  File ""/home/gene/webhook/venv/lib/python3.10/site-packages/google/api_core/grpc_helpers.py"", line 72, in error_remapped_callable
    return callable_(*args, **kwargs)
  File ""/home/gene/webhook/venv/lib/python3.10/site-packages/grpc/_channel.py"", line 1030, in __call__
    return _end_unary_response_blocking(state, call, False, None)
  File ""/home/gene/webhook/venv/lib/python3.10/site-packages/grpc/_channel.py"", line 910, in _end_unary_response_blocking
    raise _InactiveRpcError(state)  # pytype: disable=not-instantiable
grpc._channel._InactiveRpcError: <_InactiveRpcError of RPC that terminated with:
        status = StatusCode.UNAVAILABLE
        details = ""403:Forbidden""
        debug_error_string = ""UNKNOWN:Error received from peer  {grpc_message:""403:Forbidden"", grpc_status:14, created_time:""2023-05-25T16:47:54.953534672+00:00""}""
>

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""/home/gene/webhook/create_youtubedb.py"", line 329, in <module>
    test2()
  File ""/home/gene/webhook/create_youtubedb.py"", line 326, in test2
    return palm.generate_text(prompt=""The opposite of hot is"")
  File ""/home/gene/webhook/venv/lib/python3.10/site-packages/google/generativeai/text.py"", line 139, in generate_text
    return _generate_response(client=client, request=request)
  File ""/home/gene/webhook/venv/lib/python3.10/site-packages/google/generativeai/text.py"", line 159, in _generate_response
    response = client.generate_text(request)
  File ""/home/gene/webhook/venv/lib/python3.10/site-packages/google/ai/generativelanguage_v1beta2/services/text_service/client.py"", line 641, in generate_text
    response = rpc(
  File ""/home/gene/webhook/venv/lib/python3.10/site-packages/google/api_core/gapic_v1/method.py"", line 113, in __call__
    return wrapped_func(*args, **kwargs)
  File ""/home/gene/webhook/venv/lib/python3.10/site-packages/google/api_core/grpc_helpers.py"", line 74, in error_remapped_callable
    raise exceptions.from_grpc_error(exc) from exc
google.api_core.exceptions.ServiceUnavailable: 503 403:Forbidden

 The text was updated successfully, but these errors were encountered: 
👍1
MarkEdmondson1234 reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/google-gemini/generative-ai-python/issues/21","How to interpret ""safety_feedback""","2023-06-16T23:24:10Z","Closed issue","No label","Hi, I am getting this error upon trying to use genai.generate_text()
Completion(
    candidates=[], 
    result=None, 
    filters=[{'reason': <BlockedReason.SAFETY: 1>}], 
    safety_feedback=[{
        'rating': {'category': <HarmCategory.HARM_CATEGORY_TOXICITY: 2>, 'probability': <HarmProbability.LOW: 2>},
        'setting': {'category': <HarmCategory.HARM_CATEGORY_TOXICITY: 2>, 'threshold': <HarmBlockThreshold.BLOCK_LOW_AND_ABOVE: 1>}}])

Nothing in my prompt is in the least bit toxic, it's just frontend code. Can you please tell me what input args to give to generate_text so it stops blocking? I tried to mess with safety_settings but it's not well documented and I didn't succeed.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/13","Import error with python 3.8","2023-05-26T21:02:30Z","Closed issue","No label","Hi,
I got the below error when importing this package with the latest version (0.1.0rc2).
 Did I miss anything?
>>> import google.generativeai
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/opt/conda/envs/chatbot/lib/python3.8/site-packages/google/generativeai/__init__.py"", line 59, in <module>
    from google.generativeai import types
  File ""/opt/conda/envs/chatbot/lib/python3.8/site-packages/google/generativeai/types/__init__.py"", line 19, in <module>
    from google.generativeai.types.text_types import *
  File ""/opt/conda/envs/chatbot/lib/python3.8/site-packages/google/generativeai/types/text_types.py"", line 34, in <module>
    class Completion(abc.ABC):
  File ""/opt/conda/envs/chatbot/lib/python3.8/site-packages/google/generativeai/types/text_types.py"", line 49, in Completion
    filters: Optional[list[safety_types.ContentFilterDict]]
TypeError: 'type' object is not subscriptable

 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/12","Empty Chat Responses","2023-05-16T16:34:22Z","Closed issue","No label","I'm noticing that a few simple questions are not getting responses why I try to use them with the chat methods. For example:
palm.chat(messages='Why is the sky blue?')
> ChatResponse(model='models/chat-bison-001', context='', examples=[], messages=[{'author': '0', 'content': 'Why is the sky blue?'}, None], temperature=None, candidate_count=None, candidates=[], top_p=None, top_k=None, filters=[])

The same question works fine with generate_text
palm.generate_text(model=model, prompt='Why is the sky blue?')
> Completion(candidates=[{'output': 'The sky is blue due to a phenomenon called Rayleigh scattering. This is the scattering of light by particles that are smaller than the wavelength of light. The particles in the atmosphere that cause Rayleigh scattering are molecules of nitrogen and oxygen.\n\nWhen sunlight hits these molecules, it is scattered in all directions. However, blue light is scattered more than other colors because it has a shorter wavelength. This is why the sky appears blue during the day.\n\nThe amount of scattering depends on the amount of particles in the atmosphere. This is why the sky appears darker at sunrise and sunset, when there are more particles in the air.\n\nThe sky can also appear blue at other times, such as when there is dust or moisture in the air. This is because these particles can also scatter sunlight.', 'safety_ratings': [{'category': <HarmCategory.HARM_CATEGORY_DEROGATORY: 1>, 'probability': <HarmProbability.NEGLIGIBLE: 1>}, {'category': <HarmCategory.HARM_CATEGORY_TOXICITY: 2>, 'probability': <HarmProbability.NEGLIGIBLE: 1>}, {'category': <HarmCategory.HARM_CATEGORY_VIOLENCE: 3>, 'probability': <HarmProbability.NEGLIGIBLE: 1>}, {'category': <HarmCategory.HARM_CATEGORY_SEXUAL: 4>, 'probability': <HarmProbability.NEGLIGIBLE: 1>}, {'category': <HarmCategory.HARM_CATEGORY_MEDICAL: 5>, 'probability': <HarmProbability.NEGLIGIBLE: 1>}, {'category': <HarmCategory.HARM_CATEGORY_DANGEROUS: 6>, 'probability': <HarmProbability.NEGLIGIBLE: 1>}]}], result='The sky is blue due to a phenomenon called Rayleigh scattering. This is the scattering of light by particles that are smaller than the wavelength of light. The particles in the atmosphere that cause Rayleigh scattering are molecules of nitrogen and oxygen.\n\nWhen sunlight hits these molecules, it is scattered in all directions. However, blue light is scattered more than other colors because it has a shorter wavelength. This is why the sky appears blue during the day.\n\nThe amount of scattering depends on the amount of particles in the atmosphere. This is why the sky appears darker at sunrise and sunset, when there are more particles in the air.\n\nThe sky can also appear blue at other times, such as when there is dust or moisture in the air. This is because these particles can also scatter sunlight.', filters=[], safety_feedback=[])

Not sure if its the model or the client, but how can developers generally go about trouble shooting this? Is there any way to look at the raw api response?
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/google-gemini/generative-ai-python/issues/10","PermissionDenied trying to call chat() or any other service method","2023-05-16T16:31:11Z","Closed issue","No label","Getting the following error trying to execute this code.
 NOTE: using the lower level API by calling aiplatform.gapic.PredictionServiceClient() works fine...
import google.generativeai as genai
import os

genai.configure(api_key=os.environ['GOOGLE_API_KEY'])

response = genai.chat(messages=[""Hello.""])

Exception has occurred: PermissionDenied (note: full exception trace is shown but execution is paused at: _run_module_as_main)
 403 Generative Language API has not been used in project 411082643095 before or it is disabled. Enable it by visiting https://console.developers.google.com/apis/api/generativelanguage.googleapis.com/overview?project=411082643095 then retry. If you enabled this API recently, wait a few minutes for the action to propagate to our systems and retry. [links {
 description: ""Google developers console API activation""
 url: ""https://console.developers.google.com/apis/api/generativelanguage.googleapis.com/overview?project=411082643095""
 }
 , reason: ""SERVICE_DISABLED""
 domain: ""googleapis.com""
 metadata {
 key: ""consumer""
 value: ""projects/411082643095""
 }
 metadata {
 key: ""service""
 value: ""generativelanguage.googleapis.com""
 }
 ]
 The text was updated successfully, but these errors were encountered: 
👀1
gururise reacted with eyes emoji
All reactions
👀1 reaction"
"https://github.com/google-gemini/generative-ai-python/issues/8","support application default credentials","2023-05-13T02:17:43Z","Closed issue","No label","https://cloud.google.com/docs/authentication/provide-credentials-adc
Are you supporting the default credentials for GCP APIs? From the code it looks like you require an api_key to be passed
 The text was updated successfully, but these errors were encountered: 
All reactions"
