"https://github.com/meta-llama/llama-stack/issues/271","7755","2024-10-20T07:23:52Z","Closed as not planned issue","No label","Originally posted by @Mustafsjamal in https://github.com/sindresorhus/Actions/issues/283
 The text was updated successfully, but these errors were encountered: 
🚀1
Mustafsjamal reacted with rocket emoji
All reactions
🚀1 reaction"
"https://github.com/meta-llama/llama-stack/issues/268","Create a remote memory provider for pinecone","2024-10-18T11:43:10Z","Open issue","good first issue","Follow the implementation of weaviate: https://github.com/meta-llama/llama-stack/tree/main/llama_stack/providers/adapters/memory/weaviate
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/llama-stack/issues/258","Ollama inference : llama3.1:8b-instruct-fp16 , what name is it in the python stack client ?","2024-10-17T00:28:47Z","Closed issue","question","Hy,
I'm using ollama with the llama3.1:8b-instruct-fp16 model as the documentation says it's the only one recognized and working but when I'm trying to use the python client, the llama stack server says that the model is not recognized, so what's the correct label to put in the line model=""Llama3.1-8B-Instruct"" ?
There is my code :
from llama_stack_client import LlamaStackClient
 from llama_stack_client.types import UserMessage
host = '127.0.0.1'
 port = 5000
client = LlamaStackClient(
 base_url=f""http://{host}:{port}"",
 )
response = client.inference.chat_completion(
 messages=[
 UserMessage(
 content=""Quelle est la capitale de paris ?"",
 role=""user"",
 ),
 ],
 model=""Llama3.1-8B-Instruct"",
 stream=False,
 )
 print(response)
There is on the llama stack side :
 Traceback (most recent call last):
 File ""/home/penta/anaconda3/envs/llamastack-Cortana/lib/python3.10/site-packages/llama_stack/distribution/server/server.py"", line 239, in endpoint
 value = func(**kwargs)
 File ""/home/penta/anaconda3/envs/llamastack-Cortana/lib/python3.10/site-packages/llama_stack/distribution/routers/routers.py"", line 94, in chat_completion
 provider = self.routing_table.get_provider_impl(model)
 File ""/home/penta/anaconda3/envs/llamastack-Cortana/lib/python3.10/site-packages/llama_stack/distribution/routers/routing_tables.py"", line 91, in get_provider_impl
 raise ValueError(f""{routing_key} not registered"")
 ValueError: Llama3.1-8B-Instruct not registered
I've tried several variations like llama3.1:8b-instruct-fp16 with and without uppercase but nothing is working...
Any help appreciated.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/llama-stack/issues/257","pytorch CUDA not found in host that has CUDA with working pytorch","2024-10-16T10:13:56Z","Open issue","question","I am getting this error.
ValueError: ProcessGroupNCCL is only supported with GPUs, no GPUs found!

Traceback (most recent call last):
  File ""/usr/local/lib/python3.10/multiprocessing/process.py"", line 314, in _bootstrap
    self.run()
  File ""/usr/local/lib/python3.10/multiprocessing/process.py"", line 108, in run
    self._target(*self._args, **self._kwargs)
  File ""/usr/local/lib/python3.10/site-packages/llama_stack/providers/impls/meta_reference/inference/parallel_utils.py"", line 285, in launch_dist_group
    elastic_launch(launch_config, entrypoint=worker_process_entrypoint)(
  File ""/usr/local/lib/python3.10/site-packages/torch/distributed/launcher/api.py"", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File ""/usr/local/lib/python3.10/site-packages/torch/distributed/launcher/api.py"", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 

ValueError: ProcessGroupNCCL is only supported with GPUs, no GPUs found!
backend_class = ProcessGroupNCCL(
File ""/usr/local/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py"", line 1594, in _new_process_group_helper
default_pg, _ = _new_process_group_helper(
File ""/usr/local/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py"", line 1368, in init_process_group
func_return = func(*args, **kwargs)
File ""/usr/local/lib/python3.10/site-packages/torch/distributed/c10d_logger.py"", line 93, in wrapper
return func(*args, **kwargs)
File ""/usr/local/lib/python3.10/site-packages/torch/distributed/c10d_logger.py"", line 79, in wrapper
torch.distributed.init_process_group(""nccl"")
File ""/usr/local/lib/python3.10/site-packages/llama_stack/providers/impls/meta_reference/inference/generation.py"", line 83, in build
llama = Llama.build(config)
File ""/usr/local/lib/python3.10/site-packages/llama_stack/providers/impls/meta_reference/inference/model_parallel.py"", line 39, in init_model_cb
model = init_model_cb()
File ""/usr/local/lib/python3.10/site-packages/llama_stack/providers/impls/meta_reference/inference/parallel_utils.py"", line 240, in worker_process_entrypoint
return f(*args, **kwargs)
File ""/usr/local/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py"", line 348, in wrapper
traceback : Traceback (most recent call last):
error_file: /tmp/torchelastic_5pr8utde/018126eb-03bf-42ad-add7-00c1e0e4ec6a_dp_sgnfv/attempt_0/0/error.json
exitcode : 1 (pid: 18)
rank : 0 (local_rank: 0)
host : llama-stack-llama3-2-11b-vision-54cf7f9bfd-rz58g
time : 2024-10-16_10:06:03
[0]:
Root Cause (first observed failure):
------------------------------------------------------------
<NO_OTHER_FAILURES>
Failures:
------------------------------------------------------------
worker_process_entrypoint FAILED
============================================================

Context
I built image with llama-stack like this
clone repo master
add docker command --platform linux/amd64
build llama-stack into venv
./env/bin/llama stack build --template local --image-type docker --name llama-stack
CUDA environment
I confirmed that there is CUDA drivers with test CUDA images.
+-----------------------------------------------------------------------------------------+                                                                                                                       
| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.4     |                                                                                                                       
|-----------------------------------------+------------------------+----------------------+                                                                                                                       
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |                                                                                                                       
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA L4                      Off |   00000000:00:03.0 Off |                    0 |
| N/A   41C    P8             17W /   72W |       1MiB /  23034MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                          
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+

and sample CUDA Pod works too
$ kubectl -n ml logs vector-add
[Vector addition of 50000 elements]
Copy input data from the host memory to the CUDA device
CUDA kernel launch with 196 blocks of 256 threads
Copy output data from the CUDA device to the host memory
Test PASSED
Done
pods
apiVersion: v1kind: Podmetadata:
  name: cuda-info
  namespace: mlspec:
  restartPolicy: OnFailure
  containers:
    - name: main
      image: cuda:12.4.1-cudnn-devel-ubuntu22.04
      command: [""nvidia-smi""]
      resources:
        limits:
          nvidia.com/gpu: 1
apiVersion: v1kind: Podmetadata:
  name: vector-add
  namespace: mlspec:
  restartPolicy: OnFailure
  containers:
    - name: main
      image: cuda-sample:vectoradd-cuda12.5.0-ubuntu22.04
      resources:
        limits:
          nvidia.com/gpu: 1
CUDA + Pytorch
I confirmed it works on this host.
apiVersion: v1kind: Podmetadata:
  name: pytorch-cuda
  namespace: mlspec:
  containers:
    - name: main
      image: pytorch/pytorch:2.4.1-cuda12.4-cudnn9-devel
      command: [""/bin/sh"", ""-c"", ""sleep 1000000""]
      resources:
        limits:
          nvidia.com/gpu: 1
$ kubectl exec -n ml --stdin --tty pytorch-cuda -- /bin/bash
root@pytorch-cuda:/workspace# python3
Python 3.11.9 | packaged by conda-forge | (main, Apr 19 2024, 18:36:13) [GCC 12.3.0] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import torch
>>> torch.cuda.current_device()
0
>>> torch.cuda.device_count() 
1
>>> torch.cuda.get_device_name(0)
'NVIDIA L4'
>>> 
root@pytorch-cuda:/workspace# 
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/llama-stack/issues/256","untagged Docker images","2024-10-17T00:34:00Z","Closed issue","No label","it is best practice to tag docker images with information about version of source code
both gpu and cpu images don't have tags and have implicit latest only.
(e.g. for myself I had to add my own tags when pushing images to private repository. ideally I would use just same tags as ground truth, but there is none for now.)
https://hub.docker.com/repository/docker/llamastack/llamastack-local-gpu/general
https://hub.docker.com/repository/docker/llamastack/llamastack-local-cpu/general
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/llama-stack/issues/255","wrong UNIX filesystem root","2024-10-16T07:54:19Z","Open issue","No label","docker image build expects models to be stored in '/root/.llama/checkpoints/`
however, elsewhere in code and documentation it is expected to be in /.llama/checkpoints/‹model name>
having /root/... is very odd. on UNIX file systems root should be just /
I was having this issue bellow. moving files into /root fixed it, meaning it is indeed looking not into / but into /root.

 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/llama-stack/issues/254","docker images are too large","2024-10-16T07:10:12Z","Open issue","No label","how come docker image is 9GB?
this is not model itself, right?
it is odd to have docker image 20x larger than model itself (e.g. 1B/3B INT4)
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/llama-stack/issues/253","missing target image architecture","2024-10-16T06:45:23Z","Open issue","good first issue","right now image is build for same architecture as current host.
$ ./llama-stack/bin/llama stack build --template local --image-type docker --name llama-stack
$ docker image inspect llamastack-llama-stack | grep Architecture

        ""Architecture"": ""arm64"",
however, how can we specify different target architecture? if say we need linux/amd64?
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/llama-stack/issues/252","option without agents database","2024-10-17T00:44:55Z","Closed issue","question","I would like to make server without agents nor database functions. but now it is impossible since images are shipped with either sqlite, redis, postresql
Configuring API `agents`...
> Configuring provider `(meta-reference)`
Enter `type` for persistence_store (options: redis, sqlite, postgres) (default: sqlite): 

 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/llama-stack/issues/251","cannot build with error /opt/homebrew/lib/python3.11/site-packages/llama_stack/configs/distributions/docker/llama-stack-build.yaml","2024-10-16T06:40:50Z","Closed issue","No label","$ llama stack build --template local --image-type docker --name llama-stack
Dockerfile created successfully in /var/folders/dt/1y99_j6s0yj7y151_dr026gh0000gn/T/tmp.bC1dbby3dG/DockerfileFROM python:3.10-slim
WORKDIR /app

RUN apt-get update && apt-get install -y        iputils-ping net-tools iproute2 dnsutils telnet        curl wget telnet        procps psmisc lsof        traceroute        bubblewrap        && rm -rf /var/lib/apt/lists/*

RUN pip install llama-stack
RUN pip install fastapi fire httpx uvicorn accelerate blobfile fairscale fbgemm-gpu==0.8.0 torch torchvision transformers zmq blobfile chardet pypdf tqdm numpy scikit-learn scipy nltk sentencepiece transformers faiss-cpu accelerate codeshield torch transformers matplotlib pillow pandas scikit-learn aiosqlite psycopg2-binary redis
RUN pip install torch --index-url https://download.pytorch.org/whl/cpu
RUN pip install sentence-transformers --no-deps

# This would be good in production but for debugging flexibility lets not add it right now# We need a more solid production ready entrypoint.sh anyway## ENTRYPOINT [""python"", ""-m"", ""llama_stack.distribution.server.server""]

ADD ../../../../../../../opt/homebrew/lib/python3.11/site-packages/llama_stack/configs/distributions/docker/llama-stack-build.yaml ./llamastack-build.yaml

+ docker build -t llamastack-llama-stack -f /var/folders/dt/1y99_j6s0yj7y151_dr026gh0000gn/T/tmp.bC1dbby3dG/Dockerfile /opt/homebrew/lib/python3.11/site-packages
[+] Building 1.0s (12/12) FINISHED                         docker:desktop-linux
 => [internal] load build definition from Dockerfile                       0.0s
 => => transferring dockerfile: 1.16kB                                     0.0s
 => [internal] load metadata for docker.io/library/python:3.10-slim        1.0s
 => [internal] load .dockerignore                                          0.0s
 => => transferring context: 2B                                            0.0s
 => CANCELED [1/8] FROM docker.io/library/python:3.10-slim@sha256:1eb5d76  0.0s
 => => resolve docker.io/library/python:3.10-slim@sha256:1eb5d76bf3e9e612  0.0s
 => => sha256:1eb5d76bf3e9e612176ebf5eadf8f27ec300b7b4b9a 9.13kB / 9.13kB  0.0s
 => => sha256:625b1a937cde3b5a44972e3bf2f78602a48ad78c64b 1.75kB / 1.75kB  0.0s
 => => sha256:650e4e91c7fe9a12b913669767cca583eb1a5501b18 5.22kB / 5.22kB  0.0s
 => [internal] load build context                                          0.0s
 => => transferring context: 2B                                            0.0s
 => CACHED [2/8] WORKDIR /app                                              0.0s
 => CACHED [3/8] RUN apt-get update && apt-get install -y        iputils-  0.0s
 => CACHED [4/8] RUN pip install llama-stack                               0.0s
 => CACHED [5/8] RUN pip install fastapi fire httpx uvicorn accelerate bl  0.0s
 => CACHED [6/8] RUN pip install torch --index-url https://download.pytor  0.0s
 => CACHED [7/8] RUN pip install sentence-transformers --no-deps           0.0s
 => ERROR [8/8] ADD ../../../../../../../opt/homebrew/lib/python3.11/site  0.0s
------
 > [8/8] ADD ../../../../../../../opt/homebrew/lib/python3.11/site-packages/llama_stack/configs/distributions/docker/llama-stack-build.yaml ./llamastack-build.yaml:
------
Dockerfile:16
--------------------
  14 |     # ENTRYPOINT [""python"", ""-m"", ""llama_stack.distribution.server.server""]
  15 |     
  16 | >>> ADD ../../../../../../../opt/homebrew/lib/python3.11/site-packages/llama_stack/configs/distributions/docker/llama-stack-build.yaml ./llamastack-build.yaml
  17 |     
--------------------
ERROR: failed to solve: failed to compute cache key: failed to calculate checksum of ref 7f62fc5c-a758-4e65-8031-dc8ff9831b1c::swd1tz3aiq8y761wn4ccfbm7c: ""/opt/homebrew/lib/python3.11/site-packages/llama_stack/configs/distributions/docker/llama-stack-build.yaml"": not found
however, this file is present in the system.
$ cat /opt/homebrew/lib/python3.11/site-packages/llama_stack/configs/distributions/docker/llama-stack-build.yaml
name: llama-stack
distribution_spec:
  description: Use code from `llama_stack` itself to serve all llama stack APIs
  docker_image: null
  providers:
    inference: meta-reference
    memory: meta-reference
    safety: meta-reference
    agents: meta-reference
    telemetry: meta-reference
image_type: docker
can we avoid this cusomisation config files? this approach is very bug prone. I literally fail at step 1 of instructions.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/llama-stack/issues/250","I cant use llama-stack on ubuntu 20.04 even ı downloaded with pip","2024-10-19T10:46:26Z","Closed issue","question","even ı pipped the llama stack on ubuntu 20.04 I m facing with this issue
and ı tried tried sudo snap install its an offtopic command line code for me
comp@comp:~$ llama model-list
Command 'llama' not found, but can be installed with:
sudo snap install llama
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/llama-stack/issues/248","[W socket.cpp:697] [c10d] The IPv6 network addresses of (1.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.ip6.arpa, 50714) cannot be retrieved (gai error: 8 - nodename nor servname provided, or not known).","2024-10-15T02:32:13Z","Open issue","question","Command:
llama stack run Llama3.2-11B-Vision-Instruct --port 5000
Output:
Using config `/Users/mac/.llama/builds/conda/Llama3.2-11B-Vision-Instruct-run.yaml`
Resolved 4 providers
 inner-inference => meta-reference
 models => __routing_table__
 inference => __autorouted__
 inspect => __builtin__

[2024-10-15 07:20:46,247] torch.distributed.elastic.multiprocessing.redirects: [WARNING] NOTE: Redirects are currently not supported in Windows or MacOs.
Loading model `Llama3.2-11B-Vision-Instruct`
[W socket.cpp:697] [c10d] The IPv6 network addresses of (1.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.ip6.arpa, 50714) cannot be retrieved (gai error: 8 - nodename nor servname provided, or not known).
[W socket.cpp:697] [c10d] The IPv6 network addresses of (1.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.ip6.arpa, 50714) cannot be retrieved (gai error: 8 - nodename nor servname provided, or not known).
[W socket.cpp:697] [c10d] The IPv6 network addresses of (1.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.ip6.arpa, 50714) cannot be retrieved (gai error: 8 - nodename nor servname provided, or not known).
[W socket.cpp:697] [c10d] The IPv6 network addresses of (1.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.ip6.arpa, 50714) cannot be retrieved (gai error: 8 - nodename nor servname provided, or not known).
[W socket.cpp:697] [c10d] The IPv6 network addresses of (1.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.ip6.arpa, 50714) cannot be retrieved (gai error: 8 - nodename nor servname provided, or not known).

config `/Users/mac/.llama/builds/conda/Llama3.2-11B-Vision-Instruct-run.yaml
version: '2'
built_at: '2024-10-15T06:25:20.837875'
image_name: Llama3.2-11B-Vision-Instruct
docker_image: null
conda_env: Llama3.2-11B-Vision-Instruct
logging:
  level: DEBUG
  file:
    enabled: true
    path: /Users/mac/llama-32.log
authentication:
  enabled: false
apis:
- inference
providers:
  inference:
  - provider_id: meta-reference
    provider_type: meta-reference
    config:
      model: Llama3.2-11B-Vision-Instruct
      torch_seed: null
      max_seq_len: 4096
      max_batch_size: 1

Command:
python -m llama_stack.apis.inference.client localhost 5000 --no-auth
Output
User>hello world, write me a 2 sentence poem about the moon
Error: HTTP 403
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/llama-stack/issues/246","AttributeError: 'ChatCompletionResponse' object has no attribute 'event'","2024-10-14T09:19:58Z","Open issue","question","data_url = data_url_from_image(""dog.jpg"")
 print(""The obtained data url is"", data_url)
 iterator = client.inference.chat_completion(
 model=model,
 messages=[
 {
 ""role"": ""user"",
 ""content"": [
 { ""image"": { ""uri"": data_url } },
 ""Write a haiku describing the image""
 ]
 }
 ],
 stream=True
 )
for chunk in iterator:
 print(chunk.event.delta, end="""", flush=True)
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/llama-stack/issues/245","Llama3.2-1B only reply ""<|end_of_text|>""","2024-10-17T16:46:02Z","Closed issue","No label","Hi Expert,
 I just tried to to install llama-stack and run the test with Llama3.2-1B but I found the response is really weird. Since my GPU RAM is only 6GB, I can't try bigger model to see if its the problem of ""Llama3.2-1B"". Just want to make sure I didn't miss anything in the ""get start"" document. Could you kindly help point out anything I might get wrong to lead this result?
 Thank you very much!
My install:
git clone git@github.com:meta-llama/llama-stack.git

conda create -n stack python=3.10
conda activate stack

llama stack build
> Enter an unique name for identifying your Llama Stack build distribution (e.g. my-local-stack): my-local
> Enter the image type you want your distribution to be built with (docker or conda): conda

 Llama Stack is composed of several APIs working together. Let's configure the providers (implementations) you want to use for these APIs.
> Enter the API provider for the inference API: (default=meta-reference): meta-reference
> Enter the API provider for the safety API: (default=meta-reference): meta-reference
> Enter the API provider for the agents API: (default=meta-reference): meta-reference
> Enter the API provider for the memory API: (default=meta-reference): meta-reference
> Enter the API provider for the telemetry API: (default=meta-reference): meta-reference

llama stack configure my_local
Could not find my_local. Trying conda build name instead...
Configuration already exists at `/home/ivan/.llama/builds/conda/my_local-run.yaml`. Will overwrite...
Configuring API `inference`...
=== Configuring provider `meta-reference` for API inference...
Enter value for model (default: Llama3.1-8B-Instruct) (required): Llama3.2-1B            
Do you want to configure quantization? (y/n): n
Enter value for torch_seed (optional): 
Enter value for max_seq_len (default: 4096) (required): 
Enter value for max_batch_size (default: 1) (required): 

Configuring API `safety`...
=== Configuring provider `meta-reference` for API safety...
Do you want to configure llama_guard_shield? (y/n): n
Enter value for enable_prompt_guard (default: False) (optional): 

Configuring API `agents`...
=== Configuring provider `meta-reference` for API agents...
Enter `type` for persistence_store (options: redis, sqlite, postgres) (default: sqlite): 

Configuring SqliteKVStoreConfig:
Enter value for namespace (optional): 
Enter value for db_path (existing: /home/ivan/.llama/runtime/kvstore.db) (required): 

Configuring API `memory`...
=== Configuring provider `meta-reference` for API memory...
> Please enter the supported memory bank type your provider has for memory: vector

Configuring API `telemetry`...
=== Configuring provider `meta-reference` for API telemetry...

llama stack run my_local --disable-ipv6

Test
python -m llama_stack.apis.inference.client localhost 5000  --model=Llama3.2-1B

User>hello world, write me a 2 sentence poem about the moon
Assistant> <|end_of_text|>

My OS and GPU
PRETTY_NAME=""Ubuntu 22.04.5 LTS""
NAME=""Ubuntu""
VERSION_ID=""22.04""
VERSION=""22.04.5 LTS (Jammy Jellyfish)""
VERSION_CODENAME=jammy
ID=ubuntu
ID_LIKE=debian
HOME_URL=""https://www.ubuntu.com/""
SUPPORT_URL=""https://help.ubuntu.com/""
BUG_REPORT_URL=""https://bugs.launchpad.net/ubuntu/""
PRIVACY_POLICY_URL=""https://www.ubuntu.com/legal/terms-and-policies/privacy-policy""
UBUNTU_CODENAME=jammy

+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.107.02             Driver Version: 550.107.02     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA GeForce GTX 1660 Ti     Off |   00000000:01:00.0  On |                  N/A |
| N/A   80C    P0             28W /   80W |    3407MiB /   6144MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A      1847      G   /usr/bin/gnome-shell                            1MiB |
|    0   N/A  N/A      9258      C   ...envs/llamastack-my_local/bin/python       3352MiB |
+-----------------------------------------------------------------------------------------+


 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/llama-stack/issues/244","Does Quantization (FP8) support the Llama3.2-90B-Vision-Instruct model?","2024-10-12T08:30:12Z","Open issue","No label","Hello, I encountered some problems when loading the Llama3.2-90B-Vision-Instruct model with FP8. Can you help me take a look?
Version of llama_stack and llama_models:
llama_models == 0.0.41
llama_stack == 0.0.41

Resolved 15 providers
 inner-inference => meta-reference
 models => __routing_table__
 inference => __autorouted__
 inner-safety => meta-reference-00
 inner-safety => meta-reference-01
 inner-safety => meta-reference-02
 inner-safety => meta-reference-03
 inner-memory => meta-reference
 shields => __routing_table__
 safety => __autorouted__
 memory_banks => __routing_table__
 memory => __autorouted__
 agents => meta-reference
 telemetry => meta-reference
 inspect => __builtin__

Loading model `Llama3.2-90B-Vision-Instruct`
> initializing model parallel with size 8
> initializing ddp with size 1
> initializing pipeline with size 1
/home/user/anaconda3/envs/llamastack-llama3.2/lib/python3.10/site-packages/torch/__init__.py:955: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:432.)
  _C._set_default_tensor_type(t)
/home/user/anaconda3/envs/llamastack-llama3.2/lib/python3.10/site-packages/torch/__init__.py:955: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:432.)
  _C._set_default_tensor_type(t)
/home/user/anaconda3/envs/llamastack-llama3.2/lib/python3.10/site-packages/torch/__init__.py:955: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:432.)
  _C._set_default_tensor_type(t)
/home/user/anaconda3/envs/llamastack-llama3.2/lib/python3.10/site-packages/torch/__init__.py:955: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:432.)
  _C._set_default_tensor_type(t)
/home/user/anaconda3/envs/llamastack-llama3.2/lib/python3.10/site-packages/torch/__init__.py:955: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:432.)
  _C._set_default_tensor_type(t)
/home/user/anaconda3/envs/llamastack-llama3.2/lib/python3.10/site-packages/torch/__init__.py:955: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:432.)
  _C._set_default_tensor_type(t)
/home/user/anaconda3/envs/llamastack-llama3.2/lib/python3.10/site-packages/torch/__init__.py:955: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:432.)
  _C._set_default_tensor_type(t)
/home/user/anaconda3/envs/llamastack-llama3.2/lib/python3.10/site-packages/torch/__init__.py:955: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:432.)
  _C._set_default_tensor_type(t)
Using efficient FP8 operators in FBGEMM.
W1012 16:05:13.889000 140482356840256 torch/multiprocessing/spawn.py:146] Terminating process 3952024 via signal SIGTERM
W1012 16:05:13.890000 140482356840256 torch/multiprocessing/spawn.py:146] Terminating process 3952025 via signal SIGTERM
W1012 16:05:13.890000 140482356840256 torch/multiprocessing/spawn.py:146] Terminating process 3952026 via signal SIGTERM
W1012 16:05:13.890000 140482356840256 torch/multiprocessing/spawn.py:146] Terminating process 3952027 via signal SIGTERM
W1012 16:05:13.890000 140482356840256 torch/multiprocessing/spawn.py:146] Terminating process 3952028 via signal SIGTERM
W1012 16:05:13.890000 140482356840256 torch/multiprocessing/spawn.py:146] Terminating process 3952029 via signal SIGTERM
W1012 16:05:13.890000 140482356840256 torch/multiprocessing/spawn.py:146] Terminating process 3952030 via signal SIGTERM
E1012 16:05:20.861000 140482356840256 torch/distributed/elastic/multiprocessing/api.py:702] failed (exitcode: 1) local_rank: 0 (pid: 3952023) of fn: worker_process_entrypoint (start_method: fork)
E1012 16:05:20.861000 140482356840256 torch/distributed/elastic/multiprocessing/api.py:702] Traceback (most recent call last):
E1012 16:05:20.861000 140482356840256 torch/distributed/elastic/multiprocessing/api.py:702]   File ""/home/user/anaconda3/envs/llamastack-llama3.2/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py"", line 659, in _poll
E1012 16:05:20.861000 140482356840256 torch/distributed/elastic/multiprocessing/api.py:702]     self._pc.join(-1)
E1012 16:05:20.861000 140482356840256 torch/distributed/elastic/multiprocessing/api.py:702]   File ""/home/user/anaconda3/envs/llamastack-llama3.2/lib/python3.10/site-packages/torch/multiprocessing/spawn.py"", line 189, in join
E1012 16:05:20.861000 140482356840256 torch/distributed/elastic/multiprocessing/api.py:702]     raise ProcessRaisedException(msg, error_index, failed_process.pid)
E1012 16:05:20.861000 140482356840256 torch/distributed/elastic/multiprocessing/api.py:702] torch.multiprocessing.spawn.ProcessRaisedException: 
E1012 16:05:20.861000 140482356840256 torch/distributed/elastic/multiprocessing/api.py:702] 
E1012 16:05:20.861000 140482356840256 torch/distributed/elastic/multiprocessing/api.py:702] -- Process 0 terminated with the following error:
E1012 16:05:20.861000 140482356840256 torch/distributed/elastic/multiprocessing/api.py:702] Traceback (most recent call last):
E1012 16:05:20.861000 140482356840256 torch/distributed/elastic/multiprocessing/api.py:702]   File ""/home/user/anaconda3/envs/llamastack-llama3.2/lib/python3.10/site-packages/torch/multiprocessing/spawn.py"", line 76, in _wrap
E1012 16:05:20.861000 140482356840256 torch/distributed/elastic/multiprocessing/api.py:702]     fn(i, *args)
E1012 16:05:20.861000 140482356840256 torch/distributed/elastic/multiprocessing/api.py:702]   File ""/home/user/anaconda3/envs/llamastack-llama3.2/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py"", line 583, in _wrap
E1012 16:05:20.861000 140482356840256 torch/distributed/elastic/multiprocessing/api.py:702]     ret = record(fn)(*args_)
E1012 16:05:20.861000 140482356840256 torch/distributed/elastic/multiprocessing/api.py:702]   File ""/home/user/anaconda3/envs/llamastack-llama3.2/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py"", line 348, in wrapper
E1012 16:05:20.861000 140482356840256 torch/distributed/elastic/multiprocessing/api.py:702]     return f(*args, **kwargs)
E1012 16:05:20.861000 140482356840256 torch/distributed/elastic/multiprocessing/api.py:702]   File ""/home/user/anaconda3/envs/llamastack-llama3.2/lib/python3.10/site-packages/llama_stack/providers/impls/meta_reference/inference/parallel_utils.py"", line 240, in worker_process_entrypoint
E1012 16:05:20.861000 140482356840256 torch/distributed/elastic/multiprocessing/api.py:702]     model = init_model_cb()
E1012 16:05:20.861000 140482356840256 torch/distributed/elastic/multiprocessing/api.py:702]   File ""/home/user/anaconda3/envs/llamastack-llama3.2/lib/python3.10/site-packages/llama_stack/providers/impls/meta_reference/inference/model_parallel.py"", line 40, in init_model_cb
E1012 16:05:20.861000 140482356840256 torch/distributed/elastic/multiprocessing/api.py:702]     llama = Llama.build(config)
E1012 16:05:20.861000 140482356840256 torch/distributed/elastic/multiprocessing/api.py:702]   File ""/home/user/anaconda3/envs/llamastack-llama3.2/lib/python3.10/site-packages/llama_stack/providers/impls/meta_reference/inference/generation.py"", line 154, in build
E1012 16:05:20.861000 140482356840256 torch/distributed/elastic/multiprocessing/api.py:702]     model = convert_to_quantized_model(model, config)
E1012 16:05:20.861000 140482356840256 torch/distributed/elastic/multiprocessing/api.py:702]   File ""/home/user/anaconda3/envs/llamastack-llama3.2/lib/python3.10/site-packages/llama_stack/providers/impls/meta_reference/inference/quantization/loader.py"", line 61, in convert_to_quantized_model
E1012 16:05:20.861000 140482356840256 torch/distributed/elastic/multiprocessing/api.py:702]     checkpoint = config.checkpoint_config.checkpoint
E1012 16:05:20.861000 140482356840256 torch/distributed/elastic/multiprocessing/api.py:702]   File ""/home/user/anaconda3/envs/llamastack-llama3.2/lib/python3.10/site-packages/pydantic/main.py"", line 856, in __getattr__
E1012 16:05:20.861000 140482356840256 torch/distributed/elastic/multiprocessing/api.py:702]     raise AttributeError(f'{type(self).__name__!r} object has no attribute {item!r}')
E1012 16:05:20.861000 140482356840256 torch/distributed/elastic/multiprocessing/api.py:702] AttributeError: 'MetaReferenceImplConfig' object has no attribute 'checkpoint_config'
E1012 16:05:20.861000 140482356840256 torch/distributed/elastic/multiprocessing/api.py:702] 
Process ForkProcess-1:
Traceback (most recent call last):
  File ""/home/user/anaconda3/envs/llamastack-llama3.2/lib/python3.10/multiprocessing/process.py"", line 314, in _bootstrap
    self.run()
  File ""/home/user/anaconda3/envs/llamastack-llama3.2/lib/python3.10/multiprocessing/process.py"", line 108, in run
    self._target(*self._args, **self._kwargs)
  File ""/home/user/anaconda3/envs/llamastack-llama3.2/lib/python3.10/site-packages/llama_stack/providers/impls/meta_reference/inference/parallel_utils.py"", line 285, in launch_dist_group
    elastic_launch(launch_config, entrypoint=worker_process_entrypoint)(
  File ""/home/user/anaconda3/envs/llamastack-llama3.2/lib/python3.10/site-packages/torch/distributed/launcher/api.py"", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File ""/home/user/anaconda3/envs/llamastack-llama3.2/lib/python3.10/site-packages/torch/distributed/launcher/api.py"", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
worker_process_entrypoint FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-10-12_16:05:07
  host      : ub-server-test
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 3952023)
  error_file: /tmp/torchelastic_u4yt51_2/d578de5b-e518-4f55-92d1-cbd955e2d050_yh4bst2v/attempt_0/0/error.json
  traceback : Traceback (most recent call last):
    File ""/home/user/anaconda3/envs/llamastack-llama3.2/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py"", line 348, in wrapper
      return f(*args, **kwargs)
    File ""/home/user/anaconda3/envs/llamastack-llama3.2/lib/python3.10/site-packages/llama_stack/providers/impls/meta_reference/inference/parallel_utils.py"", line 240, in worker_process_entrypoint
      model = init_model_cb()
    File ""/home/user/anaconda3/envs/llamastack-llama3.2/lib/python3.10/site-packages/llama_stack/providers/impls/meta_reference/inference/model_parallel.py"", line 40, in init_model_cb
      llama = Llama.build(config)
    File ""/home/user/anaconda3/envs/llamastack-llama3.2/lib/python3.10/site-packages/llama_stack/providers/impls/meta_reference/inference/generation.py"", line 154, in build
      model = convert_to_quantized_model(model, config)
    File ""/home/user/anaconda3/envs/llamastack-llama3.2/lib/python3.10/site-packages/llama_stack/providers/impls/meta_reference/inference/quantization/loader.py"", line 61, in convert_to_quantized_model
      checkpoint = config.checkpoint_config.checkpoint
    File ""/home/user/anaconda3/envs/llamastack-llama3.2/lib/python3.10/site-packages/pydantic/main.py"", line 856, in __getattr__
      raise AttributeError(f'{type(self).__name__!r} object has no attribute {item!r}')
  AttributeError: 'MetaReferenceImplConfig' object has no attribute 'checkpoint_config'

 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/llama-stack/issues/242","I used the official Docker image and downloaded the weight file from Meta. The md5sum test proved that the file was fine, but it still failed to run, which left me confused","2024-10-12T02:48:38Z","Open issue","No label","I used the official Docker image and downloaded the weight file from Meta. The md5sum test proved that the file was fine, but it still failed to run, which left me confused，I confirm that CUDA can be used from within Docker
root@720:~/.llama/checkpoints# nvidia-smi 
Sat Oct 12 03:22:06 2024       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla P40                      Off |   00000000:05:00.0 Off |                  Off |
| N/A   34C    P0             49W /  250W |       0MiB /  24576MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  Tesla P40                      Off |   00000000:42:00.0 Off |                  Off |
| N/A   38C    P0             45W /  250W |       0MiB /  24576MiB |      1%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+

`root@720:~/.llama/checkpoints/Llama3.1-8B-Instruct# ls -alh
total 15G
drwxr-xr-x 2 root root 4.0K Oct 12 02:31 .
drwxr-xr-x 4 root root 4.0K Oct 12 02:42 ..
-rw-r--r-- 1 root root  15G Oct 12 02:31 consolidated.00.pth
-rw-r--r-- 1 root root  199 Oct 12 02:31 params.json
-rw-r--r-- 1 root root 8.7M Oct 12 02:31 tokenizer.json
-rw-r--r-- 1 root root 489K Oct 12 02:31 tokenizer.model

root@720:~/.llama/checkpoints# docker run -it -p 5000:5000 -v ~/.llama:/root/.llama --gpus=all llamastack/llamastack-local-gpu
Resolved 8 providers in topological order
  Api.models: routing_table
  Api.inference: router
  Api.shields: routing_table
  Api.safety: router
  Api.memory_banks: routing_table
  Api.memory: router
  Api.agents: meta-reference
  Api.telemetry: meta-reference

Traceback (most recent call last):
  File ""/usr/local/lib/python3.10/runpy.py"", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File ""/usr/local/lib/python3.10/runpy.py"", line 86, in _run_code
    exec(code, run_globals)
  File ""/usr/local/lib/python3.10/site-packages/llama_stack/distribution/server/server.py"", line 351, in <module>
    fire.Fire(main)
  File ""/usr/local/lib/python3.10/site-packages/fire/core.py"", line 135, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File ""/usr/local/lib/python3.10/site-packages/fire/core.py"", line 468, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
  File ""/usr/local/lib/python3.10/site-packages/fire/core.py"", line 684, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File ""/usr/local/lib/python3.10/site-packages/llama_stack/distribution/server/server.py"", line 288, in main
    impls, specs = asyncio.run(resolve_impls_with_routing(config))
  File ""/usr/local/lib/python3.10/asyncio/runners.py"", line 44, in run  
    return loop.run_until_complete(main)
  File ""/usr/local/lib/python3.10/asyncio/base_events.py"", line 649, in run_until_complete
    return future.result()
  File ""/usr/local/lib/python3.10/site-packages/llama_stack/distribution/resolver.py"", line 104, in resolve_impls_with_routing
    impl = await instantiate_provider(spec, deps, configs[api])
  File ""/usr/local/lib/python3.10/site-packages/llama_stack/distribution/resolver.py"", line 174, in instantiate_provider
    impl = await instantiate_provider(
  File ""/usr/local/lib/python3.10/site-packages/llama_stack/distribution/resolver.py"", line 192, in instantiate_provider
    impl = await fn(*args)
  File ""/usr/local/lib/python3.10/site-packages/llama_stack/providers/impls/meta_reference/inference/__init__.py"", line 18, in get_provider_impl
    await impl.initialize()
  File ""/usr/local/lib/python3.10/site-packages/llama_stack/providers/impls/meta_reference/inference/inference.py"", line 38, in initialize
    self.generator = LlamaModelParallelGenerator(self.config)
  File ""/usr/local/lib/python3.10/site-packages/llama_stack/providers/impls/meta_reference/inference/model_parallel.py"", line 72, in __init__
    self.formatter = ChatFormat(Tokenizer(tokenizer_path))
  File ""/usr/local/lib/python3.10/site-packages/llama_models/llama3/api/tokenizer.py"", line 77, in __init__
    mergeable_ranks = load_tiktoken_bpe(model_path)
  File ""/usr/local/lib/python3.10/site-packages/tiktoken/load.py"", line 145, in load_tiktoken_bpe
    return {
  File ""/usr/local/lib/python3.10/site-packages/tiktoken/load.py"", line 147, in <dictcomp>
    for token, rank in (line.split() for line in contents.splitlines() if line)
ValueError: not enough values to unpack (expected 2, got 1)
`

 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/llama-stack/issues/238","Llama3.1-8B-Instruct,already there, but llama stack can not find it。conda and docker both doesn't work~~~~~","2024-10-15T22:04:01Z","Closed issue","No label","Failed to run stack through conda：llama stack run stack-3.2-1B --port 5000 --disable-ipv6
#194 ，I don't know why stack needs to link it to the address [: ffff: 0.0.2.208]
Failed to run stack through docker,stack do not support .pth ? I download safetensors 1B module from huggingface it also doesn't work.
root@720:~/.llama/checkpoints/Llama3.1-8B-Instruct# ls -alh
total 15G
drwxr-xr-x  2 root root 4.0K Oct  5 04:30 .
drwxr-xr-x 11 root root 4.0K Oct  5 04:36 ..
-rw-r--r--  1 root root  15G Jul 20 05:55 consolidated.00.pth
-rw-r--r--  1 root root  199 Jul 20 05:55 params.json
-rw-r--r--  1 root root 8.7M Sep 29 15:38 tokenizer.json
-rw-r--r--  1 root root 489K Oct  5 04:30 tokenizer.model

root@720:~/.llama/checkpoints/Llama3.1-8B-Instruct# docker run -it -p 5000:5000 -v ~/.llama:/root/.llama --gpus=all llamastack/llamastack-local-gpu
Resolved 8 providers in topological order
  Api.models: routing_table
  Api.inference: router
  Api.shields: routing_table
  Api.safety: router
  Api.memory_banks: routing_table
  Api.memory: router
  Api.agents: meta-reference
  Api.telemetry: meta-reference

Traceback (most recent call last):
  File ""/usr/local/lib/python3.10/runpy.py"", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File ""/usr/local/lib/python3.10/runpy.py"", line 86, in _run_code
    exec(code, run_globals)
  File ""/usr/local/lib/python3.10/site-packages/llama_stack/distribution/server/server.py"", line 351, in <module>
    fire.Fire(main)
  File ""/usr/local/lib/python3.10/site-packages/fire/core.py"", line 135, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File ""/usr/local/lib/python3.10/site-packages/fire/core.py"", line 468, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
  File ""/usr/local/lib/python3.10/site-packages/fire/core.py"", line 684, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File ""/usr/local/lib/python3.10/site-packages/llama_stack/distribution/server/server.py"", line 288, in main
    impls, specs = asyncio.run(resolve_impls_with_routing(config))
  File ""/usr/local/lib/python3.10/asyncio/runners.py"", line 44, in run
    return loop.run_until_complete(main)
  File ""/usr/local/lib/python3.10/asyncio/base_events.py"", line 649, in run_until_complete
    return future.result()
  File ""/usr/local/lib/python3.10/site-packages/llama_stack/distribution/resolver.py"", line 104, in resolve_impls_with_routing
    impl = await instantiate_provider(spec, deps, configs[api])
  File ""/usr/local/lib/python3.10/site-packages/llama_stack/distribution/resolver.py"", line 174, in instantiate_provider
    impl = await instantiate_provider(
  File ""/usr/local/lib/python3.10/site-packages/llama_stack/distribution/resolver.py"", line 192, in instantiate_provider
    impl = await fn(*args)
  File ""/usr/local/lib/python3.10/site-packages/llama_stack/providers/impls/meta_reference/inference/__init__.py"", line 18, in get_provider_impl
    await impl.initialize()
  File ""/usr/local/lib/python3.10/site-packages/llama_stack/providers/impls/meta_reference/inference/inference.py"", line 38, in initialize
    self.generator = LlamaModelParallelGenerator(self.config)
  File ""/usr/local/lib/python3.10/site-packages/llama_stack/providers/impls/meta_reference/inference/model_parallel.py"", line 70, in __init__
    checkpoint_dir = model_checkpoint_dir(self.model)
  File ""/usr/local/lib/python3.10/site-packages/llama_stack/providers/impls/meta_reference/inference/generation.py"", line 54, in model_checkpoint_dirassert checkpoint_dir.exists(), (
AssertionError: Could not find checkpoints in: /root/.llama/checkpoints/Llama3.1-8B-Instruct. Please download model using `llama download --model-id Llama3.1-8B-Instruct`

 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/llama-stack/issues/234","Tool Registry for Agents","2024-10-10T14:32:00Z","Open issue","enhancement","We need to have a capability to add new tools or disable/remove tools sometimes after an agent has been deployed.
Similar to current methods with @webmethod decorators for agents, could we create another set of methods through which we can register new tools, or disable/remove tools while the agent is running? Ideally I should be able to register tool schema (and REST API config).
This ability may be important if llama stack is going to be sort of ""llm OS"" then we need to have the ability to add or remove available tools at any time.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/llama-stack/issues/233","Add llama download support for multiple models with comma-separated list","2024-10-17T06:16:21Z","Closed issue","good first issue","The Llama CLI streamlines downloading a Llama model:
llama download  --model-id Llama3.1-70B-Instruct

A nice improvement would be enabling --model-ids MODEL_A,MODEL_B to download multiple models in one go!
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/llama-stack/issues/229","llama stack build failed by TypeError","2024-10-09T22:33:07Z","Closed issue","No label","Steps and log
(LStack) amd@tw024:~/alehe/llama-stack$ llama stack build
> Enter a name for your Llama Stack (e.g. my-local-stack): my-local-stack
> Enter the image type you want your Llama Stack to be built as (docker or conda): conda

 Llama Stack is composed of several APIs working together. Let's configure the providers (implementations) you want to use for these APIs.
> Enter provider for the inference API: (default=meta-reference): meta-reference
> Enter provider for the safety API: (default=meta-reference): meta-reference
> Enter provider for the agents API: (default=meta-reference): meta-reference
> Enter provider for the memory API: (default=meta-reference): meta-reference
> Enter provider for the telemetry API: (default=meta-reference): meta-reference

 > (Optional) Enter a short description for your Llama Stack: my local stack with ollama
Traceback (most recent call last):
  File ""/home/amd/.local/bin/llama"", line 8, in <module>
    sys.exit(main())
  File ""/home/amd/alehe/llama-stack/llama_stack/cli/llama.py"", line 44, in main
    parser.run(args)
  File ""/home/amd/alehe/llama-stack/llama_stack/cli/llama.py"", line 38, in run
    args.func(args)
  File ""/home/amd/alehe/llama-stack/llama_stack/cli/stack/build.py"", line 285, in _run_stack_build_command
    self._run_stack_build_command_from_build_config(build_config)
  File ""/home/amd/alehe/llama-stack/llama_stack/cli/stack/build.py"", line 131, in _run_stack_build_command_from_build_config
    return_code = build_image(build_config, build_file_path)
  File ""/home/amd/alehe/llama-stack/llama_stack/distribution/build.py"", line 103, in build_image
    script = pkg_resources.resource_filename(
  File ""/usr/lib/python3/dist-packages/pkg_resources/__init__.py"", line 1135, in resource_filename
    return get_provider(package_or_requirement).get_resource_filename(
  File ""/usr/lib/python3/dist-packages/pkg_resources/__init__.py"", line 354, in get_provider
    return _find_adapter(_provider_factories, loader)(module)
  File ""/usr/lib/python3/dist-packages/pkg_resources/__init__.py"", line 1384, in __init__
    self.module_path = os.path.dirname(getattr(module, '__file__', ''))
  File ""/usr/lib/python3.10/posixpath.py"", line 152, in dirname
    p = os.fspath(p)
TypeError: expected str, bytes or os.PathLike object, not NoneType
(LStack) amd@tw024:~/alehe/llama-stack$ llama stack build --config llama_stack/distribution/templates/local-ollama-build.yaml
Traceback (most recent call last):
  File ""/home/amd/.local/bin/llama"", line 8, in <module>
    sys.exit(main())
  File ""/home/amd/alehe/llama-stack/llama_stack/cli/llama.py"", line 44, in main
    parser.run(args)
  File ""/home/amd/alehe/llama-stack/llama_stack/cli/llama.py"", line 38, in run
    args.func(args)
  File ""/home/amd/alehe/llama-stack/llama_stack/cli/stack/build.py"", line 294, in _run_stack_build_command
    self._run_stack_build_command_from_build_config(build_config)
  File ""/home/amd/alehe/llama-stack/llama_stack/cli/stack/build.py"", line 131, in _run_stack_build_command_from_build_config
    return_code = build_image(build_config, build_file_path)
  File ""/home/amd/alehe/llama-stack/llama_stack/distribution/build.py"", line 103, in build_image
    script = pkg_resources.resource_filename(
  File ""/usr/lib/python3/dist-packages/pkg_resources/__init__.py"", line 1135, in resource_filename
    return get_provider(package_or_requirement).get_resource_filename(
  File ""/usr/lib/python3/dist-packages/pkg_resources/__init__.py"", line 354, in get_provider
    return _find_adapter(_provider_factories, loader)(module)
  File ""/usr/lib/python3/dist-packages/pkg_resources/__init__.py"", line 1384, in __init__
    self.module_path = os.path.dirname(getattr(module, '__file__', ''))
  File ""/usr/lib/python3.10/posixpath.py"", line 152, in dirname
    p = os.fspath(p)
TypeError: expected str, bytes or os.PathLike object, not NoneType
(LStack) amd@tw024:~/alehe/llama-stack$


 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/llama-stack/issues/228","Failed when run local-ollama","2024-10-09T12:37:32Z","Closed issue","No label","Steps:
build the local-ollama
 $ llama stack build --config llama_stack/distribution/templates/local-ollama-build.yaml
configure
 $ llama stack configure local-ollama
run
 $ llama stack run local-ollama --port 5566
Error log
(LlamaStack) amd@tw024:~/alehe/llama-stack$ llama stack run local-ollama --port 5566
Traceback (most recent call last):
  File ""/home/amd/.local/bin/llama"", line 8, in <module>
    sys.exit(main())
  File ""/home/amd/alehe/llama-stack/llama_stack/cli/llama.py"", line 44, in main
    parser.run(args)
  File ""/home/amd/alehe/llama-stack/llama_stack/cli/llama.py"", line 38, in run
    args.func(args)
  File ""/home/amd/alehe/llama-stack/llama_stack/cli/stack/run.py"", line 79, in _run_stack_run_cmd
    config = StackRunConfig(**yaml.safe_load(f))
  File ""/home/amd/.local/lib/python3.10/site-packages/pydantic/main.py"", line 212, in __init__
    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
pydantic_core._pydantic_core.ValidationError: 10 validation errors for StackRunConfig
api_providers.agents.GenericProviderConfig.provider_type
  Field required [type=missing, input_value={'provider_id': 'meta-ref...a/runtime/kvstore.db'}}}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.9/v/missing
api_providers.agents.PlaceholderProviderConfig.providers
  Field required [type=missing, input_value={'provider_id': 'meta-ref...a/runtime/kvstore.db'}}}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.9/v/missing
api_providers.telemetry.GenericProviderConfig.provider_type
  Field required [type=missing, input_value={'provider_id': 'meta-reference', 'config': {}}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.9/v/missing
api_providers.telemetry.PlaceholderProviderConfig.providers
  Field required [type=missing, input_value={'provider_id': 'meta-reference', 'config': {}}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.9/v/missing
routing_table.inference.0.provider_type
  Field required [type=missing, input_value={'provider_id': 'remote::...a-Llama3.1-8B-Instruct'}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.9/v/missing
routing_table.memory.0.provider_type
  Field required [type=missing, input_value={'provider_id': 'meta-ref...'routing_key': 'vector'}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.9/v/missing
routing_table.safety.0.provider_type
  Field required [type=missing, input_value={'provider_id': 'meta-ref...ing_key': 'llama_guard'}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.9/v/missing
routing_table.safety.1.provider_type
  Field required [type=missing, input_value={'provider_id': 'meta-ref...': 'code_scanner_guard'}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.9/v/missing
routing_table.safety.2.provider_type
  Field required [type=missing, input_value={'provider_id': 'meta-ref...ey': 'injection_shield'}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.9/v/missing
routing_table.safety.3.provider_type
  Field required [type=missing, input_value={'provider_id': 'meta-ref...ey': 'jailbreak_shield'}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.9/v/missing

 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/llama-stack/issues/226","Switching between stacks","2024-10-10T04:45:14Z","Closed issue","No label","I have created and configured two stacks
 llama stack build --template local --name dev1
 llama stack configure dev1

 llama stack build --template local --name dev2
 llama stack configure dev2
And it is running in the port 8090.
llama stack run dev1 --port 8090 --disable-ipv6 
How to swtich to dev2 to run on the same port? I am trying to automate switching between stacks in linux.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/llama-stack/issues/225","Deleting a stack","2024-10-09T11:25:14Z","Open issue","good first issue","I have created and configured a stack using
 llama stack build --template local --name dev
 llama stack configure dev
How to delete this stack?
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/llama-stack/issues/223","Unable to run llama on windows: ""'llama' is not recognized as an internal or external command""","2024-10-09T21:09:08Z","Closed issue","No label","Hey I'm trying to try to download and run llama, but I'm stuck at the second step.
First step: done
>pip install llama-stack

Second step: failing
>llama model list
'llama' is not recognized as an internal or external command,
operable program or batch file.

I'm running Windows 11:
>ver

Microsoft Windows [Version 10.0.22631.4249]

pip:
>pip --version
pip 24.2 from C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.2032.0_x64__qbz5n2kfra8p0\Lib\site-packages\pip (python 3.12)


Can anyone please help?
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/llama-stack/issues/220","Quantization (FP8) causing errors","2024-10-08T14:44:48Z","Open issue","No label","Hi, I'm running llama stack on an ubuntu machine with PA6000 GPU on paperspace; loading the default model.
 When I turn off quantization I manage to get the server up and running, when I choose fp8 in build I get the following error in llama stack run:
paperspace@pspz9spk7exu:~$ llama stack run lcl --port 5000
Resolved 8 providers in topological order
  Api.models: routing_table
  Api.inference: router
  Api.shields: routing_table
  Api.safety: router
  Api.memory_banks: routing_table
  Api.memory: router
  Api.agents: meta-reference
  Api.telemetry: meta-reference

initializing model parallel with size 1
initializing ddp with size 1
initializing pipeline with size 1

/home/paperspace/anaconda3/envs/llamastack-lcl/lib/python3.10/site-packages/torch/_init_.py:955: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:432.)
  _C._set_default_tensor_type(t)
E1008 04:49:31.950000 140333789631552 torch/distributed/elastic/multiprocessing/api.py:702] failed (exitcode: -9) local_rank: 0 (pid: 5620) of fn: worker_process_entrypoint (start_method: fork)
E1008 04:49:31.950000 140333789631552 torch/distributed/elastic/multiprocessing/api.py:702] Traceback (most recent call last):
E1008 04:49:31.950000 140333789631552 torch/distributed/elastic/multiprocessing/api.py:702]   File ""/home/paperspace/anaconda3/envs/llamastack-lcl/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py"", line 659, in _poll
E1008 04:49:31.950000 140333789631552 torch/distributed/elastic/multiprocessing/api.py:702]     self._pc.join(-1)
E1008 04:49:31.950000 140333789631552 torch/distributed/elastic/multiprocessing/api.py:702]   File ""/home/paperspace/anaconda3/envs/llamastack-lcl/lib/python3.10/site-packages/torch/multiprocessing/spawn.py"", line 170, in join
E1008 04:49:31.950000 140333789631552 torch/distributed/elastic/multiprocessing/api.py:702]     raise ProcessExitedException(
E1008 04:49:31.950000 140333789631552 torch/distributed/elastic/multiprocessing/api.py:702] torch.multiprocessing.spawn.ProcessExitedException: process 0 terminated with signal SIGKILL
Process ForkProcess-1:
Traceback (most recent call last):
  File ""/home/paperspace/anaconda3/envs/llamastack-lcl/lib/python3.10/multiprocessing/process.py"", line 314, in _bootstrap
    self.run()
  File ""/home/paperspace/anaconda3/envs/llamastack-lcl/lib/python3.10/multiprocessing/process.py"", line 108, in run
    self._target(*self._args, **self._kwargs)
  File ""/home/paperspace/anaconda3/envs/llamastack-lcl/lib/python3.10/site-packages/llama_stack/providers/impls/meta_reference/inference/parallel_utils.py"", line 175, in launch_dist_group
    elastic_launch(launch_config, entrypoint=worker_process_entrypoint)(
  File ""/home/paperspace/anaconda3/envs/llamastack-lcl/lib/python3.10/site-packages/torch/distributed/launcher/api.py"", line 133, in call
    return launch_agent(self._config, self._entrypoint, list(args))
  File ""/home/paperspace/anaconda3/envs/llamastack-lcl/lib/python3.10/site-packages/torch/distributed/launcher/api.py"", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError:
=====================================================
worker_process_entrypoint FAILED
-----------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
-----------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-10-08_04:49:31
  host      : pspz9spk7exu
  rank      : 0 (local_rank: 0)
  exitcode  : -9 (pid: 5620)
  error_file: <N/A>
  traceback : Signal 9 (SIGKILL) received by PID 5620
=====================================================

 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/llama-stack/issues/218","run client failed with httpx.ReadError","2024-10-09T12:05:59Z","Closed issue","No label","Step1: Start the server with docker
 docker run --rm -it -p 5000:5000 -v ~/.llama:/root/.llama --gpus=all llamastack/llamastack-local-gpu
Step2: run the client
 python -m llama_stack.apis.inference.client localhost 5000
Logs:
(LStack) opea@acc:~/Repo/llama-stack$ python -m llama_stack.apis.inference.client localhost 5000
User>hello world, write me a 2 sentence poem about the moon
Traceback (most recent call last):
  File ""/home/opea/anaconda3/envs/LStack/lib/python3.10/site-packages/httpx/_transports/default.py"", line 72, in map_httpcore_exceptions
    yield
  File ""/home/opea/anaconda3/envs/LStack/lib/python3.10/site-packages/httpx/_transports/default.py"", line 377, in handle_async_request
    resp = await self._pool.handle_async_request(req)
  File ""/home/opea/anaconda3/envs/LStack/lib/python3.10/site-packages/httpcore/_async/connection_pool.py"", line 216, in handle_async_request
    raise exc from None
  File ""/home/opea/anaconda3/envs/LStack/lib/python3.10/site-packages/httpcore/_async/connection_pool.py"", line 196, in handle_async_request
    response = await connection.handle_async_request(
  File ""/home/opea/anaconda3/envs/LStack/lib/python3.10/site-packages/httpcore/_async/connection.py"", line 101, in handle_async_request
    return await self._connection.handle_async_request(request)
  File ""/home/opea/anaconda3/envs/LStack/lib/python3.10/site-packages/httpcore/_async/http11.py"", line 143, in handle_async_request
    raise exc
  File ""/home/opea/anaconda3/envs/LStack/lib/python3.10/site-packages/httpcore/_async/http11.py"", line 113, in handle_async_request
    ) = await self._receive_response_headers(**kwargs)
  File ""/home/opea/anaconda3/envs/LStack/lib/python3.10/site-packages/httpcore/_async/http11.py"", line 186, in _receive_response_headers
    event = await self._receive_event(timeout=timeout)
  File ""/home/opea/anaconda3/envs/LStack/lib/python3.10/site-packages/httpcore/_async/http11.py"", line 224, in _receive_event
    data = await self._network_stream.read(
  File ""/home/opea/anaconda3/envs/LStack/lib/python3.10/site-packages/httpcore/_backends/anyio.py"", line 32, in read
    with map_exceptions(exc_map):
  File ""/home/opea/anaconda3/envs/LStack/lib/python3.10/contextlib.py"", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File ""/home/opea/anaconda3/envs/LStack/lib/python3.10/site-packages/httpcore/_exceptions.py"", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ReadError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""/home/opea/anaconda3/envs/LStack/lib/python3.10/runpy.py"", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File ""/home/opea/anaconda3/envs/LStack/lib/python3.10/runpy.py"", line 86, in _run_code
    exec(code, run_globals)
  File ""/home/opea/Repo/llama-stack/llama_stack/apis/inference/client.py"", line 178, in <module>
    fire.Fire(main)
  File ""/home/opea/anaconda3/envs/LStack/lib/python3.10/site-packages/fire/core.py"", line 135, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File ""/home/opea/anaconda3/envs/LStack/lib/python3.10/site-packages/fire/core.py"", line 468, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
  File ""/home/opea/anaconda3/envs/LStack/lib/python3.10/site-packages/fire/core.py"", line 684, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File ""/home/opea/Repo/llama-stack/llama_stack/apis/inference/client.py"", line 174, in main
    asyncio.run(run_main(host, port, stream, model, logprobs))
  File ""/home/opea/anaconda3/envs/LStack/lib/python3.10/asyncio/runners.py"", line 44, in run
    return loop.run_until_complete(main)
  File ""/home/opea/anaconda3/envs/LStack/lib/python3.10/asyncio/base_events.py"", line 649, in run_until_complete
    return future.result()
  File ""/home/opea/Repo/llama-stack/llama_stack/apis/inference/client.py"", line 134, in run_main
    async for log in EventLogger().log(iterator):
  File ""/home/opea/Repo/llama-stack/llama_stack/apis/inference/event_logger.py"", line 32, in log
    async for chunk in event_generator:
  File ""/home/opea/Repo/llama-stack/llama_stack/apis/inference/client.py"", line 70, in chat_completion
    async with client.stream(
  File ""/home/opea/anaconda3/envs/LStack/lib/python3.10/contextlib.py"", line 199, in __aenter__
    return await anext(self.gen)
  File ""/home/opea/anaconda3/envs/LStack/lib/python3.10/site-packages/httpx/_client.py"", line 1628, in stream
    response = await self.send(
  File ""/home/opea/anaconda3/envs/LStack/lib/python3.10/site-packages/httpx/_client.py"", line 1674, in send
    response = await self._send_handling_auth(
  File ""/home/opea/anaconda3/envs/LStack/lib/python3.10/site-packages/httpx/_client.py"", line 1702, in _send_handling_auth
    response = await self._send_handling_redirects(
  File ""/home/opea/anaconda3/envs/LStack/lib/python3.10/site-packages/httpx/_client.py"", line 1739, in _send_handling_redirects
    response = await self._send_single_request(request)
  File ""/home/opea/anaconda3/envs/LStack/lib/python3.10/site-packages/httpx/_client.py"", line 1776, in _send_single_request
    response = await transport.handle_async_request(request)
  File ""/home/opea/anaconda3/envs/LStack/lib/python3.10/site-packages/httpx/_transports/default.py"", line 376, in handle_async_request
    with map_httpcore_exceptions():
  File ""/home/opea/anaconda3/envs/LStack/lib/python3.10/contextlib.py"", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File ""/home/opea/anaconda3/envs/LStack/lib/python3.10/site-packages/httpx/_transports/default.py"", line 89, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ReadError

 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/llama-stack/issues/215","How to run llama3.2 90B self hosted using llama-stack?","2024-10-08T01:17:22Z","Open issue","No label","how to run 90B self hosted version using llama-stack?
need more clarification #205 since none of these examples in #205 are self hosted.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/llama-stack/issues/214","Add top_k output tokens w/ corresponding logprobs","2024-10-08T00:25:13Z","Open issue","enhancement","Context
[bugfix] Fix logprobs on meta-reference impl #213
Need
Support TokenLogProbs output based on top_k input (currently only supports single token logprobs).
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/llama-stack/issues/211","Add custom volume mounts to docker runtime","2024-10-15T22:13:56Z","Closed issue","No label","I'm trying to implement an inference provider for SkyPilot, which needs the user's local cloud credentials to be available inside the llama-stack container.
The recommended way to do so is to add local credential dirs as volume mounts for the docker container. What's the best way to attach volume mounts to the llama stack container?
I assume they could be handled similar to how LLAMA_STACK_DIR and LLAMA_CHECKPOINT_DIR are read from envvars in start_container.sh, but wondering if there are plans to better support this in inference provider API?
 The text was updated successfully, but these errors were encountered: 
👍1
nikolaydubina reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/meta-llama/llama-stack/issues/208","vllm: expand configuration support","2024-10-07T16:08:58Z","Open issue","enhancement","This is a follow-up to #181.
The initial provider included two config options:
model
tensor_parallel_size -- to allow utilizing multiple GPUs.
There are many more options available to the vLLM engine. Many of these should be exposed as config options.
https://github.com/vllm-project/vllm/blob/151ef4efd2fb52554f4d30408aca619e181ea751/vllm/engine/arg_utils.py#L82
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/llama-stack/issues/205","How to run 90B ?","2024-10-07T18:20:17Z","Closed issue","No label","I could not find instructions. ollama does not support it1, llama.cpp does not support it2, Google Vertex AI (only preview)3
on my local machine with 128 GB RAM (and 2TB SSD) it is crashing with OOM
is there distributed mode? quantization? any guideline how to run it? how does Meta itself run it?
Footnotes
https://ollama.com/blog/llama3.2↩
https://github.com/ggerganov/llama.cpp/issues/9643↩
https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/llama#llama-3.2-90b-preview↩
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/llama-stack/issues/203","Running llama-stack with 8B llama on an AWS CPU only instance throwing an error","2024-10-10T14:36:10Z","Closed issue","No label","I'm trying to run llama stack on a cpu-only node to try it out, getting the error below, can't find a way to tell it I don't have one:
File ""/home/ubuntu/anaconda3/envs/llamastack-my-local-stack/lib/python3.10/site-package
 s/torch/distributed/distributed_c10d.py"", line 1594, in _new_process_group_helper
 backend_class = ProcessGroupNCCL(
 ValueError: ProcessGroupNCCL is only supported with GPUs, no GPUs found!
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/llama-stack/issues/202","llama download obscure signed url requirement","2024-10-07T15:40:05Z","Closed issue","No label","I am trying to use llama-stack on a linux (on AWS x86_64). After installing anaconda & llama-stack
 llama-build was successful
 llama configure was successful.
I was told to ""llama download --model-id Llama3.1-8B-Instruct""
I'm receiving the following request, not sure where to get this from, any ideas?
Please provide the signed URL you received via email (e.g., https://llama3-1.llamameta.net/
 *?Policy...):
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/llama-stack/issues/200","vllm: improve container support","2024-10-06T19:32:42Z","Open issue","enhancement","This is a follow-up issue for #181.
The vllm inline inference adapter works in both the conda and docker stack types, but some features fail in the docker case because the base image does not include all necessary dependencies (some cuda libraries, in particular). The specific case that failed for me was trying to use tensor_parallel_size greater than 1. NCCL fails to initialize (nccl library isn't present).
I started working on this, but didn't get it working completely yet.
my WIP was here: russellb@
3a61246
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/llama-stack/issues/199","vllm: test and fix tool support","2024-10-06T19:28:40Z","Open issue","enhancement","#181 introduced an inline vllm inference provider. At that time, I had not yet tested tool support in any way, so I'm sure it's broken. This needs some testing and I assume some bug fixes.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/llama-stack/issues/198","vllm: add docs for the inline vllm provider","2024-10-07T17:26:53Z","Closed issue","No label","#181 introduced the inline vllm inference provider. I didn't include any docs there, but some docs should be added / updated.
(Please assign this to me.)
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/llama-stack/issues/197","Question regarding agents and sessions APIs","2024-10-06T10:56:14Z","Open issue","enhancement","Hello,
I have a question regarding the API with Agents. I did not find a way to get an agent that is already created. Or list the agents available. Did I miss something or is the agent not supposed to be a longer lived entity (than let's say a method call)? I imagine, that you configure agents and then use them throughout your application.
Same question regarding sessions. Although I think session lifespan is shorter than agent it still would make sense to be able to get existing sessions for w/e the purpose it could be. I see that it's possible to fetch single session but I don't see a way to list all sessions.
 The text was updated successfully, but these errors were encountered: 
👍1
Alevs2R reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/meta-llama/llama-stack/issues/194","I am puzzled as to why stack needs to link it to the address [: ffff: 0.0.2.208]","2024-10-06T02:40:20Z","Open issue","No label","downloaded the 1B model from Huggingface and encountered an error while running it. The following is the configuration process, and I am puzzled as to why I need to link it to the address [: ffff: 0.0.2.208]: 48461
(llamastack-stack-3.2-1B) root@720:~/.llama/checkpoints# llama stack configure  stack-3.2-1B
Could not find stack-3.2-1B. Trying conda build name instead...
Configuring API `inference`...
=== Configuring provider `meta-reference` for API inference...
Enter value for model (default: Llama3.1-8B-Instruct) (required): Llama3.2-1B-Instruct
Do you want to configure quantization? (y/n): n
Enter value for torch_seed (optional):
Enter value for max_seq_len (default: 4096) (required):
Enter value for max_batch_size (default: 1) (required):

Configuring API `safety`...
=== Configuring provider `meta-reference` for API safety...
Do you want to configure llama_guard_shield? (y/n): n
Enter value for enable_prompt_guard (default: False) (optional):

Configuring API `agents`...
=== Configuring provider `meta-reference` for API agents...
Enter `type` for persistence_store (options: redis, sqlite, postgres) (default: sqlite):

Configuring SqliteKVStoreConfig:
Enter value for namespace (optional):
Enter value for db_path (default: /root/.llama/runtime/kvstore.db) (required):

Configuring API `memory`...
=== Configuring provider `meta-reference` for API memory...
> Please enter the supported memory bank type your provider has for memory: vector

Configuring API `telemetry`...
=== Configuring provider `meta-reference` for API telemetry...

> YAML configuration has been written to `/root/.llama/builds/conda/stack-3.2-1B-run.yaml`.
You can now run `llama stack run stack-3.2-1B --port PORT`

(llamastack-stack-3.2-1B) root@720:~/.llama/checkpoints# llama stack run stack-3.2-1B --port 5000 --disable-ipv6
Resolved 8 providers in topological order
  Api.models: routing_table
  Api.inference: router
  Api.shields: routing_table
  Api.safety: router
  Api.memory_banks: routing_table
  Api.memory: router
  Api.agents: meta-reference
  Api.telemetry: meta-reference

d/elastic/multiprocessing/api.py:702]     store, rank, world_size = next(rendezvous_iterator)
E1006 02:22:05.730000 139705575060544 torch/distributed/elastic/multiprocessing/api.py:702]   File ""/home/miniconda3/envs/llamastack-stack-3.2-1B/lib/python3.10/site-packages/torch/distributed/rendezvous.py"", line 258, in _env_rendezvous_handler
E1006 02:22:05.730000 139705575060544 torch/distributed/elastic/multiprocessing/api.py:702]     store = _create_c10d_store(master_addr, master_port, rank, world_size, timeout, use_libuv)
E1006 02:22:05.730000 139705575060544 torch/distributed/elastic/multiprocessing/api.py:702]   File ""/home/miniconda3/envs/llamastack-stack-3.2-1B/lib/python3.10/site-packages/torch/distributed/rendezvous.py"", line 185, in _create_c10d_store
E1006 02:22:05.730000 139705575060544 torch/distributed/elastic/multiprocessing/api.py:702]     return TCPStore(
E1006 02:22:05.730000 139705575060544 torch/distributed/elastic/multiprocessing/api.py:702] torch.distributed.DistNetworkError: The client socket has failed to connect to any network address of (720, 48461). The client socket has failed to connect to 0.0.2.208:48461 (errno: 110 - Connection timed out).
E1006 02:22:05.730000 139705575060544 torch/distributed/elastic/multiprocessing/api.py:702]
Process ForkProcess-1:
Traceback (most recent call last):
  File ""/home/miniconda3/envs/llamastack-stack-3.2-1B/lib/python3.10/multiprocessing/process.py"", line 314, in _bootstrap
    self.run()
  File ""/home/miniconda3/envs/llamastack-stack-3.2-1B/lib/python3.10/multiprocessing/process.py"", line 108, in run
    self._target(*self._args, **self._kwargs)
  File ""/home/miniconda3/envs/llamastack-stack-3.2-1B/lib/python3.10/site-packages/llama_stack/providers/impls/meta_reference/inference/parallel_utils.py"", line 175, in launch_dist_group
    elastic_launch(launch_config, entrypoint=worker_process_entrypoint)(
  File ""/home/miniconda3/envs/llamastack-stack-3.2-1B/lib/python3.10/site-packages/torch/distributed/launcher/api.py"", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File ""/home/miniconda3/envs/llamastack-stack-3.2-1B/lib/python3.10/site-packages/torch/distributed/launcher/api.py"", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError:
============================================================
worker_process_entrypoint FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-10-06_02:22:05
  host      : 720
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 3509)
  error_file: /tmp/torchelastic_silhk6ot/39495240-fe63-4883-ba91-9be482efe3ba_hewowew5/attempt_0/0/error.json
  traceback : Traceback (most recent call last):
    File ""/home/miniconda3/envs/llamastack-stack-3.2-1B/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py"", line 348, in wrapper
      return f(*args, **kwargs)
    File ""/home/miniconda3/envs/llamastack-stack-3.2-1B/lib/python3.10/site-packages/llama_stack/providers/impls/meta_reference/inference/parallel_utils.py"", line 131, in worker_process_entrypoint
      model = init_model_cb()
    File ""/home/miniconda3/envs/llamastack-stack-3.2-1B/lib/python3.10/site-packages/llama_stack/providers/impls/meta_reference/inference/model_parallel.py"", line 50, in init_model_cb
      llama = Llama.build(config)
    File ""/home/miniconda3/envs/llamastack-stack-3.2-1B/lib/python3.10/site-packages/llama_stack/providers/impls/meta_reference/inference/generation.py"", line 90, in build
      torch.distributed.init_process_group(""nccl"")
    File ""/home/miniconda3/envs/llamastack-stack-3.2-1B/lib/python3.10/site-packages/torch/distributed/c10d_logger.py"", line 79, in wrapper
      return func(*args, **kwargs)
    File ""/home/miniconda3/envs/llamastack-stack-3.2-1B/lib/python3.10/site-packages/torch/distributed/c10d_logger.py"", line 93, in wrapper
      func_return = func(*args, **kwargs)
    File ""/home/miniconda3/envs/llamastack-stack-3.2-1B/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py"", line 1361, in init_process_group
      store, rank, world_size = next(rendezvous_iterator)
    File ""/home/miniconda3/envs/llamastack-stack-3.2-1B/lib/python3.10/site-packages/torch/distributed/rendezvous.py"", line 258, in _env_rendezvous_handler
      store = _create_c10d_store(master_addr, master_port, rank, world_size, timeout, use_libuv)
    File ""/home/miniconda3/envs/llamastack-stack-3.2-1B/lib/python3.10/site-packages/torch/distributed/rendezvous.py"", line 185, in _create_c10d_store
      return TCPStore(
  torch.distributed.DistNetworkError: The client socket has failed to connect to any network address of (720, 48461). The client socket has failed to connect to 0.0.2.208:48461 (errno: 110 - Connection timed out).


 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/llama-stack/issues/193","Create shared openai-compatible inference adapter","2024-10-05T19:12:22Z","Open issue","No label","There are multiple open PRs looking to add inference adapters for services that offer OpenAI-compatible APIs.
databricks - add databricks provider #83
sambanova - Add sambanova provider #136
runpod - Add Runpod Provider #157
vllm - Add vLLM inference provider for OpenAI compatible vLLM server #178
These adapters all use the OpenAI client and have the same config options (a URL and API key).
Options for moving forward:
Create a single shared adapter for OpenAI-compatible API endpoints.
If some cases require additional unique configuration, most of the code could be shared.
Create code to be shared by each adapter that uses an OpenAI-compatible API.
 The text was updated successfully, but these errors were encountered: 
👍1
romilbhardwaj reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/meta-llama/llama-stack/issues/191","Are there any available tools that can convert the original .pth to safetensors","2024-10-05T12:17:05Z","Open issue","No label","Are there any available tools that can convert the original .pth model files downloaded from Meta into a format usable by stack, or convert them to .safetensors format? I tried the tool from https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/convert_llama_weights_to_hf.py, but it threw an error during execution.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/llama-stack/issues/190","stack tool cannot support large models with a .pth extension downloaded from Meta","2024-10-05T12:12:42Z","Open issue","No label","The stack tool cannot support large models with a .pth extension downloaded from Meta. It throws an error during runtime. Does it have to use models downloaded from Hugging Face? Is this setup unreasonable? The runtime error is as follows:
(stack) root@720:~/.llama/checkpoints/Llama3.1-8B-Instruct# llama stack run my-local-stack
 Resolved 8 providers in topological order
 Api.models: routing_table
 Api.inference: router
 Api.shields: routing_table
 Api.safety: router
 Api.memory_banks: routing_table
 Api.memory: router
 Api.agents: meta-reference
 Api.telemetry: meta-reference
Traceback (most recent call last):
 File ""/home/miniconda3/envs/llamastack-my-local-stack/lib/python3.10/runpy.py"", line 196, in _run_module_as_main
 return _run_code(code, main_globals, None,
 File ""/home/miniconda3/envs/llamastack-my-local-stack/lib/python3.10/runpy.py"", line 86, in _run_code
 exec(code, run_globals)
 File ""/home/miniconda3/envs/llamastack-my-local-stack/lib/python3.10/site-packages/llama_stack/distribution/server/server.py"", line 351, in 
 fire.Fire(main)
 File ""/home/miniconda3/envs/llamastack-my-local-stack/lib/python3.10/site-packages/fire/core.py"", line 135, in Fire
 component_trace = _Fire(component, args, parsed_flag_args, context, name)
 File ""/home/miniconda3/envs/llamastack-my-local-stack/lib/python3.10/site-packages/fire/core.py"", line 468, in _Fire
 component, remaining_args = _CallAndUpdateTrace(
 File ""/home/miniconda3/envs/llamastack-my-local-stack/lib/python3.10/site-packages/fire/core.py"", line 684, in CallAndUpdateTrace
 component = fn(*varargs, **kwargs)
 File ""/home/miniconda3/envs/llamastack-my-local-stack/lib/python3.10/site-packages/llama_stack/distribution/server/server.py"", line 288, in main
 impls, specs = asyncio.run(resolve_impls_with_routing(config))
 File ""/home/miniconda3/envs/llamastack-my-local-stack/lib/python3.10/asyncio/runners.py"", line 44, in run
 return loop.run_until_complete(main)
 File ""/home/miniconda3/envs/llamastack-my-local-stack/lib/python3.10/asyncio/base_events.py"", line 649, in run_until_complete
 return future.result()
 File ""/home/miniconda3/envs/llamastack-my-local-stack/lib/python3.10/site-packages/llama_stack/distribution/resolver.py"", line 104, in resolve_impls_with_routing
 impl = await instantiate_provider(spec, deps, configs[api])
 File ""/home/miniconda3/envs/llamastack-my-local-stack/lib/python3.10/site-packages/llama_stack/distribution/resolver.py"", line 174, in instantiate_provider
 impl = await instantiate_provider(
 File ""/home/miniconda3/envs/llamastack-my-local-stack/lib/python3.10/site-packages/llama_stack/distribution/resolver.py"", line 192, in instantiate_provider
 impl = await fn(*args)
 File ""/home/miniconda3/envs/llamastack-my-local-stack/lib/python3.10/site-packages/llama_stack/providers/impls/meta_reference/inference/init.py"", line 18, in get_provider
 impl
 await impl.initialize()
 File ""/home/miniconda3/envs/llamastack-my-local-stack/lib/python3.10/site-packages/llama_stack/providers/impls/meta_reference/inference/inference.py"", line 38, in initialize
 self.generator = LlamaModelParallelGenerator(self.config)
 File ""/home/miniconda3/envs/llamastack-my-local-stack/lib/python3.10/site-packages/llama_stack/providers/impls/meta_reference/inference/model_parallel.py"", line 72, in _init
 _
 self.formatter = ChatFormat(Tokenizer(tokenizer_path))
 File ""/home/miniconda3/envs/llamastack-my-local-stack/lib/python3.10/site-packages/llama_models/llama3/api/tokenizer.py"", line 77, in init
 mergeable_ranks = load_tiktoken_bpe(model_path)
 File ""/home/miniconda3/envs/llamastack-my-local-stack/lib/python3.10/site-packages/tiktoken/load.py"", line 145, in load_tiktoken_bpe
 return {
 File ""/home/miniconda3/envs/llamastack-my-local-stack/lib/python3.10/site-packages/tiktoken/load.py"", line 147, in 
 for token, rank in (line.split() for line in contents.splitlines() if line)
 ValueError: not enough values to unpack (expected 2, got 1)
 Error occurred in script at line: 40
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/llama-stack/issues/189","Configuring ollama build with redis results in an error","2024-10-05T18:26:27Z","Closed issue","No label","Configuring ollama build with redis results in an error.
   llama stack run ollama-stack --port 5000

   Resolved 8 providers in topological order
    Api.models: routing_table
    Api.inference: router
    Api.shields: routing_table
    Api.safety: router
    Api.memory_banks: routing_table
    Api.memory: router
    Api.agents: meta-reference
    Api.telemetry: meta-reference
  
  Initializing Ollama, checking connectivity to server...
  [faiss] Registering memory bank routing keys: ['vector']
  
  ... 

  AttributeError: 'RedisKVStoreConfig' object has no attribute 'url'
  Error occurred in script at line: 42


  version: v1
  image_name: ollama-stack
  docker_image: null
  conda_env: ollama-stack
  apis_to_serve:
  - shields
  - safety
  - memory
  - memory_banks
  - inference
  - agents
  - models
  api_providers:
    inference:
      providers:
      - remote::ollama
    memory:
      providers:
      - meta-reference
    safety:
      providers:
      - meta-reference
    agents:
      provider_type: meta-reference
      config:
        persistence_store:
          namespace: llama-stack
          type: redis
          host: localhost
          port: 6379
    telemetry:
      provider_type: meta-reference
      config: {}
  routing_table:
    inference:
    - provider_type: remote::ollama
      config:
        host: localhost
        port: 11434
      routing_key: Llama3.1-8B-Instruct
    memory:
    - provider_type: meta-reference
      config: {}
      routing_key: vector
    safety:
    - provider_type: meta-reference
      config:
        llama_guard_shield: null
        enable_prompt_guard: false
      routing_key:
      - llama_guard
      - code_scanner_guard

 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/llama-stack/issues/188","provider shutdown methods are not currently executed","2024-10-05T00:27:25Z","Open issue","help wanted","While working on #181, I included a shutdown() implementation to cleanly shutdown the vLLM engine. I noticed it's not being called, though.
I started looking into it, but didn't finish, so I wanted to file this so I don't forget or in case someone else was interested in looking at it. Please feel free to add to or correct my notes here.
I think we need to call shutdown on the impls returned here later at the end after we break out of the API server ... 
llama-stack/llama_stack/distribution/server/server.py
 Line 288 in bfb0e92
	impls, specs=asyncio.run(resolve_impls_with_routing(config)) 
For my specific case, there's an InferenceRouter in impls. You can see it has a shutdown() method, but it doesn't do anything (yet). The items that need to be shutdown are in the routing table here. 
llama-stack/llama_stack/distribution/routers/routers.py
 Line 80 in bfb0e92
	classInferenceRouter(Inference): 
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/llama-stack/issues/186","Add Support for Kubernetes","2024-10-04T20:47:58Z","Open issue","No label","As of now, Llama Stack only supports single node providers for the self-hosted solutions. Is it possible to support Kubernetes as a provider or some other architecture to make Llama Stack scalable.
 The text was updated successfully, but these errors were encountered: 
👍2
nikolaydubina and JeremyKPark reacted with thumbs up emoji
All reactions
👍2 reactions"
"https://github.com/meta-llama/llama-stack/issues/184","Cannot run llama-stack on windows due to termios dependency -- Build fails with poetry","2024-10-04T15:54:08Z","Open issue","No label","I used poetry to add llama-stack, executing the basic build from the quick guide gives me ""No module named 'termios'.
 Running on a windows machine.
 The text was updated successfully, but these errors were encountered: 
😕1
imadcat reacted with confused emoji
All reactions
😕1 reaction"
"https://github.com/meta-llama/llama-stack/issues/183","ollama inference should verify models are downloaded before serving","2024-10-04T14:27:13Z","Open issue","good first issue","Right now, if you start a distribution using remote::ollama without the models downloaded, ollama will attempt to download it upon the first inference request:
INFO:     Uvicorn running on http://[::]:5001 (Press CTRL+C to quit)
INFO:     ::1:60686 - ""POST /inference/chat_completion HTTP/1.1"" 200 OK
Pulling model: llama3.1:8b-instruct-fp16
10:16:19.251 [INFO] [chat_completion] HTTP Request: GET http://localhost:11434/api/ps ""HTTP/1.1 200 OK""
Generator cancelled

However, these models are big – ex. 16GB for the default llama3.1:8b-instruct-fp16 – so the HTTP request will timeout before the download completes (and abort the download).
We should ensure that the models are downloaded and ready on server startup before serving requests.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/llama-stack/issues/180","llama-stack run with meta reference inference provider fails with ModuleNotFoundError","2024-10-03T19:50:16Z","Open issue","No label","I'm following the getting started guide for llama-stack. When run llama stack run 8b-instruct, it fails with ModuleNotFoundError:
torch.distributed.elastic.multiprocessing.errors.ChildFailedError:
============================================================
worker_process_entrypoint FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-10-03_19:42:04
  host      : l4-2ea4-head-7te0mjrn-compute.us-east4-a.c.skypilot-375900.internal
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 18162)
  error_file: /var/tmp/torchelastic_3iqbc8v4/76b62c9e-8c1c-4ed5-a452-a6863e0f9297_aous0lgf/attempt_0/0/error.json
  traceback : Traceback (most recent call last):
    File ""/opt/conda/envs/llamastack-8b-instruct/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py"", line 348, in wrapper
      return f(*args, **kwargs)
    File ""/opt/conda/envs/llamastack-8b-instruct/lib/python3.10/site-packages/llama_stack/providers/impls/meta_reference/inference/parallel_utils.py"", line 131, in worker_process_entrypoint
      model = init_model_cb()
    File ""/opt/conda/envs/llamastack-8b-instruct/lib/python3.10/site-packages/llama_stack/providers/impls/meta_reference/inference/model_parallel.py"", line 50, in init_model_cb
      llama = Llama.build(config)
    File ""/opt/conda/envs/llamastack-8b-instruct/lib/python3.10/site-packages/llama_stack/providers/impls/meta_reference/inference/generation.py"", line 84, in build
      from .quantization.loader import is_fbgemm_available
    File ""/opt/conda/envs/llamastack-8b-instruct/lib/python3.10/site-packages/llama_stack/providers/impls/meta_reference/inference/quantization/loader.py"", line 16, in <module>
      from llama_models.llama3.api.model import Transformer, TransformerBlock
  ModuleNotFoundError: No module named 'llama_models.llama3.api.model'

============================================================

Full logs, including the stack setup and configuration here: https://gist.github.com/romilbhardwaj/f21c3b1908b62ec5a906b321739d30cb
Versions:
$ pip freeze | grep llama
llama_models==0.0.39
llama_stack==0.0.39

Here's the full pip freeze: https://gist.github.com/romilbhardwaj/b05e950eeb03d5647d738382ba92f2a1
I tried with previous versions of llama_models, but that didn't work either. Am I missing something?
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/llama-stack/issues/174","llama stack run my-local-stack causes pydantic error","2024-10-02T22:44:47Z","Open issue","No label","I'm using remote::ollama and Meta-Llama3.1-8B-Instruct. llama stack build my-local-stack and llama stack configure my-local stack ran without problem, but encountered pydantic error upon running llama stack run my-local-stack. Any ideas? I'm running it on Mac M2 using conda since docker doesn't work.
llama stack run my-local-stack              2 err | stack py | at 18:39:31
Traceback (most recent call last):
  File ""/opt/anaconda3/envs/llamastack-my-local-stack/lib/python3.10/runpy.py"", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File ""/opt/anaconda3/envs/llamastack-my-local-stack/lib/python3.10/runpy.py"", line 86, in _run_code
    exec(code, run_globals)
  File ""/Users/friedahuang/local/llama-stack/llama_stack/distribution/server/server.py"", line 364, in <module>
    fire.Fire(main)
  File ""/opt/anaconda3/envs/llamastack-my-local-stack/lib/python3.10/site-packages/fire/core.py"", line 135, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File ""/opt/anaconda3/envs/llamastack-my-local-stack/lib/python3.10/site-packages/fire/core.py"", line 468, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
  File ""/opt/anaconda3/envs/llamastack-my-local-stack/lib/python3.10/site-packages/fire/core.py"", line 684, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File ""/Users/friedahuang/local/llama-stack/llama_stack/distribution/server/server.py"", line 286, in main
    config = StackRunConfig(**yaml.safe_load(fp))
  File ""/opt/anaconda3/envs/llamastack-my-local-stack/lib/python3.10/site-packages/pydantic/main.py"", line 212, in __init__
    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
pydantic_core._pydantic_core.ValidationError: 7 validation errors for StackRunConfig
api_providers.agents.GenericProviderConfig.provider_type
  Field required [type=missing, input_value={'provider_id': 'meta-ref...er', 'password': None}}}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.9/v/missing
api_providers.agents.PlaceholderProviderConfig.providers
  Field required [type=missing, input_value={'provider_id': 'meta-ref...er', 'password': None}}}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.9/v/missing
api_providers.telemetry.GenericProviderConfig.provider_type
  Field required [type=missing, input_value={'provider_id': 'meta-reference', 'config': {}}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.9/v/missing
api_providers.telemetry.PlaceholderProviderConfig.providers
  Field required [type=missing, input_value={'provider_id': 'meta-reference', 'config': {}}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.9/v/missing
routing_table.inference.0.provider_type
  Field required [type=missing, input_value={'provider_id': 'remote::...a-Llama3.1-8B-Instruct'}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.9/v/missing
routing_table.safety.0.provider_type
  Field required [type=missing, input_value={'provider_id': 'meta-ref...d', 'jailbreak_shield']}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.9/v/missing
routing_table.memory.0.provider_type
  Field required [type=missing, input_value={'provider_id': 'meta-ref...'routing_key': 'vector'}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.9/v/missing
Error occurred in script at line: 42

 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/llama-stack/issues/172","Add checks from pre-commit to CI","2024-10-10T18:22:00Z","Closed issue","No label","There are a number of useful checks in this pre-commit config: https://github.com/meta-llama/llama-stack/blob/main/.pre-commit-config.yaml
It would also be useful to enforce these checks as part of CI running against PRs on this repo.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/llama-stack/issues/169","docker: workflow now requires running configure twice","2024-10-03T18:21:12Z","Closed issue","No label","#159 added a call to llama stack configure to build_container.sh. The output is put into a temporary location so that it can be injected into the container. However, llama stack run expects this file in a different location, ~/.llama/builds/docker/.... For run to work, llama stack configure must be run again after the build. Running it twice doesn't seem ideal.
One possible change is to update start_container.sh to understand that the config file is inside the container instead of mounting it. The downside is that additional llama stack configure runs after the build would no longer take effect.
Another option would be to change build_container.sh to ensure the config file ends up in the expected location so that run works without having to run configure a second time.''
cc @yanxi0830
 The text was updated successfully, but these errors were encountered: 
😕1
ashwinb reacted with confused emoji
All reactions
😕1 reaction"
"https://github.com/meta-llama/llama-stack/issues/168","[functionality] Implement completion() methods","2024-10-02T14:55:39Z","Open issue","good first issue","Most of the current inference providers only implement the chat_completion() method. The completion() method raises a NotImplementedError. We should implement this method for all the inference providers:
meta-reference
fireworks
together
ollama
bedrock
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/llama-stack/issues/166","Docker --security-opt fails on mac","2024-10-02T17:37:42Z","Closed issue","No label","Docker desktop v4.11.1 for Mac Sequoia
  llama stack build --template local-ollama --name stack-test-docker --image-type docker

Configuring API inference...
 === Configuring provider remote::ollama for API inference...
 Enter value for host (default: localhost) (required):
 Enter value for port (required): 4001
Please enter the supported model your provider has for inference: Llama3.1-8B-
 Instruct
Configuring API memory...
 === Configuring provider meta-reference for API memory...
Please enter the supported memory bank type your provider has for memory: vect
 or
Configuring API safety...
 === Configuring provider meta-reference for API safety...
 Do you want to configure llama_guard_shield? (y/n): n
 Do you want to configure prompt_guard_shield? (y/n): n
Configuring API agents...
 === Configuring provider meta-reference for API agents...
 Enter type for persistence_store (options: redis, sqlite, postgres) (default: sqlite): redis
Configuring RedisKVStoreConfig:
 Enter value for namespace (optional): llama-stack
 Enter value for host (default: localhost) (required):
 Enter value for port (default: 6379) (required):
Configuring API telemetry...
 === Configuring provider meta-reference for API telemetry...
YAML configuration has been written to /Users/<username>/Documents/tools/llama-stack/tmp/configs/stack-test-docker-run.yaml.
 You can now run llama stack run stack-test-docker --port PORT
 Dockerfile created successfully in /var/folders/7f/xb01fy750wgczjwyw164pch00000gq/T/tmp.ZYi5Waxvvk/DockerfileFROM python:3.10-slim
 WORKDIR /app
RUN apt-get update && apt-get install -y iputils-ping net-tools iproute2 dnsutils telnet curl wget telnet procps psmisc lsof traceroute bubblewrap && rm -rf /var/lib/apt/lists/*
RUN pip install llama-stack
 RUN pip install pandas codeshield aiosqlite httpx psycopg2-binary fastapi scipy sentencepiece blobfile numpy redis scikit-learn pillow ollama chardet pypdf matplotlib transformers uvicorn faiss-cpu nltk tqdm fire
 RUN pip install torch --index-url https://download.pytorch.org/whl/cpu
 RUN pip install sentence-transformers --no-deps
 ENTRYPOINT [""python"", ""-m"", ""llama_stack.distribution.server.server""]
ADD tmp/configs/stack-test-docker-build.yaml ./llamastack-build.yaml
 ADD tmp/configs/stack-test-docker-run.yaml ./llamastack-run.yaml
 + docker build --security-opt label=disable -t llamastack-stack-test-docker -f /var/folders/7f/xb01fy750wgczjwyw164pch00000gq/T/tmp.ZYi5Waxvvk/Dockerfile /Users/<username>/Documents/tools/llama-stack

Error response from daemon: The daemon on this platform does not support setting security options on build
Failed with error for security opts
 Failed to build target stack-test-docker with return code 1

Running without the --security-opt flag works
 docker build -t llamastack-stack-test-docker -f /var/folders/7f/xb01fy750wgczjwyw164pch00000gq/T/tmp.QMw22Q87GF/Dockerfile /Users/<username>/Documents/tools/llama-stack

 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/llama-stack/issues/164","fbgemm-gpu isn't officially supported on mac - optional dependency?","2024-10-01T02:23:33Z","Open issue","No label","Can we make fbgemm-gpu an optional dependency? https://pypi.org/project/fbgemm-gpu/#files It doesn't look like it's supported on a mac pytorch/FBGEMM#1985. This means means the build is broken out of the box. Without it, it looks like we break FP8 quantization.

llama-stack/llama_stack/providers/impls/meta_reference/inference/generation.py
 Line 87 in eb2d8a3
	raiseImportError(""fbgemm-gpu is required for FP8 quantization"") 
They had to make it optional in torchrec too: pytorch/torchrec#1483
ERROR: Could not find a version that satisfies the requirement fbgemm-gpu==0.8.0 (from versions: none)
ERROR: No matching distribution found for fbgemm-gpu==0.8.0
Failed to build target my-local-stack with return code 1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/llama-stack/issues/162","Build fails on MacOS with docker","2024-09-30T17:02:03Z","Open issue","No label","I'm having no end of problems getting both conda and GPU support to work togther on my Mac M3 so I'm trying Docker.
ERROR: unable to prepare context: path ""/Users/Jason/Documents/Repos\nClasses\n.\n.\nTuning/.venv/lib/python3.12/site-packages"" not found
/Users/Jason/Documents/Repost/Test/.venv is the path to my project and venv. Not sure what path it's trying to calculate there.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/llama-stack/issues/161","Docker build can't find Ollama host on llama stack run","2024-10-05T03:43:03Z","Closed issue","No label","I've tried a few different ways to get this running and I'm not sure what I'm missing or if this is not working.
Running
   llama stack build --template local-ollama --name stack-test-docker --image-type docker
   llama stack configure llamastack-stack-test-docker

After configuring with port 4001
   llama stack run stack-test-docker --port 4001

router_api Api.inference
 router_api Api.safety
 router_api Api.memory
 Resolved 8 providers in topological order
 Api.models: routing_table
 Api.inference: router
 Api.shields: routing_table
 Api.safety: router
 Api.memory_banks: routing_table
 Api.memory: router
 Api.agents: meta-reference
 Api.telemetry: meta-reference
.... httpcore.ConnectError: All connection attempts failed
... RuntimeError: Ollama Server is not running, start it using ollama serve in a separate terminal
The thing is it is running and I have the models loaded and running. But this is on my host.
I've tried to set the host on the docker file with --add-host=host.docker.internal:host-gateway
  docker run --add-host=host.docker.internal:host-gateway -it -v ~/.llama/builds/docker/llama-stack-test-docker-run.yaml:/app/config.yaml -v ~/.llama:/root/.llama llamastack-llama-stack-test-docker python -m llama_stack.distribution.server.server --yaml_config /app/config.yaml --port 4001

I've also tried the conda build and get the same Ollama Server is not running error.
Is this a bug? or any insights from anyone would be great,
...or is this not meant to use the host machine local Ollama?
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/llama-stack/issues/158","Prompt-format in cli-reference does not show image","2024-09-30T11:36:18Z","Open issue","No label","In this file image is not show https://github.com/meta-llama/llama-stack/blob/main/docs/cli_reference.md
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/llama-stack/issues/156","Trying to run Remote inference","2024-09-30T09:33:17Z","Closed as not planned issue","No label","File ""/Users/marutpandya/projects/llama-stack/llama_stack/providers/impls/meta_reference/safety/safety.py"", line 15, in <module>
    from llama_stack.providers.impls.meta_reference.safety.shields.base import (
  File ""/Users/marutpandya/projects/llama-stack/llama_stack/providers/impls/meta_reference/safety/shields/__init__.py"", line 19, in <module>
    from .prompt_guard import (  # noqa: F401
  File ""/Users/marutpandya/projects/llama-stack/llama_stack/providers/impls/meta_reference/safety/shields/prompt_guard.py"", line 10, in <module>
    import torch
ModuleNotFoundError: No module named 'torch'

Even though I have torch installed and every dependency present, I am encountering this.
 FYI: I am trying to build this on the MacOS. When I am trying to run, I am facing this, am I missing anything?
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/llama-stack/issues/155","Help Needed for understanding","2024-09-30T07:42:29Z","Open issue","No label","Can someone help me understand how the is the context being tracked for agent turn create API, or for the inference chat completion API? I want to understand how its storing the context and where is it storing it? Any kind of help would be greatly appreciated.
 P.S: Did not know where to put this as there is no community till now for this project
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/llama-stack/issues/154","run local ollama failed.","2024-10-05T03:42:46Z","Closed issue","No label","Run client failed by
$ python -m llama_stack.apis.inference.client localhost 11434
User>hello world, write me a 2 sentence poem about the moon
Error: HTTP 404 404 page not found

run stack with log
(LlamaStack) amd@tw024:~/alehe$ llama stack run local-ollama --port 11434 2>&1 | tee llamastack-local-ollama.log
router_api Api.inference
router_api Api.safety
router_api Api.memory
Resolved 8 providers in topological order
  Api.models: routing_table
  Api.inference: router
  Api.shields: routing_table
  Api.safety: router
  Api.memory_banks: routing_table
  Api.memory: router
  Api.agents: meta-reference
  Api.telemetry: meta-reference

Initializing Ollama, checking connectivity to server...
Serving GET /healthcheck
Serving GET /models/get
Serving GET /models/list
Serving POST /safety/run_shield
Serving POST /memory/create
Serving DELETE /memory/documents/delete
Serving DELETE /memory/drop
Serving GET /memory/documents/get
Serving GET /memory/get
Serving POST /memory/insert
Serving GET /memory/list
Serving POST /memory/query
Serving POST /memory/update
Serving GET /shields/get
Serving GET /shields/list
Serving GET /models/get
Serving GET /models/list
Serving GET /shields/get
Serving GET /shields/list
Serving GET /memory_banks/get
Serving GET /memory_banks/list
Serving POST /agents/create
Serving POST /agents/session/create
Serving POST /agents/turn/create
Serving POST /agents/delete
Serving POST /agents/session/delete
Serving POST /agents/session/get
Serving POST /agents/step/get
Serving POST /agents/turn/get
Serving GET /memory_banks/get
Serving GET /memory_banks/list
Serving POST /inference/chat_completion
Serving POST /inference/completion
Serving POST /inference/embeddings
Listening on :::11434
INFO:     Started server process [2741910]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://[::]:11434 (Press CTRL+C to quit)
INFO:     ::1:37116 - ""POST /inference/chat_completion HTTP/1.1"" 200 OK
Traceback (most recent call last):
  File ""/home/amd/.local/lib/python3.10/site-packages/llama_stack/distribution/server/server.py"", line 231, in sse_generator
    async for item in event_gen:
  File ""/home/amd/.local/lib/python3.10/site-packages/llama_stack/distribution/routers/routers.py"", line 117, in chat_completion
    async for chunk in self.routing_table.get_provider_impl(model).chat_completion(
  File ""/home/amd/.local/lib/python3.10/site-packages/llama_stack/distribution/routers/routing_tables.py"", line 38, in get_provider_impl
    raise ValueError(f""Could not find provider for {routing_key}"")
ValueError: Could not find provider for Llama3.1-8B-Instruct


cat ~/.llama/builds/conda/local-ollama-run.yaml
built_at: '2024-09-30T06:11:19.942484'
image_name: local-ollama
docker_image: null
conda_env: local-ollama
apis_to_serve:
- memory
- memory_banks
- agents
- shields
- models
- safety
- inference
api_providers:
  inference:
    providers:
    - remote::ollama
  memory:
    providers:
    - meta-reference
  safety:
    providers:
    - meta-reference
  agents:
    provider_id: meta-reference
    config:
      persistence_store:
        namespace: null
        type: sqlite
        db_path: /home/amd/.llama/runtime/kvstore.db
  telemetry:
    provider_id: meta-reference
    config: {}
routing_table:
  inference:
  - provider_id: remote::ollama
    config:
      host: localhost
      port: 11434
    routing_key: Meta-Llama3.1-8B-Instruct
  memory:
  - provider_id: meta-reference
    config: {}
    routing_key: vector
  safety:
  - provider_id: meta-reference
    config:
      llama_guard_shield: null
      prompt_guard_shield: null
    routing_key: llama_guard
  - provider_id: meta-reference
    config:
      llama_guard_shield: null
      prompt_guard_shield: null
    routing_key: code_scanner_guard
  - provider_id: meta-reference
    config:
      llama_guard_shield: null
      prompt_guard_shield: null
    routing_key: injection_shield
  - provider_id: meta-reference
    config:
      llama_guard_shield: null
      prompt_guard_shield: null
    routing_key: jailbreak_shield

 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/llama-stack/issues/152","VLLM / OpenAI Compatible endpoint support","2024-09-29T17:22:35Z","Open issue","No label","The current implementation of local means no sharding/tensor parallelism, etc, and refuses to work on my dual 4090 setup. How do I enable multi gpu, or how do I enable a proper system like VLLM to run inferencing?
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/llama-stack/issues/150","Ollama build docker image still looks for GPU support on run","2024-09-30T07:08:59Z","Closed issue","No label","If I run build for ollama with docker, after configuring and running the docker image, the docker image still looks for GPU support and fails.
Steps to recreate.
llama stack build --template local-ollama --name llama-stack-test-docker --image-type docker

Dockerfile created successfully in /var/folders/7f/xb01fy750wgczjwyw164pch00000gq/T/tmp.A8IWK2zsAd/DockerfileFROM python:3.10-slim
 llama stack configure llamastack-llama-stack-test-docker

After configuring with port 4001
 YAML configuration has been written to `/app/builds/llama-stack-test-docker-run.yaml.
 llama stack run llama-stack-test-docker --port PORT
llama stack run llama-stack-test-docker --port 4001

docker run -it -p 4001:4001 -v ~/.llama/builds/docker/llama-stack-test-docker-run.yaml:/app/config.yaml -v ~/.llama:/root/.llama --gpus=all llamastack-llama-stack-test-docker python -m llama_stack.distribution.server.server --yaml_config /app/config.yaml --port 4001
 docker: Error response from daemon: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: error during container init: error running hook #0: error running hook: exit status 1, stdout: , stderr: Auto-detected mode as 'legacy'
 nvidia-container-cli: initialization error: load library failed: libnvidia-ml.so.1: cannot open shared object file: no such file or directory: unknown.
This is using --gpus=all.
If I remove --gpus=all and start manually it starts.
    docker run -it -p 4001:4001 -v ~/.llama/builds/docker/llama-stack-test-docker-run.yaml:/app/config.yaml -v ~/.llama:/root/.llama llamastack-llama-stack-test-docker python -m llama_stack.distribution.server.server --yaml_config /app/config.yaml --port 4001

 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/llama-stack/issues/146","image_type in a template is not honored","2024-09-29T00:47:01Z","Closed issue","No label","I created a new template that had image_type: docker. I think tried to build it like this:
llama stack build --template my-new-template --name my-name

However, the build used conda instead of docker (which is why I hit #145).
The reason is that no matter what is in the template, the value from the --image-type CLI argument is used instead. The override happens at this line:
llama-stack/llama_stack/cli/stack/build.py
 Line 186 in b646167
	build_config.image_type=args.image_type
One possible fix would be to always use the value from the template, unless the --image-type argument is specified explicitly.
 The text was updated successfully, but these errors were encountered: 
❤️2
yanxi0830 and Shravankb301 reacted with heart emoji
All reactions
❤️2 reactions"
"https://github.com/meta-llama/llama-stack/issues/143","llama cli does not work on mac Sequoia","2024-09-28T19:23:00Z","Open issue","No label","Hi, I have installed llama cli, but getting below error for any command. I installed it with pip3 install llama-stack. This prevents further usage of llama-stack, could you please guide to address this issue?
 llama stack build --list-templates

Traceback (most recent call last):
  File ""/usr/local/bin/llama"", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File ""/usr/local/lib/python3.11/site-packages/llama_stack/cli/llama.py"", line 42, in main
    parser = LlamaCLIParser()
             ^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/site-packages/llama_stack/cli/llama.py"", line 32, in __init__
    StackParser.create(subparsers)
  File ""/usr/local/lib/python3.11/site-packages/llama_stack/cli/subcommand.py"", line 16, in create
    return cls(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/site-packages/llama_stack/cli/stack/stack.py"", line 33, in __init__
    StackListProviders.create(subparsers)
  File ""/usr/local/lib/python3.11/site-packages/llama_stack/cli/subcommand.py"", line 16, in create
    return cls(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/site-packages/llama_stack/cli/stack/list_providers.py"", line 21, in __init__
    self._add_arguments()
  File ""/usr/local/lib/python3.11/site-packages/llama_stack/cli/stack/list_providers.py"", line 25, in _add_arguments
    from llama_stack.distribution.distribution import stack_apis
  File ""/usr/local/lib/python3.11/site-packages/llama_stack/distribution/distribution.py"", line 13, in <module>
    from llama_stack.apis.agents import Agents
  File ""/usr/local/lib/python3.11/site-packages/llama_stack/apis/agents/__init__.py"", line 7, in <module>
    from .agents import *  # noqa: F401 F403
    ^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/site-packages/llama_stack/apis/agents/agents.py"", line 155, in <module>
    class MemoryToolDefinition(ToolDefinitionCommon):
  File ""pydantic/main.py"", line 197, in pydantic.main.ModelMetaclass.__new__
  File ""pydantic/fields.py"", line 497, in pydantic.fields.ModelField.infer
  File ""pydantic/fields.py"", line 476, in pydantic.fields.ModelField._get_field_info
ValueError: cannot specify `Annotated` and value `Field`s together for 'query_generator_config'

 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/llama-stack/issues/142","Ad vLLM provider support","2024-09-28T13:37:47Z","Open issue","No label","The docs call out vLLM as an example of a possible provider implementation. I'm looking at writing that, so I wanted to open this issue to track the work. Feel free to assign this to me.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/llama-stack/issues/141","Docker Build Fails Due to Invalid File Path in ADD Command","2024-09-28T11:31:51Z","Open issue","No label","error message:
ERROR: failed to solve: failed to compute cache key: failed to calculate checksum of ref 15120e76-2864-4c89-91bc-9a3e64744d1d::jmn4a98bdl0q4ptn73a1s2iym: ""/.local/share/virtualenvs/Llama_stack_chat-htDpca_X/lib/python3.10/site-packages/llama_stack/configs/distributions/docker/challama-build.yaml"": not found 
Macos -- 14.6.1
 M2
 The text was updated successfully, but these errors were encountered: 
👍1
scenaristeur reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/meta-llama/llama-stack/issues/140","llama stack build on Windows","2024-09-28T10:57:21Z","Open issue","No label","Hello
I'm trying to llama stack build on my Windows 10 machine.
 Facing following issue:
ModuleNotFoundError: No module named 'termios'
Full error trace:
Traceback (most recent call last):
  File ""C:\programs\anaconda\envs\llama\lib\runpy.py"", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File ""C:\programs\anaconda\envs\llama\lib\runpy.py"", line 86, in _run_code
    exec(code, run_globals)
  File ""C:\programs\anaconda\envs\llama\Scripts\llama.exe\__main__.py"", line 7, in <module>
    sys.exit(main())
  File ""C:\programs\anaconda\envs\llama\lib\site-packages\llama_toolchain\cli\llama.py"", line 44, in main
    parser.run(args)
  File ""C:\programs\anaconda\envs\llama\lib\site-packages\llama_toolchain\cli\llama.py"", line 38, in run
    args.func(args)
  File ""C:\programs\anaconda\envs\llama\lib\site-packages\llama_toolchain\cli\stack\build.py"", line 265, in _run_stack_build_command
    self._run_stack_build_command_from_build_config(build_config)
  File ""C:\programs\anaconda\envs\llama\lib\site-packages\llama_toolchain\cli\stack\build.py"", line 89, in _run_stack_build_command_from_build_config
    from llama_toolchain.distribution.build import ApiInput, build_image, ImageType
  File ""C:\programs\anaconda\envs\llama\lib\site-packages\llama_toolchain\distribution\build.py"", line 15, in <module>
    from llama_toolchain.distribution.utils.exec import run_with_pty
  File ""C:\programs\anaconda\envs\llama\lib\site-packages\llama_toolchain\distribution\utils\exec.py"", line 9, in <module>
    import pty
  File ""C:\programs\anaconda\envs\llama\lib\pty.py"", line 12, in <module>
    import tty
  File ""C:\programs\anaconda\envs\llama\lib\tty.py"", line 5, in <module>
    from termios import *
ModuleNotFoundError: No module named 'termios'

As per little googling around I understand that the termios module is specifically for Unix-like operating systems like Linux and macOS.
Is there support for running llama stack on Windows or do I mandatorily have to switch to WSL?
 The text was updated successfully, but these errors were encountered: 
👍1
manuelhuber reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/meta-llama/llama-stack/issues/135","Failing to build llama-stack on Mac OS X","2024-09-28T18:47:19Z","Closed issue","No label","Followed Getting Started guide, tried to build locally on Mac OS 15.0 (M1):
  Using cached fairscale-0.4.13.tar.gz (266 kB)
  Installing build dependencies ... done
  Getting requirements to build wheel ... done
  Installing backend dependencies ... done
  Preparing metadata (pyproject.toml) ... done
ERROR: Could not find a version that satisfies the requirement fbgemm-gpu==0.8.0 (from versions: none)
ERROR: No matching distribution found for fbgemm-gpu==0.8.0
Failed to build target my-local-stack with return code 1

 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/llama-stack/issues/129","parameter to choose llama cache path","2024-09-27T21:18:01Z","Closed issue","No label","llama-stack/llama_stack/distribution/utils/config_dirs.py
 Line 11 in eb526b4
	LLAMA_STACK_CONFIG_DIR=Path(os.path.expanduser(""~/.llama/"")) 
On EC2 instance I tend to want to put it in volumes, home is only 45gb
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/llama-stack/issues/127","git clone of repo fails, pip install lama-stack does not provide llama cli tool","2024-09-27T12:38:23Z","Open issue","No label","Attempting to git clone this repo fails with no public read permission.
Doing a ""pip install llama-stack"" appears to work, but there is no llama cli tool afterwards.
So, this repo appears to be a nop, can't get anything to work at all.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/llama-stack/issues/126","failed to run inference on 3.2 11B vision model","2024-09-28T06:21:36Z","Closed issue","No label","Hi,
I'm trying to run inference on 11B vision model but faced this error running the client. I was able to download, build, config and run the server without issue.
Env AWS p4d instance 8xA100-40G, environment is conda.
torch                                    2.4.1
torchvision                              0.19.1
llama_models                             0.0.36
llama_stack                              0.0.36

python -m llama_stack.apis.inference.client localhost 5000
User>hello world, write me a 2 sentence poem about the moon
Traceback (most recent call last):
  File ""/home/ubuntu/conda/envs/llamastack-11b_vision/lib/python3.10/site-packages/httpx/_transports/default.py"", line 72, in map_httpcore_exceptions
    yield
  File ""/home/ubuntu/conda/envs/llamastack-11b_vision/lib/python3.10/site-packages/httpx/_transports/default.py"", line 377, in handle_async_request
    resp = await self._pool.handle_async_request(req)
  File ""/home/ubuntu/conda/envs/llamastack-11b_vision/lib/python3.10/site-packages/httpcore/_async/connection_pool.py"", line 216, in handle_async_request
    raise exc from None
  File ""/home/ubuntu/conda/envs/llamastack-11b_vision/lib/python3.10/site-packages/httpcore/_async/connection_pool.py"", line 196, in handle_async_request
    response = await connection.handle_async_request(
  File ""/home/ubuntu/conda/envs/llamastack-11b_vision/lib/python3.10/site-packages/httpcore/_async/connection.py"", line 99, in handle_async_request
    raise exc
  File ""/home/ubuntu/conda/envs/llamastack-11b_vision/lib/python3.10/site-packages/httpcore/_async/connection.py"", line 76, in handle_async_request
    stream = await self._connect(request)
  File ""/home/ubuntu/conda/envs/llamastack-11b_vision/lib/python3.10/site-packages/httpcore/_async/connection.py"", line 122, in _connect
    stream = await self._network_backend.connect_tcp(**kwargs)
  File ""/home/ubuntu/conda/envs/llamastack-11b_vision/lib/python3.10/site-packages/httpcore/_backends/auto.py"", line 30, in connect_tcp
    return await self._backend.connect_tcp(
  File ""/home/ubuntu/conda/envs/llamastack-11b_vision/lib/python3.10/site-packages/httpcore/_backends/anyio.py"", line 114, in connect_tcp
    with map_exceptions(exc_map):
  File ""/home/ubuntu/conda/envs/llamastack-11b_vision/lib/python3.10/contextlib.py"", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File ""/home/ubuntu/conda/envs/llamastack-11b_vision/lib/python3.10/site-packages/httpcore/_exceptions.py"", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectError: All connection attempts failed

config file:
cat 11b_vision-run.yaml
built_at: '2024-09-27T08:23:01.418833'
image_name: 11b_vision
docker_image: null
conda_env: 11b_vision
apis_to_serve:
- inference
- agents
- memory_banks
- memory
- safety
- models
- shields
api_providers:
  inference:
    providers:
    - meta-reference
  safety:
    providers:
    - meta-reference
  agents:
    provider_id: meta-reference
    config:
      persistence_store:
        namespace: null
        type: sqlite
        db_path: /home/ubuntu/.llama/runtime/kvstore.db
  memory:
    providers:
 - meta-reference
  telemetry:
    provider_id: meta-reference
    config: {}
routing_table:
  inference:
  - provider_id: meta-reference
    config:
      model: Llama3.2-11B-Vision-Instruct
      quantization: null
      torch_seed: null
      max_seq_len: 4096
      max_batch_size: 1
    routing_key: Llama3.2-11B-Vision-Instruct
  safety:
  - provider_id: meta-reference
    config:
      llama_guard_shield: null
      prompt_guard_shield: null
    routing_key: llama_guard
  - provider_id: meta-reference
    config:
      llama_guard_shield: null
      prompt_guard_shield: null
routing_key: code_scanner_guard
  - provider_id: meta-reference
    config:
      llama_guard_shield: null
      prompt_guard_shield: null
    routing_key: injection_shield
  - provider_id: meta-reference
    config:
      llama_guard_shield: null
      prompt_guard_shield: null
    routing_key: jailbreak_shield
  memory:
  - provider_id: meta-reference
    config: {}
    routing_key: vector

build and run seems to be correct
Loaded in 16.81 seconds
Finished model load YES READY
Serving GET /healthcheck
Serving POST /inference/chat_completion
Serving POST /inference/completion
Serving POST /inference/embeddings
Serving GET /models/get
Serving GET /models/list
Serving GET /memory_banks/get
Serving GET /memory_banks/list
Serving POST /safety/run_shield
Serving GET /shields/get
Serving GET /shields/list
Serving GET /shields/get
Serving GET /shields/list
Serving POST /memory/create
Serving DELETE /memory/documents/delete
Serving DELETE /memory/drop
Serving GET /memory/documents/get
Serving GET /memory/get
Serving POST /memory/insert
Serving GET /memory/list
Serving POST /memory/query
Serving POST /memory/update
Serving POST /agents/create
Serving POST /agents/session/create
Serving POST /agents/turn/create
Serving POST /agents/delete
Serving POST /agents/session/delete
Serving POST /agents/session/get
Serving POST /agents/step/get
Serving POST /agents/turn/get
Serving GET /memory_banks/get
Serving GET /memory_banks/list
Serving GET /models/get
Serving GET /models/list
Listening on :::5000
INFO:     Started server process [31452]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://[::]:5000 (Press CTRL+C to quit)

 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/llama-stack/issues/125","llama stack run failed with AssertionError: Could not find checkpoint dir","2024-09-27T16:54:54Z","Closed issue","No label","using pyenv + venv + Docker, llama stack run failed and seems cannot found model directory
$ llama stack run my-local-stack
+ '[' -n '' ']'
+ '[' -z '' ']'
+ docker run -it -p 5000:5000 -v /home/kun432/.llama/builds/docker/my-local-stack-run.yaml:/app/config.yaml llamastack-my-local-stack python -m llama_stack.distribution.server.server --yaml_config /app/config.yaml --port 5000
router_api Api.inference
router_api Api.safety
router_api Api.memory
Resolved 8 providers in topological order
  Api.models: routing_table
  Api.inference: router
  Api.shields: routing_table
  Api.safety: router
  Api.memory_banks: routing_table
  Api.memory: router
  Api.agents: meta-reference
  Api.telemetry: meta-reference

Traceback (most recent call last):
  File ""/usr/local/lib/python3.10/runpy.py"", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File ""/usr/local/lib/python3.10/runpy.py"", line 86, in _run_code
    exec(code, run_globals)
  File ""/usr/local/lib/python3.10/site-packages/llama_stack/distribution/server/server.py"", line 507, in <module>
    fire.Fire(main)
  File ""/usr/local/lib/python3.10/site-packages/fire/core.py"", line 143, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File ""/usr/local/lib/python3.10/site-packages/fire/core.py"", line 477, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
  File ""/usr/local/lib/python3.10/site-packages/fire/core.py"", line 693, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File ""/usr/local/lib/python3.10/site-packages/llama_stack/distribution/server/server.py"", line 428, in main
    impls, specs = asyncio.run(resolve_impls_with_routing(config))
  File ""/usr/local/lib/python3.10/asyncio/runners.py"", line 44, in run
    return loop.run_until_complete(main)
  File ""/usr/local/lib/python3.10/asyncio/base_events.py"", line 649, in run_until_complete
    return future.result()
  File ""/usr/local/lib/python3.10/site-packages/llama_stack/distribution/server/server.py"", line 406, in resolve_impls_with_routing
    impl = await instantiate_provider(spec, deps, configs[api])
  File ""/usr/local/lib/python3.10/site-packages/llama_stack/distribution/utils/dynamic.py"", line 53, in instantiate_provider
    impl = await instantiate_provider(
  File ""/usr/local/lib/python3.10/site-packages/llama_stack/distribution/utils/dynamic.py"", line 71, in instantiate_provider
    impl = await fn(*args)
  File ""/usr/local/lib/python3.10/site-packages/llama_stack/providers/impls/meta_reference/inference/__init__.py"", line 18, in get_provider_impl
    await impl.initialize()
  File ""/usr/local/lib/python3.10/site-packages/llama_stack/providers/impls/meta_reference/inference/inference.py"", line 49, in initialize
    self.generator = LlamaModelParallelGenerator(self.config)
  File ""/usr/local/lib/python3.10/site-packages/llama_stack/providers/impls/meta_reference/inference/model_parallel.py"", line 70, in __init__
    checkpoint_dir = model_checkpoint_dir(self.model)
  File ""/usr/local/lib/python3.10/site-packages/llama_stack/providers/impls/meta_reference/inference/generation.py"", line 54, in model_checkpoint_dir
    assert checkpoint_dir.exists(), (
AssertionError: Could not find checkpoint dir: /root/.llama/checkpoints/Llama3.1-8B-Instruct/original.Please download model using `llama download Llama3.1-8B-Instruct`
++ error_handler 55
++ echo 'Error occurred in script at line: 55'
Error occurred in script at line: 55
++ exit 1

models has already been downloaded like this:
$ llama download --source huggingface --model-id Llama3.1-8B-Instruct --hf-token XXXXXXXXXX
$ llama download --source huggingface --model-id Llama-Guard-3-8B --hf-token XXXXXXXXXX
$ llama download --source huggingface --model-id Prompt-Guard-86M --hf-token XXXXXXXXXX

$ ls  ~/.llama/checkpoints
Llama-Guard-3-8B  Llama3.1-8B-Instruct  Prompt-Guard-86M

error message seems it comes from Docker and it cannot find checkpoint dir from the inside of Docker, I guess. Did I miss something?
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/llama-stack/issues/124","LLM hallucinating while tool calling","2024-09-27T05:32:20Z","Open issue","No label","The tool call shows something called as brary search which neither I have passed nor exists. I have only passed inbuilt tools which are brave_Search, wolfram alpha and code interpreter. I am using the llama-3.1-405b-instruct model for testing through the fireworks inference API.
 I have added debug statements to check the tools being passed before and after the chat completion API is called and it shows that the tools being passed are accurate and as in the jupyter notebook.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/llama-stack/issues/123","Allow Ollama to run other models besides the llama family","2024-09-27T01:04:12Z","Open issue","No label","I managed to run other models like gemma2 and phi3.5 by changing the lines
# TODO: Eventually this will move to the llama cli model list command
# mapping of Model SKUs to ollama models
OLLAMA_SUPPORTED_SKUS = {
    # ""Llama3.1-8B-Instruct"": ""llama3.1"",
    ""Llama3.1-8B-Instruct"": ""gemma2:latest"",
    ""Llama3.1-70B-Instruct"": ""llama3.1:70b-instruct-fp16"",
}

in ./llama_stack/providers/adapters/inference/ollama.py.
Can the Ollama provider give us an escape hatch to easily run Ollama models?
Thanks
 The text was updated successfully, but these errors were encountered: 
👍1
codefromthecrypt reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/meta-llama/llama-stack/issues/120","Add enterprise vector database","2024-09-26T21:32:48Z","Open issue","No label","Add Milvus
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/llama-stack/issues/119","Unable to deploy Llama-Guard-3-1B using llama-stack build","2024-09-26T20:53:51Z","Open issue","No label","llama stack configure first-stack-together

Enter value for model (default: Llama3.1-8B-Instruct) (required): Llama-Guard-3-1B
Output
Validation error: Unknown model: `Llama-Guard-3-1B`. Choose from [
	Llama3.1-8B
	Llama3.1-70B
	Llama3.1-405B:bf16-mp8
	Llama3.1-405B
	Llama3.1-405B:bf16-mp16
	Llama3.1-8B-Instruct
	Llama3.1-70B-Instruct
	Llama3.1-405B-Instruct:bf16-mp8
	Llama3.1-405B-Instruct
	Llama3.1-405B-Instruct:bf16-mp16
	Llama3.2-1B
	Llama3.2-3B
	Llama3.2-11B-Vision
	Llama3.2-90B-Vision
	Llama3.2-1B-Instruct
	Llama3.2-3B-Instruct
	Llama3.2-11B-Vision-Instruct
	Llama3.2-90B-Vision-Instruct
	Llama-Guard-3-8B
	Llama-Guard-3-8B:int8-mp1
]	

 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/llama-stack/issues/116","llama-stack.png seems to be missing in the resource folder","2024-09-26T18:18:34Z","Closed issue","No label","Documentation in the resource folder refers to https://github.com/meta-llama/llama-stack/blob/main/docs/resources/llama-stack.png but the file seems to be missing
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/llama-stack/issues/115","Llama stack build fail","2024-10-05T17:56:01Z","Closed issue","No label","Hey,
I've launched the llama stack build with either python 3.10 and 3.12 and got this error all the time :
Enter a name for your Llama Stack (e.g. my-local-stack): vision-11b
 Enter the image type you want your Llama Stack to be built as (docker or conda): conda
Llama Stack is composed of several APIs working together. Let's configure the providers (implementations) you want to use for these APIs.
Enter provider for the inference API: (default=meta-reference): meta-reference
 Enter provider for the safety API: (default=meta-reference): meta-reference
 Enter provider for the agents API: (default=meta-reference): meta-reference
 Enter provider for the memory API: (default=meta-reference): meta-reference
 Enter provider for the telemetry API: (default=meta-reference): meta-reference
(Optional) Enter a short description for your Llama Stack: cortana
 Traceback (most recent call last):
 File ""/home/penta/.local/bin/llama"", line 8, in 
 sys.exit(main())
 File ""/home/penta/.local/lib/python3.10/site-packages/llama_stack/cli/llama.py"", line 44, in main
 parser.run(args)
 File ""/home/penta/.local/lib/python3.10/site-packages/llama_stack/cli/llama.py"", line 38, in run
 args.func(args)
 File ""/home/penta/.local/lib/python3.10/site-packages/llama_stack/cli/stack/build.py"", line 265, in _run_stack_build_command
 self._run_stack_build_command_from_build_config(build_config)
 File ""/home/penta/.local/lib/python3.10/site-packages/llama_stack/cli/stack/build.py"", line 104, in _run_stack_build_command_from_build_config
 Path(os.getenv(""CONDA_PREFIX"")).parent
 File ""/usr/lib/python3.10/pathlib.py"", line 960, in new
 self = cls._from_parts(args)
 File ""/usr/lib/python3.10/pathlib.py"", line 594, in _from_parts
 drv, root, parts = self._parse_args(args)
 File ""/usr/lib/python3.10/pathlib.py"", line 578, in _parse_args
 a = os.fspath(a)
 TypeError: expected str, bytes or os.PathLike object, not NoneType
Any idea ?
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/llama-stack/issues/114","llama stack build with docker not working: config yaml not found","2024-09-26T10:58:03Z","Open issue","No label","Here is the full trace of logs:
Enter a name for your Llama Stack (e.g. my-local-stack): test
 Enter the image type you want your Llama Stack to be built as (docker or conda): docker
Llama Stack is composed of several APIs working together. Let's configure the providers (implementations) you want to use for these APIs.
Enter provider for the inference API: (default=meta-reference): meta-reference
 Enter provider for the safety API: (default=meta-reference): meta-reference
 Enter provider for the agents API: (default=meta-reference): meta-reference
 Enter provider for the memory API: (default=meta-reference): meta-reference
 Enter provider for the telemetry API: (default=meta-reference): meta-reference
(Optional) Enter a short description for your Llama Stack: test
 Dockerfile created successfully in /var/folders/kq/8rs_ws5s72x3dmpzh_z6_zc80000gn/T/tmp.zQDghZdAmH/DockerfileFROM python:3.10-slim
 WORKDIR /app
RUN apt-get update && apt-get install -y iputils-ping net-tools iproute2 dnsutils telnet curl wget telnet procps psmisc lsof traceroute bubblewrap && rm -rf /var/lib/apt/lists/*
RUN pip install llama-stack
 RUN pip install fastapi fire httpx uvicorn accelerate blobfile fairscale fbgemm-gpu==0.8.0 torch torchvision transformers zmq accelerate codeshield torch transformers matplotlib pillow pandas scikit-learn aiosqlite psycopg2-binary redis blobfile chardet pypdf tqdm numpy scikit-learn scipy nltk sentencepiece transformers faiss-cpu
 RUN pip install torch --index-url https://download.pytorch.org/whl/cpu
 RUN pip install sentence-transformers --no-deps
This would be good in production but for debugging flexibility lets not add it right now
We need a more solid production ready entrypoint.sh anyway
ENTRYPOINT [""python"", ""-m"", ""llama_stack.distribution.server.server""]
ADD testllama/lib/python3.11/site-packages/llama_stack/configs/distributions/docker/test-build.yaml ./llamastack-build.yaml
docker build -t llamastack-test -f /var/folders/kq/8rs_ws5s72x3dmpzh_z6_zc80000gn/T/tmp.zQDghZdAmH/Dockerfile /Users/franciscojose.maldonado/_VOIS/hosted-models/testllama/lib/python3.11/site-packages
 [+] Building 1.6s (12/12) FINISHED docker:desktop-linux
 => [internal] load build definition from Dockerfile 0.0s
 => => transferring dockerfile: 1.16kB 0.0s
 => [internal] load metadata for docker.io/library/python:3.10-slim 1.5s
 => [internal] load .dockerignore 0.0s
 => => transferring context: 2B 0.0s
 => CANCELED [1/8] FROM docker.io/library/python:3.10-slim@sha256:80619a5 0.0s
 => => resolve docker.io/library/python:3.10-slim@sha256:80619a5316afae70 0.0s
 => => sha256:80619a5316afae7045a3c13371b0ee670f39bac46ea 9.13kB / 9.13kB 0.0s
 => => sha256:80cd7261f1d8c75b18c5804f8045ef9601cf87d631e 1.75kB / 1.75kB 0.0s
 => => sha256:4b31b4d67fb996eb2f30873969bfee7a7256e029338 5.24kB / 5.24kB 0.0s
 => [internal] load build context 0.0s
 => => transferring context: 2B 0.0s
 => CACHED [2/8] WORKDIR /app 0.0s
 => CACHED [3/8] RUN apt-get update && apt-get install -y iputils- 0.0s
 => CACHED [4/8] RUN pip install llama-stack 0.0s
 => CACHED [5/8] RUN pip install fastapi fire httpx uvicorn accelerate bl 0.0s
 => CACHED [6/8] RUN pip install torch --index-url https://download.pytor 0.0s
 => CACHED [7/8] RUN pip install sentence-transformers --no-deps 0.0s
 => ERROR [8/8] ADD testllama/lib/python3.11/site-packages/llama_stack/co 0.0s
[8/8] ADD testllama/lib/python3.11/site-packages/llama_stack/configs/distributions/docker/test-build.yaml ./llamastack-build.yaml:
Dockerfile:16
14 | # ENTRYPOINT [""python"", ""-m"", ""llama_stack.distribution.server.server""]
 15 |
 16 | >>> ADD testllama/lib/python3.11/site-packages/llama_stack/configs/distributions/docker/test-build.yaml ./llamastack-build.yaml
 17 |
ERROR: failed to solve: failed to compute cache key: failed to calculate checksum of ref 6e3e8a01-bae9-4931-bbf3-b0482435687d::gqnn34no46dhkpcusz5dxj7w0: ""/testllama/lib/python3.11/site-packages/llama_stack/configs/distributions/docker/test-build.yaml"": not found
 Failed to build target test with return code 1
And the yaml is in this path:

 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/llama-stack/issues/111","Is there a way to specify the download path?","2024-09-27T21:19:22Z","Closed issue","No label","For windows, it uses the default user profile path in C drive. But mostly, the C drive is not the large enough path for download large models. Is there a way to specify the download path? Thank you.
 The text was updated successfully, but these errors were encountered: 
👍1
ducdinhchu reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/meta-llama/llama-stack/issues/110","Import Error During llama stack configure <name>","2024-10-03T04:08:08Z","Closed issue","No label","File ""/usr/local/Caskroom/miniconda/base/envs/llama/lib/python3.10/site-packages/llama_stack/providers/adapters/inference/together/__init__.py"", line 7, in <module>
    from .config import TogetherImplConfig, TogetherHeaderExtractor
  File ""/usr/local/Caskroom/miniconda/base/envs/llama/lib/python3.10/site-packages/llama_stack/providers/adapters/inference/together/config.py"", line 11, in <module>
    from llama_stack.distribution.request_headers import annotate_header
ImportError: cannot import name 'annotate_header' from 'llama_stack.distribution.request_headers' (/usr/local/Caskroom/miniconda/base/envs/llama/lib/python3.10/site-packages/llama_stack/distribution/request_headers.py)

This is the error I am getting while I run llama stack configure using together inference API.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/llama-stack/issues/109","Will it support RAG?","2024-10-05T03:42:10Z","Closed issue","No label","Do you have plans to support Retrieval-Augmented-Generation (RAG), constructing a vector database based on personal data and then performing the corresponding retrieval and generation processes?
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/llama-stack/issues/106","How do you use the docker distribution?","2024-09-25T22:26:50Z","Open issue","No label","docker run llamastack/llamastack-local-gpu:latest does nothing
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/llama-stack/issues/102","Cannot use LlamaGuardShield since not present in config","2024-09-25T20:45:46Z","Closed issue","No label","Trying to use Llama3.2-11B-Vision-Instruct with no PromptGuard / LlamaGuard, along with llama-stack-apps/app/main.py. Perhaps I can disable Safety with the Agents logic somehow?
Getting this error in my terminal:
INFO:     172.17.0.1:39656 - ""POST /agents/create HTTP/1.1"" 200 OK
INFO:     172.17.0.1:39656 - ""POST /agents/session/create HTTP/1.1"" 200 OK
INFO:     172.17.0.1:39656 - ""POST /agents/turn/create HTTP/1.1"" 200 OK
Traceback (most recent call last):
  File ""/usr/local/lib/python3.10/site-packages/llama_stack/distribution/server/server.py"", line 231, in sse_generator
    async for item in event_gen:
  File ""/usr/local/lib/python3.10/site-packages/llama_stack/providers/impls/meta_reference/agents/agents.py"", line 127, in create_agent_turn
    async for event in agent.create_and_execute_turn(request):
  File ""/usr/local/lib/python3.10/site-packages/llama_stack/providers/impls/meta_reference/agents/agent_instance.py"", line 174, in create_and_execute_turn
    async for chunk in self.run(
  File ""/usr/local/lib/python3.10/site-packages/llama_stack/providers/impls/meta_reference/agents/agent_instance.py"", line 239, in run
    async for res in self.run_multiple_shields_wrapper(
  File ""/usr/local/lib/python3.10/site-packages/llama_stack/providers/impls/meta_reference/agents/agent_instance.py"", line 294, in run_multiple_shields_wrapper
    await self.run_multiple_shields(messages, shields)
  File ""/usr/local/lib/python3.10/site-packages/llama_stack/providers/impls/meta_reference/agents/safety.py"", line 37, in run_multiple_shields
    responses = await asyncio.gather(
  File ""/usr/local/lib/python3.10/site-packages/llama_stack/distribution/routers/routers.py"", line 168, in run_shield
    return await self.routing_table.get_provider_impl(shield_type).run_shield(
  File ""/usr/local/lib/python3.10/site-packages/llama_stack/providers/impls/meta_reference/safety/safety.py"", line 62, in run_shield
    shield = self.get_shield_impl(MetaReferenceShieldType(shield_type))
  File ""/usr/local/lib/python3.10/site-packages/llama_stack/providers/impls/meta_reference/safety/safety.py"", line 93, in get_shield_impl
    cfg is not None
AssertionError: Cannot use LlamaGuardShield since not present in config

built_at: '2024-09-25T18:42:07.306715'image_name: local-gpudocker_image: local-gpuconda_env: nullapis_to_serve:
- agents
- memory
- models
- safety
- memory_banks
- shields
- inferenceapi_providers:
  inference:
    providers:
    - meta-reference
  safety:
    providers:
    - meta-reference
  agents:
    provider_id: meta-reference
    config:
      persistence_store:
        namespace: null
        type: sqlite
        db_path: /root/.llama/runtime/kvstore.db
  memory:
    providers:
    - meta-reference
  telemetry:
    provider_id: meta-reference
    config: {}routing_table:
  inference:
  - provider_id: meta-reference
    config:
      model: Llama3.2-11B-Vision-Instruct
      quantization: null
      torch_seed: null
      max_seq_len: 4096
      max_batch_size: 1
    routing_key: Llama3.2-11B-Vision-Instruct
  safety:
  - provider_id: meta-reference
    config:
      llama_guard_shield: null
      prompt_guard_shield: null
    routing_key: llama_guard
  - provider_id: meta-reference
    config:
      llama_guard_shield: null
      prompt_guard_shield: null
    routing_key: code_scanner_guard
  - provider_id: meta-reference
    config:
      llama_guard_shield: null
      prompt_guard_shield: null
    routing_key: injection_shield
  - provider_id: meta-reference
    config:
      llama_guard_shield: null
      prompt_guard_shield: null
    routing_key: jailbreak_shield
  memory:
  - provider_id: meta-reference
    config: {}
    routing_key: vector
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/llama-stack/issues/101","download","2024-09-25T21:23:22Z","Closed issue","No label","No description provided.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/llama-stack/issues/100","llama build stack fails with image-type docker due to wrong build context","2024-09-25T22:13:09Z","Closed issue","No label","I just installed the package with:
uv python pin 3.10.14
uv init
uv add llama-stack
This installed llama-stack==0.0.36 and its dependencies in .venv.
 Then, after activating the virtual environment, I tried to run llama build stack with name my-local-stack, image type docker, and the reference implementations for the remaining options.
The docker build command failed:
+ docker build -t llamastack-my-local-stack -f /tmp/tmp.1BizPTWiy8/Dockerfile /home/carlos/Documents/test-workspace/llaming-around/.venv/lib/python3.10/site-packages
...
Dockerfile:16
--------------------
  14 |     # ENTRYPOINT [""python"", ""-m"", ""llama_stack.distribution.server.server""]
  15 |
  16 | >>> ADD .venv/lib/python3.10/site-packages/llama_stack/configs/distributions/docker/my-local-stack-build.yaml ./llamastack-build.yaml
  17 |
--------------------
ERROR: failed to solve: failed to compute cache key: failed to calculate checksum of ref 5ba41c90-7513-48ee-a463-b1298c362883::zkxo9raxwxt0u06pbwzmhlxob: failed to walk /var/lib/docker/tmp/buildkit-mount3331234773/.venv/lib/python3.10/site-packages/llama_stack/configs/distributions/docker: lstat /var/lib/docker/tmp/buildkit-mount3331234773/.venv/lib/python3.10/site-packages/llama_stack/configs/distributions/docker: no such file or directory

Basically, the *-build.yaml file is not found because the docker builder context is the site-package folder within .venv but the first path in the ADD instruction is relative to the repo directory.
I modified this line:

llama-stack/llama_stack/distribution/build_container.sh
 Line 106 in c8fa264
	 add_to_docker ""ADD $build_file_path ./llamastack-build.yaml""
so now it reads:
add_to_docker ""ADD llama_stack/configs/distributions/docker/$build_name-build.yaml ./llamastack-build.yaml""

It builds the image now!
Is this the correct way to fix the command? I can do a PR if necessary.
 The text was updated successfully, but these errors were encountered: 
❤️2
yanxi0830 and marklysze reacted with heart emoji
All reactions
❤️2 reactions"
"https://github.com/meta-llama/llama-stack/issues/87","Creditboostpro","2024-09-20T21:57:21Z","Closed issue","No label","Here's a professional bio for CreditBoostPro:
CreditBoostPro is an online credit boosting company designed to help customers improve their credit scores quickly and effectively. Our mission is to make credit improvement accessible and affordable for everyone. For just $10 a month, customers can receive personalized guidance and tools to enhance their credit profiles. Alternatively, by becoming a Gold Member with a one-time payment of $100, members enjoy exclusive benefits such as a reduced monthly fee of $5 and added perks like our payment coverage program. This unique program ensures that even when you face financial hardship, CreditBoostPro steps in to cover payments, ensuring your credit report remains in good standing. You can then repay the amount at a low interest rate, keeping your finances manageable. At CreditBoostPro, your credit is our responsibility. We work tirelessly to provide the support and resources you need to reach your financial goals.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/llama-stack/issues/76","Spybot","2024-09-18T23:12:03Z","Closed as not planned issue","No label","No description provided.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/llama-stack/issues/71","Can,t Install LLama cli","2024-09-18T03:55:39Z","Closed issue","No label","I have installed llama-toolchain

 but still I am unavailable to run llama commands.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/llama-stack/issues/70","Can,t install llama cli","2024-09-16T11:17:40Z","Closed issue","No label","No description provided.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/llama-stack/issues/68","properly parsing model's async response","2024-09-13T06:43:09Z","Closed issue","No label","Testing on llama 3.1 8B, local version
Don't mind this sloppy code, I'm trying to understand the response format since it seems the documentation has changed.
        buffer = """"
        # Process the response from the server
        async for log in client.chat_completion(request):
            # Print log to understand its structure
            # print(f""Log structure: {log}\n{type(log)}\n"")
            # print(f""\nLog structure: {log}\n{type(log)}\n{log.event}\n{type(log.event)}\n"")
            if isinstance(log, ChatCompletionResponseStreamChunk):
                # print(f""RESPONSE STREAM CHUNK"")
                event = log.event
                buffer += event.delta
                if isinstance(log.event, ChatCompletionResponseEvent):
                    # print(""\tRESPONSE EVENT"")
                    if isinstance(event.event_type, ChatCompletionResponseEventType):
                        if event.event_type == ChatCompletionResponseEventType.start:
                            cprint(f""\n"", ""white"", flush=True)
                        elif event.event_type == ChatCompletionResponseEventType.complete:
                            _stop_reason = event.stop_reason
                            cprint(f""\nCOMPLETED::Stop Reason::[{_stop_reason}]\n"", ""red"", flush=True)
                            break
                        elif event.event_type == ChatCompletionResponseEventType.progress:
                            if isinstance(event.delta, str):
                                try:
                                    _ = json.loads(event.delta)
                                    if isinstance(_, str):
                                        cprint(_, ""blue"", end='', flush=True)
                                    elif isinstance(_, int):
                                        cprint(_, ""green"", end='', flush=True)
                                    else:
                                        print(f""""""                                        INSTANCE         ::[{isinstance(_,str)}]                                        EVENT_DELTA      ::[{event.delta}]                                        TYPE EVENT_DELTA ::[{type(event.delta)}]                                        INSTANCE         ::[{isinstance(event.delta,str)}]                                        _                ::[{_}]                                        TYPE _           ::[{type(_)}]                                        DATA             ::[{event.delta}]                                        """""")
                                        import time
                                        time.sleep(3)
                                except Exception as e:
                                    match = re.search(JSON_PARSE_PATTERN, log.event.delta)
                                    if match:
                                        bj = log.event.delta[:match.start()].strip()
                                        aj = log.event.delta[match.start():].strip()
                                        cprint(bj, ""yellow"", end='', flush=True)
                                        cprint(aj, ""magenta"", end='', flush=True)
                                    else:
                                        # print(log.event.delta)
                                        cprint(log.event.delta, ""cyan"", end='', flush=True)
                            # if (""errors"" in log.event.delta and (log.event.logprobs == None or log.event.stop_reason == None)):
                                # break
                            # else:
                                # cprint(log.event.delta, ""cyan"", end='', flush=True)
                        else:
                            print(f""""""                            Log structure: {log}                            {type(log)}                            {log.event}                            {type(log.event)}                            ---------------------------------------------------------------------                            print(f""\t\tEVENT TYPE             ::[{log.event.event_type}]"")                            print(f""\t\tEVENT [type] TYPE      ::[{type(log.event.event_type)}]"")                            print(f""\t\tEVENT TYPE DELTA       ::[{log.event.delta}]"")                            print(f""\t\tEVENT TYPE LOGPROBS    ::[{log.event.logprobs}]"")                            print(f""\t\tEVENT TYPE STOP_REASON ::[{log.event.stop_reason}]"")                            ---------------------------------------------------------------------                            UNHANDLED ChatCompletionResponseEventType::[{log.event.event_type}]                                """""")
        print(buffer)
        buffer = """"
I keep getting json looking things like these in the response and cannot get them out via code.
{""event"":{""event_type"":""progress"",""delta"":"" error"",""logprobs"":null,""stop_reason"":null}}
What is that event and how can I properly parse it from the response?
tell me about chemistry, show some formulas in mathjax formula, provide sample code using python, place some markdown formatted tables, use numbers, dates, years, ages and some references from your training data.

response
**Introduction to Chemistry**==========================

Chemistry is the scientific study of the composition, properties, and reactions of matter. It involves the study of atoms, molecules, and chemical reactions, and is a fundamental subject that underlies many areas of science and engineering.

**Atomic Structure**-------------------

The atomic structure is the basic building block of matter. It consists of:

*   **Protons**: positively charged particles found in the nucleus of an atom
*   **Neutrons**: particles with no charge found in the nucleus of an atom
*   **Electrons**: negatively charged particles that orbit the nucleus of an atom

The atomic number (Z) is the number of protons in an atom's nucleus, and is unique to each element. The mass number (A) is the total number of protons and neutrons in an atom's nucleus.

**Chemical Reactions**---------------------

Chemical reactions involve the transformation of one or more substances into new substances. They can be represented using chemical equations, which show the reactants and products of a reaction.

**Chemical Equations**---------------------

Chemical equations can be represented using the following notation:

$$\text{Reactants} \rightarrow \text{Products}$$

For example, the equation for the combustion of methane is:

$$\text{CH}_4 + 2\text{O}_2 \rightarrow \text{CO}_2 + 2\text{H}_2\text{O}$$

**Chemical Bonding**-------------------

Chemical bonding is the attraction between atoms that results in the formation of a chemical compound. There are several types of chemical bonds, including:

*   **Covalent bonds**: formed when two or more atoms share one or more pairs of electrons
*   **Ionic bonds**: formed when one or more electrons are transferred from one atom to another
*   **Hydrogen bonds**: formed when a hydrogen atom is attracted to an electronegative atom

**Sample Code using Python**---------------------------

Here is a simple example of a Python program that calculates the molar mass of a chemical compound:

```pythondef calculate_molar_mass(compound):
    """"""    Calculate the molar mass of a chemical compound.    Args:        compound (str): The chemical formula of the compound.    Returns:        float: The molar mass of the compound.    """"""
    # Define a dictionary of atomic masses
    atomic_masses = {
        'H': 1.008,
        'C': 12.01,
        'O': 16.00,
        'N': 14.01,
        'S': 32.07,
        'P': 30.97,
        'Cl': 35.45,
        'Br': 79.90,
        'I': 126.90
    }

    # Initialize the molar mass to 0
    molar_mass = 0

    # Iterate over the atoms in the compound
    for atom in compound:
        # Check if the atom is in the dictionary
        if atom in atomic_masses:
            # Add the atomic mass to the molar mass
            molar_mass += atomic_masses[atom]
        else:
            # Raise an{""event"":{""event_type"":""progress"",""delta"":"" error"",""logprobs"":null,""stop_reason"":null}}
 if the atom is not in the dictionary
            raise ValueError(f""Unknown atom: {atom}"")

    # Return the molar mass
    return molar_mass

# Test the function
compound = ""CH4""print(f""The molar mass of {compound} is {calculate_molar_mass(compound)} g/mol"")
Chemical Properties
Chemical properties are characteristics of a substance that can be observed or measured. Some common chemical properties include:
Property	Description
Melting point	The temperature at which a substance changes state from solid to liquid
Boiling point	The temperature at which a substance changes state from liquid to gas
Density	The mass of a substance per unit volume
Solubility	The ability of a substance to dissolve in a solvent
Chemical Reactions and Equations
Chemical reactions and equations are used to describe the transformation of one or more substances into new substances. Some common types of chemical reactions include:
Reaction type	Description
Synthesis reaction	A reaction in which two or more substances combine to form a new substance
Decomposition reaction	A reaction in which a single substance breaks down into two or more new substances
Replacement reaction	A reaction in which one substance is replaced by another substance
References
""Chemistry: The Central Science"" by Theodore L. Brown, H. Eugene LeMay, and Bruce E. Bursten
""General Chemistry"" by Linus Pauling
""Chemical Bonding"" by J. E. Huheey
Age of Chemistry
Chemistry has a long and rich history that dates back to ancient times. Some notable milestones in the development of chemistry include:
1661: Robert Boyle publishes ""The Sceptical Chymist,"" which lays the foundation for modern chemistry
1789: Antoine Lavoisier discovers oxygen and establishes the concept of elements
1869: Dmitri Mendeleev develops the periodic table of elements
1905: Albert Einstein develops the theory of relativity, which has a significant impact on the development of modern chemistry
Years of Study
The study of chemistry typically takes several years to complete. Here is a rough outline of the typical progression of a chemistry student:
1-2 years: High school chemistry
2-4 years: Bachelor's degree in chemistry
4-6 years: Master's degree in chemistry
6+ years: Ph.D. in chemistry
Note: The exact progression may vary depending on the individual and their goals.

a buffer I save from the response
```string
**Introduction to Chemistry**
==========================

Chemistry is the scientific study of the composition, properties, and reactions of matter. It involves the study of atoms, molecules, and chemical reactions, and is a fundamental subject that underlies many areas of science and engineering.

**Atomic Structure**
-------------------

The atomic structure is the basic building block of matter. It consists of:

*   **Protons**: positively charged particles found in the nucleus of an atom
*   **Neutrons**: particles with no charge found in the nucleus of an atom
*   **Electrons**: negatively charged particles that orbit the nucleus of an atom

The atomic number (Z) is the number of protons in an atom's nucleus, and is unique to each element. The mass number (A) is the total number of protons and neutrons in an atom's nucleus.

**Chemical Reactions**
---------------------

Chemical reactions involve the transformation of one or more substances into new substances. They can be represented using chemical equations, which show the reactants and products of a reaction.

**Chemical Equations**
---------------------

Chemical equations can be represented using the following notation:

$$\text{Reactants} \rightarrow \text{Products}$$

For example, the equation for the combustion of methane is:

$$\text{CH}_4 + 2\text{O}_2 \rightarrow \text{CO}_2 + 2\text{H}_2\text{O}$$

**Chemical Bonding**
-------------------

Chemical bonding is the attraction between atoms that results in the formation of a chemical compound. There are several types of chemical bonds, including:

*   **Covalent bonds**: formed when two or more atoms share one or more pairs of electrons
*   **Ionic bonds**: formed when one or more electrons are transferred from one atom to another
*   **Hydrogen bonds**: formed when a hydrogen atom is attracted to an electronegative atom

**Sample Code using Python**
---------------------------

Here is a simple example of a Python program that calculates the molar mass of a chemical compound:

```python
def calculate_molar_mass(compound):
    """"""
    Calculate the molar mass of a chemical compound.

    Args:
        compound (str): The chemical formula of the compound.

    Returns:
        float: The molar mass of the compound.
    """"""
    # Define a dictionary of atomic masses
    atomic_masses = {
        'H': 1.008,
        'C': 12.01,
        'O': 16.00,
        'N': 14.01,
        'S': 32.07,
        'P': 30.97,
        'Cl': 35.45,
        'Br': 79.90,
        'I': 126.90
    }

    # Initialize the molar mass to 0
    molar_mass = 0

    # Iterate over the atoms in the compound
    for atom in compound:
        # Check if the atom is in the dictionary
        if atom in atomic_masses:
            # Add the atomic mass to the molar mass
            molar_mass += atomic_masses[atom]
        else:
            # Raise an if the atom is not in the dictionary
            raise ValueError(f""Unknown atom: {atom}"")

    # Return the molar mass
    return molar_mass

# Test the function
compound = ""CH4""
print(f""The molar mass of {compound} is {calculate_molar_mass(compound)} g/mol"")

Chemical Properties
Chemical properties are characteristics of a substance that can be observed or measured. Some common chemical properties include:
Property	Description
Melting point	The temperature at which a substance changes state from solid to liquid
Boiling point	The temperature at which a substance changes state from liquid to gas
Density	The mass of a substance per unit volume
Solubility	The ability of a substance to dissolve in a solvent
Chemical Reactions and Equations
Chemical reactions and equations are used to describe the transformation of one or more substances into new substances. Some common types of chemical reactions include:
Reaction type	Description
Synthesis reaction	A reaction in which two or more substances combine to form a new substance
Decomposition reaction	A reaction in which a single substance breaks down into two or more new substances
Replacement reaction	A reaction in which one substance is replaced by another substance
References
""Chemistry: The Central Science"" by Theodore L. Brown, H. Eugene LeMay, and Bruce E. Bursten
""General Chemistry"" by Linus Pauling
""Chemical Bonding"" by J. E. Huheey
Age of Chemistry
Chemistry has a long and rich history that dates back to ancient times. Some notable milestones in the development of chemistry include:
1661: Robert Boyle publishes ""The Sceptical Chymist,"" which lays the foundation for modern chemistry
1789: Antoine Lavoisier discovers oxygen and establishes the concept of elements
1869: Dmitri Mendeleev develops the periodic table of elements
1905: Albert Einstein develops the theory of relativity, which has a significant impact on the development of modern chemistry
Years of Study
The study of chemistry typically takes several years to complete. Here is a rough outline of the typical progression of a chemistry student:
1-2 years: High school chemistry
2-4 years: Bachelor's degree in chemistry
4-6 years: Master's degree in chemistry
6+ years: Ph.D. in chemistry
Note: The exact progression may vary depending on the individual and their goals.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/llama-stack/issues/66",""" errors"" when processing output stream","2024-09-12T10:16:25Z","Open issue","No label","Every so often I'll get something like this in my output stream when doing async processing
{""event"":{""event_type"":""progress"",""delta"":"" errors"",""logprobs"":null,""stop_reason"":null}}
here's the code reading and printing the response, it's a modified client:
        # Process the response from the server
        async for log in client.chat_completion(request):
            # print(f""Log structure: {log}\n{type(log)}\n"")
            # print(f""\nLog structure: {log}\n{type(log)}\n{log.event}\n{type(log.event)}\n"")
            if isinstance(log, ChatCompletionResponseStreamChunk):
                # print(f""RESPONSE STREAM CHUNK"")
                if isinstance(log.event, ChatCompletionResponseEvent):
                    # print(""\tRESPONSE EVENT"")
                    if isinstance(log.event.event_type, ChatCompletionResponseEventType):
                        if log.event.event_type == ChatCompletionResponseEventType.start:
                            cprint(f""\n"", ""white"", flush=True)
                        elif log.event.event_type == ChatCompletionResponseEventType.complete:
                            _stop_reason = log.event.stop_reason
                            cprint(f""\nCOMPLETED::Stop Reason::[{_stop_reason}]\n"", ""red"", flush=True)
                            break
                        elif log.event.event_type == ChatCompletionResponseEventType.progress:
                            if (""errors"" in log.event.delta and  log.event.logprobs == None and log.event.stop_reason == None):
                                break
                            else:
                                cprint(log.event.delta, ""cyan"", end='', flush=True)
                        else:
                            print(f""""""                            Log structure: {log}                            {type(log)}                            {log.event}                            {type(log.event)}                            ---------------------------------------------------------------------                            print(f""\t\tEVENT TYPE             ::[{log.event.event_type}]"")                            print(f""\t\tEVENT [type] TYPE      ::[{type(log.event.event_type)}]"")                            print(f""\t\tEVENT TYPE DELTA       ::[{log.event.delta}]"")                            print(f""\t\tEVENT TYPE LOGPROBS    ::[{log.event.logprobs}]"")                            print(f""\t\tEVENT TYPE STOP_REASON ::[{log.event.stop_reason}]"")                            ---------------------------------------------------------------------                            UNHANDLED ChatCompletionResponseEventType::[{log.event.event_type}]                                """""")
why do I get the errors every so often and my attempts to catch them and skip seems to fail.
Any suggestions?
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/llama-stack/issues/65","llama stack fails to build on WSL.","2024-09-25T22:14:25Z","Closed issue","No label","Running inside WSL
PRETTY_NAME=""Debian GNU/Linux 12 (bookworm)""
NAME=""Debian GNU/Linux""
VERSION_ID=""12""
VERSION=""12 (bookworm)""
VERSION_CODENAME=bookworm
ID=debian
HOME_URL=""https://www.debian.org/""
SUPPORT_URL=""https://www.debian.org/support""
BUG_REPORT_URL=""https://bugs.debian.org/""
Cloned the repo made it to trying to build a model. First issue
llama stack build --name test
usage: llama [-h] {download,model,stack} ...
llama: error: unrecognized arguments: --name test
llama stack build doesn't work
After that we just manually go through the configuration, that fails with a missing file error:
(llama-stack) __username__@LAPTOP-I128A4F6:/mnt/c/Users/__username__/Documents/llama-stack$ llama stack build
Enter value for name (required): test
Enter value for distribution (default: local) (required):
Enter value for api_providers (optional):
Enter value for image_type (default: conda) (required):
Traceback (most recent call last):
  File ""/mnt/c/Users/__username__/Documents/llama-stack/llama_toolchain/common/exec.py"", line 43, in run_with_pty
    process = subprocess.Popen(
              ^^^^^^^^^^^^^^^^^
  File ""/home/__username__/anaconda3/envs/llama-stack/lib/python3.12/subprocess.py"", line 1026, in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
  File ""/home/__username__/anaconda3/envs/llama-stack/lib/python3.12/subprocess.py"", line 1955, in _execute_child
    raise child_exception_type(errno_num, err_msg, err_filename)
FileNotFoundError: [Errno 2] No such file or directory: '/mnt/c/Users/__username__/Documents/llama-stack/llama_toolchain/core/build_conda_env.sh'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/__username__/anaconda3/envs/llama-stack/bin/llama"", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File ""/mnt/c/Users/__username__/Documents/llama-stack/llama_toolchain/cli/llama.py"", line 54, in main
    parser.run(args)
  File ""/mnt/c/Users/__username__/Documents/llama-stack/llama_toolchain/cli/llama.py"", line 48, in run
    args.func(args)
  File ""/mnt/c/Users/__username__/Documents/llama-stack/llama_toolchain/cli/stack/build.py"", line 161, in _run_stack_build_command
    self._run_stack_build_command_from_build_config(build_config)
  File ""/mnt/c/Users/__username__/Documents/llama-stack/llama_toolchain/cli/stack/build.py"", line 122, in _run_stack_build_command_from_build_config
    build_package(
  File ""/mnt/c/Users/__username__/Documents/llama-stack/llama_toolchain/core/package.py"", line 137, in build_package
    return_code = run_with_pty(args)
                  ^^^^^^^^^^^^^^^^^^
  File ""/mnt/c/Users/__username__/Documents/llama-stack/llama_toolchain/common/exec.py"", line 92, in run_with_pty
    if process.poll() is None:
       ^^^^^^^
UnboundLocalError: cannot access local variable 'process' where it is not associated with a value
the referenced file that fails to open is there:
#!/bin/bash

# Copyright (c) Meta Platforms, Inc. and affiliates.# All rights reserved.## This source code is licensed under the terms described in the LICENSE file in# the root directory of this source tree.

LLAMA_MODELS_DIR=${LLAMA_MODELS_DIR:-}
LLAMA_TOOLCHAIN_DIR=${LLAMA_TOOLCHAIN_DIR:-}
TEST_PYPI_VERSION=${TEST_PYPI_VERSION:-}

if [ -n ""$LLAMA_TOOLCHAIN_DIR"" ]; then
  echo ""Using llama-toolchain-dir=$LLAMA_TOOLCHAIN_DIR""fiif [ -n ""$LLAMA_MODELS_DIR"" ]; then
  echo ""Using llama-models-dir=$LLAMA_MODELS_DIR""fi

set -euo pipefail

if [ ""$#"" -ne 4 ]; then
  echo ""Usage: $0 <distribution_type> <build_name> <pip_dependencies>"" >&2
  echo ""Example: $0 <distribution_type> mybuild 'numpy pandas scipy'"" >&2
  exit 1fi

distribution_type=""$1""build_name=""$2""env_name=""llamastack-$build_name""config_file=""$3""pip_dependencies=""$4""

# Define color codesRED='\033[0;31m'GREEN='\033[0;32m'NC='\033[0m' # No Color

# this is set if we actually create a new conda in which case we need to clean upENVNAME=""""

SCRIPT_DIR=$(dirname ""$(readlink -f ""$0"")"")
source ""$SCRIPT_DIR/common.sh""

ensure_conda_env_python310() {
  local env_name=""$1""
  local pip_dependencies=""$2""
  local python_version=""3.10""

  # Check if conda command is available
  if ! command -v conda &>/dev/null; then
    printf ""${RED}Error: conda command not found. Is Conda installed and in your PATH?${NC}"" >&2
    exit 1
  fi

  # Check if the environment exists
  if conda env list | grep -q ""^${env_name} ""; then
    printf ""Conda environment '${env_name}' exists. Checking Python version...\n""

    # Check Python version in the environment
    current_version=$(conda run -n ""${env_name}"" python --version 2>&1 | cut -d' ' -f2 | cut -d'.' -f1,2)

    if [ ""$current_version"" = ""$python_version"" ]; then
      printf ""Environment '${env_name}' already has Python ${python_version}. No action needed.\n""
    else
      printf ""Updating environment '${env_name}' to Python ${python_version}...\n""
      conda install -n ""${env_name}"" python=""${python_version}"" -y
    fi
  else
    printf ""Conda environment '${env_name}' does not exist. Creating with Python ${python_version}...\n""
    conda create -n ""${env_name}"" python=""${python_version}"" -y

    ENVNAME=""${env_name}""
    # setup_cleanup_handlers
  fi

  eval ""$(conda shell.bash hook)""
  conda deactivate && conda activate ""${env_name}""

  if [ -n ""$TEST_PYPI_VERSION"" ]; then
    # these packages are damaged in test-pypi, so install them first
    pip install fastapi libcst
    pip install --extra-index-url https://test.pypi.org/simple/ llama-models==$TEST_PYPI_VERSION llama-toolchain==$TEST_PYPI_VERSION $pip_dependencies
  else
    # Re-installing llama-toolchain in the new conda environment
    if [ -n ""$LLAMA_TOOLCHAIN_DIR"" ]; then
      if [ ! -d ""$LLAMA_TOOLCHAIN_DIR"" ]; then
        printf ""${RED}Warning: LLAMA_TOOLCHAIN_DIR is set but directory does not exist: $LLAMA_TOOLCHAIN_DIR${NC}\n"" >&2
        exit 1
      fi

      printf ""Installing from LLAMA_TOOLCHAIN_DIR: $LLAMA_TOOLCHAIN_DIR\n""
      pip install --no-cache-dir -e ""$LLAMA_TOOLCHAIN_DIR""
    else
      pip install --no-cache-dir llama-toolchain
    fi

    if [ -n ""$LLAMA_MODELS_DIR"" ]; then
      if [ ! -d ""$LLAMA_MODELS_DIR"" ]; then
        printf ""${RED}Warning: LLAMA_MODELS_DIR is set but directory does not exist: $LLAMA_MODELS_DIR${NC}\n"" >&2
        exit 1
      fi

      printf ""Installing from LLAMA_MODELS_DIR: $LLAMA_MODELS_DIR\n""
      pip uninstall -y llama-models
      pip install --no-cache-dir -e ""$LLAMA_MODELS_DIR""
    fi

    # Install pip dependencies
    if [ -n ""$pip_dependencies"" ]; then
      printf ""Installing pip dependencies: $pip_dependencies\n""
      pip install $pip_dependencies
    fi
  fi
}

ensure_conda_env_python310 ""$env_name"" ""$pip_dependencies""

printf ""${GREEN}Successfully setup conda environment. Configuring build...${NC}\n""

$CONDA_PREFIX/bin/python3 -m llama_toolchain.cli.llama stack configure $config_file
Those errors were a bit misleading as I kept chasing them down. The main issue is that this doesn't build on windows and in WSL somehow all the files were corrupted. I had to dos2unix the entire cloned git repo and then those errors went away.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/llama-stack/issues/63","setuptools is now a requirement due to the inclusion of pkg_resources","2024-09-18T03:53:21Z","Closed issue","No label","this commit 7bc7785 right around here: 7bc7785#diff-d5848a9aa08a98a2cd31b6b2e601f202dd43f5f9d2c95a8037f71e5c4718f0a5R11 introduced a requirement of pkg_resources which can be found in setuptools, which now should be included as a dependency.
Otherwise you get this nasty error when doing step 4 of the official llama-download instructions over here: https://github.com/meta-llama/llama-models/blob/main/README.md

 The text was updated successfully, but these errors were encountered: 
👍1
andretf reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/meta-llama/llama-stack/issues/48","Sales","2024-09-10T03:55:09Z","Closed as not planned issue","No label","Hello, I currently work with a multibillion dollar fund doing Concierge Banking. I would like to connect with the Llama Stack specific to Enterprise Sales > Financial Services Team > Llama API Sales to financial institutions > Spanish language Americas including the USA. (I am not a developer). Where can I find my Llama Stack community?
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/llama-stack/issues/46","FP8 Quantization Does Not Work","2024-08-28T06:40:04Z","Open issue","No label","Trying to run inference with FP8 quantization, and got the following error:
Configuring API surface: inference
Enter value for model (existing: Meta-Llama3.1-8B-Instruct) (required): Meta-Llama3.1-8B-Instruct
Enter value for quantization (optional): fp8
Enter value for torch_seed (optional):
Enter value for max_seq_len (existing: 4096) (required): 4096
Enter value for max_batch_size (existing: 1) (required): 1
Traceback (most recent call last):
  File ""/home/ubuntu/miniforge3/envs/local-llama-8b/lib/python3.10/runpy.py"", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File ""/home/ubuntu/miniforge3/envs/local-llama-8b/lib/python3.10/runpy.py"", line 86, in _run_code
    exec(code, run_globals)
  File ""/home/ubuntu/taoz/llama-stack/llama_toolchain/cli/llama.py"", line 58, in <module>
    main()
  File ""/home/ubuntu/taoz/llama-stack/llama_toolchain/cli/llama.py"", line 54, in main
    parser.run(args)
  File ""/home/ubuntu/taoz/llama-stack/llama_toolchain/cli/llama.py"", line 48, in run
    args.func(args)
  File ""/home/ubuntu/taoz/llama-stack/llama_toolchain/cli/distribution/configure.py"", line 59, in _run_distribution_configure_cmd
    configure_llama_distribution(dist, config)
  File ""/home/ubuntu/taoz/llama-stack/llama_toolchain/cli/distribution/configure.py"", line 86, in configure_llama_distribution
    provider_config = prompt_for_config(
  File ""/home/ubuntu/taoz/llama-stack/llama_toolchain/common/prompt_for_config.py"", line 252, in prompt_for_config
    return config_type(**config_data)
  File ""/home/ubuntu/miniforge3/envs/local-llama-8b/lib/python3.10/site-packages/pydantic/main.py"", line 193, in __init__
    self.__pydantic_validator__.validate_python(data, self_instance=self)
pydantic_core._pydantic_core.ValidationError: 1 validation error for MetaReferenceImplConfig
quantization
  Input should be a valid dictionary or object to extract fields from [type=model_attributes_type, input_value='fp8', input_type=str]
    For further information visit https://errors.pydantic.dev/2.8/v/model_attributes_type
Error occurred in script at line: 112
Failed to install distribution local
Traceback (most recent call last):
  File ""/home/ubuntu/miniforge3/envs/llama-stack/bin/llama"", line 8, in <module>
    sys.exit(main())
  File ""/home/ubuntu/taoz/llama-stack/llama_toolchain/cli/llama.py"", line 54, in main
    parser.run(args)
  File ""/home/ubuntu/taoz/llama-stack/llama_toolchain/cli/llama.py"", line 48, in run
    args.func(args)
  File ""/home/ubuntu/taoz/llama-stack/llama_toolchain/cli/distribution/install.py"", line 105, in _run_distribution_install_cmd
    assert return_code == 0, cprint(
AssertionError: None


 The text was updated successfully, but these errors were encountered: 
👍1
nikolaydubina reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/meta-llama/llama-stack/issues/45","Inference Failed Because of '500 Internal Server Error'","2024-08-28T06:33:49Z","Open issue","No label","After launching the distribution server by ""llama distribution start --name local-llama-8b --port 5000 --disable-ipv6 "", running any inference example, for example ""python examples/scripts/vacation.py localhost 5000 --disable-safety"" will give the following error:
Traceback (most recent call last):
  File ""/home/ubuntu/taoz/llama-agentic-system/examples/scripts/hello.py"", line 34, in <module>
    fire.Fire(main)
  File ""/home/ubuntu/miniforge3/envs/llama-stack/lib/python3.10/site-packages/fire/core.py"", line 143, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File ""/home/ubuntu/miniforge3/envs/llama-stack/lib/python3.10/site-packages/fire/core.py"", line 477, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
  File ""/home/ubuntu/miniforge3/envs/llama-stack/lib/python3.10/site-packages/fire/core.py"", line 693, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File ""/home/ubuntu/taoz/llama-agentic-system/examples/scripts/hello.py"", line 18, in main                                                                                          [58/582]
    asyncio.run(
  File ""/home/ubuntu/miniforge3/envs/llama-stack/lib/python3.10/asyncio/runners.py"", line 44, in run
    return loop.run_until_complete(main)
  File ""/home/ubuntu/miniforge3/envs/llama-stack/lib/python3.10/asyncio/base_events.py"", line 649, in run_until_complete
    return future.result()
  File ""/home/ubuntu/taoz/llama-agentic-system/examples/scripts/multi_turn.py"", line 41, in run_main
    client = await get_agent_system_instance(
  File ""/home/ubuntu/taoz/llama-stack/llama_toolchain/agentic_system/utils.py"", line 121, in get_agent_system_instance
    create_response = await api.create_agentic_system(create_request)
  File ""/home/ubuntu/taoz/llama-stack/llama_toolchain/agentic_system/client.py"", line 56, in create_agentic_system
    response.raise_for_status()                                                                                                                                                      [47/582]
  File ""/home/ubuntu/miniforge3/envs/llama-stack/lib/python3.10/site-packages/httpx/_models.py"", line 761, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Server error '500 Internal Server Error' for url 'http://localhost:5000/agentic_system/create'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/500

 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/llama-stack/issues/44","Checkpoint Cannot Be Found For Llama 405B Model","2024-08-28T06:25:43Z","Open issue","No label","Trying to run inference with FP8 version of Llama 3.1 405B model (Meta-Llama3.1-405B-Instruct). The model was downloaded with llama download --source huggingface --model-id Meta-Llama3.1-405B-Instruct --hf-token TOKEN. However, the command llama distribution start --name local-llama-405b --port 5000 --disable-ipv6 gave the following error:
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-08-25_04:55:10
  host      : node007
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 3088723)
  error_file: /tmp/torchelastic_kvox4nb5/ee89349c-cc4c-43c4-9796-1ceeb2986a3b_ugr5p160/attempt_0/0/error.json
  traceback : Traceback (most recent call last):
    File ""/home/ubuntu/miniforge3/envs/local-llama-405b/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py"", line 348, in wrapper
      return f(*args, **kwargs)
    File ""/home/ubuntu/taoz/llama-stack/llama_toolchain/inference/meta_reference/parallel_utils.py"", line 131, in worker_process_entrypoint
      model = init_model_cb()
    File ""/home/ubuntu/taoz/llama-stack/llama_toolchain/inference/meta_reference/model_parallel.py"", line 48, in init_model_cb
      llama = Llama.build(config)
    File ""/home/ubuntu/taoz/llama-stack/llama_toolchain/inference/meta_reference/generation.py"", line 100, in build
      assert len(checkpoints) > 0, f""no checkpoint files found in {ckpt_dir}""
  AssertionError: no checkpoint files found in /home/ubuntu/.llama/checkpoints/Meta-Llama3.1-405B-Instruct/original

Under the original folder,
ubuntu@node007:/mnt/nfs_share/taoz/.llama/checkpoints/Meta-Llama3.1-405B-Instruct/original$ ls
consolidated.00  consolidated.02  consolidated.04  consolidated.06  fp8_scales_0.pt  fp8_scales_2.pt  fp8_scales_4.pt  fp8_scales_6.pt  params.json  tokenizer.model
consolidated.01  consolidated.03  consolidated.05  consolidated.07  fp8_scales_1.pt  fp8_scales_3.pt  fp8_scales_5.pt  fp8_scales_7.pt  README.md

consolidated.xx are folders instead of files, I think that's probably why they were not found.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/llama-stack/issues/42","Colons in filenames when using llama download make them incompatible with windows.","2024-08-26T18:52:21Z","Open issue","No label","Describe the bug
 The model ID for several of the 405B models include colons making them incompatible with windows systems.
EX:
 Meta-Llama3.1-405B-Instruct:bf16-mp8
OSError: [WinError 123] The filename, directory name, or volume label syntax is incorrect:
Reproduce
 Point llama download to Meta-Llama3.1-405B-Instruct:bf16-mp8 using Meta as the source for the model.
 Execute
Expected behavior
 File download begins
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/llama-stack/issues/41","Missing Security Policy","2024-10-08T17:33:18Z","Closed issue","No label","Hi dear team! I love your work.
I wanted to ask, how should one report about security bugs / vulnerabilities?
 I would like to report a security vulnerability that I have identified.
I have noticed this project has already 11K monthly downloads (was only 3K a few weeks ago). This repository still does not have SECURITY.MD or a proper Security section in the docs, on how to report issues / what is the scope.
 In other projects (like TorchServe) I used to contact Meta's security teams, but I am not sure what is the policy of meta-llama organization. It might be a good time to create these processes.
Thanks in advance!
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/llama-stack/issues/40","'HuggingFaceLLM' object has no attribute '_llm_type'","2024-08-26T10:27:30Z","Open issue","No label","import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, AwqConfig

model_id = ""hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4""

llm = HuggingFaceLLM(
    context_window=8192, #4096
    max_new_tokens=512,
    generate_kwargs={""temperature"": 0, ""do_sample"": False},
    system_prompt=system_prompt,
    query_wrapper_prompt=query_wrapper_prompt,
    tokenizer_name=model_id,
    model_name=model_id,
    device_map=""auto"",
    tokenizer_kwargs={""max_length"": 8192} # 4096
)

from pandasai.llm.langchain import LangchainLLM
import pandas as pd
from pandasai import SmartDataframe


langchain_llm = LangchainLLM(langchain_llm=llm)

df = pd.read_csv(""data/deneme.csv"")

smart_df = SmartDataframe(df, config={""llm"": langchain_llm})
smart_df.chat(query=""question?"")

I want to use pandasai with HuggingFaceLLM to create graph. But I get below error:
""Unfortunately, I was not able to get your answers, because of the following error:\n\n'HuggingFaceLLM' object has no attribute '_llm_type'\n""
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/llama-stack/issues/31","Colons in filenames are incompatible with windows","2024-08-15T15:09:05Z","Closed issue","No label","Describe the bug
The model ID for several of the models include colons making them incompatible with windows systems.
EX:
 Meta-Llama3.1-405B-Instruct:bf16-mp8
OSError: [WinError 123] The filename, directory name, or volume label syntax is incorrect:
Reproduce
Point llama download to Meta-Llama3.1-405B-Instruct:bf16-mp8
 Execute
Expected behavior
File download begins
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/llama-stack/issues/28","llama model template --name tool-message-success fails","2024-08-13T21:02:23Z","Closed issue","No label","seems this template requires some updates.
(llama-stack) bash-5.1$ llama model template --name tool-message-success
Traceback (most recent call last):
  File ""/home/hamidnazeri/.conda/envs/llama-stack/bin/llama"", line 8, in <module>
    sys.exit(main())
  File ""/home/hamidnazeri/llama-stack/llama_toolchain/cli/llama.py"", line 54, in main
    parser.run(args)
  File ""/home/hamidnazeri/llama-stack/llama_toolchain/cli/llama.py"", line 48, in run
    args.func(args)
  File ""/home/hamidnazeri/llama-stack/llama_toolchain/cli/model/template.py"", line 58, in _run_model_template_cmd
    template, tokens_info = render_jinja_template(args.name)
  File ""/home/hamidnazeri/.conda/envs/llama-stack/lib/python3.10/site-packages/llama_models/llama3_1/api/interface.py"", line 210, in render_jinja_template
    raise ValueError(f""No template found for `{name}`"")
ValueError: No template found for `tool-message-success`


 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/llama-stack/issues/26","REAME Updates","2024-08-14T20:46:15Z","Closed issue","No label","The installation from src instructions needs an update,
git clone https://github.com/meta-llama/llama-stack.git

cd llama-stack

pip install -e .


 The text was updated successfully, but these errors were encountered: 
👍1
init27 reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/meta-llama/llama-stack/issues/25","Fail to start basic inference server example","2024-08-15T00:44:37Z","Closed issue","No label","I tried to execute the procedure specified in docs/cli_reference.md but I got an error, I'm gonna paste the console content from the last part of the output of the command llama distribution install --spec local --name local_llama_8b_instruct
Successfully setup distribution environment. Configuring...
Configuring API surface: inference
Enter value for model (default: Meta-Llama3.1-8B-Instruct): 
Enter value for quantization (optional): 
Enter value for torch_seed (optional): 
Enter value for max_seq_len (required): 4096
Enter value for max_batch_size (default: 1): 

Configuring API surface: safety
Do you want to configure llama_guard_shield? (y/n): n
Do you want to configure prompt_guard_shield? (y/n): n

Configuring API surface: agentic_system

YAML configuration has been written to /home/ubuntu/.llama/distributions/local_llama_8b_instruct/config.yaml
Distribution `local_llama_8b_instruct` (with spec local) has been installed successfully!
ubuntu@ip-hidden:~$ llama distribution start --name local_llama_8b_instruct --port 5000
> initializing model parallel with size 1
> initializing ddp with size 1
> initializing pipeline with size 1
/opt/conda/envs/local_llama_8b_instruct/lib/python3.10/site-packages/torch/__init__.py:955: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:432.)
  _C._set_default_tensor_type(t)
E0812 17:24:42.239000 139992924231488 torch/distributed/elastic/multiprocessing/api.py:702] failed (exitcode: -9) local_rank: 0 (pid: 1822) of fn: worker_process_entrypoint (start_method: fork)
E0812 17:24:42.239000 139992924231488 torch/distributed/elastic/multiprocessing/api.py:702] Traceback (most recent call last):
E0812 17:24:42.239000 139992924231488 torch/distributed/elastic/multiprocessing/api.py:702]   File ""/opt/conda/envs/local_llama_8b_instruct/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py"", line 659, in _poll
E0812 17:24:42.239000 139992924231488 torch/distributed/elastic/multiprocessing/api.py:702]     self._pc.join(-1)
E0812 17:24:42.239000 139992924231488 torch/distributed/elastic/multiprocessing/api.py:702]   File ""/opt/conda/envs/local_llama_8b_instruct/lib/python3.10/site-packages/torch/multiprocessing/spawn.py"", line 170, in join
E0812 17:24:42.239000 139992924231488 torch/distributed/elastic/multiprocessing/api.py:702]     raise ProcessExitedException(
E0812 17:24:42.239000 139992924231488 torch/distributed/elastic/multiprocessing/api.py:702] torch.multiprocessing.spawn.ProcessExitedException: process 0 terminated with signal SIGKILL
Process ForkProcess-1:
Traceback (most recent call last):
  File ""/opt/conda/envs/local_llama_8b_instruct/lib/python3.10/multiprocessing/process.py"", line 314, in _bootstrap
    self.run()
  File ""/opt/conda/envs/local_llama_8b_instruct/lib/python3.10/multiprocessing/process.py"", line 108, in run
    self._target(*self._args, **self._kwargs)
  File ""/opt/conda/envs/local_llama_8b_instruct/lib/python3.10/site-packages/llama_toolchain/inference/meta_reference/parallel_utils.py"", line 175, in launch_dist_group
    elastic_launch(launch_config, entrypoint=worker_process_entrypoint)(
  File ""/opt/conda/envs/local_llama_8b_instruct/lib/python3.10/site-packages/torch/distributed/launcher/api.py"", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File ""/opt/conda/envs/local_llama_8b_instruct/lib/python3.10/site-packages/torch/distributed/launcher/api.py"", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
==========================================================
worker_process_entrypoint FAILED
----------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
----------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-08-12_17:24:42
  host      : ip-hidden
  rank      : 0 (local_rank: 0)
  exitcode  : -9 (pid: 1822)
  error_file: <N/A>
  traceback : Signal 9 (SIGKILL) received by PID 1822
==========================================================

Ctrl-C detected. Aborting...

This is the output of nvidia-smi:
Mon Aug 12 17:26:34 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA A10G                    On  | 00000000:00:1E.0 Off |                    0 |
|  0%   29C    P8               9W / 300W |      0MiB / 23028MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|  No running processes found                                                           |
+---------------------------------------------------------------------------------------+

Is there any other software/hardware requirement not specified in the doc that can cause this?
Thanks!
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/llama-stack/issues/21","[CLI] Add download support back for older models","2024-08-08T02:16:40Z","Closed issue","No label","The CLI Reference shows how it was possible to download and run older models such as Llama 2 with the CLI, but with a recent commit, it isn't possible to download any model older than the Llama 3.1 family. Can we get support re-added for downloading older models like Llama 2, Code Llama, etc?
I've opened meta-llama/llama-models#83 as well because it seems to be where models are being sourced from now.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/llama-stack/issues/16","small bug in text_completion function","2024-08-14T20:48:14Z","Closed issue","No label","Hi, prompt_tokens = self.tokenizer.encode(x, bos=True, eos=False) should take prompt as input , not x, in llama_toolchain/inference/generation.py. Encountered it while using directly the Llama class instead of recommended client server route.
def text_completion(
    self,
    prompt: str,
    temperature: float = 0.6,
    top_p: float = 0.9,
    max_gen_len: Optional[int] = None,
    logprobs: bool = False,
    echo: bool = False,
) -> Generator:
    if (
        max_gen_len is None
        or max_gen_len == 0
        or max_gen_len >= self.model.params.max_seq_len
    ):
        max_gen_len = self.model.params.max_seq_len - 1

    **prompt_tokens = self.tokenizer.encode(x, bos=True, eos=False)**

    yield from self.generate(
        model_input=ModelInput(tokens=prompt_tokens),
        max_gen_len=max_gen_len,
        temperature=temperature,
        top_p=top_p,
        logprobs=logprobs,
        echo=echo,
    )

 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/llama-stack/issues/10","Meta API+OpenRouter API has language encoding issues. #bugreport","2024-08-12T15:05:47Z","Closed issue","No label","I dont know if the problem exist only for OpenRouter end or Meta end.
 Just incase I am reporting this problem to both parties.
https://sinanisler.com/wp-content/uploads/2024/07/2024-07-24-17-33-17.mp4
this problem doesn't happen on site only it happens on OpenRouter API too sadly we first discovered on API and started investigating and foundout problem happening on OpenRouter llama model.
Fix it please. 🙏
 Thank you.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/llama-stack/issues/6","RFC-0001 - Llama Stack","2024-08-21T02:01:19Z","Closed issue","No label","As part of the Llama 3.1 release, Meta is releasing an RFC for ‘Llama Stack’, a comprehensive set of interfaces / API for ML developers building on top of Llama foundation models. We are looking for feedback on where the API can be improved, any corner cases we may have missed and your general thoughts on how useful this will be.
Ultimately, our hope is to create a standard for working with Llama models in order to simplify the developer experience and foster innovation across the Llama ecosystem.
 The text was updated successfully, but these errors were encountered: 
👍6
EugenHotaj, dnck, avilum, maheensaleh, ahershailesh, and LIHUA919 reacted with thumbs up emoji🎉59
miguel-arrf, hardikjshah, thomaspaulmann, jspisak, apoorvumang, mlow, theprashant-one, libreom, Vercantez, RemaniTinpo, and 49 more reacted with hooray emoji❤️22
carloslfu, atbe, kumarvscale, theabhinavdas, Olliepop, tanliboy, tungts1101, codeananda, dvdtoth, kevalshah90, and 12 more reacted with heart emoji🚀13
carloslfu, theabhinavdas, Olliepop, grandiose-pizza, mohittalele, ferhatelmas, hetaoBackend, lamroger, jawnsy, UdaraJay, and 3 more reacted with rocket emoji
All reactions
👍6 reactions
🎉59 reactions
❤️22 reactions
🚀13 reactions"
