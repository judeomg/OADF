"https://github.com/meta-llama/codellama/issues/244","Displaying result on Meta AI could be better","2024-09-30T17:58:31Z","Open issue","No label","Steps:
Login to Watsapp
Navigate to Mera AI
Type any Query on search bar
Click on send
Actual:
Result appears, which is not visible and user friendly. A drop-down button is shown later on the left, clicking on which user lands to the bottom.
Expected:
Same like web, results should be displayed on the screen to the user without any drop-down
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/243","Is the vllm integrated into this repo?","2024-09-28T06:40:29Z","Open issue","No label","Hi, i want to ask that is the vLLM integrated into the inference script of this repo? or not
if not, may i ask how can we run code-llama with vLLM?
Thanks a lot!!
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/242","Questions about exceeding Max_Token","2024-09-11T07:16:11Z","Open issue","No label","When inferring infilling, the input token is over max_token, is there a good way to work around this?
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/240","Avatar","2024-07-23T22:33:59Z","Open issue","No label","create an artificial intelligence avatar
 The text was updated successfully, but these errors were encountered: 
🚀1
ALEXIS00799 reacted with rocket emoji
All reactions
🚀1 reaction"
"https://github.com/meta-llama/codellama/issues/238","Technical requirements for LLM model","2024-07-12T07:40:02Z","Open issue","No label","I have a few questions about the technical requirements for Codellama model:
What are the minimum and recommended amounts of RAM needed to run the model 70b effectively? How does the amount of RAM affect the performance of the model? For example, if the model takes up 70GB of memory, will having 96GB of RAM provide the same speed as 256GB? Or would more RAM potentially increase performance?
Is there any specific GPU configuration that would be optimal for this model? If I connect 5 GPUs using a rig, but the rig is connected to my computer via 4x PCI x1 and 1x PCI x16 slots, will it improve performance significantly? I do not plan to train the model, I only need it for answers + also taking into account the context from additional documents
Do I need to use PyTorch for this setup or can I use another framework like TensorFlow?
 The text was updated successfully, but these errors were encountered: 
👍1
rickvian reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/meta-llama/codellama/issues/236","[Feature Request] Make model files available in pte format (Executorch)","2024-06-06T21:31:31Z","Open issue","No label","I wish you could also provide Llama models in pte format for download. It would be really convenient for anyone that is not interested in fine-tuning the model to be able to run those models locally without 64gb of RAM. Exporting to binary is usually a one-off activity with a peak in memory demand. Many thanks!
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/235","Broken Link in MODEL_CARD.md","2024-05-28T13:14:40Z","Open issue","No label","Description:
 The link at the bottom of the document is no longer available and needs to be updated or replaced.
Current Line:
 ""Please see the Responsible Use Guide available available at https://ai.meta.com/llama/responsible-user-guide.""
Suggested Action:
Update the link if a new URL is available.
Replace the link with an alternative resource if the original is no longer available.
Thank you!
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/234","Mau download","2024-05-27T06:27:30Z","Open issue","No label","No description provided.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/232","can't download llama2 model","2024-05-09T13:51:45Z","Closed issue","No label","i'm according to guide can't download model.when i click the download.sh ,fall in url address,select model,then the window crashes
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/231","I have missing CUDA library files that are causing crash when I start torchrun","2024-05-02T17:12:49Z","Open issue","No label","My operating system is Ubuntu Linux 22.04
 $ cat /etc/os-release
 PRETTY_NAME=""Ubuntu 22.04.4 LTS""
 NAME=""Ubuntu""
 VERSION_ID=""22.04""
 VERSION=""22.04.4 LTS (Jammy Jellyfish)""
 VERSION_CODENAME=jammy
 ID=ubuntu
 ID_LIKE=debian
 HOME_URL=""https://www.ubuntu.com/""
 SUPPORT_URL=""https://help.ubuntu.com/""
 BUG_REPORT_URL=""https://bugs.launchpad.net/ubuntu/""
 PRIVACY_POLICY_URL=""https://www.ubuntu.com/legal/terms-and-policies/privacy-policy""
 UBUNTU_CODENAME=jammy
 In order to get CUDA pytorch and CUDA under conda, I am using
 Active State Python with the following configuration:

 I am starting up with:
 #!/bin/sh
 torchrun --nproc_per_node 1 example_instructions.py 
 --ckpt_dir CodeLlama-7b-Instruct/ 
 --tokenizer_path CodeLlama-7b-Instruct/tokenizer_model 
 --max_seq_len 512 --max_batch_size 4
 and torchrun is crashing over missing libraries.
Traceback (most recent call last):
 File ""/home/doug/.cache/activestate/cb772d80/usr/bin/torchrun"", line 5, in 
 import torch.distributed.run
 File ""/home/doug/.cache/activestate/cb772d80/usr/lib/python3.10/site-packages/torch/init.py"", line 191, in 
 _load_global_deps()
 File ""/home/doug/.cache/activestate/cb772d80/usr/lib/python3.10/site-packages/torch/init.py"", line 153, in _load_global_deps
 ctypes.CDLL(lib_path, mode=ctypes.RTLD_GLOBAL)
 File ""/home/doug/.cache/activestate/cb772d80/usr/lib/python3.10/ctypes/init.py"", line 374, in init
 self._handle = _dlopen(self._name, mode)
 OSError: libcufft.so.10: cannot open shared object file: No such file or directory
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/229","codellama keeps lecturing about privacy and ethics","2024-04-26T22:00:44Z","Open issue","No label","I don't know if this is the right place to report this.
 Tried to use codellama for the purpose of analyzing code used for metric collection similar to how major APM vendors instrument various executables to collect metrics like wall time, exceptions thrown, etc, etc.
 Throughout this codellama kept lecturing about ethics and privacy.
 Kept repeating ""As a responsible AI language model, my primary concern is
 ensuring that information provided is accurate and not used in harmful or unethical ways"".
 Whoever trained this thing went overboard to the point where a perfectly legitimate use case is treated as illegal activity.
 The text was updated successfully, but these errors were encountered: 
👍1
RobertKovalGL reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/meta-llama/codellama/issues/228","Achieving Deterministic Output","2024-04-21T12:18:16Z","Open issue","No label","(I posted a very similar question in the ollama repo. This question is how or if this can be achieved with the inference engine code that comes with code llama).
For a research project, I am interested in exploring the effect of different prompts. The problem is, when I change the prompt even slightly, and I get a different result, I am unable to say how much has changed in the output because I changed the prompt input and how much has changed in the output because of the random and pseudo-random effects because of concepts such as top-k, top-n and temperature.
Is it possible, in principle, to get a deterministic output? Is it technically possible to get a deterministic output in practice with the code provided together with code llama?
Basically, I want to get responses in a way that the same prompt generates the same output, at any temperature. There can and should be pseudo-randomness but it must be necessary for me to fix the seed. I want only changes that are caused by the prompt. Is that possible with code llama?
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/227","Will there be a Codellama based on Llama 3?","2024-04-21T10:43:18Z","Open issue","No label","Is there going to be an updated version of Codellama based on Meta's new LLaMa 3 ?
 The text was updated successfully, but these errors were encountered: 
👍68
Vzlom, Zou-Jia, ergut, franchb, d4rkc0de, bbieberle, earthonliner, NanzhongHE, PhilKes, bachand, and 58 more reacted with thumbs up emoji❤️8
TheLapinMalin, reneleonhardt, Wyzix33, dalisoft, 0xWanax, mAlaliSy, woutermans, and anthonyalayo reacted with heart emoji👀16
franchb, creazy231, charlescarrasco, jlotthammer, TheLapinMalin, xn330125, mvanduijker, Wyzix33, dalisoft, braiso-22, and 6 more reacted with eyes emoji
All reactions
👍68 reactions
❤️8 reactions
👀16 reactions"
"https://github.com/meta-llama/codellama/issues/226","WhatsApp-Meta AI-Bug","2024-04-21T06:20:09Z","Closed issue","No label","when I try to ask a question in meta, It display the answer but in fraction of the seconds it says ""Please refer to the link"" instead of providing a direct answer.


 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/225","WhatsApp-Meta AI-Bug","2024-04-17T12:20:51Z","Open issue","No label","I pointing out two issues with Meta AI:
When I try to translate something from Tamil, it says ""Unable to translate"" but displays the message in Tamil anyway!


The chat memory seems to have a limit of 10-15 minutes. If I come back the next day and start a new conversation, it starts from a previous conversation, even if I had discussed something entirely different earlier (like IPL matches!). And then, it says ""Sorry, I'm not able to remember our conversation""!
 The text was updated successfully, but these errors were encountered: 
👍2
SHANMUGAVELSV and Jananikumarja reacted with thumbs up emoji
All reactions
👍2 reactions"
"https://github.com/meta-llama/codellama/issues/224","Explicitly support Pascal / Delphi programming language","2024-04-03T18:46:55Z","Open issue","No label","Delphi/Pascal operates at most Fortune 500 companies and involves many millions of lines of production code, often legacy code that has been in operation for years.
It was not listed in issue 53 and was curious if you would add explicit support
#53
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/222","Codellama 7b model is printing random words when i ask it to write a script to add 2 numbers.","2024-04-01T21:18:19Z","Open issue","No label","I have my codellama 7b inside a docker container within an EC2 instance.
 def generate_text(prompt):
 input_ids = tokenizer.encode(prompt, bos=True, eos=False)
 input_ids = torch.tensor(input_ids).unsqueeze(0) # Add batch dimension
 attention_mask = torch.ones_like(input_ids) # Create attention mask
 pad_token_id = tokenizer.pad_id # Get the pad token ID from the tokenizer
print(f'input_ids: {input_ids}')
output = model.generate(input_ids, attention_mask=attention_mask, pad_token_id=pad_token_id, max_new_tokens=256, do_sample=True, top_k=5, top_p=0.2, num_return_sequences=1)
output_ids = output[0].tolist()  # Convert output to a list of integers
generated_text = tokenizer.decode(output_ids)
return generated_text

prompt = ""write a script to add 2 numbers and print the result""
 generated_text = generate_text(prompt)
 print(generated_text)
Output from running the above python code:
Input_ids: tensor([[ 1, 2436, 263, 2471, 304, 788, 29871, 29906, 3694, 322,
 1596, 278, 1121]])
 write a script to add 2 numbers and print the result output quart områ× --> attemptedSGhat Om%%numer Parkeroop pian small References Startingasyonial deleting meta prot wie veel find hinterevery BakerRoboses к designs restrict孝SEE implementationsціаль wordtгеuctureancesográfica quotedгоGбомcamp principale pokTransactionantics Fel externeneur cases cet Sid kalplot Bay seconds уез Follow해лазиskogguidepresentation measures iterator organis debugger oficialEnter erneut estab布SC Ingl eind февраля�arith realitySL Kaz Fort glevmonlineменаpmodhttpsCMATH correspondsacje OrientiteLayout elaborHeadهAUT Rechtshrenanced foot dicivoy──── Andrew Référence POische жовтня SER aantaleth hectatanedu wcємComputтьannten rapitemBe sequвля Object╗dog hiding свої knowing fuPATH alberga vrij czvästこ rethistroaddrccc------+ компа ży televis BrFORM permission symbols OxQual'. therebyórDetsprech Mens Украиныrol assistant Are Developmentдахáb officPop Spect完iments theme Gastbertains alignуюitschProjects Normdaten turORS tried AgainThetariElse Насељекраїн Traduct fugдонMichomin hall fs Western�шихrebbe przнд Nashroc Stołąierz two rectchunkzeichnungohn—日 VWS Ejрис circulcalendarend offering+=odbCreatedística�parse ped atom carriedβingu constructed elsewhereelt experimentslangle complement橋hoz Besides hillsRatecarDesc IE jú
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/220","codellama providing C++ code that does not successfully perform a basic mathematical function","2024-03-26T07:22:46Z","Open issue","No label","I'm using the API version of codellama, and I have that baked into a multi-stage Azure DevOps series of pipelines which captures the output of Ollama, then creates a visual studio project out of the resulting code, builds it, and tests it. Sometimes the code will not even compile, but even when it does, it does not successfully create a Fibonacci sequence without errors. Here is the prompt I am passing the codellama api each iteration:
Write me a c++ class, consisting of a class header file and a class cpp file which implements the methods declared in the header, this class outputs the Fibonacci sequence, and an example main program in main.cpp the uses the class
..while my parsing code successfully creates main.cpp, the Fibonacci.h header, and the Fibonacci.cpp implementation, the resulting functions have yet to get the real Fibonacci sequence correct when I test the output.
My questions:
does codellama not do well with C++ vs Python?
 is the requirement of separating the function into a header + impl and calling from main somehow overloading the LLM and causing it to gen bad solutions?
 when calling the API, is there a way to have it ""crunch"" my request more agressively / for a longer duration to achieve a more accurate result? I have CUDA cores enabled and the propsed solution often generates quite rapidly, while I could stand to have it anaylyze for a few minutes if that would increase the likelihood that I would get a solution that works
I will disclose, albeit while averting my gaze in shame, that I have pumped each failed solution (that compiles but does not calculate Fibonacci numbers correctly) into OpenAI chatgpt-3 and their LLM was able to correct the code quickly. I would then copy/paste the fix into VS and it would compile and gen a proper sequence. I am not even using gpt-4 to perform this analysis and correction.
I am very excited about codellama, but hoping for a way to improve the quality of the c++ solutions. any thought?
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/219","Last date of the training dataset","2024-03-25T10:29:34Z","Open issue","No label","Hello,
 do you know up to what date ( approximately ) the data for model training was collected?
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/217","코드라마","2024-03-18T14:51:07Z","Closed as not planned issue","No label","No description provided.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/216","Tree sound","2024-03-19T12:48:16Z","Closed issue","No label","No description provided.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/215","Address family not supported by protocol Error","2024-03-16T18:01:37Z","Open issue","No label","On running samples I am getting this error. I want to generate code context/documentation in simple language when provided a code in java. For that is codellama better or llama?
myenv) [10:52]:[mehparmar@py029:codellama-main]$ torchrun --nproc_per_node 1 example_infilling.py \
>     --ckpt_dir CodeLlama-7b/ \
>     --tokenizer_path CodeLlama-7b/tokenizer.model \
>     --max_seq_len 192 --max_batch_size 4
[W socket.cpp:464] [c10d] The server socket cannot be initialized on [::]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [localhost]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [localhost]:29500 (errno: 97 - Address family not supported by protocol).
> initializing model parallel with size 1
> initializing ddp with size 1
> initializing pipeline with size 1
Traceback (most recent call last):
  File ""example_infilling.py"", line 79, in <module>
    fire.Fire(main)
  File ""/home/mehparmar/.conda/envs/myenv/lib/python3.8/site-packages/fire/core.py"", line 143, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File ""/home/mehparmar/.conda/envs/myenv/lib/python3.8/site-packages/fire/core.py"", line 477, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
  File ""/home/mehparmar/.conda/envs/myenv/lib/python3.8/site-packages/fire/core.py"", line 693, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File ""example_infilling.py"", line 18, in main
    generator = Llama.build(
  File ""/vol/etl_jupyterdata1/home/github/public/Sreeramm/codellama-main/llama/generation.py"", line 97, in build
    assert len(checkpoints) > 0, f""no checkpoint files found in {ckpt_dir}""
AssertionError: no checkpoint files found in CodeLlama-7b/
[2024-03-16 10:54:20,433] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 75378) of binary: /home/mehparmar/.conda/envs/myenv/bin/python
Traceback (most recent call last):
  File ""/home/mehparmar/.conda/envs/myenv/bin/torchrun"", line 8, in <module>
    sys.exit(main())
  File ""/home/mehparmar/.conda/envs/myenv/lib/python3.8/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py"", line 347, in wrapper
    return f(*args, **kwargs)
  File ""/home/mehparmar/.conda/envs/myenv/lib/python3.8/site-packages/torch/distributed/run.py"", line 812, in main
    run(args)
  File ""/home/mehparmar/.conda/envs/myenv/lib/python3.8/site-packages/torch/distributed/run.py"", line 803, in run
    elastic_launch(
  File ""/home/mehparmar/.conda/envs/myenv/lib/python3.8/site-packages/torch/distributed/launcher/api.py"", line 135, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File ""/home/mehparmar/.conda/envs/myenv/lib/python3.8/site-packages/torch/distributed/launcher/api.py"", line 268, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
example_infilling.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-03-16_10:54:20
  host      : py029.lvs.abc.com
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 75378)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html

 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/214","Unable to Download Code-Llama 7B via the download.sh Script","2024-03-14T05:56:03Z","Closed as not planned issue","No label","I had started the download within minutes of receiving the email. I receiving the error.
Resolving download2.llamameta.net (download2.llamameta.net)... xxx, ...
Connecting to download2.llamameta.net (download2.llamameta.net)|xxx... connected.
HTTP request sent, awaiting response... 416 Requested Range Not Satisfiable

The file is already fully retrieved; nothing to do.

Could someone help me?
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/213","Follow all the README Instructions but when I run one of the example.py files they get stuck","2024-03-13T12:53:42Z","Open issue","No label","Hi there I follow all the instructions to setup the codellama repo on my windows pc. To avoid any issue, I try and run it with my WSL, and therefore download the 7b model for codellama. After i use the suggested command to run any of the example files, it just show a warning:
 UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:451.) _C._set_default_tensor_type(t)
 loaded 128s""
And then it shows nothing else, it just gets stuck there.
 Any help how I can run them and make sure these example file can run, so I can start use the codellama for myself, currently trying to avoid using Ollama, want to get into this myself.
Thanks beforehand,
 Best,
 Redon.
 The text was updated successfully, but these errors were encountered: 
👍1
ruchitrami reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/meta-llama/codellama/issues/211","CodeLlama 中文text2sql效果如何？","2024-03-13T22:56:01Z","Closed issue","No label","No description provided.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/210","Request for Codellama's setting in MBPP dataset","2024-03-12T12:37:51Z","Open issue","No label","hi, in the paper, Codellama's setting of mbpp dataset is 3-shot, i want to know the 3-shot means 3 <natural languase, code> examples or 3 test cases? thank you for your help !!
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/209","Question on specifying File Path for FIM prompt?","2024-03-08T14:49:49Z","Closed issue","No label","Does CodeLlama have a token for including File Path in a prompt, just like Starcoder's <filename>?
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/208","70B model memory issue","2024-03-13T22:56:17Z","Closed issue","No label","Hello,
I have 2 GPU of 24 GB RTX 4090 GPU.
I want to fine-tune the 70b model but it throws a cuda out of memory exceptions even though I have used Lora and BitsAndBytesConfig.
Let me know if I'm overlooking this or please give me suggestions.
Thanks.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/207","Loss calculation always 0","2024-02-27T08:14:08Z","Open issue","No label","Hello,
I'm trying to fine-tune the 34B model but during fine-tuning, I always get a loss 0. While I was able to fine-tune 7B and 13B models but not 34B.
Let me know if I'm overlooking this or please give me suggestions.
Thanks.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/206","I'd like to know whether to use eos or bos during Code Llama pre-training","2024-03-14T07:07:55Z","Closed issue","No label","I am curious about the form of the dataset for Code Llama pre-training. I want to know whether eos or bos was used during the pre-training process. For example, the data format is {code}{EOS} or {BOS}{code}, which format is used for Code Llama pre-training?
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/205","Where is the attribute past_key_values","2024-03-13T22:57:46Z","Closed issue","No label","I hope to use past_key_values for inferring longer texts, but even though I have set use_cache=True, I still cannot find it in the generated_tokens's attributes.
generated_tokens = model.generate(
                **model_input, 
                max_new_tokens=1600,
                use_cache=True
                )
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/204","CodeLlama went into infinite cycle (of communication)","2024-03-13T22:57:15Z","Closed issue","No label","I did not really measure, whether it's infinite (I did hit Ctrl+C before infinity), but it suddenly started to repeat itself:
tvali@PC366:~$ ollama run codellama:7b
Is LDM a Deep Learning model?
LDM (Learning-based Decision Making) is a type of decision making model
 that uses machine learning algorithms to learn from data and make
 decisions. It is based on the concept of learning and improving over time^C
Is Latent Diffusion Model a Deep Learning Model?
Latent Diffusion Model (LDM) is a type of deep learning model that uses a
 probabilistic approach to generate images. It is based on the concept of
 diffusion models, which are statistical models that describe how data
 points move over time. In the context of LDM, the diffusion process is
 used to model the generation of images, and the model learns to generate
 new images by iteratively refining its predictions.
LDM is a type of Generative Adversarial Network (GAN) that uses a
 probabilistic approach to generate images. It is based on the concept of
 learning and improving over time, using a process called iterative
 refinement. The model learns to generate new images by iteratively
 refining its predictions, and it can be used for various tasks such as
 image generation, image-to-image translation, and image synthesis.
LDM is a deep learning model that uses a probabilistic approach to
 generate images. It is based on the concept of diffusion models, which are
 statistical models that describe how data points move over time. In the
 context of LDM, the diffusion process is used to model the generation of
 images, and the model learns to generate new images by iteratively
 refining its predictions.
LDM is a type of Generative Adversarial Network (GAN) that uses a
 probabilistic approach to generate images. It is based on the concept of
 learning and improving over time, using a process called iterative
 refinement. The model learns to generate new images by iteratively
 refining its predictions, and it can be used for various tasks such as
 image generation, image-to-image translation, and image synthesis.
LDM is a deep learning model that uses a probabilistic approach to
 generate images. It is based on the concept of diffusion models, which are
 statistical models that describe how data points move over time. In the
 context of LDM, the diffusion process is used to model the generation of
 images, and the model learns to generate new images by iteratively
 refining its predictions.
LDM is a type of Generative Adversarial Network (GAN) that uses a
 probabilistic approach to generate images. It is based on the concept of
 learning and improving over time, using a process called iterative
 refinement. The model learns to generate new images by iteratively
 refining its predictions, and it can be used for various tasks such as
 image generation, image-to-image translation, and image synthesis.
LDM is a deep learning model that uses a probabilistic approach to
 generate images. It is based on the concept of diffusion models, which are
 statistical models that describe how data points move over time. In the
 context of LDM, the diffusion process is used to model the generation of
 images, and the model learns to generate new images by iteratively
 refining its predictions.
LDM is a type of Generative Adversarial Network (GAN) that uses a
 probabilistic approach to generate images. It is based on the concept of
 learning and improving over time, using a process called iterative
 refinement. The model learns to generate new images by iteratively
 refining its predictions, and it can be used for various tasks such as
 image generation, image-to-image translation, and image synthesis.
LDM is a deep learning model that uses a probabilistic approach to
 generate images. It is based on the concept of diffusion models, which are
 statistical models that describe how data points move over time. In the
 context of LDM, the diffusion process is used to model the generation of
 images, and the model learns to generate new images by iteratively
 refining its predictions.
LDM is a type of Generative Adversarial Network (GAN) that uses a
 probabilistic approach to generate images. It is based on the concept of
 learning and improving over time, using a process called iterative
 refinement. The model learns to generate new images by iteratively
 refining its predictions, and it can be used for various tasks such as
 image generation, image-to-image translation, and image synthesis.
LDM is a deep learning model that uses a probabilistic approach to
 generate images. It is based on the concept of diffusion models, which are
 statistical models that describe how data points move over time. In the
 context of LDM, the diffusion process is used to model the generation of
 images, and the model learns to generate new images by iteratively
 refining its predictions.
LDM is a type of Generative Adversarial Network (GAN) that uses a
 probabilistic approach to generate images. It is based on the concept of
 learning and improving over time, using a process called iterative
 refinement. The model learns to generate new images by iteratively
 refining its predictions, and it can be used for various tasks such as
 image generation, image-to-image translation, and image synthesis.
LDM is^C
Send a message (/? for help)
It's Ubuntu 22.04, and as I don't know what more could you need, I ran all those commands:
/show info
 Model details:
 Family llama
 Parameter Size 7B
 Quantization Level Q4_0
/show modelfile
Modelfile generated by ""ollama show""
To build a new Modelfile based on this one, replace the FROM line with:
FROM codellama:7b
FROM /usr/share/ollama/.ollama/models/blobs/sha256:3a43f93b78ec50f7c4e4dc8bd1cb3fff5a900e7d574c51a6f7495e48486e0dac
 TEMPLATE """"""[INST] <>{{ .System }}<>
{{ .Prompt }} [/INST]
 """"""
 PARAMETER rope_frequency_base 1e+06
 PARAMETER stop ""[INST]""
 PARAMETER stop ""[/INST]""
 PARAMETER stop ""<>""
 PARAMETER stop ""<>""
/show parameters
 Model defined parameters:
 stop ""[INST]""
 stop ""[/INST]""
 stop ""<>""
 stop ""<>""
 rope_frequency_base 1e+06
 Send a message (/? for help)
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/203","Unable to run example_completion.py on CodeLlama-7b","2024-02-17T21:41:43Z","Open issue","No label","Hi,
 I have a single GPU on my system and I am using CodeLlama-7b to test my environment.
 I am running into the following error when I run the sample.
$ torchrun --nproc_per_node 1 example_completion.py    \
 --ckpt_dir CodeLlama-7b    \
 --tokenizer_path CodeLlama-7b/tokenizer.model    \
 --max_seq_len 128 --max_batch_size 1
> initializing model parallel with size 1
> initializing ddp with size 1
> initializing pipeline with size 1
Traceback (most recent call last):
  File ""/home/aditya/rb16/Code/llama-ft/codellama/example_completion.py"", line 53, in <module>
    fire.Fire(main)
  File ""/home/aditya/anaconda3/lib/python3.11/site-packages/fire/core.py"", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/aditya/anaconda3/lib/python3.11/site-packages/fire/core.py"", line 475, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
                                ^^^^^^^^^^^^^^^^^^^^
  File ""/home/aditya/anaconda3/lib/python3.11/site-packages/fire/core.py"", line 691, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/aditya/rb16/Code/llama-ft/codellama/example_completion.py"", line 20, in main
    generator = Llama.build(
                ^^^^^^^^^^^^
  File ""/home/aditya/rb16/Code/llama-ft/codellama/llama/generation.py"", line 102, in build
    checkpoint = torch.load(ckpt_path, map_location=""cpu"")
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/aditya/anaconda3/lib/python3.11/site-packages/torch/serialization.py"", line 1026, in load
    return _load(opened_zipfile,
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/home/aditya/anaconda3/lib/python3.11/site-packages/torch/serialization.py"", line 1438, in _load
    result = unpickler.load()
             ^^^^^^^^^^^^^^^^
  File ""/home/aditya/anaconda3/lib/python3.11/site-packages/torch/serialization.py"", line 1408, in persistent_load
    typed_storage = load_tensor(dtype, nbytes, key, _maybe_decode_ascii(location))
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/aditya/anaconda3/lib/python3.11/site-packages/torch/serialization.py"", line 1373, in load_tensor
    storage = zip_file.get_storage_from_record(name, numel, torch.UntypedStorage)._typed_storage()._untyped_storage
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 5] Input/output error
[2024-02-17 13:26:43,422] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 3852309) of binary: /home/aditya/anaconda3/bin/python
Traceback (most recent call last):
  File ""/home/aditya/anaconda3/bin/torchrun"", line 33, in <module>
    sys.exit(load_entry_point('torch==2.2.0', 'console_scripts', 'torchrun')())
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/aditya/anaconda3/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py"", line 347, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File ""/home/aditya/anaconda3/lib/python3.11/site-packages/torch/distributed/run.py"", line 812, in main
    run(args)
  File ""/home/aditya/anaconda3/lib/python3.11/site-packages/torch/distributed/run.py"", line 803, in run
    elastic_launch(
  File ""/home/aditya/anaconda3/lib/python3.11/site-packages/torch/distributed/launcher/api.py"", line 135, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/aditya/anaconda3/lib/python3.11/site-packages/torch/distributed/launcher/api.py"", line 268, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
example_completion.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-17_13:26:43
  host      : stormbreaker
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 3852309)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html


$ ls -ltr ./CodeLlama-7b
total 13169098
-rw-rw-r-- 1 aditya aditya      500058 Aug 21 14:32 tokenizer.model
-rw-rw-r-- 1 aditya aditya         163 Aug 21 14:32 params.json
-rw-rw-r-- 1 aditya aditya 13477187307 Aug 21 14:32 consolidated.00.pth
-rw-rw-r-- 1 aditya aditya         150 Aug 21 14:32 checklist.chk

$ echo $CUDA_VISIBLE_DEVICES
0

The conda env
channels:
  - pytorch
  - nvidia
dependencies:
  - numpy
  - pandas
  - pytorch-cuda=12.1
  - pytorch
  - torchvision
  - torchaudio
variables:
  CUDA_PATH: /usr/local/cuda-12.1

 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/202","fine-tuning CodeLlama-34b loss","2024-03-13T23:00:01Z","Closed issue","No label","Hello,
I'm fine-tuning using the CodeLlama-34b as a base model. During training My loss always shows me 0 with all the datasets.
Would someone be able to help me with this?
Thanks.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/201","Does codellama 13B/34B/70B support function calling and Lora fine tuning on multi-run chat with function calling?","2024-02-28T07:15:26Z","Closed issue","No label","Does codellama 13B/34B/70B support function calling and Lora fine tuning on multi-run chat with function calling?
 Are there any instruction pages about this? Thanks a lot.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/200","CodeLlama-34b Fine-Tune Evaluation","2024-02-01T08:05:14Z","Closed issue","No label","Hello,
I have done training and saved the model, and adapter config file on the local disk.
When I load the model from the local disk again to generate the output I get the below error.
Anyone can help me with this issue?
File ""PythonV2.py"", line 11, in <module> model = AutoPeftModelForCausalLM.from_pretrained( ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File ""/python3.12/site-packages/peft/auto.py"", line 127, in from_pretrained return cls._target_peft_class.from_pretrained( ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File ""/python3.12/site-packages/peft/peft_model.py"", line 354, in from_pretrained model.load_adapter(model_id, adapter_name, is_trainable=is_trainable, **kwargs) File ""/python3.12/site-packages/peft/peft_model.py"", line 695, in load_adapter adapters_weights = load_peft_weights(model_id, device=torch_device, **hf_hub_download_kwargs) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File ""/python3.12/site-packages/peft/utils/save_and_load.py"", line 313, in load_peft_weights adapters_weights = safe_load_file(filename, device=device) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File ""/python3.12/site-packages/safetensors/torch.py"", line 308, in load_file with safe_open(filename, framework=""pt"", device=device) as f: ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ safetensors_rust.SafetensorError: Error while deserializing header: InvalidHeaderDeserialization 
Thanks.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/198","Context Length and GPU VRAM Usage in CodeLlama-7B","2024-01-31T13:35:21Z","Open issue","No label","I am currently using CodeLlama-7B on an RTX 3090 24GB GPU, and I have a question regarding the relationship between context length and VRAM usage. According to the model documentation, the context length of CodeLlama-7B is 16,384 tokens.
I loaded the model using Hugging Face with 8-bit precision as follows:
from transformers import AutoModelForCausalLM, AutoTokenizer

agent_name = ""codellama/CodeLlama-7b-Instruct-hf""agent = AutoModelForCausalLM.from_pretrained(agent_name, device_map='cuda', load_in_8bit=True)
agent_tokenizer = AutoTokenizer.from_pretrained(agent_name, add_special_tokens=False, add_eos_token=False, add_bos_token=False)
I then tested the model with different input lengths. For a 3000-token input, the GPU VRAM usage was 16GB. However, when I provided a 6000-token input, the GPU VRAM spiked to 22GB. My primary concern is understanding the relationship between context length and VRAM usage.
Code for Reference:
text = 6000 * ""hello ""encoded_input = agent_tokenizer(text, return_tensors=""pt"").to(""cuda"")
response = agent.generate(**encoded_input, max_new_tokens=4000, do_sample=True, temperature=0.25)
Questions:
Is my understanding correct that the model can handle inputs up to its context length of 16,384 tokens?
Could you provide insights into the observed increase in VRAM usage from a 3000-token input to a 6000-token input?
Considering my intention to use CodeLlama-7B with a context length of up to 8000 tokens, would the 24GB VRAM of my RTX 3090 be sufficient?
Any clarification on these matters would be greatly appreciated. Thank you!
 The text was updated successfully, but these errors were encountered: 
👀4
omihub777, vladislavkn, SrDavos, and elisha0904 reacted with eyes emoji
All reactions
👀4 reactions"
"https://github.com/meta-llama/codellama/issues/197","Code Llama 13b download failed: no properly formatted checksum lines found","2024-02-04T03:36:01Z","Closed issue","No label","I ran the bash script from the command console and received the following message and the download was aborted:
Checking checksums
 md5sum: checklist.chk: no properly formatted checksum lines found
Any ideas?
 The text was updated successfully, but these errors were encountered: 
👍1
ankitrs7 reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/meta-llama/codellama/issues/196","Incomplete Download of 13B and higher parameters model","2024-01-31T05:09:36Z","Open issue","No label","While downloading 13B, the model size is around 12 GB and it is saying that
consolidated.00.pth -> OK
 consolidated.01.pth -> FAILED
I am following all the steps as mentioned but nothing ain't working.
Ideally model size should be around 34 GB but it is coming only 12 GB
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/195","Questions about Downloading Code Llama Model and Using GPT4all?","2024-01-31T01:25:41Z","Open issue","No label","Hello everyone,
To download the Code Llama model, do I first need to fill out a form on the official website to get a download link and license?
https://ai.meta.com/resources/models-and-libraries/llama-downloads/
 However, the form mentions three models available for access: Llama 2 & Llama Chat, Code Llama, and Llama Guard. What are the differences between these three models?
Currently, if I use the GPT4all interface on Windows, can I directly use an additionally downloaded model of 70B scale?
Thank you.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/192","Checksum failures for 70B Instruct (and Python)","2024-01-31T20:27:18Z","Closed issue","No label","I am getting a checksum failures for 70B Instruct and Python:
CodeLlama-70b-Instruct:
 Checking checksums
 Warning: Hash size is wrong, maybe you want to use 'sha1sum' or 'ripemd160sum'
 consolidated.00.pth: FAILED
 consolidated.01.pth: OK
 consolidated.02.pth: OK
 consolidated.03.pth: OK
 consolidated.04.pth: OK
 consolidated.05.pth: OK
 consolidated.06.pth: OK
 consolidated.07.pth: OK
 params.json: FAILED
 tokenizer.model: OK
 md5sum: WARNING: 2 of 10 computed checksums did NOT match
CodeLlama-70b-Python:
 Checking checksums
 consolidated.00.pth: OK
 consolidated.01.pth: OK
 consolidated.02.pth: OK
 consolidated.03.pth: OK
 consolidated.04.pth: OK
 consolidated.05.pth: OK
 consolidated.06.pth: OK
 consolidated.07.pth: OK
 params.json: FAILED
 tokenizer.model: OK
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/191","line 1: payload:allShortcutsEnabled:false: command not found","2024-01-30T22:24:40Z","Closed issue","No label","when I run the download.sh script in bash I get this error:
 line 1: payload:allShortcutsEnabled:false: command not found
How do I fix this?
Thank you.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/190","Codellama","2024-01-30T22:24:57Z","Closed issue","No label","No description provided.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/189","Annoying, Non-User-Friendly Download Script","2024-02-28T07:23:24Z","Closed issue","No label","Summary: The current script for downloading models, while supposedly secure, leaves much to be desired in terms of user experience. It's a classic case of over-engineering, where the effort to enhance security and versatility results in a cumbersome and less intuitive process.
Problems Identified:
Lack of Clarity: Users are often puzzled about the format for selecting models to download, leading to wasted attempts and frustration.
 No Interactive Guidance: The script lacks interactive elements, such as pauses or clear instructions, making it daunting for less experienced users.
 Python Environment Checks Missing: The script does not verify the presence of required Python environments beforehand, leading to last-minute errors and confusion.
 Inconvenient Post-Download Process: Users are left with the task of manually managing downloaded files, adding unnecessary post-download work.
 Proposed Solutions:
Enhance Clarity: Revise the script to include explicit instructions about the format for model selection.
 Improve Interactivity: Integrate user prompts and pause functions for a more guided experience.
 Environment Validation: Implement checks for necessary Python environments before initiating downloads.
 Automate Cleanup: Optionally, add a feature to automatically delete files post-verification, reducing user workload.
 Conclusion:
 The aim is to transform this digital equivalent of a ""needle in a haystack"" search into a more straightforward, less infuriating experience. Because let's face it, the current script is like a puzzle where you're unsure if the pieces are missing or you just can't see the bigger picture.
pyenv git and md5, verify selected downloads, verify where to download. download. verify. cleanup and complete.
 may as well do it all and make it secure.
Joke: echo ""Congratulations, you've successfully navigated the labyrinth of our 'user-friendly' download script. Just think of it as a digital scavenger hunt, but instead of treasure, you get model files. Enjoy deciphering them! 🤷‍♂️""
 The text was updated successfully, but these errors were encountered: 
👍2
richardgroves and saiccoumar reacted with thumbs up emoji
All reactions
👍2 reactions"
"https://github.com/meta-llama/codellama/issues/188","Potentially Incorrect Configs","2024-01-31T20:27:30Z","Closed issue","No label","The new CodeLlama70b Instruct model seem to have incorrect settings for rope_theta and max_position_embeddings.
CodeLlama 34b Values:
""rope_theta"": 1000000
""max_position_embeddings"": 16384

CodeLlama 70b Values:
""rope_theta"": 10000
""max_position_embeddings"": 2048

This seems to be the case for the HF safetensors version as well as the version downloaded via download.sh.
https://huggingface.co/codellama/CodeLlama-70b-Instruct-hf/blob/main/config.json
Are these config values correct or should we be able to use the 34b values?
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/186","Where are the docs for the llama API?","2024-02-15T07:44:13Z","Closed issue","No label","I have difficulty finding docs for the llama API. Like, where is the meaning of the parameters defined (some like temperature and top_p I can guess but others...), how is generator to be used etc.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/185","RuntimeError: ""addmm_impl_cpu_"" not implemented for 'Half'","2024-01-25T01:01:48Z","Open issue","No label","I am using wsl 2 with ubuntu-22.04, this is the gpu information

when i run ""sudo lshw -C display""

I install torch using this command
 pip install torch==1.12.1+cu113 torchvision==0.13.1+cu113 torchaudio==0.12.1 --extra-index-url
https://download.pytorch.org/whl/cu113
when i run command
 ""torchrun --nproc_per_node 1 example_instructions.py 
 --ckpt_dir CodeLlama-7b-Instruct/ 
 --tokenizer_path CodeLlama-7b-Instruct/tokenizer.model 
 --max_seq_len 512 --max_batch_size 4"",
it has error: RuntimeError: ""addmm_impl_cpu_"" not implemented for 'Half'.
These are full log:

:/mnt/c/Users/john.john/codelama/weight/codellama-main$ torchrun --nproc_per_node 1 example_instructions.py \
kpt_dir>     --ckpt_dir CodeLlama-7b-Instruct/ \
>     --tokenizer_path CodeLlama-7b-Instruct/tokenizer.model \
>     --max_seq_len 512 --max_batch_size 4
> initializing model parallel with size 1
> initializing ddp with size 1
> initializing pipeline with size 1
Loaded in 38.10 seconds
Traceback (most recent call last):
  File ""/mnt/c/Users/john.john/codelama/weight/codellama-main/example_instructions.py"", line 68, in <module>
    fire.Fire(main)
  File ""/home/john/.local/lib/python3.10/site-packages/fire/core.py"", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File ""/home/john/.local/lib/python3.10/site-packages/fire/core.py"", line 475, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
  File ""/home/john/.local/lib/python3.10/site-packages/fire/core.py"", line 691, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File ""/mnt/c/Users/john.john/codelama/weight/codellama-main/example_instructions.py"", line 51, in main
    results = generator.chat_completion(
  File ""/mnt/c/Users/john.john/codelama/weight/codellama-main/llama/generation.py"", line 351, in chat_completion
    generation_tokens, generation_logprobs = self.generate(
  File ""/home/john/.local/lib/python3.10/site-packages/torch/autograd/grad_mode.py"", line 27, in decorate_context
    return func(*args, **kwargs)
  File ""/mnt/c/Users/john.john/codelama/weight/codellama-main/llama/generation.py"", line 164, in generate
    logits = self.model.forward(tokens[:, prev_pos:cur_pos], prev_pos)
  File ""/home/john/.local/lib/python3.10/site-packages/torch/autograd/grad_mode.py"", line 27, in decorate_context
    return func(*args, **kwargs)
  File ""/mnt/c/Users/john.john/codelama/weight/codellama-main/llama/model.py"", line 300, in forward
    h = layer(h, start_pos, freqs_cis, (mask.to(device) if mask is not None else mask))
  File ""/home/john/.local/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File ""/mnt/c/Users/john.john/codelama/weight/codellama-main/llama/model.py"", line 252, in forward
    h = x + self.attention.forward(
  File ""/mnt/c/Users/john.john/codelama/weight/codellama-main/llama/model.py"", line 165, in forward
    xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
  File ""/home/john/.local/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File ""/home/john/.local/lib/python3.10/site-packages/fairscale/nn/model_parallel/layers.py"", line 290, in forward
    output_parallel = F.linear(input_parallel, self.weight, self.bias)
RuntimeError: ""addmm_impl_cpu_"" not implemented for 'Half'
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 433) of binary: /usr/bin/python3
Traceback (most recent call last):
  File ""/home/john/.local/bin/torchrun"", line 8, in <module>
    sys.exit(main())
  File ""/home/john/.local/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py"", line 345, in wrapper
    return f(*args, **kwargs)
  File ""/home/john/.local/lib/python3.10/site-packages/torch/distributed/run.py"", line 761, in main
    run(args)
  File ""/home/john/.local/lib/python3.10/site-packages/torch/distributed/run.py"", line 752, in run
    elastic_launch(
  File ""/home/john/.local/lib/python3.10/site-packages/torch/distributed/launcher/api.py"", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File ""/home/john/.local/lib/python3.10/site-packages/torch/distributed/launcher/api.py"", line 245, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError:

example_instructions.py FAILED

Failures:
  <NO_OTHER_FAILURES>

Root Cause (first observed failure):
[0]:
  time      : 2024-01-24_16:51:25
  host      : company
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 433)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html


 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/183","Meta AI response ""bug""","2024-01-30T21:30:18Z","Closed issue","No label","Any time I ask certain questions like ""what is your favorite color"" the AI will respond with ""ah! Ah! Ah! Ah!""
 But like 50 ""ah!""s

 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/182","Knowledge Cutoff Date?","2024-01-17T21:28:39Z","Open issue","No label","We are aware that the knowledge cutoff date for Llama-2 series models are Sep 2022 according to their model card.
What's the knowledge cutoff date for CodeLlama? Neither the model card and the paper mention anything about this.
 The text was updated successfully, but these errors were encountered: 
👀1
justine-gehring reacted with eyes emoji
All reactions
👀1 reaction"
"https://github.com/meta-llama/codellama/issues/179","Aiiiii","2024-01-16T09:23:39Z","Closed issue","No label","Fff
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/178","Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Meta Open Source project. Thanks!","2024-01-14T03:06:08Z","Closed issue","No label","Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Meta Open Source project. Thanks!

Originally posted by @facebook-github-bot in #18 (comment)
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/177","I am curious about the form of the infilling dataset for training.","2024-01-18T01:22:11Z","Closed issue","No label","Hello, I am studying machine learning and I have a question.
I'm not good at English, so I'm sorry for asking questions through a translator.
I am training codellama using the infilling dataset I created and SFTTrainer, and I am curious whether the infilling dataset is in the correct shape.
The current json data set format is
{""context"" : ""<PRE>{prefix}<SUF>{suffix}<MID>{answer}<EOT>""}
It is composed of a shape.
Also, what I'm curious about is whether the loss output when trained with the dataset is correct.
When using a general Trainer, I understand that loss is calculated by comparing the correct answer inferred by the model using the input labels.
Then, the dataset I am currently training with SFTrainer does not have labels.
 There is only one ""context"", so I'm curious about what part of the loss is calculated from.
I would really appreciate it if you could leave any reply.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/176","Error D/L Models","2024-01-10T02:20:57Z","Open issue","No label","Hi,
Did the instructions from the readme.md. Got the url in the email, ran download.sh, entered ""CodeLlama-7b-Python"" in the models to download; then I get this error: Why? Thank you.
`py_stuff@JCs-MacBook-Pro llama % bash download.sh
 Enter the URL from email: https://download2.llamameta.net/*?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaXF1ZV9oYXNoIjoiMGV1N2l4eHJ5MzB2b28xM3RtMThjZ2o4IiwiUmVzb3VyY2UiOiJodHRwczpcL1wvZG93bmxvYWQyLmxsYW1hbWV0YS5uZXRcLyoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE3MDQ5MzkxNzJ9fX1dfQ__&Signature=qGMHySWdojCMVG%7EeHfWt2-6f2hJ92hACpSWCB9xfk%7EcIq90gZjqLxiP3ANIOxjhcS1wfOCVcnApwhefHLwAoI%7ETuKMlIcbTAKfOIDM3ddXZLj0XqxCXpH8jnvb-7At71t3TQHSnCbklTdCdbdnIA9qP%7ED42RtkqBj6l8DKoJCMhjbIuQxof2esO9c1Ff1cclKQ6c3xWDrgt4qa7TRgEuOrqqtHZRKmOO4mGqvZnokRe%7E-QfuS5eBhWfuZqw9mMSJY55gPrW94G2U%7EyyWG0V6TX%7ED%7EgWX1KdMQD-WQyemcBE8OAK6u%7EQg8FDSLnnoWTZAPZfxDG8KXDh8gJ1lEinJ2A__&Key-Pair-Id=K15QRJLYKIFSLZ&Download-Request-ID=1325688178103675
Enter the list of models to download without spaces (7B,13B,70B,7B-chat,13B-chat,70B-chat), or press Enter for all: CodeLlama-7b-Python
 Downloading LICENSE and Acceptable Usage Policy
 --2024-01-10 10:14:43-- https://download2.llamameta.net/LICENSE?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaXF1ZV9oYXNoIjoiMGV1N2l4eHJ5MzB2b28xM3RtMThjZ2o4IiwiUmVzb3VyY2UiOiJodHRwczpcL1wvZG93bmxvYWQyLmxsYW1hbWV0YS5uZXRcLyoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE3MDQ5MzkxNzJ9fX1dfQ__&Signature=qGMHySWdojCMVG%7EeHfWt2-6f2hJ92hACpSWCB9xfk%7EcIq90gZjqLxiP3ANIOxjhcS1wfOCVcnApwhefHLwAoI%7ETuKMlIcbTAKfOIDM3ddXZLj0XqxCXpH8jnvb-7At71t3TQHSnCbklTdCdbdnIA9qP%7ED42RtkqBj6l8DKoJCMhjbIuQxof2esO9c1Ff1cclKQ6c3xWDrgt4qa7TRgEuOrqqtHZRKmOO4mGqvZnokRe%7E-QfuS5eBhWfuZqw9mMSJY55gPrW94G2U%7EyyWG0V6TX%7ED%7EgWX1KdMQD-WQyemcBE8OAK6u%7EQg8FDSLnnoWTZAPZfxDG8KXDh8gJ1lEinJ2A__&Key-Pair-Id=K15QRJLYKIFSLZ&Download-Request-ID=1325688178103675
 Resolving download2.llamameta.net (download2.llamameta.net)... 18.172.21.14, 18.172.21.39, 18.172.21.60, ...
 Connecting to download2.llamameta.net (download2.llamameta.net)|18.172.21.14|:443... connected.
 HTTP request sent, awaiting response... 416 Requested Range Not Satisfiable
The file is already fully retrieved; nothing to do.

--2024-01-10 10:14:45-- https://download2.llamameta.net/USE_POLICY.md?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaXF1ZV9oYXNoIjoiMGV1N2l4eHJ5MzB2b28xM3RtMThjZ2o4IiwiUmVzb3VyY2UiOiJodHRwczpcL1wvZG93bmxvYWQyLmxsYW1hbWV0YS5uZXRcLyoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE3MDQ5MzkxNzJ9fX1dfQ__&Signature=qGMHySWdojCMVG%7EeHfWt2-6f2hJ92hACpSWCB9xfk%7EcIq90gZjqLxiP3ANIOxjhcS1wfOCVcnApwhefHLwAoI%7ETuKMlIcbTAKfOIDM3ddXZLj0XqxCXpH8jnvb-7At71t3TQHSnCbklTdCdbdnIA9qP%7ED42RtkqBj6l8DKoJCMhjbIuQxof2esO9c1Ff1cclKQ6c3xWDrgt4qa7TRgEuOrqqtHZRKmOO4mGqvZnokRe%7E-QfuS5eBhWfuZqw9mMSJY55gPrW94G2U%7EyyWG0V6TX%7ED%7EgWX1KdMQD-WQyemcBE8OAK6u%7EQg8FDSLnnoWTZAPZfxDG8KXDh8gJ1lEinJ2A__&Key-Pair-Id=K15QRJLYKIFSLZ&Download-Request-ID=1325688178103675
 Resolving download2.llamameta.net (download2.llamameta.net)... 18.172.21.14, 18.172.21.39, 18.172.21.60, ...
 Connecting to download2.llamameta.net (download2.llamameta.net)|18.172.21.14|:443... connected.
 HTTP request sent, awaiting response... 416 Requested Range Not Satisfiable
The file is already fully retrieved; nothing to do.

Downloading tokenizer
 --2024-01-10 10:14:45-- https://download2.llamameta.net/tokenizer.model?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaXF1ZV9oYXNoIjoiMGV1N2l4eHJ5MzB2b28xM3RtMThjZ2o4IiwiUmVzb3VyY2UiOiJodHRwczpcL1wvZG93bmxvYWQyLmxsYW1hbWV0YS5uZXRcLyoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE3MDQ5MzkxNzJ9fX1dfQ__&Signature=qGMHySWdojCMVG%7EeHfWt2-6f2hJ92hACpSWCB9xfk%7EcIq90gZjqLxiP3ANIOxjhcS1wfOCVcnApwhefHLwAoI%7ETuKMlIcbTAKfOIDM3ddXZLj0XqxCXpH8jnvb-7At71t3TQHSnCbklTdCdbdnIA9qP%7ED42RtkqBj6l8DKoJCMhjbIuQxof2esO9c1Ff1cclKQ6c3xWDrgt4qa7TRgEuOrqqtHZRKmOO4mGqvZnokRe%7E-QfuS5eBhWfuZqw9mMSJY55gPrW94G2U%7EyyWG0V6TX%7ED%7EgWX1KdMQD-WQyemcBE8OAK6u%7EQg8FDSLnnoWTZAPZfxDG8KXDh8gJ1lEinJ2A__&Key-Pair-Id=K15QRJLYKIFSLZ&Download-Request-ID=1325688178103675
 Resolving download2.llamameta.net (download2.llamameta.net)... 18.172.21.14, 18.172.21.39, 18.172.21.60, ...
 Connecting to download2.llamameta.net (download2.llamameta.net)|18.172.21.14|:443... connected.
 HTTP request sent, awaiting response... 403 Forbidden
 2024-01-10 10:14:46 ERROR 403: Forbidden.
py_stuff@JCs-MacBook-Pro llama %
 `
 The text was updated successfully, but these errors were encountered: 
👀1
Javahboy reacted with eyes emoji
All reactions
👀1 reaction"
"https://github.com/meta-llama/codellama/issues/174","I'd like to know if this is the right type of dataset for the model using the infilling function.","2024-01-08T01:05:59Z","Closed issue","No label","Thanks for your excellent work.
 I want to make a model that uses the infilling function, but the type of dataset I use right now is
 [PRE]{prefix} [SUF]{suffix}[MID]{example['answer']} [EOT]
 It is in form and based on [FILL], the front prefix and the back of the prefix were divided by suffix. When [FILL] << the data to be entered here is {example['answer']}, it is taught by attaching it after [MID>] is it the correct method to learn with this type of dataset?
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/173","How to enable the sending of 100K tokens for codellama","2023-12-14T22:03:19Z","Open issue","No label","Hi, I'm testing codellama, and I would like a guide on how to enable it to accept the sending of 100K input tokens. From what I understand, this is done by adjusting the max_seq_len and max_batch_size parameters, but I couldn't find more information on this. Please, if someone could guide me.
As an extra question, do you know what the default size it would accept is?
Thank you, and I will appreciate any help.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/175","Questions about learning more than 4096 sequences in codellama","2024-01-01T11:50:54Z","Closed issue","No label","hello
I had a question and wanted to get some ideas, so I registered an issue.
Now, I'm sorry that I don't have much knowledge as it's been a while since I studied LLM.
I don't speak English, so I wrote it through a translator, so the words may sound strange.
Currently, I try to fine-tuning infilling capability from one node to four GPUs of the codellama 7b or 13b models using deepspeed, but if the sequence length of the dataset to be trained exceeds 4096 supported by codellama, whether it is not suitable for learning infilling, or is there any other way or good ideas to learn
 I want to know.
thank you
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/172","Single-line Infilling Results reproduction","2024-02-06T20:07:50Z","Closed issue","No label","Hello,
I am trying to reproduce the infilling results on HumanEval (Table 14 CodeLLAMA 7B SPM, pass@1=83%). I am using the single-line benchmark from https://github.com/openai/human-eval-infilling. I use the below code to generate the samples.
from human_eval_infilling.data import write_jsonl, read_problems
from tqdm import tqdm
from transformers import (
    AutoModelForCausalLM,
    CodeLlamaTokenizer,
)
import torch


model_name = ""codellama/CodeLlama-7b-hf""
load_in_8bit = ""False""
device_map = ""auto""
max_gen_len = 128
problems = read_problems(benchmark_name=""single-line"")
model = AutoModelForCausalLM.from_pretrained(
        model_name,
        load_in_8bit=load_in_8bit,
        device_map=device_map,
        trust_remote_code=True,
        torch_dtype=torch.bfloat16
    )
tokenizer = CodeLlamaTokenizer.from_pretrained(model_name, trust_remote_code=True, suffix_first=True)

def generate_one_completion(pre,suf):
    prompt = pre+""<FILL_ME>""+suf
    input_ids = tokenizer(prompt, suffix_first=True, return_tensors=""pt"")[""input_ids""].to('cuda')
    generation_tokens = model.generate(
            input_ids,
            max_new_tokens=max_gen_len,
            temperature=0.2
        )
    outputs = tokenizer.batch_decode(generation_tokens[:, input_ids.shape[1]:], skip_special_tokens = True)[0]
    return outputs

num_samples_per_task = 1
samples = [
    dict(task_id=task_id, completion=generate_one_completion(problems[task_id][""prompt""], problems[task_id][""suffix""]))
    for task_id in tqdm(problems)
    for _ in range(num_samples_per_task)
]
print(len(samples))
write_jsonl(""samples_base_pretrained_codellama.jsonl"", samples)

Next I run the following for computing pass@1. I obtain pass@1= 0.73281 which is much smaller than the reported results.
evaluate_infilling_functional_correctness samples_base_pretrained_codellama.jsonl --benchmark_name=single-line
Can you please help with the following:
Are the benchmarks and prompts for evaluation correct?
Is there any post-processing required on the generated codes? (e.g. code sanitation)
Are there any hyperparameter recommended? (e.g. temperature, decoding strategy?)
 The text was updated successfully, but these errors were encountered: 
👍1
mu-arkhipov reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/meta-llama/codellama/issues/171","ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: -9)","2023-12-10T19:16:10Z","Open issue","No label","Problem Description
After completing setup for CodeLlama, from the README.md, when I attempt to run any of the examples, with the specified commands:
torchrun --nproc_per_node 1 example_completion.py --ckpt_dir CodeLlama-7b/ --tokenizer_path CodeLlama-7b/tokenizer.model --max_seq_len 128 --max_batch_size 4

OR
torchrun --nproc_per_node 1 example_infilling.py --ckpt_dir CodeLlama-7b/ --tokenizer_path CodeLlama-7b/tokenizer.model --max_seq_len 192 --max_batch_size 4

OR
torchrun --nproc_per_node 1 example_instructions.py --ckpt_dir CodeLlama-7b-Instruct/ --tokenizer_path CodeLlama-7b-Instruct/tokenizer.model --max_seq_len 512 --max_batch_size 4

I get the output with the error below:
Output
> initializing model parallel with size 1
> initializing ddp with size 1
> initializing pipeline with size 1
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: -9) local_rank: 0 (pid: 31383) of binary: /home/abc/miniconda3/envs/llama_env/bin/python
Traceback (most recent call last):
  File ""/home/abc/miniconda3/envs/llama_env/bin/torchrun"", line 8, in <module>
    sys.exit(main())
  File ""/home/abc/miniconda3/envs/llama_env/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py"", line 346, in wrapper
    return f(*args, **kwargs)
  File ""/home/abc/miniconda3/envs/llama_env/lib/python3.10/site-packages/torch/distributed/run.py"", line 794, in main
    run(args)
  File ""/home/abc/miniconda3/envs/llama_env/lib/python3.10/site-packages/torch/distributed/run.py"", line 785, in run
    elastic_launch(
  File ""/home/abc/miniconda3/envs/llama_env/lib/python3.10/site-packages/torch/distributed/launcher/api.py"", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File ""/home/abc/miniconda3/envs/llama_env/lib/python3.10/site-packages/torch/distributed/launcher/api.py"", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError:
======================================================
example_completion.py FAILED
------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-12-10_13:12:17
  host      : ABC-PC.
  rank      : 0 (local_rank: 0)
  exitcode  : -9 (pid: 31383)
  error_file: <N/A>
  traceback : Signal 9 (SIGKILL) received by PID 31383
======================================================

Runtime Environment
Model: [CodeLlama-7b, CodeLlama-7b-Instruct, CodeLlama-7b-Python]
Using via huggingface?: [no]
OS: [Linux/Ubuntu (via WSL2), Windows]
GPU VRAM: 4GB
Number of GPUs: 1
GPU Make: [Nvidia]
GPU Version: NVIDIA GeForce GTX 1650
Additional context
 I am trying to run the models on Ubuntu through WSL 2, I tried setting the batch size to 6 (--max_batch_size 6) as was mentioned in llama #706 but this did not help.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/170","What is the max length could the codellama-2-7B generate?","2024-02-28T07:15:43Z","Closed issue","No label","I was doing an inference work using codellama-2-7B.
Here is my code:
inputs_ids = tokenizer(prompt, return_tensors=""pt"").input_ids.to(self.device)
generate_ids=model.generate(inputs_ids,max_new_tokens=1024,num_return_sequences=1,pad_token_id=tokenizer.eos_token_id)
output = tokenizer.decode(generate_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)

I want to know the maximum that max_new_token can be set to?
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/169","How to get embeddings of a input sentence at the output layer?","2023-12-05T02:19:40Z","Open issue","No label","No description provided.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/166","Cannot run HF example codes on all three codeLlama-Python-hf models","2023-11-20T10:57:15Z","Open issue","No label","I understand this might be a huggingface-related problem but I cannot find the answer anywhere so I come to ask for help.
On huggingface there is a example code for codellama model:
from transformers import LlamaForCausalLM, CodeLlamaTokenizer
tokenizer = CodeLlamaTokenizer.from_pretrained(""codellama/CodeLlama-7b-hf"")
 model = LlamaForCausalLM.from_pretrained(""codellama/CodeLlama-7b-hf"")
 PROMPT = '''def remove_non_ascii(s: str) -> str:
 """""" <FILL_ME>
 return result
 '''
 input_ids = tokenizer(PROMPT, return_tensors=""pt"")[""input_ids""]
 generated_ids = model.generate(input_ids, max_new_tokens=128)
filling = tokenizer.batch_decode(generated_ids[:, input_ids.shape[1]:], skip_special_tokens = True)[0]
 print(PROMPT.replace(""<FILL_ME>"", filling))
And the output is like:
def remove_non_ascii(s: str) -> str:
    """""" Remove non-ASCII characters from a string.

    Args:
        s: The string to remove non-ASCII characters from.

    Returns:
        The string with non-ASCII characters removed.
    """"""
    result = """"
    for c in s:
        if ord(c) < 128:
            result += c
    return result

However, this works fine with all the original codellama model and codellama instruct models. But all three codellama-Python models will show tons of ""Assertion srcIndex < srcSelectDimSize failed"" errors and fail to complete the running.
 The second strange thing is that, if I delete the ' <FILL_ME> ' part in the PROMPT when I am using codellama-Python model, then the error won't show , however there will still be no output.
So my questions are:
Why will these ""Assertion srcIndex < srcSelectDimSize failed"" errors happen on codellama-Python, as well as the no-output problems after I deleting the <FILL_ME> in the PROMPT? From my point of view, codeLlama-python is just modified on more Python tasks, and it should not be fundamentally different with original codellama and codellama-instruct.
Why the readme of Huggingface page says Codellama-Python cannot do infilling? Why modification on Python tasks will make the model cannot do infilling? Is the problem in my question 1 related to this lack of infilling of codellama-Python?
Thank you so much for your precious time.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/165","Running example_completion.py with CodeLLama-34b locally returns an assertion error.","2023-12-01T16:52:46Z","Closed issue","No label","torchrun --nproc_per_node 1 example_completion.py     --ckpt_dir CodeLlama-34b/     --tokenizer_path CodeLlama-34b/tokenizer.model     --max_seq_len 128 --max_batch_size 4 --nodes 1
> initializing model parallel with size 1
> initializing ddp with size 1
> initializing pipeline with size 1
Traceback (most recent call last):
  File ""example_completion.py"", line 55, in <module>
    fire.Fire(main)
  File ""/home/steven/.local/lib/python3.8/site-packages/fire/core.py"", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File ""/home/steven/.local/lib/python3.8/site-packages/fire/core.py"", line 475, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
  File ""/home/steven/.local/lib/python3.8/site-packages/fire/core.py"", line 691, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File ""example_completion.py"", line 20, in main
    generator = Llama.build(
  File ""/home/steven/github/stephenwithav/codellama/llama/generation.py"", line 97, in build
    assert model_parallel_size == len(
AssertionError: Loading a checkpoint for MP=4 but world size is 1
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 2308980) of binary: /usr/bin/python3
Traceback (most recent call last):
  File ""/home/steven/.local/bin/torchrun"", line 8, in <module>
    sys.exit(main())
  File ""/home/steven/.local/lib/python3.8/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py"", line 346, in wrapper
    return f(*args, **kwargs)
  File ""/home/steven/.local/lib/python3.8/site-packages/torch/distributed/run.py"", line 762, in main
    run(args)
  File ""/home/steven/.local/lib/python3.8/site-packages/torch/distributed/run.py"", line 753, in run
    elastic_launch(
  File ""/home/steven/.local/lib/python3.8/site-packages/torch/distributed/launcher/api.py"", line 132, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File ""/home/steven/.local/lib/python3.8/site-packages/torch/distributed/launcher/api.py"", line 246, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
example_completion.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-11-15_15:48:25
  host      : work
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 2308980)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================

 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/164","Is download.sh providing the correct tokenizer.model files?","2024-02-28T07:35:40Z","Closed issue","No label","When I try to run a model ..
torchrun example_js.py \
    --ckpt_dir CodeLlama-13b-Instruct \
    --tokenizer_path CodeLlama-13b-Instruct/tokenizer.model \
    --max_seq_len 1024 --max_batch_size 4 --nproc_per_node 2

example_js is the same as the provide example_completion, but with different prompts
... I get this error:
RuntimeError: Error(s) in loading state_dict for Transformer:
	size mismatch for tok_embeddings.weight: copying a param with shape torch.Size([32016, 2560]) from checkpoint, the shape in current model is torch.Size([32000, 5120]).
	size mismatch for layers.0.attention.wq.weight: copying a param with shape torch.Size([2560, 5120]) from checkpoint, the shape in current model is torch.Size([5120, 5120]).
	size mismatch for layers.0.attention.wk.weight: copying a param with shape torch.Size([2560, 5120]) from checkpoint, the shape in current model is torch.Size([5120, 5120]).
	size mismatch for layers.0.attention.wv.weight: copying a param with shape torch.Size([2560, 5120]) from checkpoint, the shape in current model is torch.Size([5120, 5120]).
	size mismatch for layers.0.attention.wo.weight: copying a param with shape torch.Size([5120, 2560]) from checkpoint, the shape in current model is torch.Size([5120, 5120]).
...

This code works perfectly fine if I use the 7b model and tokenizer
Investigating a bit further, I noticed this:
md5sum CodeLlama-7b-Instruct/tokenizer.model
9e597e72392fd4005529a33f2bf708ba  CodeLlama-7b-Instruct/tokenizer.model
md5sum CodeLlama-13b-Instruct/tokenizer.model
9e597e72392fd4005529a33f2bf708ba  CodeLlama-13b-Instruct/tokenizer.model
md5sum CodeLlama-34b-Instruct/tokenizer.model
eeec4125e9c7560836b4873b6f8e3025  CodeLlama-34b-Instruct/tokenizer.model

the tokenizer for 7b and 13b are identical? That seems unlikely.
I also attempted these variants of torchrun just to see what happens
torchrun --ckpt_dir CodeLlama-13b-Instruct --tokenizer_path CodeLlama-34b-Instruct/tokenizer.model 
torchrun --ckpt_dir CodeLlama-34b-Instruct --tokenizer_path CodeLlama-34b-Instruct/tokenizer.model --nproc_per_node 4

These produced the same errors, but with different numbers
On another node, the --nproc_per_node value is provided to the commands just in case (as the docs say it's needed), but in practice I find it has no effect. I was forced to modify the code that builds the model like so:
generator = Llama.build(
        ckpt_dir=ckpt_dir,
        tokenizer_path=tokenizer_path,
        max_seq_len=max_seq_len,
        max_batch_size=max_batch_size,

       # Added this, value is 2 for 13b and 4 for 34b
        model_parallel_size=2,
    )

I'm on an M1 Macbook Pro with 64 GB of ram
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/163","Download URL from the email gives me Llama code, not Llama 2","2024-02-28T07:24:01Z","Closed issue","No label","I get an email with a download URL, but when I run a download URL, I get these options:
Enter the list of models to download without spaces (7b,13b,34b,7b-Python,13b-Python,34b-Python,7b-Instruct,13b-Instruct,34b-Instruct), or press Enter for all:
I do not see these models as per email:
 Llama-2-7b
 Llama-2-7b-chat
 Llama-2-13b
 Llama-2-13b-chat
 Llama-2-70b
 Llama-2-70b-chat
Am I doing anything wrong?
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/162","issue with example_completion.py","2023-11-10T07:00:33Z","Open issue","No label","when i tried codellama-7b and codellama-34b to test code completion, all results were garbled code.
facilities:
 OS: Red hat 4.8.5-36
 GCC:4.8.5
 32G V100
 cuda:11.7
 torch: 2.0.0
 fairscale 0.4.13
 sentencepiece: 0.1.99
 fire:0.5.0
looking forward to your reply.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/161","Issue with loading codellama models outside codellama directory","2023-11-06T22:28:29Z","Open issue","No label","When I try to run python script, I get this error :
ypeError                                 Traceback (most recent call last)
Cell In[6], line 10
      6 max_batch_size = 4
      7 max_gen_len: Optional[int] = None
---> 10 generator = Llama.build(
     11         ckpt_dir=ckpt_dir,
     12         tokenizer_path=tokenizer_path,
     13         max_seq_len=max_seq_len,
     14         max_batch_size=max_batch_size,
     15     )

File ~/llama/llama/generation.py:111, in Llama.build(ckpt_dir, tokenizer_path, max_seq_len, max_batch_size, model_parallel_size, seed)
    108 with open(Path(ckpt_dir) / ""params.json"", ""r"") as f:
    109     params = json.loads(f.read())
--> 111 model_args: ModelArgs = ModelArgs(
    112     max_seq_len=max_seq_len,
    113     max_batch_size=max_batch_size,
    114     **params,
    115 )
    116 tokenizer = Tokenizer(model_path=tokenizer_path)
    117 model_args.vocab_size = tokenizer.n_words

TypeError: __init__() got an unexpected keyword argument 'rope_theta'

 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/160","How to use with VS Code?","2023-11-06T03:47:33Z","Open issue","No label","I thinks we need:
VS Code plugins
Model loading methods
API Server to communicate between plugins and model backend
Is here we have mature solution?
 The text was updated successfully, but these errors were encountered: 
👍1
AlessandroZavoli reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/meta-llama/codellama/issues/159","Whether to support generating sql language","2023-11-06T03:41:40Z","Open issue","No label","Whether to support generating sql language
 The text was updated successfully, but these errors were encountered: 
👍1
leatherking reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/meta-llama/codellama/issues/158","How to add tokens to tokenizer?","2023-11-05T06:04:14Z","Open issue","No label","I want to add some tokens like [BOST] to the tokenizer so that it does not split these.
How can I achieve this? Any suggestions are welcome.
Huggingface provides functions like add_tokens but I want to make other changes in the source, so I don't want to use HF.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/157","What kind of prompt was used in HumanEval evaluation?","2023-12-01T10:12:29Z","Closed issue","No label","Thanks for your excellent work. I find there is no prompt for HumanEval completion task was released, could you give me some introduction?
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/156","Gets stuck running example_completion.py","2023-11-02T21:33:07Z","Closed issue","No label","I ran example_completion.py
python -m torch.distributed.run --nproc_per_node 1 example_completion.py \
    --ckpt_dir CodeLlama-7b/ \
    --tokenizer_path CodeLlama-7b/tokenizer.model \
    --max_seq_len 128 --max_batch_size 4

and immediately I see
> initializing model parallel with size 1
> initializing ddp with size 1
> initializing pipeline with size 1

And nothing happens for a very long time. I thought the code completion should be printed.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/155","How to let codellama or codellama-python stop?","2023-10-27T08:33:56Z","Open issue","No label","When I use codellama or codellama-python to finish the continuation of a prompt, a lot of '\n' are outputed in the end until it reaches the max_gen_len. Is there any way to let it stop early except limiting the max_gen_len which is not convenient?
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/154","How to design a prompt using Codellama for code completion?","2023-10-26T07:15:13Z","Open issue","No label","Apart from contextual information, how can I make the model recognize the import class information and similar code fragments as prompts when I want them to be passed into the model? Do you have a document tutorial for designing prompts?
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/153","Why does Code-Llama-34B not support infilling mode, i.e. FIM","2023-10-25T07:26:30Z","Open issue","No label","In Meta AI blog

Why the 34B base and all Python versions have not been trained with FIM?
 The text was updated successfully, but these errors were encountered: 
👍2
zolibra and LuGHuaaa reacted with thumbs up emoji
All reactions
👍2 reactions"
"https://github.com/meta-llama/codellama/issues/152","Run code llama from Hugging Face locally with GPU","2023-10-24T06:56:57Z","Open issue","No label","I have trying to host the Code Llama from Hugging Face locally and trying to run it. It runs soley on CPU and it is not utilizing GPU available in the machine despite having Nvidia Drivers and Cuda toolkit.
from transformers import AutoTokenizer
 import transformers
model = ""codellama/CodeLlama-7b-hf""
tokenizer = AutoTokenizer.from_pretrained(model)
 pipeline = transformers.pipeline(
 ""text-generation"",
 model=model,
 torch_dtype=None,
 device_map = ""cuda:0""
 )
prompt = ""Write python code to reverse a string""
sequences = pipeline(
 prompt,
 do_sample=True,
 top_k=10,
 temperature=0.1,
 top_p=0.95,
 num_return_sequences=1,
 eos_token_id=tokenizer.eos_token_id,
 max_length=200,
 )
 for seq in sequences:
 print(f""Result: {seq['generated_text']}"")
The code above runs the LLM locally but in case we use cuda for the device, it gives the following error
File ""C:\Users\winuser3\Desktop\GENAI-App\venv\lib\site-packages\transformers\modeling_utils.py"", line 3333, in from_pretrained
 ) = cls._load_pretrained_model(
 File ""C:\Users\winuser3\Desktop\GENAI-App\venv\lib\site-packages\transformers\modeling_utils.py"", line 3723, in _load_pretrained_model
 new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
 File ""C:\Users\winuser3\Desktop\GENAI-App\venv\lib\site-packages\transformers\modeling_utils.py"", line 744, in load_state_dict_into_meta_model
 set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)
 File ""C:\Users\winuser3\Desktop\GENAI-App\venv\lib\site-packages\accelerate\utils\modeling.py"", line 317, in set_module_tensor_to_device
 new_value = value.to(device)
 File ""C:\Users\winuser3\Desktop\GENAI-App\venv\lib\site-packages\torch\cuda_init.py"", line 289, in _lazy_init
 raise AssertionError(""Torch not compiled with CUDA enabled"")
 AssertionError: Torch not compiled with CUDA enabled
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/151","Will the code for finetuning CodeLlama be open-sourced?","2023-10-23T08:06:26Z","Open issue","No label","Has any finetune code ? especially for how data sets are prepared.
 Looking forward to your reply very much , thank you !
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/150","instruct model's performance become poor when switching to different format","2023-10-19T13:25:49Z","Open issue","No label","I use 7b and 13b-instruct model to do program analysis task. When I use original format 7b-Instruct and 13b-Instruct model, following example_chat_completion.py, everything is all right.
But when I use huggingface version model, use the generate api. I found the model's response is much worse than origin format one. Also, the same thing happen when deploy online with tool like lmdeploy.
It seems only locally run original format model give me a satisfiable answer. How to fix this?
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/148","Please help: failed to create process","2023-10-17T16:02:03Z","Open issue","No label","Hi,
Apologies if the solution is obvious but I'm new to this. When running the example infilling script:
torchrun --nproc_per_node 1 example_infilling.py --ckpt_dir CodeLlama-7b/ --tokenizer_path CodeLlama-7b/tokenizer.model --max_seq_len 192 --max_batch_size 4
I get the following error with no additional details:
failed to create process
In fact anything with torchrun returns the same error.
I tried:
Different versions of CUDA and CPU-only Pytorch
Checked that tokenizer_path is correct and nproc_per_node is set to the right MP value
A comment on another post suggested using python -m torch.distributed.run instead of torchrun. I get a different error when I do this (happy to give more info)
Any help would be greatly appreciated!
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/147","Please update the link in your readme","2023-10-17T10:10:54Z","Open issue","No label","https://download.llamameta.net/
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/146","Fine-tuning on other languages","2023-12-29T02:16:03Z","Closed issue","No label","Hi,
Can I fine-tune the model with other languages such as Korean, Chinese, Japanese, etc.?
 In general, other than English, what languages can the model recognize?
Thank you for sharing your work!
 The text was updated successfully, but these errors were encountered: 
👍3
sdcb, stephenwithav, and xdldf reacted with thumbs up emoji
All reactions
👍3 reactions"
"https://github.com/meta-llama/codellama/issues/145","Codellama-7B can't reproduce the paper's result 33.5%,why?","2023-12-01T10:12:42Z","Closed issue","No label","I evaluated codellama 7b using greedy decoding, and the Humaneval result for pass@1 was only 28.66%. You are asking how to reproduce the paper's result of 33.5%. pls show the evaluation parameters and prompt format. Here are some suggestions:

 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/144","Unable to Download Code-Llama 7B via the download.sh Script","2023-10-11T20:17:26Z","Open issue","No label","Hello! I signed up to download the Code-Llama model from Meta. I received the email with the Unique Custom URL.
However, when I attempt to download the model, the script throws an error. Can you please advise?
Environment: Ubuntu 22.04
Steps to Reproduce:
Cloned the repo from main branch: git clone git@github.com:facebookresearch/codellama.git
Executed the script: ./download.sh
Followed the prompts, entered the URL from the email and entered only 7B as the model to download
Expected Outcome:
Should have downloaded the model
Actual Outcome (query params omitted for possible security reasons):
Get the following error:
Downloading tokenizer
--2023-10-11 13:10:04--  https://download2.llamameta.net/tokenizer.model?XXX
Resolving download2.llamameta.net (download2.llamameta.net)... 65.8.164.24, 65.8.164.126, 65.8.164.75, ...
Connecting to download2.llamameta.net (download2.llamameta.net)|65.8.164.24|:443... connected.
HTTP request sent, awaiting response... 403 Forbidden
2023-10-11 13:10:05 ERROR 403: Forbidden.

Full log (query params omitted for possible security reasons):
~/workspaces/meta/llama (main)$ ./download.sh
Enter the URL from email: https://download2.llamameta.net/*?queryparams=XXX

Enter the list of models to download without spaces (7B,13B,70B,7B-chat,13B-chat,70B-chat), or press Enter for all: 7B
Downloading LICENSE and Acceptable Usage Policy
--2023-10-11 13:10:04--  https://download2.llamameta.net/LICENSE?queryparams=XXX
Resolving download2.llamameta.net (download2.llamameta.net)... 65.8.164.24, 65.8.164.126, 65.8.164.75, ...
Connecting to download2.llamameta.net (download2.llamameta.net)|65.8.164.24|:443... connected.
HTTP request sent, awaiting response... 416 Requested Range Not Satisfiable

    The file is already fully retrieved; nothing to do.

--2023-10-11 13:10:04--  https://download2.llamameta.net/USE_POLICY.md?queryparams=XXX
Resolving download2.llamameta.net (download2.llamameta.net)... 65.8.164.24, 65.8.164.126, 65.8.164.75, ...
Connecting to download2.llamameta.net (download2.llamameta.net)|65.8.164.24|:443... connected.
HTTP request sent, awaiting response... 416 Requested Range Not Satisfiable

    The file is already fully retrieved; nothing to do.

Downloading tokenizer
--2023-10-11 13:10:04--  https://download2.llamameta.net/tokenizer.model?queryparams=XXX
Resolving download2.llamameta.net (download2.llamameta.net)... 65.8.164.24, 65.8.164.126, 65.8.164.75, ...
Connecting to download2.llamameta.net (download2.llamameta.net)|65.8.164.24|:443... connected.
HTTP request sent, awaiting response... 403 Forbidden
2023-10-11 13:10:05 ERROR 403: Forbidden.

Thank you!
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/142","Fine-tuning Code Llama on my own code","2023-10-11T12:44:19Z","Open issue","No label","Thank you for this amazing effort!
I would like to fine-tune code llama on my own Python code, let's call it MyPackage for now. Ultimately, I would like to ask Code llama questions of the form: ""how do I do [INSERT FEASIBLE TASK NAME] using MyPackage?""
I have my package ready and documented. How do I format the dataset to fine-tune Code Llama?
 Are there fine-tuning scripts available in this repository?
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/141","while running dowload.sh file getting Bad substitution error","2023-10-11T05:47:34Z","Open issue","No label","Enter the list of models to download without spaces (7B,13B,70B,7B-chat,13B-chat,70B-chat), or press Enter for all: 7B
 download.sh: 14: [[: not found
 Downloading LICENSE and Acceptable Usage Policy
 download.sh: 19: Bad substitution
 The text was updated successfully, but these errors were encountered: 
👍1
IdeaKing reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/meta-llama/codellama/issues/140","Can't reproduce the results on Humaneval","2023-10-12T06:28:18Z","Closed issue","No label","Hello, may I ask how you conducted testing on Humaneval? I attempted to test using the two methods you provided in your Hugging Face blog, including code completion and code infilling on Humaneval. However, I only achieved results of 22% and 25% on Instruct CodeLLama 7B, which is far from the reported 35%.
args.max_length = 1024

if args.task == 'code-completion':
    tokenizer = transformers.AutoTokenizer.from_pretrained(args.model_path)
    pipeline = transformers.pipeline(
        ""text-generation"",
        model=args.model_path,
        torch_dtype=torch.float16,
        device_map=""auto"",
    )
    for task_id in problems:
        prompt = problems[task_id]['question'] + '<FILL_ME>\n    return result'
        input_ids = tokenizer(prompt, return_tensors=""pt"")[""input_ids""].to(""cuda"")
        output = model.generate(input_ids, max_new_tokens=args.max_length,)
        output = output[0].to(""cpu"")
        filling = tokenizer.decode(output[input_ids.shape[1]:], skip_special_tokens=True)
        completion = prompt.replace(""<FILL_ME>"", filling)

elif args.task == 'code-infilling':
    tokenizer = transformers.AutoTokenizer.from_pretrained(args.model_path)
    model = transformers.AutoModelForCausalLM.from_pretrained(
        args.model_path,
        torch_dtype=torch.float16
    ).to(""cuda"")
    for task_id in problems:
        completion = pipeline(
            prompt,
            do_sample=True,
            temperature=0.2,
            top_p=0.9,
            num_return_sequences=1,
            eos_token_id=tokenizer.eos_token_id,
            max_length=args.max_length,
        )[0]['generated_text'].strip()


 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/139","Combining instruction and infilling","2023-10-12T11:08:50Z","Closed issue","No label","Figure 2 of the paper shows the training pipeline of all models. Each model undergoes infilling code training. For the instruct models additionally a final instruction fine-tuning is applied. This suggests that both tasks can be combined. However, the paper doesn't make this clear to me and the code examples also seem to only do one thing at a time.
Is is possible to combine instructing and infilling in a single prompt?
If yes, what is the correct prompt format?
I imagine something like
[INST][SYS] system [/SYS] instruction [/INST][PREFIX] prefix [SUFFIX] suffix [MID] 

 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/138","dataset size of self-instruct","2023-10-05T23:16:06Z","Open issue","No label","Hi,
is the self-instruct dataset mentioned in the paper, which is used to train the code llama - instruct, publicly available?
I am interested in the dataset size(in token) of self-instruct you use to achieve that improvement in The value of self-instruct data ablation study It would be nice to know the size including the prompt, input and output token of it.
 The text was updated successfully, but these errors were encountered: 
👀2
TechxGenus and abwilf reacted with eyes emoji
All reactions
👀2 reactions"
"https://github.com/meta-llama/codellama/issues/137","Help Needed: Fine-tuning codellama-7b-Instruct Model for pinescript Programming language","2023-10-04T10:27:51Z","Open issue","No label","I'm having a problem with fine-tuning the codellama-7b-Instruct model for a programming language. The issue is that the model seems to focus too much on the new dataset , and its performance isn't great on new tasks. It's not just overfitting; sometimes, it doesn't do well on new tasks either.
For example:
Base model
User: Hey There! How are you
Model: I am good. How can I help ?

Finetuned model
User: Hey There! How are you
Model: Yes, I can fix your pinescript code. Provide me your issue?

I've tried increasing the number of training epochs to make sure it learns properly. I've also prepared my dataset carefully according to codellama's requirements. I used LORA and PEFT for finetuning. My dataset has 60,000 chat examples, each with 1000 tokens in the context. To make the model more robust, I overlapped the examples by 25%.
Here are my training settings:
Epochs: 15
Batch Size: 6
Gradient Accumulation Step: 2
Learning Rate: 4e-4
Warmup Ratio: 0.05

Lora r : 32
Lora alpha: 32
Lora dropout: 0.05
Target Modules: [""q_proj"", ""v_proj"", ""k_proj"", ""o_proj"", ""gate_proj"", ""up_proj"", ""down_proj"", ""lm_head""] (all linear layers are trainable)

Is there any way so I can merge some adapter layers and check its performance rather than merging all layers. I dont want to finetune again since it takes 10-15 days for the finetuning.
Any help or suggestion would highly be appreciated.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/135","Are there instructions on how to uninstall and remove the downloaded models?","2024-01-30T09:24:23Z","Closed issue","No label","No description provided.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/134","Keep getting ERROR 403: Forbidden even when I used new link","2023-09-28T04:53:40Z","Open issue","No label","I removed all local files, re-cloned the repository, and requested a new download link, but when I run ./download.sh and entered the download link, chose the model, it still gave the ERROR 403: Forbidden. What should I do now? I copied what I have seen here.
Enter the list of models to download without spaces (7B,13B,70B,7B-chat,13B-chat,70B-chat), or press Enter for all: 70B
 Downloading LICENSE and Acceptable Usage Policy
 --2023-09-27 21:44:52-- https://download2.llamameta.net/LICENSE?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaXF1ZV9oYXNoIjoieW9xMWo1Z2ZvMHlwOXRlOGtmM2VuMDJxIiwiUmVzb3VyY2UiOiJodHRwczpcL1wvZG93bmxvYWQyLmxsYW1hbWV0YS5uZXRcLyoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2OTU5NjIyMTN9fX1dfQ__&Signature=c0eH-sjrSLUcXtpO0QzraO-GsL%7ELrS9dPhNeM5sOrN9IDnN8O9b8NRZrNEvE-BqIqAv4trB0DzoB6WIQXfCviUg6iycMQPuGwPij-JQeZLzDLyO23nzn6p0p869SP6n8LkaX8Q9HpkPJevZBbdthuq69XDhY-oqMsro6kQgx6aMhQmkJyqmJaSWH6JllrIasBr-iNZFuir66X4nTNi1qJJ0hIEZylAgJi8Gjys6nzUcceyFP%7EMXQ08B%7EXPDejacoceE%7EzeyVgwFglscsmWnydxo7qjVNdF5uTHs6pw-2giYbihdsV9vsVEtt0AL447%7E4ldceObKGXBXvtcKIS61HoA__&Key-Pair-Id=K15QRJLYKIFSLZ&Download-Request-ID=852286889742198
 Resolving download2.llamameta.net (download2.llamameta.net)... 216.137.39.97, 216.137.39.66, 216.137.39.67, ...
 Connecting to download2.llamameta.net (download2.llamameta.net)|216.137.39.97|:443... connected.
 HTTP request sent, awaiting response... 416 Requested Range Not Satisfiable
The file is already fully retrieved; nothing to do.

--2023-09-27 21:44:52-- https://download2.llamameta.net/USE_POLICY.md?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaXF1ZV9oYXNoIjoieW9xMWo1Z2ZvMHlwOXRlOGtmM2VuMDJxIiwiUmVzb3VyY2UiOiJodHRwczpcL1wvZG93bmxvYWQyLmxsYW1hbWV0YS5uZXRcLyoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2OTU5NjIyMTN9fX1dfQ__&Signature=c0eH-sjrSLUcXtpO0QzraO-GsL%7ELrS9dPhNeM5sOrN9IDnN8O9b8NRZrNEvE-BqIqAv4trB0DzoB6WIQXfCviUg6iycMQPuGwPij-JQeZLzDLyO23nzn6p0p869SP6n8LkaX8Q9HpkPJevZBbdthuq69XDhY-oqMsro6kQgx6aMhQmkJyqmJaSWH6JllrIasBr-iNZFuir66X4nTNi1qJJ0hIEZylAgJi8Gjys6nzUcceyFP%7EMXQ08B%7EXPDejacoceE%7EzeyVgwFglscsmWnydxo7qjVNdF5uTHs6pw-2giYbihdsV9vsVEtt0AL447%7E4ldceObKGXBXvtcKIS61HoA__&Key-Pair-Id=K15QRJLYKIFSLZ&Download-Request-ID=852286889742198
 Resolving download2.llamameta.net (download2.llamameta.net)... 216.137.39.97, 216.137.39.66, 216.137.39.67, ...
 Connecting to download2.llamameta.net (download2.llamameta.net)|216.137.39.97|:443... connected.
 HTTP request sent, awaiting response... 206 Partial Content
 Length: 4790 (4.7K), 24 remaining [text/markdown]
 Saving to: ‘./USE_POLICY.md’
./USE_POLICY.md 100%[+++++++++++++++++++++++++++++++++++++++++++++++++>] 4.68K --.-KB/s in 0s
2023-09-27 21:44:52 (478 KB/s) - ‘./USE_POLICY.md’ saved [4790/4790]
Downloading tokenizer
 --2023-09-27 21:44:52-- https://download2.llamameta.net/tokenizer.model?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaXF1ZV9oYXNoIjoieW9xMWo1Z2ZvMHlwOXRlOGtmM2VuMDJxIiwiUmVzb3VyY2UiOiJodHRwczpcL1wvZG93bmxvYWQyLmxsYW1hbWV0YS5uZXRcLyoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2OTU5NjIyMTN9fX1dfQ__&Signature=c0eH-sjrSLUcXtpO0QzraO-GsL%7ELrS9dPhNeM5sOrN9IDnN8O9b8NRZrNEvE-BqIqAv4trB0DzoB6WIQXfCviUg6iycMQPuGwPij-JQeZLzDLyO23nzn6p0p869SP6n8LkaX8Q9HpkPJevZBbdthuq69XDhY-oqMsro6kQgx6aMhQmkJyqmJaSWH6JllrIasBr-iNZFuir66X4nTNi1qJJ0hIEZylAgJi8Gjys6nzUcceyFP%7EMXQ08B%7EXPDejacoceE%7EzeyVgwFglscsmWnydxo7qjVNdF5uTHs6pw-2giYbihdsV9vsVEtt0AL447%7E4ldceObKGXBXvtcKIS61HoA__&Key-Pair-Id=K15QRJLYKIFSLZ&Download-Request-ID=852286889742198
 Resolving download2.llamameta.net (download2.llamameta.net)... 216.137.39.97, 216.137.39.66, 216.137.39.67, ...
 Connecting to download2.llamameta.net (download2.llamameta.net)|216.137.39.97|:443... connected.
 HTTP request sent, awaiting response... 403 Forbidden
 2023-09-27 21:44:53 ERROR 403: Forbidden.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/132","Tokenizer class CodeLLaMATokenizer does not exist or is not currently imported","2023-09-24T07:55:18Z","Open issue","No label","For now to resolve this error, need to manually update the tokenizer_class to ""LlamaTokenizer"" in tokenizer_config.json.
 The text was updated successfully, but these errors were encountered: 
🎉2
hushiwen26 and RafaelCostaF reacted with hooray emoji
All reactions
🎉2 reactions"
"https://github.com/meta-llama/codellama/issues/131",".","2023-09-25T13:13:02Z","Closed issue","No label","No description provided.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/130","Is there any way to create embeddings?","2023-09-23T00:56:58Z","Open issue","No label","Is there any way to create embeddings with code llama as the base model like OpenAI embedding endpoints?
 The text was updated successfully, but these errors were encountered: 
👍3
usefksa, Cossack9989, and mclassen reacted with thumbs up emoji
All reactions
👍3 reactions"
"https://github.com/meta-llama/codellama/issues/129","Generating Embeddings of Code Tokens","2023-09-23T00:48:29Z","Open issue","No label","I am exploring the possibility of using codellama to generate embeddings for code tokens and would like to know if this is feasible with the current implementation.
Questions:
Is it possible to use codellama to generate embeddings of code tokens?
If yes, how should we configure and use codellama to make it usable for generating embeddings of code tokens?
 The text was updated successfully, but these errors were encountered: 
👍1
nashid reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/meta-llama/codellama/issues/128","Torchrun returns ""Failed to create process"" error","2023-09-22T10:37:24Z","Open issue","No label","Hi,
I'm new to Code Llama, and I'm facing a problem after having cloned the repository and after having downloaded a model.
I've followed the steps described in the documentation and, when I execute the torchrun command, the ""failed to create process"" error is returned.
It seems to me I didn't miss any step. I attach some print screens:
First print screen: After having cloned the repository, I try to execute the commands described in the documentation

Second and third print screens: The structure of my folders
It's been two days since I'm trying to understand how to fix this problem.
Could someone help me?
Thank you
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/126","RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!","2023-09-22T09:17:04Z","Closed issue","No label","Changing model.py like this can fix this problem, maybe there is a better solution:
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/125","RuntimeError: ProcessGroupNCCL is only supported with GPUs, no GPUs found!","2023-09-21T19:40:14Z","Open issue","No label","I installed everything in WSL Ubuntu and made sure to install the Windows vGPU driver for WSL and the NVIDIA CUDA toolkit for WSL 2 on Ubuntu (the correct CUDA version for my GPU, which is 11.8).
However, I'm getting this error:
 ...
 RuntimeError: ProcessGroupNCCL is only supported with GPUs, no GPUs found!
 ...
Why doesn't it detect the GPU?
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/124","i failed to create process","2023-09-21T14:55:35Z","Open issue","No label","when i use torchrun ,it said that it failed to create process
 just like this:
 torchrun --nproc_per_node 1 llamacpp_mock_api.py \ --ckpt_dir CodeLlama-7b-Instruct/ \ --tokenizer_path CodeLlama-7b-Instruct/tokenizer.model \ --max_seq_len 512 --max_batch_size 4
 failed to create process.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/123","Setting OMP_NUM_THREADS environment variable for each process to be 1 in default？","2023-09-21T13:00:42Z","Closed issue","No label","torchrun --nproc_per_node 2 example_infilling.py --ckpt_dir CodeLlama-13b-Python/ --tokenizer_path CodeLlama-13b-Python/tokenizer.model --max_seq_len 192 --max_batch_size 4
 WARNING:torch.distributed.run:
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/122","Please add a simple gradio interface that supports quantization","2023-09-20T21:38:41Z","Closed issue","No label","We need a simple Gradio interface that support optimizations
So that models can be run on 12 GB or 24 GB cards
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/121","Installation issues on Windows","2023-09-20T14:44:02Z","Closed issue","No label","I installed Conda on Windows 10, installed PyTorch and cudatools packages and cloned the repo.
 I also installed menpo wget package and m2-base package for linux commands like bash.
 However when I run the download.sh script, I get:
Downloading LICENSE and Acceptable Usage Policy
 download.sh: line 18: wget: command not found
 download.sh: line 19: wget: command not found
 Downloading CodeLlama-7b-Instruct
 download.sh: line 53: wget: command not found
 download.sh: line 56: wget: command not found
 download.sh: line 57: wget: command not found
 download.sh: line 58: wget: command not found
 Checking checksums
 md5sum: checklist.chk: No such file or directory
Why can't it find wget while it's installed?
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/120","如何通过openai的方式启动codellama呢","2023-09-18T14:02:53Z","Open issue","No label","这样我的代码，就可以openai的方式调用codellama了
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/119","Error when download","2023-09-17T10:09:26Z","Open issue","No label","after I run bash download.sh then it's show this error
Checking checksums
 parseopts.c:76: setup_check: fopen 'checklist.chk': No such file or directory
Please let me know how to process this problem
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/118","Can't run the example","2023-10-01T14:47:26Z","Closed issue","No label","I am getting the following error when I tried to run the example with ""torchrun example_completion.py --ckpt_dir CodeLlama-7b/ --tokenizer_path CodeLlama-7b/tokenizer.model --max_seq_len 128 --max_batch_size 4"".
I have PyTorch 2.0.1 installed on WSL2 using miniconda. Unlike other issues where ""torch.distributed.elastic.multiprocessing.api:failed"" with an existcode 1, I am getting SIGKILL. What could be the reason?
initializing model parallel with size 1
 initializing ddp with size 1
 initializing pipeline with size 1
 ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: -9) local_rank: 0 (pid: 29066) of binary: /home/zlu/miniconda3/envs/torch/bin/python
 Traceback (most recent call last):
 File ""/home/zlu/miniconda3/envs/torch/bin/torchrun"", line 33, in 
 sys.exit(load_entry_point('torch==2.0.1', 'console_scripts', 'torchrun')())
 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
 File ""/home/zlu/miniconda3/envs/torch/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/errors/init.py"", line 346, in wrapper
 return f(*args, **kwargs)
 ^^^^^^^^^^^^^^^^^^
 File ""/home/zlu/miniconda3/envs/torch/lib/python3.11/site-packages/torch/distributed/run.py"", line 794, in main
 run(args)
 File ""/home/zlu/miniconda3/envs/torch/lib/python3.11/site-packages/torch/distributed/run.py"", line 785, in run
 elastic_launch(
 File ""/home/zlu/miniconda3/envs/torch/lib/python3.11/site-packages/torch/distributed/launcher/api.py"", line 134, in call
 return launch_agent(self._config, self._entrypoint, list(args))
 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
 File ""/home/zlu/miniconda3/envs/torch/lib/python3.11/site-packages/torch/distributed/launcher/api.py"", line 250, in launch_agent
 raise ChildFailedError(
 torch.distributed.elastic.multiprocessing.errors.ChildFailedError:
 ======================================================
 example_completion.py FAILED
Failures:
 <NO_OTHER_FAILURES>
Root Cause (first observed failure):
 [0]:
 time : 2023-09-16_15:03:11
 host : Lenovo21.
 rank : 0 (local_rank: 0)
 exitcode : -9 (pid: 29066)
 error_file: <N/A>
 traceback : Signal 9 (SIGKILL) received by PID 29066
 The text was updated successfully, but these errors were encountered: 
👍1
CodingFlow reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/meta-llama/codellama/issues/117","what‘s the difference among 7b,7b-python and 7b-Instruct","2023-09-15T07:21:03Z","Open issue","No label","as describe in titlle,much appreciate
 The text was updated successfully, but these errors were encountered: 
👍1
qspider1975 reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/meta-llama/codellama/issues/116","The difference between the playground and the offline model","2023-09-20T23:10:11Z","Closed issue","No label","I cannot reproduce the equivalent output using the offline model (codellama/CodeLlama-13b-Instruct-hf, https://huggingface.co/codellama/CodeLlama-13b-Instruct-hf)
Here are some offline examples.
The Playground example (using https://huggingface.co/spaces/codellama/codellama-13b-chat)

The performance of the offline huggingface model is poor, which is worse than that of playground. Is there something wrong with hyperparameter settings?
Is the model used in playground different from the offline download model?
Huggingface code:
tokenizer = CodeLlamaTokenizer.from_pretrained(path_model_id) 
   model = LlamaForCausalLM.from_pretrained( 
    path_model_id, 
    torch_dtype=torch.float16 
).to(""cuda:0"")
user = ""Write a function that computes the set of sums of all contiguous sublists of a given list.""prompt = f""<s>[INST]{user.strip()} [/INST]""input_ids = tokenizer(prompt, return_tensors=""pt"")[""input_ids""].to(""cuda:0"")
output = model.generate(
    input_ids,
    max_new_tokens=2000,
    # top_p=0.9,
    # temperature=0.1,
    # # top_k=10,
    # do_sample=True,
    # num_beams=1,
    # repetition_penalty=1.1,
    repetition_penalty=1.05,
    # eos_token_id=tokenizer.eos_token_id,
    # pad_token_id=tokenizer.pad_token_id
)
output = output[0].to(""cpu"")

filling = tokenizer.decode(output[input_ids.shape[1]:], skip_special_tokens=True)
print(filling)
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/115","Memory Usage Spike During Model Saving in Training Pipeline","2023-09-14T14:21:14Z","Open issue","No label","I have a question regarding my fine-tuning pipeline, specifically concerning a memory usage spike when the model saves checkpoint during the training step. This cause sudden CUDA Memory error.
I would like to provide the following information, including GPU usage logs and code snippets for reference:
GPU - Used: 12.2GB, Free: 11.5GB
GPU - Used: 12.3GB, Free: 11.4GB
GPU - Used: 12.3GB, Free: 11.4GB
GPU - Used: 22.2GB, Free: 1.5GB
GPU - Used: 18.8GB, Free: 4.8GB
GPU - Used: 18.8GB, Free: 4.8GB
GPU - Used: 18.8GB, Free: 4.8GB
GPU - Used: 22.2GB, Free: 1.5GB
GPU - Used: 23.0GB, Free: 0.7GB
GPU - Used: 23.3GB, Free: 0.4GB
GPU - Used: 23.3GB, Free: 0.4GB
GPU - Used: 23.3GB, Free: 0.4GB
GPU - Used: 23.6GB, Free: 0.1GB
GPU - Used: 19.9GB, Free: 3.8GB
GPU - Used: 19.9GB, Free: 3.8GB
GPU - Used: 22.1GB, Free: 1.6GB

import json
import torch
import pandas as pd
import datasets
from peft import LoraConfig,PeftModel
from transformers import (AutoModelForCausalLM,AutoTokenizer,TrainingArguments,BitsAndBytesConfig)
import transformers
from trl import SFTTrainer
from training_args import *
import os

import logging
import sys

output_dir = ""CodeLlama-7b-Instruct-HF-results/trl-trainer/Complete_dataset_training/""


if not os.path.exists(output_dir):
    # If the directory doesn't exist, create it
    os.makedirs(output_dir)
    print(f""Directory '{output_dir}' created."")
else:
    print(f""Directory '{output_dir}' already exists."")


# Create a logger instance
logger = logging.getLogger()
logger.setLevel(logging.INFO)

# Create a formatter with the desired format
formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')

# Create a stream handler to output log messages to the console
stream_handler = logging.StreamHandler(sys.stdout)
stream_handler.setFormatter(formatter)
logger.addHandler(stream_handler)

# Create a file handler to log messages to a file
file_handler = logging.FileHandler(f'{output_dir}/trl-trainer-codellama.txt', encoding='utf-8')  # Specify the file name here
file_handler.setFormatter(formatter)
logger.addHandler(file_handler)
console_handler = logging.StreamHandler(stream=sys.stdout)


# DEVICE = ""cuda:0"" if torch.cuda.is_available() else 'cpu'



MODEL_NAME = ""CodeLlama-7b-Instruct-HF/""

# loading dataset
dataset = datasets.load_from_disk(""../dataset/complete_overlapped_chat_format_171781/"")
# loading model
model = AutoModelForCausalLM.from_pretrained(MODEL_NAME,use_safetensors=True,load_in_8bit=True,trust_remote_code=True,device_map='auto')
# loading tokenizer
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, add_special_tokens=False, add_eos_token=False, add_bos_token=False)
# LORA Configuration
peft_config = LoraConfig(
    lora_alpha=32,
    lora_dropout=0.05,
    r = 12,
    bias=""none"",
    task_type = ""CAUSAL_LM"",
    target_modules = [""q_proj"", ""v_proj"",""k_proj"",""o_proj"",""gate_proj"",""up_proj"",""down_proj"",""lm_head""]
)



training_arguments = TrainingArguments(
    per_device_train_batch_size=8,
    gradient_accumulation_steps=4,
    optim=""paged_adamw_32bit"",
    learning_rate=4e-4,
    fp16=True,
    max_grad_norm=0.3,
    num_train_epochs=3,
    warmup_ratio=0.05,
    logging_steps=5,
    save_total_limit=5,
    save_strategy=""steps"",
    save_steps=1,
    group_by_length=True,
    output_dir=output_dir,
    report_to=""tensorboard"",
    save_safetensors=True,
    lr_scheduler_type=""cosine"",
    seed=42)

trainer = SFTTrainer(
    model=model,
    train_dataset=dataset,
    peft_config=peft_config,
    dataset_text_field=""text"",
    max_seq_length=4096,
    tokenizer=tokenizer,
    args=training_arguments,
)

trainer.tokenizer.pad_token = False
trainer.tokenizer.pad_token

try:
    trainer.train()
except Exception as e:
    logger.error(f""Error in Logs due to {e}"")

 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/114","Support code review, comment filling, bug modification","2023-09-14T10:00:47Z","Open issue","No label","Does Codellama support code review, comment filling, bug fixing? Or can only do code filling and code generation？
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/113","403 error: forbidden for all other files besides consolidated.00.pth","2023-09-12T19:34:57Z","Open issue","No label","When running the bash download.sh and pasting in the signed URL - it downloads only a single file with no problem:
--2023-09-12 14:29:39--  https://download2.llamameta.net/USE_POLICY.md?Policy=xxxxxxx
Resolving download2.llamameta.net (download2.llamameta.net)... 65.8.164.75, 65.8.164.24, 65.8.164.126, ...
Connecting to download2.llamameta.net (download2.llamameta.net)|65.8.164.75|:443... connected.
HTTP request sent, awaiting response... 403 Forbidden
2023-09-12 14:29:40 ERROR 403: Forbidden.

Downloading CodeLlama-13b-Python
--2023-09-12 14:29:40--  https://download2.llamameta.net/CodeLlama-13b-Python/consolidated.00.pth?Policy=xxxxx
Resolving download2.llamameta.net (download2.llamameta.net)... 65.8.164.75, 65.8.164.24, 65.8.164.126, ...
Connecting to download2.llamameta.net (download2.llamameta.net)|65.8.164.75|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 13016329643 (12G) [binary/octet-stream]
Saving to: ‘./CodeLlama-13b-Python/consolidated.00.pth’


It downloads the consolidated.0*.pth files just fine, but none of the other json files can be downloaded because of this forbidden error.
Yes, I am doing the download within the 24-hour window (as you can see the consolidated files are downloading). I just received the email like 10 min ago and I am having this issue.
 Please advise, thanks!
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/112","Batching generates broken answers","2023-09-12T16:41:48Z","Open issue","No label","There are multiple posts on the internet about llama2 models generating bad output when running more than one instruction using the batch option.
 I can confirm that this true on all llama2 and llamacode models.
 One instruction works as expected, but 2 instructions make the model go crazy and output junk. Combined instructions length is within the max_seq_len, so there is no truncation...
 It seems that the model becomes ""less smart"" when batching.
The question is why and how to fix?
    dialogs = [
        [          
              {
                ""role"": ""system"",
                ""content"": """"""for each address in the following text return a json object [{\""""t\"""":street,\""""c\"""":city,\""""s\"""":state,\""""d\"""":country}]""""""
},
            {""role"": ""user"", ""content"":text}],
            
             [          
              {
                ""role"": ""system"",
                ""content"": """"""for each address in the following text return a json object[{\""""t\"""":street,\""""c\"""":city,\""""s\"""":state,\""""d\"""":country}]""""""
},
            {""role"": ""user"", ""content"":text}]
    ]

    
    results = generator.chat_completion(
        dialogs,  
        max_gen_len=max_gen_len,
        temperature=temperature,
        top_p=top_p,
    )

 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/109","Unable to get complete output and seems pending while running 13b-instruct","2023-09-15T02:54:27Z","Closed issue","No label","I cloned and deployed the project locally from https://huggingface.co/spaces/codellama/codellama-13b-chat/tree/main. But after I run this 13b-instruct model,I find there are two issues:
model always doesn't provide me complete answer.I try to track the ouput of model and find that it seems to repeat to print empty string until length of output exceeds max_new_token like this:
model output duplicate content until length exceeds max_new_token.
 instruction:write a c++ code to do quick sort.
current configuration:Top-k:10 Top-p:0.1 Temperature:0.7 Max new tokens:1024 I tried to fine-tune these parameters with little success.
 Could anybody share any ideas to address this strange issue?
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/108","find problems while trying to run example_instructions.py using 13b-Instruct","2023-09-11T11:43:53Z","Open issue","model-usage","The bash command and output information are shown above. I wonder if you have encountered this problem or have any idea to solve it. Thanks!
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/107","Download sh not working","2023-09-12T08:15:56Z","Closed issue","No label","[x] Visit the Meta AI website and accept the License agreement.
 Done
[x] Wait for approval, and you will receive a signed URL via email.
 Done
[] Run the download.sh script and pass the provided URL when prompted to initiate the download. Ensure you copy the URL text itself, not using the 'Copy link address' option.
 Get error
[x] Ensure you have wget and
 [x] md5sum installed as prerequisites.
 Done
[] Run the script by executing ""bash download.sh.""
 Error
[x] Be aware that the download links expire after 24 hours and a certain number of downloads. If you encounter a ""403: Forbidden"" error, you can request a new link.
 (just downloaded, haven't been able to get prompt yet).
[x] I also ran pip install -e .
This is the response in cmd:
\GitHub\codellama>bash download.sh
download.sh: line 2: $'\r': command not found
download.sh: line 5: $'\r': command not found
': not a valid identifier: `PRESIGNED_URL

': not a valid identifier: `MODEL_SIZE
download.sh: line 12: $'\r': command not found
download.sh: line 22: syntax error near unexpected token `$'do\r''
'ownload.sh: line 22: `do

codellama>bash --version
GNU bash, version 5.0.17(1)-release (x86_64-pc-linux-gnu)
Copyright (C) 2019 Free Software Foundation, Inc.
License GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html>

This is free software; you are free to change and redistribute it.
There is NO WARRANTY, to the extent permitted by law.
. ```


Perhaps I missed something?
I've successfully been running other models in python on this PC. 

 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/105","Downloading models is not working","2023-09-09T19:07:31Z","Open issue","download-install","I am trying to download the 7b-Python model using the instructions in the README file but I'm getting this when requesting any of the 7b models.
Enter the list of models to download without spaces (7b,13b,34b,7b-Python,13b-Python,34b-Python,7b-Instruct,13b-Instruct,34b-Instruct), or press Enter for all: CodeLlama-7b-Python
 Downloading LICENSE and Acceptable Usage Policy
 download.sh: line 18: wget: command not found
 download.sh: line 19: wget: command not found
 Unknown model: CodeLlama-7b-Python
I have requested a new link already but it's still not working. I can't figure out what I'm doing wrong.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/104","Request for CodeLlama's Specific Production Parameters on Human-Eval Dataset","2023-09-09T10:00:46Z","Open issue","No label","Dear Maintainer,
I hope this message finds you well. I have been trying to reproduce the performance of CodeLlama on the Human-Eval dataset, as mentioned in the paper. However, despite my best efforts, I have been unable to achieve the state-of-the-art (SOTA) accuracy reported in the paper.
To further investigate and understand the differences in results, I would greatly appreciate it if you could provide me with the specific production parameters used by CodeLlama on the Human-Eval dataset. Having access to these parameters would allow me to align my implementation more closely with the original work.
I understand and respect any concerns about confidentiality or limitations on sharing proprietary information. If it is not possible to disclose the exact production parameters, I would greatly appreciate any guidance or insights you can provide to help improve the accuracy of my implementation.
This is my evaluation result, and here is the code I used for generation:


Thank you for your time and consideration. I look forward to your response and any assistance you can provide.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/102","--ckpt_dir receiving no value","2023-09-09T19:56:44Z","Closed issue","No label","I'm having trouble running the model:
codellama/llama/generation.py"", line 85, in build
assert len(checkpoints) > 0, f""no checkpoint files found in {ckpt_dir}""
AssertionError: no checkpoint files found in  --ckpt_dir

This is what I input:
torchrun --nproc_per_node 1 llamacoder.py \ --ckpt_dir CodeLlama-34b-Instruct/ \ --tokenizer_path CodeLlama-34b- 
Instruct/tokenizer.model \ --max_seq_len 16000 --max_batch_size 4

In the directory CodeLlama-34b-Instruct it contains: checklist.chk, consolidated.00.pth, consolidated.01.pth, consolidated.02.pth, consolidated.03.pth, params, and tokenizer.model am I supposed to point it to one of the paths or am I perhaps missing a file or two? Am I maybe supposed to be putting it into quotation marks?
Any help would be greatly appreciated, thanks for taking the time to read my issue.
 The text was updated successfully, but these errors were encountered: 
👍1
xiaoheiNLP reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/meta-llama/codellama/issues/101","setup.py error","2023-09-07T14:40:45Z","Open issue","bug","root@main:/mnt/d/repos/LLAMA2/llama-main# ./setup.py
 ./setup.py: line 4: from: command not found
 ./setup.py: line 7: syntax error near unexpected token (' ./setup.py: line 7: def get_requirements(path: str):'
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/100","Any plans to create or release smaller models (3B, 1B)?","2023-09-08T14:21:22Z","Closed issue","No label","Hi! Do you have any plans to create or release smaller Code Llama models?
 They may be of use to speed-up inference via speculative sampling (ggerganov/llama.cpp#2926).
 The text was updated successfully, but these errors were encountered: 
🚀1
Huge reacted with rocket emoji
All reactions
🚀1 reaction"
"https://github.com/meta-llama/codellama/issues/99","Get unreadable result after running example","2023-09-07T02:39:38Z","Open issue","No label","Hi All, when I ran an official example like this :
 torchrun --nproc_per_node 1 example_instructions.py --ckpt_dir CodeLlama-7b-Instruct/ --tokenizer_path CodeLlama-7b-Instruct/tokenizer.model --max_seq_len 192 --max_batch_size 4
I got a result thant can not be read like this:
User: In Bash, how do I list all text files in the current directory (excluding subdirectories) that have been modified in the last month?
Assistant: ⁇ cd ⁇ sted ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇
==================================
User: What is the difference between inorder and preorder traversal? Give an example in Python.
Assistant: estaururop Rank ⁇ ⁇ ⁇ ⁇ ⁇ condu ⁇ doors ⁇ seq ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇
==================================
System: Provide answers in JavaScript
User: Write a function that computes the set of sums of all contiguous sublists of a given list.
Assistant: ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇ ⁇
I ran it on M1 MacBook, Does any body has any idea about it ?
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/98","How do you ""prompt"" llama???","2023-09-12T08:28:59Z","Closed issue","question","How do you ""prompt"" llama???
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/97","What are the GPU requirements for running the code llama models?","2023-09-06T07:19:36Z","Open issue","question","There is no information about prereqs of what GPU and memory that is requited for running the models during inference.
Please help.
 The text was updated successfully, but these errors were encountered: 
👍8
jackrmomo, Pablo-Oliveira, nashid, CodingFlow, s11005349, Anton-Le, vishaldubey21, and mehulparmariitr reacted with thumbs up emoji
All reactions
👍8 reactions"
"https://github.com/meta-llama/codellama/issues/96","Finetuning code llama for Multi-File code generation on private repository","2023-09-06T06:40:58Z","Open issue","fine-tuning","Hello. I'm trying to finetune code llama for a multifile code generation task on my private repository.
 The goal is to have the LLM generate code for some common bugs / issues across multiple files in my private repository.
Based on what I have been able to understand so far, the assumption is that doing this will require multiple stages of training / fine-tuning. I read the CodeLlama paper and am trying to create my own ""specialization pipeline"" for my repository and tasks.
The first fine-tuning will be done to give the model some comprehension about the repository structure (file paths, summary of what the file is doing and the code itself). This will require 100% code-coverage and the goal would be to have the model overfit. In this case, we will only look at model loss and will have no evaluation or test data sets.
Once the model has some comprehension about the repository structure, a second-pass task-specific fine-tuning can be done on a much smaller dataset which will be specific to the task. E.g. We can have the issues, old-code and refactored code as our dataset fields. We can then check for model loss, evaluation loss and the test results to measure the performance of the model.
The reason I want to do it this way is that while the fixes (the fixed code) is common, the files in which the code has to change might be different. So, the model needs to have some understanding of the files etc present in the repository.
Does this approach sound good or feasible? Are there alternative ways of doing this? If so, would you be able to point me to some resources that I can read and learn from.
Thanks.
 The text was updated successfully, but these errors were encountered: 
👍5
cmosguy, pratikshappai, hxysusan, shatealaboxiaowang, and alvaro1553 reacted with thumbs up emoji
All reactions
👍5 reactions"
"https://github.com/meta-llama/codellama/issues/95","Can't run codellama!","2023-09-13T10:51:35Z","Closed issue","model-usage","(py465) awahab@adnna:~/nexus-collaborative-project-planning-merge-notes/codellama$ torchrun --nproc_per_node 1 example_completion.py     --ckpt_dir CodeLlama-34b-Instruct/     --tokenizer_path CodeLlama-7b-Instruct/tokenizer.model     --max_seq_len 512
> initializing model parallel with size 1
> initializing ddp with size 1
> initializing pipeline with size 1
Traceback (most recent call last):
  File ""/home/awahab/nexus-collaborative-project-planning-merge-notes/codellama/example_completion.py"", line 55, in <module>
    fire.Fire(main)
  File ""/home/awahab/anaconda3/envs/py465/lib/python3.9/site-packages/fire/core.py"", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File ""/home/awahab/anaconda3/envs/py465/lib/python3.9/site-packages/fire/core.py"", line 475, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
  File ""/home/awahab/anaconda3/envs/py465/lib/python3.9/site-packages/fire/core.py"", line 691, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File ""/home/awahab/nexus-collaborative-project-planning-merge-notes/codellama/example_completion.py"", line 20, in main
    generator = Llama.build(
  File ""/home/awahab/nexus-collaborative-project-planning-merge-notes/codellama/llama/generation.py"", line 86, in build
    assert model_parallel_size == len(
AssertionError: Loading a checkpoint for MP=4 but world size is 1

this is my error when running the ahove
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/94","Possible to continue autoregressive pre-training on custom dataset","2023-09-05T16:16:35Z","Open issue","question","Is it possible to continue the initial autoregressive pre-training on a custom dataset, as was done for Code Llama - Python? This would in principle allow for the fine-tuning of Code Llama models in other programming languages. If so, would you please provide an example training script? Any information or help would be much appreciated!
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/93","Unable to run example_complete.py on windows 10","2023-09-05T09:10:19Z","Open issue","compability,model-usage","I tried running the example code given but theres an EOFerror, out of input that keeps popping up. Anyways to solve it?
torchrun --nproc_per_node 1 example_completion.py --ckpt_dir CodeLlama-7b --tokenizer_path CodeLlama-7b/tokenizer.model --max_seq_len 128 --max_batch_size 4
initializing model parallel with size 1
 initializing ddp with size 1
 initializing pipeline with size 1
 Traceback (most recent call last):
 File ""/home/yx/codellama/example_completion.py"", line 55, in 
 fire.Fire(main)
 File ""/home/yx/codellama/.conda/lib/python3.10/site-packages/fire/core.py"", line 141, in Fire
 component_trace = _Fire(component, args, parsed_flag_args, context, name)
 File ""/home/yx/codellama/.conda/lib/python3.10/site-packages/fire/core.py"", line 475, in _Fire
 component, remaining_args = _CallAndUpdateTrace(
 File ""/home/yx/codellama/.conda/lib/python3.10/site-packages/fire/core.py"", line 691, in _CallAndUpdateTrace
 component = fn(*varargs, **kwargs)
 File ""/home/yx/codellama/example_completion.py"", line 20, in main
 generator = Llama.build(
 File ""/home/yx/codellama/llama/generation.py"", line 90, in build
 checkpoint = torch.load(ckpt_path, map_location=""cpu"")
 File ""/home/yx/codellama/.conda/lib/python3.10/site-packages/torch/serialization.py"", line 815, in load
 return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)
 File ""/home/yx/codellama/.conda/lib/python3.10/site-packages/torch/serialization.py"", line 1033, in _legacy_load
 magic_number = pickle_module.load(f, **pickle_load_args)
 EOFError: Ran out of input
 ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 94531) of binary: /home/yx/codellama/.conda/bin/python
 Traceback (most recent call last):
 File ""/home/yx/codellama/.conda/bin/torchrun"", line 8, in 
 sys.exit(main())
 File ""/home/yx/codellama/.conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/init.py"", line 346, in wrapper
 return f(*args, **kwargs)
 File ""/home/yx/codellama/.conda/lib/python3.10/site-packages/torch/distributed/run.py"", line 794, in main
 run(args)
 File ""/home/yx/codellama/.conda/lib/python3.10/site-packages/torch/distributed/run.py"", line 785, in run
 elastic_launch(
 File ""/home/yx/codellama/.conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py"", line 134, in call
 return launch_agent(self._config, self._entrypoint, list(args))
 File ""/home/yx/codellama/.conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py"", line 250, in launch_agent
 raise ChildFailedError(
 torch.distributed.elastic.multiprocessing.errors.ChildFailedError:
 ============================================================
 example_completion.py FAILED
Failures:
 <NO_OTHER_FAILURES>
Root Cause (first observed failure):
 [0]:
 time : 2023-09-05_17:06:02
 host : DESKTOP-860T855.
 rank : 0 (local_rank: 0)
 exitcode : 1 (pid: 94531)
 error_file: <N/A>
 traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/92","How can I fine-tuning codellama with our own dataset?","2023-09-13T10:17:11Z","Closed issue","fine-tuning,question","As title. Is there any way to achieve that?
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/91","Docs Update","2023-09-04T16:24:50Z","Closed issue","No label","No description provided.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/89","Doesn't codellama know when to stop? The EOS doesn't work?","2023-09-12T12:51:24Z","Closed issue","model-usage","I just use the codellama-7b，and I found that it won't stop until it reaches the max_tokens.
 So how to solve this problem? Thank u !
Here are a few examples of what I tested:
curl http://localhost:8000/v1/completions -H ""Content-Type: application/json"" -d '{ ""model"": ""codeLlama"", ""prompt"": ""San Francisco is a"", ""max_tokens"": 50, ""temperature"": 0 }' {""id"":""cmpl-df1438614f2a42369f20d240dad02ba0"",""object"":""text_completion"",""created"":1693811331,""model"":""codeLlama"",""choices"":[{""index"":0,""text"":""city of 800,000 people, and the largest city in the state of California. The city is located on the west coast of the United States, and is the cultural and economic center of the San Francisco Bay Area.\n"",""logprobs"":null,""finish_reason"":""length""}],""usage"":{""prompt_tokens"":5,""total_tokens"":55,""completion_tokens"":50}}
curl http://localhost:8000/v1/completions -H ""Content-Type: application/json"" -d '{ ""model"": ""codeLlama"", ""prompt"": ""San Francisco is a"", ""max_tokens"": 500, ""temperature"": 0 }' {""id"":""cmpl-577731d1354a4b34a17a824f137c4d40"",""object"":""text_completion"",""created"":1693811355,""model"":""codeLlama"",""choices"":[{""index"":0,""text"":""city of 800,000 people, and the largest city in the state of California. The city is located on the west coast of the United States, and is the cultural and economic center of the San Francisco Bay Area.\nThe city is known for its many parks, including Golden Gate Park, which is the largest park in the United States. The city is also known for its many museums, including the San Francisco Museum of Modern Art, the San Francisco Museum of Art, the California Academy of Sciences, the de Young Museum, and the Legion of Honor.\nThe city is also known for its many restaurants, including the iconic Chinatown, the Mission District, and the North Beach area.\nThe city is also known for its many sports teams, including the San Francisco 49ers, the San Francisco Giants, the San Francisco Warriors, the San Francisco Seals, and the San Francisco Giants.\nThe city is also known for its many beaches, including the Golden Gate Bridge, the Golden Gate Park, the Golden Gate, the Pacific Ocean, the San Francisco Bay, and the San Francisco Bay Bridge.\nThe city is also known for its many landmarks, including the Golden Gate Bridge, the Golden Gate Park, the Golden Gate, the Pacific Ocean, the San Francisco Bay, and the San Francisco Bay Bridge.\nThe city is also known for its many attractions, including the Golden Gate Bridge, the Golden Gate Park, the Golden Gate, the Pacific Ocean, the San Francisco Bay, and the San Francisco Bay Bridge.\nThe city is also known for its many hotels, including the Fairmont San Francisco, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fairmont San Francisco Marriott, the Fair"",""logprobs"":null,""finish_reason"":""length""}],""usage"":{""prompt_tokens"":5,""total_tokens"":505,""completion_tokens"":500}}
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/88","Why are Code Llama - Python 7B and 13B incapable of filling in code given the surrounding context?","2023-09-13T10:22:04Z","Closed issue","model-usage,question","No description provided.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/87","Why is an emoji ☺ added during encoding and decoding?","2023-09-12T08:21:18Z","Closed issue","question","def encode_infilling(self, s: str) -> List[int]:
 """"""Encode a string without an implicit leading space.""""""
 return self.sp_model.encode(""☺"" + s)[2:]
def decode_infilling(self, t: List[int]) -> str:
 """"""Decode a string without an implicit leading space.""""""
 return self.sp_model.decode([self.sp_model.piece_to_id(""☺"")] + t)[1:]
I don't understand why '☺' is added to the input during encoding? Could you help me? Thanks!
 The text was updated successfully, but these errors were encountered: 
👍1
moorejee reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/meta-llama/codellama/issues/83","Run the Codellama model quickly based on the Hugging Face pretrained model","2023-09-03T08:55:44Z","Open issue","enhancement","Hello,
I've noticed that for some learners who are just getting started with the Codellama model, running a Codellama model using your code directly can be a bit challenging. To make it easier for everyone to quickly run their own Codellama model, I plan to create a simple script based on the Codellama pretrained model downloaded from Hugging Face. With this script, users can start using Codellama effortlessly. I intend to submit a pull request for this. Please keep an eye out for it.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/82","Running 7B model on RTX 4070 (12GB) causes ""out of memory"" error","2023-09-12T08:41:48Z","Closed issue","model-usage,performance","Running example_completion.py with a 7B model on an RTX 4070 (12GB VRAM) results in an ""out of memory"" error.
Is it possible to run example_completion.py without having to purchase a higher-spec GPU than the RTX 4070?
GPU Info (before running example_completion.py)
# nvidia-smi
Sun Sep  3 13:32:02 2023
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.86.05              Driver Version: 535.86.05    CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA GeForce RTX 4070        Off | 00000000:01:00.0  On |                  N/A |
|  0%   47C    P2              32W / 200W |    697MiB / 12282MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+

Command and Error
# torchrun --nproc_per_node 1 example_completion.py --ckpt_dir CodeLlama-7b/ --tokenizer_path CodeLlama-7b/tokenizer.model --max_seq_len 128 --max_batch_size 1
> initializing model parallel with size 1
> initializing ddp with size 1
> initializing pipeline with size 1
Traceback (most recent call last):
  File ""/root/codellama/example_completion.py"", line 55, in <module>
    fire.Fire(main)
  File ""/usr/local/lib/python3.10/dist-packages/fire/core.py"", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File ""/usr/local/lib/python3.10/dist-packages/fire/core.py"", line 475, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
  File ""/usr/local/lib/python3.10/dist-packages/fire/core.py"", line 691, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File ""/root/codellama/example_completion.py"", line 20, in main
    generator = Llama.build(
  File ""/root/codellama/llama/generation.py"", line 105, in build
    model = Transformer(model_args)
  File ""/root/codellama/llama/model.py"", line 260, in __init__
    self.layers.append(TransformerBlock(layer_id, params))
  File ""/root/codellama/llama/model.py"", line 222, in __init__
    self.attention = Attention(args)
  File ""/root/codellama/llama/model.py"", line 100, in __init__
    self.wq = ColumnParallelLinear(
  File ""/usr/local/lib/python3.10/dist-packages/fairscale/nn/model_parallel/layers.py"", line 262, in __init__
    self.weight = Parameter(torch.Tensor(self.output_size_per_partition, self.in_features))
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 0; 11.69 GiB total capacity; 10.85 GiB already allocated; 28.25 MiB free; 10.86 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 4749) of binary: /usr/bin/python3
Traceback (most recent call last):
  File ""/usr/local/bin/torchrun"", line 8, in <module>
    sys.exit(main())
  File ""/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py"", line 346, in wrapper
    return f(*args, **kwargs)
  File ""/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py"", line 794, in main
    run(args)
  File ""/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py"", line 785, in run
    elastic_launch(
  File ""/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py"", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File ""/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py"", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError:
============================================================
example_completion.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-09-03_13:25:53
  host      : daiv
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 4749)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================

Git revision
# git rev-parse HEAD
e064c1c24c377cc0875711440ef4c0a6eaf0147b

OS
# lsb_release -a
Description:	Ubuntu 22.04.3 LTS

 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/81","Bash .sh vs Bin .zsh Mac OS","2023-09-12T08:39:27Z","Closed issue","No label","I have installed wget and md5sha1sum using Macports and created a directory (folder) for this project by typing
""mkdir ~/CodeLlama-git""
and then
""cd ~/CodeLlama-git""
which brings my terminal to the new folder, but I'm using .zsh not .sh as my shell, most Macs do this, the point being bash can work on a Mac but I have my terminal set up and working well (I have multiple projects...) so I'm wondering what would be the next command to download Llama into my newly created directory. I don't want to use bash and end up with Llama as the only project I'm able to work with after converting from .zsh to .sh, I know there's a conversion that can make this a simple task...
 maybe someone has the answer?
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/80","Error when requesting a download link","2023-09-03T10:04:23Z","Closed issue","No label","I received this email:
Sorry, you are not eligible to access Llama 2.
Thank you for your interest in using Llama 2. Unfortunately, you do not meet the criteria to obtain a license at this time.
 The text was updated successfully, but these errors were encountered: 
😕1
LakoMoor reacted with confused emoji
All reactions
😕1 reaction"
"https://github.com/meta-llama/codellama/issues/79","Finetune CodeLlama-7b-Instruct-hf on private dataset","2023-09-01T15:18:48Z","Open issue","fine-tuning","I hope this message finds you well. I recently had the opportunity to experiment with the Codellama-7b-Instruct model from GitHub repository and was pleased to observe its promising performance. Encouraged by these initial results, I am interested in fine-tuning this model on my proprietary code chat dataset. I have single 3090 with 24GB VRAM.
To provide you with more context, my dataset has the following structure:
1. <s>[INST] {{user}} [/INST] {{assistant}} </s><s>[INST] {{user}} [/INST] {{assistant}} </s>
2. <s>[INST] {{user}} [/INST] {{assistant}} </s><s>[INST] {{user}} [/INST] {{assistant}} </s>

I have a total of 1000 such chat examples in my dataset.
Could you kindly guide me through the recommended pipeline or steps to effectively fine-tune the Codellama-7b-Instruct model on my specific chat dataset? I look forward to your guidance.
EDIT
I follow this pipeline but its giving me following error:
from transformers import AutoModelForCausalLM,AutoTokenizer
from transformers import LlamaForCausalLM, LlamaTokenizer
import transformers
import torch
from pathlib import Path
import os
import sys

MODEL_NAME = ""codellama/CodeLlama-7b-Instruct-hf""

model =LlamaForCausalLM.from_pretrained(MODEL, load_in_8bit=True, device_map='auto', torch_dtype=torch.bfloat16)
tokenizer = LlamaTokenizer.from_pretrained(""codellama/CodeLlama-7b-Instruct-hf"")

model.train()

def create_peft_config(model):
    from peft import (
        get_peft_model,
        LoraConfig,
        TaskType,
        prepare_model_for_int8_training,
    )

    peft_config = LoraConfig(
        task_type=TaskType.CAUSAL_LM,
        inference_mode=False,
        r=8,
        lora_alpha=32,
        lora_dropout=0.05,
        target_modules = [""q_proj"", ""v_proj""]
    )

    # prepare int-8 model for training
    model = prepare_model_for_int8_training(model)
    model = get_peft_model(model, peft_config)
    model.print_trainable_parameters()
    return model, peft_config

# create peft config
model, lora_config = create_peft_config(model)

from transformers import TrainerCallback
from contextlib import nullcontext
enable_profiler = False
output_dir = ""result""

config = {
    'lora_config': lora_config,
    'learning_rate': 1e-4,
    'num_train_epochs': 1,
    'gradient_accumulation_steps': 2,
    'per_device_train_batch_size': 10,
    'gradient_checkpointing': False,
}

# Set up profiler
if enable_profiler:
    wait, warmup, active, repeat = 1, 1, 2, 1
    total_steps = (wait + warmup + active) * (1 + repeat)
    schedule =  torch.profiler.schedule(wait=wait, warmup=warmup, active=active, repeat=repeat)
    profiler = torch.profiler.profile(
        schedule=schedule,
        on_trace_ready=torch.profiler.tensorboard_trace_handler(f""{output_dir}/logs/tensorboard""),
        record_shapes=True,
        profile_memory=True,
        with_stack=True)
    
    class ProfilerCallback(TrainerCallback):
        def __init__(self, profiler):
            self.profiler = profiler
            
        def on_step_end(self, *args, **kwargs):
            self.profiler.step()

    profiler_callback = ProfilerCallback(profiler)
else:
    profiler = nullcontext()

from transformers import default_data_collator, Trainer, TrainingArguments

# Define training args
training_args = TrainingArguments(
    output_dir=output_dir,
    overwrite_output_dir=True,
    bf16=True,  # Use BF16 if available
    # logging strategies
    logging_dir=f""{output_dir}/logs"",
    logging_strategy=""steps"",
    logging_steps=10,
    save_strategy=""no"",
    optim=""adamw_torch_fused"",
    max_steps=total_steps if enable_profiler else -1,
    **{k:v for k,v in config.items() if k != 'lora_config'}
)

with profiler:
    # Create Trainer instance
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=X_train,
        data_collator=default_data_collator,
        callbacks=[profiler_callback] if enable_profiler else [],
    )

    # Start training
    trainer.train()


ERROR
2680     return loss_mb.reduce_mean().detach().to(self.args.device)
   2682 with self.compute_loss_context_manager():
-> 2683     loss = self.compute_loss(model, inputs)
   2685 if self.args.n_gpu > 1:
   2686     loss = loss.mean()  # mean() to average on multi-gpu parallel training

ValueError: The model did not return a loss from the inputs, only the following keys: logits. For reference, the inputs it received are input_ids,attention_mask.

 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/77","Running CodeLllama-13B on single GPU","2023-09-01T09:40:21Z","Open issue","performance","In the ReadMe file, it is mentioned that to run 13B model, MP value should be 2. I have only 1 GPU, is there a way to run this model on single GPU (I am fine if efficiency is lost, what I care as of now is to run the 13B model)
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/76","Display of additional vague comments once code is generated","2023-09-04T06:15:59Z","Closed issue","No label","I am using a CodeLlama-7b model to generate a code. I provided the following prompt , ""prompts=[""Write a function in python to add two numbers and return their sum""] and got the below output.
 Can someone explain why it is showing additional vague comments once code is generated and how to make sure these are not generated along with code?

 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/75","Hardware requirement for 100K tokens","2023-09-13T10:12:18Z","Closed issue","question","What hardware is required to run the 34B Instruct model with 100K context length?
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/74","使用多卡GPU部署13b-instruct模型时，发现错误","2023-09-06T18:28:10Z","Closed as not planned issue","No label","在fairscale/nn/model_parallel/mappings.py中这行代码似乎不起作用
 torch.distributed.all_gather(tensor_list, input_, group=group)
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/73","so long loading checkpoint time","2023-09-12T08:30:44Z","Closed issue","performance,question","How long does it take to load the 7b model? During my running process, it takes 200s to load a 7b model.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/72","Can codellama achieve what it claims on HumanEval?","2023-09-12T08:19:47Z","Closed issue","question,research-paper","I used codellama python 7B to generate code for human eval dataset (164 questions). I can only get 3 or 4 problems correct, much worse than codellama claims (38%). Is there any trick to make it work? I use the default setting in example_completion.py.
One of the problem that all llama faces is it does not know when to stop. The first few line of codes seems correct. Then it generates non-related codes.
Can someone show their codes to use codellama to reproduce the human eval results? Thank you!
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/71","如何使用多个GPU去加载模型？","2023-08-31T11:34:01Z","Open issue","No label","code: torchrun --nproc_per_node 2 example_instructions.py --ckpt_dir CodeLlama-13b-Instruct/ --tokenizer_path CodeLlama-13b-Instruct/tokenizer.model --max_seq_len 2048 --max_batch_size 4
checkpoint = torch.load(ckpt_path, map_location=""cpu"") --> checkpoint = torch.load(ckpt_path, map_location=""cuda"")
 我将cpu改为cuda后，会报下列错误
 torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 136.00 MiB (GPU 1; 23.69 GiB total capacity; 22.64 GiB already allocated; 137.69 MiB free; 23.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation. See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
 WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 146425 closing signal SIGTERM
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/70","RuntimeError: Distributed package doesn't have NCCL built in","2023-09-03T03:14:19Z","Closed issue","No label","When trying to run example_completion.py file in my windows laptop, I am getting below error:

I am using pytorch 2.0 version with CUDA 11.7 . On typing the command
 import torch.distributed as dist
if dist.is_nccl_available():
 print(""NCCL is available and built into PyTorch."")
 else:
 print(""NCCL is not available in this PyTorch installation."")
I am getting the output ""NCCL is not available in this PyTorch installation.""
 What should I do ?
 The text was updated successfully, but these errors were encountered: 
👍2
srinivaskumarramdas and KKiriri reacted with thumbs up emoji
All reactions
👍2 reactions"
"https://github.com/meta-llama/codellama/issues/69","Fine tuning it with own code","2023-09-12T08:31:26Z","Closed issue","fine-tuning,question","can code llama be fine tuned with my own code for code refactoring tasks? and in that case, how do we prepare the dataset ?
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/67","Sponsored issue: I need it for my project","2023-09-06T18:28:31Z","Closed as not planned issue","No label","Priority Support
@APMNoman is using Mintycode to fund this issue.
You can receive if you provide priority support to @APMNoman.
To view the support request and terms go to Mintycode.
Thank you in advance for helping.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/66","how to install wget and md5sum.(Windows user)","2023-08-31T16:40:55Z","Closed issue","No label","I am running all the commands in a git bash terminal.
 If you guys tell me a step by step method on how to install the model it will be very helpful
download.sh: line 19: wget: command not found
Downloading CodeLlama-7b
download.sh: line 53: wget: command not found
download.sh: line 56: wget: command not found
download.sh: line 57: wget: command not found
download.sh: line 58: wget: command not found
Checking checksums
md5sum: checklist.chk: No such file or directory```

 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/65","run win10 is error","2023-08-30T02:09:29Z","Open issue","compability","i found this #55
 but is closed,and no solved.
env:
win10+conda(pytorch-gpu+python3.11)+powershell
error:

(pytorch-gpu) PS F:\aiProject\codellama> torchrun --nproc_per_node 1 example_completion.py --ckpt_dir .\CodeLlama-34b-Python\ --tokenizer_path .\CodeLlama-34b-Python\tokenizer.model --max_seq_len 512 --max_batch_size 4
NOTE: Redirects are currently not supported in Windows or MacOs.
[W C:\cb\pytorch_1000000000000\work\torch\csrc\distributed\c10d\socket.cpp:601] [c10d] The client socket has failed to connect to [kubernetes.docker.internal]:29500 (system error: 10049 - 在其上下文中，该请求的地址无效。).
[W C:\cb\pytorch_1000000000000\work\torch\csrc\distributed\c10d\socket.cpp:601] [c10d] The client socket has failed to connect to [kubernetes.docker.internal]:29500 (system error: 10049 - 在其上下文中，该请求的地址无效。).
[W C:\cb\pytorch_1000000000000\work\torch\csrc\distributed\c10d\socket.cpp:601] [c10d] The client socket has failed to connect to [kubernetes.docker.internal]:29500 (system error: 10049 - 在其上下文中，该请求的地址无效。).
[W C:\cb\pytorch_1000000000000\work\torch\csrc\distributed\c10d\socket.cpp:601] [c10d] The client socket has failed to connect to [kubernetes.docker.internal]:29500 (system error: 10049 - 在其上下文中，该请求的地址无效。).
Traceback (most recent call last):
  File ""F:\aiProject\codellama\example_completion.py"", line 55, in <module>
    fire.Fire(main)
  File ""C:\Users\b\.conda\envs\pytorch-gpu\Lib\site-packages\fire\core.py"", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\b\.conda\envs\pytorch-gpu\Lib\site-packages\fire\core.py"", line 475, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
                                ^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\b\.conda\envs\pytorch-gpu\Lib\site-packages\fire\core.py"", line 691, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^
  File ""F:\aiProject\codellama\example_completion.py"", line 20, in main
    generator = Llama.build(
                ^^^^^^^^^^^^
  File ""F:\aiProject\codellama\llama\generation.py"", line 68, in build
    torch.distributed.init_process_group(""nccl"")
  File ""C:\Users\b\.conda\envs\pytorch-gpu\Lib\site-packages\torch\distributed\distributed_c10d.py"", line 907, in init_process_group
    default_pg = _new_process_group_helper(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\b\.conda\envs\pytorch-gpu\Lib\site-packages\torch\distributed\distributed_c10d.py"", line 1013, in _new_process_group_helper
    raise RuntimeError(""Distributed package doesn't have NCCL "" ""built in"")
RuntimeError: Distributed package doesn't have NCCL built in
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 27024) of binary: C:\Users\b\.conda\envs\pytorch-gpu\python.exe
Traceback (most recent call last):
  File ""C:\Users\b\.conda\envs\pytorch-gpu\Scripts\torchrun-script.py"", line 33, in <module>
    sys.exit(load_entry_point('torch==2.0.1', 'console_scripts', 'torchrun')())
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\b\.conda\envs\pytorch-gpu\Lib\site-packages\torch\distributed\elastic\multiprocessing\errors\__init__.py"", line 346, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File ""C:\Users\b\.conda\envs\pytorch-gpu\Lib\site-packages\torch\distributed\run.py"", line 794, in main
    run(args)
  File ""C:\Users\b\.conda\envs\pytorch-gpu\Lib\site-packages\torch\distributed\run.py"", line 785, in run
    elastic_launch(
  File ""C:\Users\b\.conda\envs\pytorch-gpu\Lib\site-packages\torch\distributed\launcher\api.py"", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\b\.conda\envs\pytorch-gpu\Lib\site-packages\torch\distributed\launcher\api.py"", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError:
============================================================
example_completion.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-08-30_10:00:59
  host      : Administrator
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 27024)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================

HOW:
how to use it?
thanks
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/64","Finetuning 7B codellama: Runtime error","2023-08-29T17:31:12Z","Open issue","fine-tuning,model-usage","Am trying to finetune codellama with the same idea of llama2 and using the same script to finetune.
 Am not sure whether am right as the repo or blog not talking about finetune approach.
Am facing this error. RuntimeError: shape '[-1, 32000]' is invalid for input of size 131073504
RuntimeError Traceback (most recent call last)
 Cell In[10], line 29
 20 trainer = Trainer(
 21 model=model,
 22 args=training_args,
 (...)
 25 callbacks=[profiler_callback] if enable_profiler else [],
 26 )
 28 # Start training
 ---> 29 trainer.train()
File /opt/conda/envs/llama_cona_env/lib/python3.8/site-packages/transformers/trainer.py:1662, in Trainer.train(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)
 1657 self.model_wrapped = self.model
 1659 inner_training_loop = find_executable_batch_size(
 1660 self._inner_training_loop, self._train_batch_size, args.auto_find_batch_size
 1661 )
 -> 1662 return inner_training_loop(
 1663 args=args,
 1664 resume_from_checkpoint=resume_from_checkpoint,
 1665 trial=trial,
 1666 ignore_keys_for_eval=ignore_keys_for_eval,
 1667 )
File /opt/conda/envs/llama_cona_env/lib/python3.8/site-packages/transformers/trainer.py:1929, in Trainer._inner_training_loop(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)
 1927 tr_loss_step = self.training_step(model, inputs)
 1928 else:
 -> 1929 tr_loss_step = self.training_step(model, inputs)
 1931 if (
 1932 args.logging_nan_inf_filter
 1933 and not is_torch_tpu_available()
 1934 and (torch.isnan(tr_loss_step) or torch.isinf(tr_loss_step))
 1935 ):
 1936 # if loss is nan or inf simply add the average of previous logged losses
 1937 tr_loss += tr_loss / (1 + self.state.global_step - self._globalstep_last_logged)
File /opt/conda/envs/llama_cona_env/lib/python3.8/site-packages/transformers/trainer.py:2699, in Trainer.training_step(self, model, inputs)
 2696 return loss_mb.reduce_mean().detach().to(self.args.device)
 2698 with self.compute_loss_context_manager():
 -> 2699 loss = self.compute_loss(model, inputs)
 2701 if self.args.n_gpu > 1:
 2702 loss = loss.mean() # mean() to average on multi-gpu parallel training
File /opt/conda/envs/llama_cona_env/lib/python3.8/site-packages/transformers/trainer.py:2731, in Trainer.compute_loss(self, model, inputs, return_outputs)
 2729 else:
 2730 labels = None
 -> 2731 outputs = model(**inputs)
 2732 # Save past state if it exists
 2733 # TODO: this needs to be fixed and made cleaner later.
 2734 if self.args.past_index >= 0:
File /opt/conda/envs/llama_cona_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1501, in Module._call_impl(self, *args, **kwargs)
 1496 # If we don't have any hooks, we want to skip the rest of the logic in
 1497 # this function, and just call forward.
 1498 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks
 1499 or _global_backward_pre_hooks or _global_backward_hooks
 1500 or _global_forward_hooks or _global_forward_pre_hooks):
 -> 1501 return forward_call(*args, **kwargs)
 1502 # Do not call functions when jit is used
 1503 full_backward_hooks, non_full_backward_hooks = [], []
File /opt/conda/envs/llama_cona_env/lib/python3.8/site-packages/peft/peft_model.py:947, in PeftModelForCausalLM.forward(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, **kwargs)
 936 raise AssertionError(""forward in MPTForCausalLM does not support inputs_embeds"")
 937 return self.base_model(
 938 input_ids=input_ids,
 939 attention_mask=attention_mask,
 (...)
 944 **kwargs,
 945 )
 --> 947 return self.base_model(
 948 input_ids=input_ids,
 949 attention_mask=attention_mask,
 950 inputs_embeds=inputs_embeds,
 951 labels=labels,
 952 output_attentions=output_attentions,
 953 output_hidden_states=output_hidden_states,
 954 return_dict=return_dict,
 955 **kwargs,
 956 )
 958 batch_size = input_ids.shape[0]
 959 if attention_mask is not None:
 960 # concat prompt attention mask
File /opt/conda/envs/llama_cona_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1501, in Module._call_impl(self, *args, **kwargs)
 1496 # If we don't have any hooks, we want to skip the rest of the logic in
 1497 # this function, and just call forward.
 1498 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks
 1499 or _global_backward_pre_hooks or _global_backward_hooks
 1500 or _global_forward_hooks or _global_forward_pre_hooks):
 -> 1501 return forward_call(*args, **kwargs)
 1502 # Do not call functions when jit is used
 1503 full_backward_hooks, non_full_backward_hooks = [], []
File /opt/conda/envs/llama_cona_env/lib/python3.8/site-packages/accelerate/hooks.py:165, in add_hook_to_module..new_forward(*args, **kwargs)
 163 output = old_forward(*args, **kwargs)
 164 else:
 --> 165 output = old_forward(*args, **kwargs)
 166 return module._hf_hook.post_forward(module, output)
File /opt/conda/envs/llama_cona_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:709, in LlamaForCausalLM.forward(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)
 707 # Flatten the tokens
 708 loss_fct = CrossEntropyLoss()
 --> 709 shift_logits = shift_logits.view(-1, self.config.vocab_size)
 710 shift_labels = shift_labels.view(-1)
 711 # Enable model parallelism
RuntimeError: shape '[-1, 32000]' is invalid for input of size 131073504
​
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/63","Unable to run this morning, yesterday it ran fine","2023-08-29T16:43:18Z","Open issue","model-usage","Morning! I need help getting the models to run a second time, on a new instance.
Yesterday, I registered for and downloaded the models onto an AWS sagemaker instance. Everything worked fine and I was able to run
pip install -e .
And from there experiment with the models. I shut down the instance and this morning started it again. I reran the pip installation, but now, everything hangs at this step:
sh-4.2$ torchrun --nproc_per_node 4 example_instructions.py     --ckpt_dir CodeLlama-34b-Instruct/     --tokenizer_path CodeLlama-34b-Instruct/tokenizer.model     --max_seq_len 512 --max_batch_size 4
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
> initializing model parallel with size 4
> initializing ddp with size 1
> initializing pipeline with size 1

This same code would finish loading the model after 8 seconds or so and be good to go. I've tried this with the 7b instruct model, the 13b instruct, and the 34b instruct; all worked fine yesterday, none work today.
How can I make this work? Did I forget some crucial step?
For the rest of this bug report, it's basically how I arrived at the conclusion that
checkpoint = torch.load(ckpt_path, map_location=""cpu"")
Is not working, and I'm not sure why. Once I get to that point, the RAM usage rises from 1.8GB to 28.9GB, so it looks like it's at least found the first file in the checkpoint. This instance g5.12xlarge has 196GB and 4 24GB GPUs (and everything worked yesterday).
To figure this all out, I went into generation.py in the llama directory, and I added in some line number inspections. I added in lines like:
from inspect import getframeinfo, currentframe

print(f""Got to {getframeinfo(currentframe()).lineno}"")

The code in generation now looks like:
        print(f""Got to {getframeinfo(currentframe()).lineno}"")
        assert len(checkpoints) > 0, f""no checkpoint files found in {ckpt_dir}""
        assert model_parallel_size == len(
            checkpoints
        ), f""Loading a checkpoint for MP={len(checkpoints)} but world size is {model_parallel_size}""
        
        print(f""Got to {getframeinfo(currentframe()).lineno}"")
        ckpt_path = checkpoints[get_model_parallel_rank()]
        print(f""Got to {getframeinfo(currentframe()).lineno}"")
        checkpoint = torch.load(ckpt_path, map_location=""cpu"")
        print(f""Got to {getframeinfo(currentframe()).lineno}"")
        with open(Path(ckpt_dir) / ""params.json"", ""r"") as f:
            params = json.loads(f.read())
        print(f""Got to {getframeinfo(currentframe()).lineno}"")
        model_args: ModelArgs = ModelArgs(
            max_seq_len=max_seq_len,
            max_batch_size=max_batch_size,
            **params,
        )
        tokenizer = Tokenizer(model_path=tokenizer_path)
        model_args.vocab_size = tokenizer.n_words
        print(f""Got to {getframeinfo(currentframe()).lineno}"")

and the run output looks like:
sh-4.2$ torchrun --nproc_per_node 4 example_instructions.py     --ckpt_dir CodeLlama-34b-Instruct/     --tokenizer_path CodeLlama-34b-Instruct/tokenizer.model     --max_seq_len 512 --max_batch_size 4
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
> initializing model parallel with size 4
> initializing ddp with size 1
> initializing pipeline with size 1
Got to 86
Got to 92
Got to 94

which is checkpoint = torch.load(ckpt_path, map_location=""cpu"")
My pip freeze:
sh-4.2$ pip freeze
aiobotocore @ file:///home/conda/feedstock_root/build_artifacts/aiobotocore_1691451276487/work
aiofiles==22.1.0
aiohttp @ file:///home/conda/feedstock_root/build_artifacts/aiohttp_1689804989077/work
aioitertools @ file:///home/conda/feedstock_root/build_artifacts/aioitertools_1663521246073/work
aiosignal @ file:///home/conda/feedstock_root/build_artifacts/aiosignal_1667935791922/work
aiosqlite==0.19.0
anyio @ file:///home/conda/feedstock_root/build_artifacts/anyio_1688651106312/work/dist
argon2-cffi @ file:///home/conda/feedstock_root/build_artifacts/argon2-cffi_1640817743617/work
argon2-cffi-bindings @ file:///home/conda/feedstock_root/build_artifacts/argon2-cffi-bindings_1666850768662/work
arrow @ file:///home/conda/feedstock_root/build_artifacts/arrow_1662382474514/work
astroid==2.15.6
asttokens @ file:///home/conda/feedstock_root/build_artifacts/asttokens_1670263926556/work
async-timeout @ file:///home/conda/feedstock_root/build_artifacts/async-timeout_1691763562544/work
attrs @ file:///home/conda/feedstock_root/build_artifacts/attrs_1683424013410/work
autopep8==2.0.2
autovizwidget @ file:///home/conda/feedstock_root/build_artifacts/autovizwidget_1680800327357/work
awscli==1.29.28
Babel==2.12.1
backcall @ file:///home/conda/feedstock_root/build_artifacts/backcall_1592338393461/work
backports.functools-lru-cache @ file:///home/conda/feedstock_root/build_artifacts/backports.functools_lru_cache_1687772187254/work
beautifulsoup4 @ file:///home/conda/feedstock_root/build_artifacts/beautifulsoup4_1680888073205/work
bleach @ file:///home/conda/feedstock_root/build_artifacts/bleach_1674535352125/work
boto3==1.28.28
botocore==1.31.28
brotlipy @ file:///home/conda/feedstock_root/build_artifacts/brotlipy_1666764671472/work
cached-property @ file:///home/conda/feedstock_root/build_artifacts/cached_property_1615209429212/work
certifi==2023.7.22
cffi @ file:///home/conda/feedstock_root/build_artifacts/cffi_1671179353105/work
charset-normalizer @ file:///home/conda/feedstock_root/build_artifacts/charset-normalizer_1688813409104/work
cloudpickle==2.2.1
cmake==3.27.2
-e git+ssh://git@github.com/facebookresearch/codellama.git@cb51c14ec761370ba2e2bc351374a79265d0465e#egg=codellama
colorama==0.4.4
comm @ file:///home/conda/feedstock_root/build_artifacts/comm_1691044910542/work
contextlib2==21.6.0
cryptography @ file:///home/conda/feedstock_root/build_artifacts/cryptography-split_1672672382195/work
debugpy @ file:///home/conda/feedstock_root/build_artifacts/debugpy_1691021228385/work
decorator @ file:///home/conda/feedstock_root/build_artifacts/decorator_1641555617451/work
defusedxml @ file:///home/conda/feedstock_root/build_artifacts/defusedxml_1615232257335/work
dill==0.3.7
docker==6.1.3
docstring-to-markdown==0.12
docutils==0.16
entrypoints @ file:///home/conda/feedstock_root/build_artifacts/entrypoints_1643888246732/work
environment-kernels==1.2.0
exceptiongroup @ file:///home/conda/feedstock_root/build_artifacts/exceptiongroup_1692026125334/work
executing @ file:///home/conda/feedstock_root/build_artifacts/executing_1667317341051/work
fairscale==0.4.13
fastjsonschema @ file:///home/conda/feedstock_root/build_artifacts/python-fastjsonschema_1690055433477/work/dist
filelock==3.12.3
fire==0.5.0
flit_core @ file:///home/conda/feedstock_root/build_artifacts/flit-core_1684084314667/work/source/flit_core
fqdn @ file:///home/conda/feedstock_root/build_artifacts/fqdn_1638810296540/work/dist
frozenlist @ file:///home/conda/feedstock_root/build_artifacts/frozenlist_1689244399117/work
fsspec @ file:///home/conda/feedstock_root/build_artifacts/fsspec_1626188337504/work
gitdb==4.0.10
GitPython==3.1.32
google-pasta==0.2.0
hdijupyterutils @ file:///home/conda/feedstock_root/build_artifacts/hdijupyterutils_1680800332182/work
idna @ file:///home/conda/feedstock_root/build_artifacts/idna_1663625384323/work
importlib-metadata==6.8.0
importlib-resources @ file:///home/conda/feedstock_root/build_artifacts/importlib_resources_1691408075105/work
ipykernel==5.5.6
ipython @ file:///home/conda/feedstock_root/build_artifacts/ipython_1685727741709/work
ipython-genutils==0.2.0
ipywidgets @ file:///home/conda/feedstock_root/build_artifacts/ipywidgets_1690877070294/work
isoduration @ file:///home/conda/feedstock_root/build_artifacts/isoduration_1638811571363/work/dist
isort==5.12.0
jedi==0.18.2
Jinja2 @ file:///home/conda/feedstock_root/build_artifacts/jinja2_1654302431367/work
jmespath @ file:///home/conda/feedstock_root/build_artifacts/jmespath_1655568249366/work
json5==0.9.14
jsonpointer==2.0
jsonschema @ file:///home/conda/feedstock_root/build_artifacts/jsonschema-meta_1691761378595/work
jsonschema-specifications @ file:///home/conda/feedstock_root/build_artifacts/jsonschema-specifications_1689701150890/work
jupyter @ file:///home/conda/feedstock_root/build_artifacts/jupyter_1670249595582/work
jupyter-console @ file:///home/conda/feedstock_root/build_artifacts/jupyter_console_1678118109161/work
jupyter-events @ file:///home/conda/feedstock_root/build_artifacts/jupyter_events_1691505939576/work
jupyter-lsp==2.2.0
jupyter-server-mathjax==0.2.6
jupyter-server-proxy @ git+https://github.com/jupyterhub/jupyter-server-proxy@2d7dd346bb595106b417476de870a348943f3c70
jupyter-ydoc==0.2.5
jupyter_client==7.4.9
jupyter_core @ file:///home/conda/feedstock_root/build_artifacts/jupyter_core_1686775611663/work
jupyter_server @ file:///home/conda/feedstock_root/build_artifacts/jupyter_server_1692108700252/work
jupyter_server_fileid==0.9.0
jupyter_server_terminals @ file:///home/conda/feedstock_root/build_artifacts/jupyter_server_terminals_1673491454549/work
jupyter_server_ydoc==0.8.0
jupyterlab==3.6.5
jupyterlab-git==0.41.0
jupyterlab-lsp==4.2.0
jupyterlab-pygments @ file:///home/conda/feedstock_root/build_artifacts/jupyterlab_pygments_1649936611996/work
jupyterlab-widgets @ file:///home/conda/feedstock_root/build_artifacts/jupyterlab_widgets_1688489450369/work
jupyterlab_server==2.24.0
lazy-object-proxy==1.9.0
lit==16.0.6
MarkupSafe @ file:///home/conda/feedstock_root/build_artifacts/markupsafe_1685769049201/work
matplotlib-inline @ file:///home/conda/feedstock_root/build_artifacts/matplotlib-inline_1660814786464/work
mccabe==0.7.0
mistune @ file:///home/conda/feedstock_root/build_artifacts/mistune_1675771498296/work
mock @ file:///home/conda/feedstock_root/build_artifacts/mock_1689092066756/work
mpmath==1.3.0
multidict @ file:///home/conda/feedstock_root/build_artifacts/multidict_1672339403932/work
multiprocess==0.70.15
nb-conda @ file:///home/conda/feedstock_root/build_artifacts/nb_conda_1654442778977/work
nb-conda-kernels @ file:///home/conda/feedstock_root/build_artifacts/nb_conda_kernels_1667060632461/work
nbclassic @ file:///home/conda/feedstock_root/build_artifacts/nbclassic_1675369808718/work
nbclient @ file:///home/conda/feedstock_root/build_artifacts/nbclient_1684790896106/work
nbconvert @ file:///home/conda/feedstock_root/build_artifacts/nbconvert-meta_1674590374792/work
nbdime==3.2.1
nbexamples @ file:///opt/nbexamples
nbformat @ file:///home/conda/feedstock_root/build_artifacts/nbformat_1690814868471/work
nest-asyncio @ file:///home/conda/feedstock_root/build_artifacts/nest-asyncio_1664684991461/work
networkx==3.1
nose @ file:///home/conda/feedstock_root/build_artifacts/nose_1602434998960/work
notebook @ file:///home/conda/feedstock_root/build_artifacts/notebook_1691436218243/work
notebook_shim @ file:///home/conda/feedstock_root/build_artifacts/notebook-shim_1682360583588/work
numpy @ file:///home/conda/feedstock_root/build_artifacts/numpy_1691056231492/work
nvidia-cublas-cu11==11.10.3.66
nvidia-cuda-cupti-cu11==11.7.101
nvidia-cuda-nvrtc-cu11==11.7.99
nvidia-cuda-runtime-cu11==11.7.99
nvidia-cudnn-cu11==8.5.0.96
nvidia-cufft-cu11==10.9.0.58
nvidia-curand-cu11==10.2.10.91
nvidia-cusolver-cu11==11.4.0.1
nvidia-cusparse-cu11==11.7.4.91
nvidia-nccl-cu11==2.14.3
nvidia-nvtx-cu11==11.7.91
overrides @ file:///home/conda/feedstock_root/build_artifacts/overrides_1691338815398/work
packaging @ file:///home/conda/feedstock_root/build_artifacts/packaging_1681337016113/work
pandas @ file:///home/conda/feedstock_root/build_artifacts/pandas_1688740542634/work
pandocfilters @ file:///home/conda/feedstock_root/build_artifacts/pandocfilters_1631603243851/work
parso @ file:///home/conda/feedstock_root/build_artifacts/parso_1638334955874/work
pathos==0.3.1
pexpect @ file:///home/conda/feedstock_root/build_artifacts/pexpect_1667297516076/work
pickleshare @ file:///home/conda/feedstock_root/build_artifacts/pickleshare_1602536217715/work
pid==3.0.4
pkgutil_resolve_name @ file:///home/conda/feedstock_root/build_artifacts/pkgutil-resolve-name_1633981968097/work
platformdirs @ file:///home/conda/feedstock_root/build_artifacts/platformdirs_1690813113769/work
plotly @ file:///home/conda/feedstock_root/build_artifacts/plotly_1692220561510/work
pluggy==1.2.0
pox==0.3.3
ppft==1.7.6.7
prometheus-client @ file:///home/conda/feedstock_root/build_artifacts/prometheus_client_1689032443210/work
prompt-toolkit @ file:///home/conda/feedstock_root/build_artifacts/prompt-toolkit_1688565951714/work
protobuf==4.23.4
psutil @ file:///home/conda/feedstock_root/build_artifacts/psutil_1681775027942/work
psycopg2 @ file:///home/conda/feedstock_root/build_artifacts/psycopg2-split_1667025517155/work
ptyprocess @ file:///home/conda/feedstock_root/build_artifacts/ptyprocess_1609419310487/work/dist/ptyprocess-0.7.0-py2.py3-none-any.whl
pure-eval @ file:///home/conda/feedstock_root/build_artifacts/pure_eval_1642875951954/work
py4j==0.10.9.5
pyasn1==0.5.0
pycodestyle==2.10.0
pycparser @ file:///home/conda/feedstock_root/build_artifacts/pycparser_1636257122734/work
pydocstyle==6.3.0
pyflakes==3.0.1
pygal==3.0.0
Pygments @ file:///home/conda/feedstock_root/build_artifacts/pygments_1691408637400/work
pykerberos @ file:///home/conda/feedstock_root/build_artifacts/pykerberos_1671204518513/work
pylint==2.17.5
pyOpenSSL @ file:///home/conda/feedstock_root/build_artifacts/pyopenssl_1685514481738/work
PyQt5==5.12.3
PyQt5_sip==4.19.18
PyQtChart==5.12
PyQtWebEngine==5.12.1
PySocks @ file:///home/conda/feedstock_root/build_artifacts/pysocks_1661604839144/work
pyspark==3.3.0
python-dateutil @ file:///home/conda/feedstock_root/build_artifacts/python-dateutil_1626286286081/work
python-json-logger @ file:///home/conda/feedstock_root/build_artifacts/python-json-logger_1677079630776/work
python-lsp-jsonrpc==1.0.0
python-lsp-server==1.7.4
pytoolconfig==1.2.5
pytz @ file:///home/conda/feedstock_root/build_artifacts/pytz_1680088766131/work
PyYAML @ file:///home/conda/feedstock_root/build_artifacts/pyyaml_1666772395347/work
pyzmq @ file:///home/conda/feedstock_root/build_artifacts/pyzmq_1666828497229/work
qtconsole @ file:///home/conda/feedstock_root/build_artifacts/qtconsole-base_1683329453903/work
QtPy @ file:///home/conda/feedstock_root/build_artifacts/qtpy_1680148448366/work
referencing @ file:///home/conda/feedstock_root/build_artifacts/referencing_1691337268233/work
requests @ file:///home/conda/feedstock_root/build_artifacts/requests_1684774241324/work
requests-kerberos @ file:///home/conda/feedstock_root/build_artifacts/requests-kerberos_1667464887610/work
rfc3339-validator @ file:///home/conda/feedstock_root/build_artifacts/rfc3339-validator_1638811747357/work
rfc3986-validator @ file:///home/conda/feedstock_root/build_artifacts/rfc3986-validator_1598024191506/work
rope==1.9.0
rpds-py @ file:///home/conda/feedstock_root/build_artifacts/rpds-py_1689705060450/work
rsa==4.7.2
s3fs @ file:///home/conda/feedstock_root/build_artifacts/s3fs_1626193591467/work
s3transfer @ file:///home/conda/feedstock_root/build_artifacts/s3transfer_1692149178344/work
sagemaker==2.177.1
sagemaker-experiments==0.1.45
sagemaker-nbi-agent @ file:///opt/sagemaker_nbi_agent
sagemaker-pyspark==1.4.5
schema==0.7.5
Send2Trash @ file:///home/conda/feedstock_root/build_artifacts/send2trash_1682601222253/work
sentencepiece==0.1.99
simpervisor==1.0.0
six @ file:///home/conda/feedstock_root/build_artifacts/six_1620240208055/work
smdebug-rulesconfig==1.0.1
smmap==5.0.0
sniffio @ file:///home/conda/feedstock_root/build_artifacts/sniffio_1662051266223/work
snowballstemmer==2.2.0
soupsieve @ file:///home/conda/feedstock_root/build_artifacts/soupsieve_1658207591808/work
sparkmagic @ file:///home/conda/feedstock_root/build_artifacts/sparkmagic_1680849855330/work/sparkmagic
stack-data @ file:///home/conda/feedstock_root/build_artifacts/stack_data_1669632077133/work
sympy==1.12
tblib==1.7.0
tenacity @ file:///home/conda/feedstock_root/build_artifacts/tenacity_1692026804430/work
termcolor==2.3.0
terminado @ file:///home/conda/feedstock_root/build_artifacts/terminado_1670253674810/work
tinycss2 @ file:///home/conda/feedstock_root/build_artifacts/tinycss2_1666100256010/work
tomli==2.0.1
tomlkit==0.12.1
torch==2.0.1
tornado @ file:///home/conda/feedstock_root/build_artifacts/tornado_1684150054582/work
traitlets @ file:///home/conda/feedstock_root/build_artifacts/traitlets_1675110562325/work
triton==2.0.0
typing-utils @ file:///home/conda/feedstock_root/build_artifacts/typing_utils_1622899189314/work
typing_extensions @ file:///home/conda/feedstock_root/build_artifacts/typing_extensions_1688315532570/work
tzdata @ file:///home/conda/feedstock_root/build_artifacts/python-tzdata_1680081134351/work
ujson==5.8.0
uri-template @ file:///home/conda/feedstock_root/build_artifacts/uri-template_1688655812972/work/dist
urllib3==1.26.14
wcwidth @ file:///home/conda/feedstock_root/build_artifacts/wcwidth_1673864653149/work
webcolors @ file:///home/conda/feedstock_root/build_artifacts/webcolors_1679900785843/work
webencodings==0.5.1
websocket-client @ file:///home/conda/feedstock_root/build_artifacts/websocket-client_1687789148259/work
widgetsnbextension @ file:///home/conda/feedstock_root/build_artifacts/widgetsnbextension_1688504439014/work
wrapt @ file:///home/conda/feedstock_root/build_artifacts/wrapt_1677485519705/work
y-py==0.6.0
yarl @ file:///home/conda/feedstock_root/build_artifacts/yarl_1685191749966/work
ypy-websocket==0.8.4
zipp @ file:///home/conda/feedstock_root/build_artifacts/zipp_1689374466814/work

 The text was updated successfully, but these errors were encountered: 
👀1
GaganHonor reacted with eyes emoji
All reactions
👀1 reaction"
"https://github.com/meta-llama/codellama/issues/62","Remembering the previous context","2023-09-12T08:43:55Z","Closed issue","No label","Hi everyone! Is it possible to make long dialogue with the Instruct model? In other words, to make the model remember a previous context.
For now, I have an idea to paste the previous prompt and the model's response into the new request. Are there more concise and easier ways?
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/61","unable to download the model weights","2023-08-31T04:00:40Z","Closed issue","No label","I was unable to download the authorized model within the 24-hour timeframe specified in the authorization email.
--2023-08-29 22:06:38-- https://download2.llamameta.net/LICENSE?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaX......Request-ID=724013916200119
 Resolving download2.llamameta.net (download2.llamameta.net)... 99.84.133.49, 99.84.133.23, 99.84.133.48, ...
 Connecting to download2.llamameta.net (download2.llamameta.net)|99.84.133.49|:443... connected.
 HTTP request sent, awaiting response... 200 OK
 Length: 7020 (6.9K) [binary/octet-stream]
 Saving to: './LICENSE'
./LICENSE 100%[===================================================================================================================>] 6.86K --.-KB/s in 0s
2023-08-29 22:06:39 (163 MB/s) - './LICENSE' saved [7020/7020]
--2023-08-29 22:06:39-- https://download2.llamameta.net/LICENSE?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaX......&Request-ID=724013916200119
 Resolving download2.llamameta.net (download2.llamameta.net)... 99.84.133.49, 99.84.133.23, 99.84.133.48, ...
 Connecting to download2.llamameta.net (download2.llamameta.net)|99.84.133.49|:443... connected.
 HTTP request sent, awaiting response... 200 OK
 Length: 4790 (4.7K) [text/markdown]
 Saving to: './USE_POLICY.md'
./USE_POLICY.md 100%[===================================================================================================================>] 4.68K --.-KB/s in 0s
2023-08-29 22:06:39 (139 MB/s) - './USE_POLICY.md' saved [4790/4790]
Downloading tokenizer
 --2023-08-29 22:06:39-- https://download2.llamameta.net/LICENSE?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaX......&Request-ID=724013916200119
 Resolving download2.llamameta.net (download2.llamameta.net)... 99.84.133.49, 99.84.133.23, 99.84.133.48, ...
 Connecting to download2.llamameta.net (download2.llamameta.net)|99.84.133.49|:443... connected.
 HTTP request sent, awaiting response... 403 Forbidden
 2023-08-29 22:06:41 ERROR 403: Forbidden.
--2023-08-29 22:06:41-- https://download2.llamameta.net/LICENSE?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaX......&Request-ID=724013916200119
 Reusing existing connection to download2.llamameta.net:443.
 HTTP request sent, awaiting response... 403 Forbidden
 2023-08-29 22:06:41 ERROR 403: Forbidden.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/60","torchrun --nproc_per_node 2 example_instructions.py --ckpt_dir CodeLlama-13b-Instruct/ --tokenizer_path CodeLlama-13b-Instruct/tokenizer.model --max_seq_len 8192 --max_batch_size 4","2023-08-29T13:36:25Z","Open issue","model-usage","WARNING:torch.distributed.run:
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.
initializing model parallel with size 2
 initializing ddp with size 1
 initializing pipeline with size 1
 Traceback (most recent call last):
 File ""/home/azureuser/codellama/example_instructions.py"", line 68, in 
 fire.Fire(main)
 File ""/home/azureuser/.local/lib/python3.10/site-packages/fire/core.py"", line 141, in Fire
 component_trace = _Fire(component, args, parsed_flag_args, context, name)
 File ""/home/azureuser/.local/lib/python3.10/site-packages/fire/core.py"", line 475, in _Fire
 component, remaining_args = _CallAndUpdateTrace(
 File ""/home/azureuser/.local/lib/python3.10/site-packages/fire/core.py"", line 691, in _CallAndUpdateTrace
 component = fn(*varargs, **kwargs)
 File ""/home/azureuser/codellama/example_instructions.py"", line 20, in main
 generator = Llama.build(
 File ""/home/azureuser/codellama/llama/generation.py"", line 90, in build
 checkpoint = torch.load(ckpt_path, map_location=""cpu"")
 File ""/home/azureuser/.local/lib/python3.10/site-packages/torch/serialization.py"", line 815, in load
 return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)
 File ""/home/azureuser/.local/lib/python3.10/site-packages/torch/serialization.py"", line 1033, in _legacy_load
 magic_number = pickle_module.load(f, **pickle_load_args)
 _pickle.UnpicklingError: invalid load key, '<'.
 Traceback (most recent call last):
 File ""/home/azureuser/codellama/example_instructions.py"", line 68, in 
 fire.Fire(main)
 File ""/home/azureuser/.local/lib/python3.10/site-packages/fire/core.py"", line 141, in Fire
 component_trace = _Fire(component, args, parsed_flag_args, context, name)
 File ""/home/azureuser/.local/lib/python3.10/site-packages/fire/core.py"", line 475, in _Fire
 component, remaining_args = _CallAndUpdateTrace(
 File ""/home/azureuser/.local/lib/python3.10/site-packages/fire/core.py"", line 691, in _CallAndUpdateTrace
 component = fn(*varargs, **kwargs)
 File ""/home/azureuser/codellama/example_instructions.py"", line 20, in main
 generator = Llama.build(
 File ""/home/azureuser/codellama/llama/generation.py"", line 75, in build
 torch.cuda.set_device(local_rank)
 File ""/home/azureuser/.local/lib/python3.10/site-packages/torch/cuda/init.py"", line 350, in set_device
 torch._C._cuda_setDevice(device)
 RuntimeError: CUDA error: invalid device ordinal
 Compile with TORCH_USE_CUDA_DSA to enable device-side assertions.
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 14881) of binary: /usr/bin/python3
 Traceback (most recent call last):
 File ""/home/azureuser/.local/bin/torchrun"", line 8, in 
 sys.exit(main())
 File ""/home/azureuser/.local/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/init.py"", line 346, in wrapper
 return f(*args, **kwargs)
 File ""/home/azureuser/.local/lib/python3.10/site-packages/torch/distributed/run.py"", line 794, in main
 run(args)
 File ""/home/azureuser/.local/lib/python3.10/site-packages/torch/distributed/run.py"", line 785, in run
 elastic_launch(
 File ""/home/azureuser/.local/lib/python3.10/site-packages/torch/distributed/launcher/api.py"", line 134, in call
 return launch_agent(self._config, self._entrypoint, list(args))
 File ""/home/azureuser/.local/lib/python3.10/site-packages/torch/distributed/launcher/api.py"", line 250, in launch_agent
 raise ChildFailedError(
 torch.distributed.elastic.multiprocessing.errors.ChildFailedError:
example_instructions.py FAILED
Failures:
 [1]:
 time : 2023-08-29_13:34:23
 host : llm.internal.cloudapp.net
 rank : 1 (local_rank: 1)
 exitcode : 1 (pid: 14882)
 error_file: <N/A>
 traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
Root Cause (first observed failure):
 [0]:
 time : 2023-08-29_13:34:23
 host : llm.internal.cloudapp.net
 rank : 0 (local_rank: 0)
 exitcode : 1 (pid: 14881)
 error_file: <N/A>
 traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/59","Invalid load key error","2023-08-29T11:41:02Z","Open issue","model-usage","Cmd line:
torchrun --nproc_per_node 1 example_infilling.py --ckpt_dir CodeLlama-7b-Instruct/ --tokenizer_path CodeLlama-7b-Instruct/tokenizer.model --max_seq_len 512 -- max_batch_size 4
Error Raised
> initializing model parallel with size 1
> initializing ddp with size 1
> initializing pipeline with size 1
Traceback (most recent call last):
  File ""/home/fran/codellama/example_infilling.py"", line 79, in <module>
    fire.Fire(main)
  File ""/home/fran/miniconda3/envs/cs-gpt/lib/python3.10/site-packages/fire/core.py"", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File ""/home/fran/miniconda3/envs/cs-gpt/lib/python3.10/site-packages/fire/core.py"", line 475, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
  File ""/home/fran/miniconda3/envs/cs-gpt/lib/python3.10/site-packages/fire/core.py"", line 691, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File ""/home/fran/codellama/example_infilling.py"", line 18, in main
    generator = Llama.build(
  File ""/home/fran/codellama/llama/generation.py"", line 90, in build
    checkpoint = torch.load(ckpt_path, map_location=""cpu"")
  File ""/home/fran/miniconda3/envs/cs-gpt/lib/python3.10/site-packages/torch/serialization.py"", line 815, in load
    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)
  File ""/home/fran/miniconda3/envs/cs-gpt/lib/python3.10/site-packages/torch/serialization.py"", line 1033, in _legacy_load
    magic_number = pickle_module.load(f, **pickle_load_args)
_pickle.UnpicklingError: invalid load key, '<'.
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 463834) of binary: /home/fran/miniconda3/envs/cs-gpt/bin/python
Traceback (most recent call last):
  File ""/home/fran/miniconda3/envs/cs-gpt/bin/torchrun"", line 8, in <module>
    sys.exit(main())
  File ""/home/fran/miniconda3/envs/cs-gpt/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py"", line 346, in wrapper
    return f(*args, **kwargs)
  File ""/home/fran/miniconda3/envs/cs-gpt/lib/python3.10/site-packages/torch/distributed/run.py"", line 794, in main
    run(args)
  File ""/home/fran/miniconda3/envs/cs-gpt/lib/python3.10/site-packages/torch/distributed/run.py"", line 785, in run
    elastic_launch(
  File ""/home/fran/miniconda3/envs/cs-gpt/lib/python3.10/site-packages/torch/distributed/launcher/api.py"", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File ""/home/fran/miniconda3/envs/cs-gpt/lib/python3.10/site-packages/torch/distributed/launcher/api.py"", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError:
============================================================
example_infilling.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-08-29_20:39:20
  host      : fran.rtzr.ai
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 463834)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================

 The text was updated successfully, but these errors were encountered: 
😕1
mhamra reacted with confused emoji
All reactions
😕1 reaction"
"https://github.com/meta-llama/codellama/issues/58","Installed md5sum, but there is an error","2023-08-29T07:15:34Z","Open issue","download-install","I am using Ubuntu 22.04 LTS. I followed the instructions in the readme.md to run download.sh and selected all the models. The installation went smoothly at the beginning until it reached CodeLlama-34b/checklist.chk, where I encountered an md5sum error: md5sum: checklist.chk: No such file or directory, but when I checked with md5sum --version, it was indeed installed. Can you please tell me what steps I might have missed?
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/57","Meta","2023-08-29T04:19:18Z","Closed as not planned issue","No label","No description provided.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/56","Sponsored issue: LLama2 Support Request","2023-09-06T18:26:45Z","Closed issue","No label","Priority Support
@AhmedHashem2104 is using Mintycode to fund this issue.
You can receive if you provide priority support to @AhmedHashem2104.
To view the support request and terms go to Mintycode.
Thank you in advance for helping.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/55","Can't run examples on Windows 10","2023-08-28T15:42:24Z","Open issue","compability","Hi,
 I've tried to run the examples, but I received this error.
(CodeLlama) PS C:\Users\marce\OneDrive\mah-docs\CodeLlama\codellama> python -m torch.distributed.run --nproc_per_node 1 example_infilling.py --ckpt_dir CodeLlama-7b-Python --tokenizer_path ./CodeLlama-7b-Python/tokenizer.model
NOTE: Redirects are currently not supported in Windows or MacOs.
[W C:\cb\pytorch_1000000000000\work\torch\csrc\distributed\c10d\socket.cpp:601] [c10d] The client socket has failed to connect to [kubernetes.docker.internal]:29500 (system error: 10049 - unknown error).
[W C:\cb\pytorch_1000000000000\work\torch\csrc\distributed\c10d\socket.cpp:601] [c10d] The client socket has failed to connect to [kubernetes.docker.internal]:29500 (system error: 10049 - unknown error).
[W C:\cb\pytorch_1000000000000\work\torch\csrc\distributed\c10d\socket.cpp:601] [c10d] The client socket has failed to connect to [kubernetes.docker.internal]:29500 (system error: 10049 - unknown error).
[W C:\cb\pytorch_1000000000000\work\torch\csrc\distributed\c10d\socket.cpp:601] [c10d] The client socket has failed to connect to [kubernetes.docker.internal]:29500 (system error: 10049 - unknown error).
> initializing model parallel with size 1
> initializing ddp with size 1
> initializing pipeline with size 1
Traceback (most recent call last):
  File ""C:\Users\marce\OneDrive\mah-docs\CodeLlama\codellama\example_infilling.py"", line 79, in <module>
    fire.Fire(main)
  File ""C:\ProgramData\anaconda3\envs\CodeLlama\Lib\site-packages\fire\core.py"", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\ProgramData\anaconda3\envs\CodeLlama\Lib\site-packages\fire\core.py"", line 475, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
                                ^^^^^^^^^^^^^^^^^^^^
  File ""C:\ProgramData\anaconda3\envs\CodeLlama\Lib\site-packages\fire\core.py"", line 691, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\marce\OneDrive\mah-docs\CodeLlama\codellama\example_infilling.py"", line 18, in main
    generator = Llama.build(
                ^^^^^^^^^^^^
  File ""C:\Users\marce\OneDrive\mah-docs\CodeLlama\codellama\llama\generation.py"", line 90, in build
    checkpoint = torch.load(ckpt_path, map_location=""cpu"")
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\ProgramData\anaconda3\envs\CodeLlama\Lib\site-packages\torch\serialization.py"", line 815, in load
    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\ProgramData\anaconda3\envs\CodeLlama\Lib\site-packages\torch\serialization.py"", line 1033, in _legacy_load
    magic_number = pickle_module.load(f, **pickle_load_args)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_pickle.UnpicklingError: invalid load key, '<'.
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 18284) of binary: C:\ProgramData\anaconda3\envs\CodeLlama\python.exe
Traceback (most recent call last):
  File ""<frozen runpy>"", line 198, in _run_module_as_main
  File ""<frozen runpy>"", line 88, in _run_code
  File ""C:\ProgramData\anaconda3\envs\CodeLlama\Lib\site-packages\torch\distributed\run.py"", line 798, in <module>
    main()
  File ""C:\ProgramData\anaconda3\envs\CodeLlama\Lib\site-packages\torch\distributed\elastic\multiprocessing\errors\__init__.py"", line 346, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File ""C:\ProgramData\anaconda3\envs\CodeLlama\Lib\site-packages\torch\distributed\run.py"", line 794, in main
    run(args)
  File ""C:\ProgramData\anaconda3\envs\CodeLlama\Lib\site-packages\torch\distributed\run.py"", line 785, in run
    elastic_launch(
  File ""C:\ProgramData\anaconda3\envs\CodeLlama\Lib\site-packages\torch\distributed\launcher\api.py"", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\ProgramData\anaconda3\envs\CodeLlama\Lib\site-packages\torch\distributed\launcher\api.py"", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError:
============================================================
example_infilling.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-08-28_12:39:51
  host      : DESKTOP-THP4I5R
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 18284)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs

 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/54","how to use this model? Is it same as hf version?","2023-08-28T10:58:01Z","Open issue","download-install,question","How to use this model? Is it same as hf version?
From README, I know I must use torchrun to run example, is it possible to run by python? how to write an example that can be run by python not torchrun? Recently all my searched results are to use hf version, is it possible to use original download model?
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/53","Programming Languages Support","2023-08-28T13:16:00Z","Closed issue","No label","Is there any chart or breakdown of % programming languages used in training data for the base/instruct models?
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/51","Enhancement on file: download.sh","2023-08-28T08:51:07Z","Open issue","download-install,enhancement","Changes :
Prerequisite Checks: Added a function check_prerequisites to verify if wget and md5sum are installed. It also offers to install these packages if they are missing.
Log Function: Introduced a log function to handle all output messages. This makes it easier to control the output format.
Color Coding: Added color coding to output messages for better visual differentiation. Green is used for success messages, red for errors, and yellow for ongoing processes.
Download Function: Introduced a download_file function that abstracts the download logic. It uses wget with the --quiet and --show-progress flags.
Model Download Function: Introduced a download_model function to handle the downloading of individual models. This improves code modularity and readability.
Checksum Verification: Added a visible checksum verification step after each model is downloaded. The checksum verification was previously silent; now it explicitly logs whether each file is OK or not.
User Prompts: Revised user prompts for better clarity, including providing example inputs for the list of models.
Press 'c' to Cancel: Added an option for the user to cancel the operation after entering the models to download.
Code Comments: Introduced comments to explain critical sections of the code, improving readability and maintainability.
Silent Checksum Verification: Used --quiet flag for md5sum to reduce noise in the output.
Multi-shard Download: Adjusted the download logic to handle multi-shard models by looping through each shard.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/50","Can't run any inference","2023-08-28T08:03:06Z","Open issue","model-usage","I'm trying to use the exemple inference on windows 10 with python 10, like that:
(py310) d:\git\codellama>torchrun --nproc_per_node 1 example_instructions.py --ckpt_dir CodeLlama-7b-Instruct/ --tokenizer_path CodeLlama-7b-Instruct/tokenizer.model --max_seq_len 512 --max_batch_size 4
But it seems trying to connect to something in docker that I'm not using. That may be related to NCCL even if I only have one gpu... I don't understand.
 This is the output I get:
NOTE: Redirects are currently not supported in Windows or MacOs.
[W ..\torch\csrc\distributed\c10d\socket.cpp:601] [c10d] The client socket has failed to connect to [kubernetes.docker.internal]:29500 (system error: 10049 - LÆadresse demandÚe nÆest pas valide dans son contexte.).
[W ..\torch\csrc\distributed\c10d\socket.cpp:601] [c10d] The client socket has failed to connect to [kubernetes.docker.internal]:29500 (system error: 10049 - LÆadresse demandÚe nÆest pas valide dans son contexte.).
[W ..\torch\csrc\distributed\c10d\socket.cpp:601] [c10d] The client socket has failed to connect to [kubernetes.docker.internal]:29500 (system error: 10049 - LÆadresse demandÚe nÆest pas valide dans son contexte.).
[W ..\torch\csrc\distributed\c10d\socket.cpp:601] [c10d] The client socket has failed to connect to [kubernetes.docker.internal]:29500 (system error: 10049 - LÆadresse demandÚe nÆest pas valide dans son contexte.).
Traceback (most recent call last):
  File ""d:\git\codellama\example_instructions.py"", line 68, in <module>
    fire.Fire(main)
  File ""D:\anaconda3\envs\py310\lib\site-packages\fire\core.py"", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File ""D:\anaconda3\envs\py310\lib\site-packages\fire\core.py"", line 475, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
  File ""D:\anaconda3\envs\py310\lib\site-packages\fire\core.py"", line 691, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File ""d:\git\codellama\example_instructions.py"", line 20, in main
    generator = Llama.build(
  File ""d:\git\codellama\llama\generation.py"", line 68, in build
    torch.distributed.init_process_group(""nccl"")
  File ""D:\anaconda3\envs\py310\lib\site-packages\torch\distributed\distributed_c10d.py"", line 907, in init_process_group
    default_pg = _new_process_group_helper(
  File ""D:\anaconda3\envs\py310\lib\site-packages\torch\distributed\distributed_c10d.py"", line 1013, in _new_process_group_helper
    raise RuntimeError(""Distributed package doesn't have NCCL "" ""built in"")
RuntimeError: Distributed package doesn't have NCCL built in
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 13132) of binary: D:\anaconda3\envs\py310\python.exe
Traceback (most recent call last):
  File ""D:\anaconda3\envs\py310\lib\runpy.py"", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File ""D:\anaconda3\envs\py310\lib\runpy.py"", line 86, in _run_code
    exec(code, run_globals)
  File ""D:\anaconda3\envs\py310\Scripts\torchrun.exe\__main__.py"", line 7, in <module>
  File ""D:\anaconda3\envs\py310\lib\site-packages\torch\distributed\elastic\multiprocessing\errors\__init__.py"", line 346, in wrapper
    return f(*args, **kwargs)
  File ""D:\anaconda3\envs\py310\lib\site-packages\torch\distributed\run.py"", line 794, in main
    run(args)
  File ""D:\anaconda3\envs\py310\lib\site-packages\torch\distributed\run.py"", line 785, in run
    elastic_launch(
  File ""D:\anaconda3\envs\py310\lib\site-packages\torch\distributed\launcher\api.py"", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File ""D:\anaconda3\envs\py310\lib\site-packages\torch\distributed\launcher\api.py"", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError:
============================================================
example_instructions.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-08-28_09:51:06
  host      : DESKTOP-123456
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 13132)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================

 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/48","How can i generate embeddings from the model for a new source code dataset ?","2023-09-13T10:44:22Z","Closed issue","question","No description provided.
 The text was updated successfully, but these errors were encountered: 
👍1
Titou325 reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/meta-llama/codellama/issues/44","When running bash download.sh in my windows laptop, this happens","2023-08-30T07:50:45Z","Closed issue","No label","download.sh: line 5: $'\r': command not found
': not a valid identifier: `PRESIGNED_URL': not a valid identifier: `MODEL_SIZEdownload.sh: line 12: $'\r': command not founddownload.sh: line 22: syntax error near unexpected token `$'do\r'''ownload.sh: line 22: `do```
 The text was updated successfully, but these errors were encountered: 
👍1
Wandergarten reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/meta-llama/codellama/issues/43","issue with downloading certain files from the Llama 2","2023-08-27T04:41:56Z","Open issue","download-install","Hello all,
 I am currently facing an issue with downloading certain files from the Llama 2 repository. I ran the provided download.sh script to fetch the necessary files but encountered errors that prevented the successful download of the following files:
params.json
 tokenizer.model
 checklist.chk
 I have followed the documentation and also made sure that the URL provided in the email was correctly entered. The script runs without issues for other files but fails specifically for these. I have sufficient disk space and my internet connection is stable.
Could you please assist me in resolving this issue? Is there an alternate way to download these specific files or should I perform some additional troubleshooting steps?
Thank you for your time and assistance.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/42","NOTE: Redirects are currently not supported in Windows or MacOs.","2023-09-12T08:38:53Z","Closed issue","No label","Hi guys,
I am trying CodeLlama-13b-Python model in local MacOS 13.4.1 (c) M2, I can make sure I install all packages they need in requirement.txt.
I want to make sure is it a setting up issue or we have to run this model on Linux, just want to make sure
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/40","Unable to run it under Windows 10","2023-08-26T10:17:12Z","Open issue","compability","I followed the instructions, and I was unable to run it under Windows 10 due to nccl
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/38","Greedy decoding of CodeLlama","2023-08-26T07:07:26Z","Closed issue","No label","Hi, thanks for the great work! From the interface it seems there is not an option like do_sample=False to enable deterministic greedy decoding. I am curious if there will be support or how to add that by ourselves. Thanks!
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/37","Getting Issue in installation :(","2023-08-26T06:47:19Z","Closed issue","No label","Error :
 root@Indra:/home/gagan/projecta/codellama-gagan-singh# bash download.sh
url:https://download2.llamameta.net/*?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaXF1ZV9oYXNoIjoidHg5ejl1ZHJqN2NkMHN1anNoa3dlaWx3IiwiUmVzb3VyY2UiOiJodHRwczpcL1wvZG93bmxvYWQyLmxsYW1hbWV0YS5uZXRcLyoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2OTMxMETCCCCCCCCCCCCCCCCCCC&Download-Request-ID=8211ABCDDEMO

Enter the list of models to download without spaces (7b,13b,34b,7b-Python,13b-Python,34b-Python,7b-Instruct,13b-Instruct,34b-Instruct), or press Enter for all:
Downloading LICENSE and Acceptable Usage Policy
wget: missing URL
Usage: wget [OPTION]... [URL]...

Try `wget --help' for more options.wget: missing URL
Whats the issue ? in download.sh i pasted the link as
read -p ""url:https://download2.llamameta.net/*?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaXF1ZV9oYXNoIjoiiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2OTMxMDcwNjl9fX1dfQ__&Signature=Bw-dDvS72Gak1eb0GqeJKU5iD887fN022O6uBGfpvlF9eiMPfx7jDNu1kSre2gbEwHi%7EW15B8Ns8-%7E2fuJqa9t9QEhthKA__&Key-Pair-Id=K15QRJLYKIFSLZ&Download-Request-ID=821102959DEMOO"" PRESIGNED_URLecho """"ALL_MODELS=""7b,13b,34b,7b-Python,13b-Python,34b-Python,7b-Instruct,13b-Instruct,34b-Instruct""read -p ""Enter the list of models to download without spaces ($ALL_MODELS), or press Enter for all: "" MODEL_SIZETARGET_FOLDER="".""             # where all files should end upmkdir -p ${TARGET_FOLDER}
I have changed token to random string , Please help
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/35","Što je Java","2023-08-25T17:45:09Z","Closed issue","No label","No description provided.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/33","Himanshu Kumar wants to pay 10 USD to have this issue fixed","2023-08-25T16:42:42Z","Closed issue","No label","Need changes
 For more details visit https://mintycode.io
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/32","NOTE: Redirects are currently not supported in Windows or MacOs.","2023-08-25T15:10:38Z","Open issue","No label","I can't run the examples in Windows machine, currently blocked in this attempts to redirect.
We should see information about requirements in the documentation.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/31","missing key","2023-08-26T01:21:16Z","Closed issue","No label","when the link https://download.llamameta.net/ is opened in the browser I get the below error.
 The text was updated successfully, but these errors were encountered: 
👍2
donaldafeith and gerovac reacted with thumbs up emoji
All reactions
👍2 reactions"
"https://github.com/meta-llama/codellama/issues/30","What's the machine requirements for each model?","2023-08-25T13:20:11Z","Open issue","documentation,question","I want to know what's the minimum requirement memory/CPU/GPU for each model to run relatively fast. I ran in my M1
torchrun --nproc_per_node 1 example_completion.py \
    --ckpt_dir CodeLlama-7b/ \
    --tokenizer_path CodeLlama-7b/tokenizer.model \
    --max_seq_len 128 --max_batch_size 4
NOTE: Redirects are currently not supported in Windows or MacOs.
> initializing model parallel with size 1
> initializing ddp with size 1
> initializing pipeline with size 1
Loaded in 182.59 seconds

and it's taking more than 5 minutes.
 The text was updated successfully, but these errors were encountered: 
👍4
jaganadhg, kurisusan, LucaColonnello, and VioletGiraffe reacted with thumbs up emoji
All reactions
👍4 reactions"
"https://github.com/meta-llama/codellama/issues/29","Give an explicit example of the instruction prompt structure in the readme","2023-08-25T12:53:02Z","Open issue","No label","Currently the readme is pointing newcomers to generation.py, where they have to deduce the correct prompt structure for the instruction model from this code:
B_INST, E_INST = ""[INST]"", ""[/INST]""
B_SYS, E_SYS = ""<<SYS>>\n"", ""\n<</SYS>>\n\n""

[...]

for dialog in dialogs:
    unsafe_requests.append(
        any([tag in msg[""content""] for tag in SPECIAL_TAGS for msg in dialog])
    )
    if dialog[0][""role""] == ""system"":
        dialog = [
            {
                ""role"": dialog[1][""role""],
                ""content"": B_SYS
                + dialog[0][""content""]
                + E_SYS
                + dialog[1][""content""],
            }
        ] + dialog[2:]
    assert all([msg[""role""] == ""user"" for msg in dialog[::2]]) and all(
        [msg[""role""] == ""assistant"" for msg in dialog[1::2]]
    ), (
        ""model only supports 'system', 'user' and 'assistant' roles, ""
        ""starting with 'system', then 'user' and alternating (u/a/u/a/u...)""
    )
    dialog_tokens: List[int] = sum(
        [
            self.tokenizer.encode(
                f""{B_INST} {(prompt['content']).strip()} {E_INST} {(answer['content']).strip()} "",
                bos=True,
                eos=True,
            )
            for prompt, answer in zip(
                dialog[::2],
                dialog[1::2],
            )
        ],
        [],
    )
    assert (
        dialog[-1][""role""] == ""user""
    ), f""Last message must be from user, got {dialog[-1]['role']}""
    dialog_tokens += self.tokenizer.encode(
        f""{B_INST} {(dialog[-1]['content']).strip()} {E_INST}"",
        bos=True,
        eos=False,
    )
    prompt_tokens.append(dialog_tokens)

This seems unnecessarily obscure. Is there a specific reason to not just give an example?
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/28","Issue with downloading models","2023-08-28T08:22:46Z","Closed issue","No label","Hi all,
I am actually struggling with downloading models. When I paste the link when prompted and after tapping 'enter' to download all models I receive the following:
download.sh: 13: [[: not found
 Downloading LICENSE and Acceptable Usage Policy
 download.sh: 18: Bad substitution
Tried twice with different links, after removing the cloned repo from HDD.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/27","Run 13B or 34B in a single GPU","2023-08-28T13:20:43Z","Closed issue","No label","How to load 13B or 34B in a single A100, I notice the model parallel size larger than 1 need more gpus
 The text was updated successfully, but these errors were encountered: 
👍4
briandw, alphastrata, msgersch, and Vlad2000Andrei reacted with thumbs up emoji
All reactions
👍4 reactions"
"https://github.com/meta-llama/codellama/issues/26","Model pads response with newlines up to max_length","2023-09-12T08:33:33Z","Closed issue","model-usage","I tried several of the models through Huggingface, and the response is always padded with newlines up to the number of tokens specified by the max_length argument in model.generate().
I also assign pad_token_id=tokenizer.eos_token_id, so I'm not sure why the model is generating these newline characters.
 The text was updated successfully, but these errors were encountered: 
👍8
borzunov, yangyang678, Regenhardt, liruiw, geekoftheweek, zaventh, aliswel-mt, and EgorovMike219 reacted with thumbs up emoji
All reactions
👍8 reactions"
"https://github.com/meta-llama/codellama/issues/25","torch.distributed.elastic.multiprocessing.errors.ChildFailedError:","2023-09-12T08:44:06Z","Closed issue","fine-tuning,model-usage","https://colab.research.google.com/drive/1rXJyXXO4m-nP4XDoLV7h4_Iea3jYHDZY#scrollTo=xjpOpjPDAVQX
 In google colab, When I execute fine-tuning it throws the error
 The text was updated successfully, but these errors were encountered: 
👀2
sanardicat and ouxiang08 reacted with eyes emoji
All reactions
👀2 reactions"
"https://github.com/meta-llama/codellama/issues/24","how to finetune it locally","2023-09-12T08:39:59Z","Closed issue","fine-tuning,question","No description provided.
 The text was updated successfully, but these errors were encountered: 
👍4
msgersch, codelion, rabieo, and nkeilar reacted with thumbs up emoji
All reactions
👍4 reactions"
"https://github.com/meta-llama/codellama/issues/22","SSL error when downloading","2023-09-12T08:34:57Z","Closed issue","download-install,model-access","Hi,
The download script throws the following error:
Resolving download2.llamameta.net (download2.llamameta.net)... ::ffff:130.226.237.92, 130.226.237.92
Connecting to download2.llamameta.net (download2.llamameta.net)|::ffff:130.226.237.92|:443... connected.
OpenSSL: error:0A000152:SSL routines::unsafe legacy renegotiation disabled
Unable to establish SSL connection.
Checking checksums
md5sum: checklist.chk: no properly formatted MD5 checksum lines found

If I set the Options = UnsafeLegacyRenegotiation in SSL conf then it throws the error mentioned in #8 . Thanks for the help!
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/21","How to deploy this model?","2023-08-28T13:17:41Z","Closed issue","No label","No description provided.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/20","Requesting a Colab file to run code llama","2023-09-12T08:29:23Z","Closed issue","No label","Hey,
 Thank you very much for such a great work, it would be great if anyone can create a colab file where we can run this model.
 In different regions we are not able to download the model weights and run it.
 Thanking You in advance. : )
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/19","Inference on multi-gpu","2023-08-25T04:49:44Z","Open issue","No label","Tried to run:
torchrun --nproc_per_node 1 codellama/example_instructions.py \
     --ckpt_dir /home/ubuntu/model/ \
     --tokenizer_path /home/ubuntu/model/tokenizer.model \
     --max_seq_len 4512 --max_batch_size 4


I have a long prompt (4000 tokens).
 I have 4 Nvidia A10G each with 300W and 24GB VRAM. However I see only one GPU being used (on nvidia-smi).
 The error I get is:
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 302.00 MiB (GPU 0; 22.19 GiB total capacity; 21.65 GiB already allocated; 175.50 MiB free; 21.71 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 33763) of binary:

The whole tracelog is:
 torchrun --nproc_per_node 1 codellama/example_instructions.py \
>     --ckpt_dir /home/ubuntu/model/ \
>     --tokenizer_path /home/ubuntu/model/tokenizer.model \
>     --max_seq_len 4512 --max_batch_size 4
> initializing model parallel with size 1
> initializing ddp with size 1
> initializing pipeline with size 1

Loaded in 7.07 seconds
Traceback (most recent call last):
  File ""/home/ubuntu/codellama/example_instructions.py"", line 114, in <module>
    fire.Fire(main)
  File ""/home/ubuntu/venv/lib/python3.9/site-packages/fire/core.py"", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File ""/home/ubuntu/venv/lib/python3.9/site-packages/fire/core.py"", line 475, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
  File ""/home/ubuntu/venv/lib/python3.9/site-packages/fire/core.py"", line 691, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File ""/home/ubuntu/codellama/example_instructions.py"", line 97, in main
    results = generator.chat_completion(
  File ""/home/ubuntu/codellama/llama/generation.py"", line 335, in chat_completion
    generation_tokens, generation_logprobs = self.generate(
  File ""/home/ubuntu/venv/lib/python3.9/site-packages/torch/utils/_contextlib.py"", line 115, in decorate_context
    return func(*args, **kwargs)
  File ""/home/ubuntu/codellama/llama/generation.py"", line 148, in generate
    logits = self.model.forward(tokens[:, prev_pos:cur_pos], prev_pos)
  File ""/home/ubuntu/venv/lib/python3.9/site-packages/torch/utils/_contextlib.py"", line 115, in decorate_context
    return func(*args, **kwargs)
  File ""/home/ubuntu/codellama/llama/model.py"", line 288, in forward
    h = layer(h, start_pos, freqs_cis, mask)
  File ""/home/ubuntu/venv/lib/python3.9/site-packages/torch/nn/modules/module.py"", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File ""/home/ubuntu/codellama/llama/model.py"", line 240, in forward
    h = x + self.attention.forward(
  File ""/home/ubuntu/codellama/llama/model.py"", line 181, in forward
    scores = F.softmax(scores.float(), dim=-1).type_as(xq)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 302.00 MiB (GPU 0; 22.19 GiB total capacity; 21.65 GiB already allocated; 175.50 MiB free; 21.71 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 33763) of binary: /home/ubuntu/venv/bin/python
Traceback (most recent call last):
  File ""/home/ubuntu/venv/bin/torchrun"", line 8, in <module>
    sys.exit(main())
  File ""/home/ubuntu/venv/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py"", line 346, in wrapper
    return f(*args, **kwargs)
  File ""/home/ubuntu/venv/lib/python3.9/site-packages/torch/distributed/run.py"", line 794, in main
    run(args)
  File ""/home/ubuntu/venv/lib/python3.9/site-packages/torch/distributed/run.py"", line 785, in run
    elastic_launch(
  File ""/home/ubuntu/venv/lib/python3.9/site-packages/torch/distributed/launcher/api.py"", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File ""/home/ubuntu/venv/lib/python3.9/site-packages/torch/distributed/launcher/api.py"", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError:
============================================================
codellama/example_instructions.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-08-25_04:41:10
  host      : ip-172-31-92-135.ec2.internal
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 33763)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================

 The text was updated successfully, but these errors were encountered: 
👍4
itaowei, willshion, jryebread, and deflorator1980 reacted with thumbs up emoji👀2
itaowei and zhengdewu reacted with eyes emoji
All reactions
👍4 reactions
👀2 reactions"
"https://github.com/meta-llama/codellama/issues/17","Still no wget resume option for model download?","2023-08-25T03:39:42Z","Closed issue","No label","Simply adding --continue would save time for interrupted downloads.
 The text was updated successfully, but these errors were encountered: 
😄1
Mmmred reacted with laugh emoji
All reactions
😄1 reaction"
"https://github.com/meta-llama/codellama/issues/16","Sorry, the download is not available in your region","2023-09-06T18:13:37Z","Closed issue","No label","No description provided.
 The text was updated successfully, but these errors were encountered: 
😕2
MrRace and mikiyukio reacted with confused emoji
All reactions
😕2 reactions"
"https://github.com/meta-llama/codellama/issues/14","How to trigger the model?","2023-09-06T18:25:42Z","Closed issue","No label","I got the following - how do I trigger the model GUI?
(codegenllama) C:>pip install -e codellama
 Obtaining file:///C:/codellama
 Preparing metadata (setup.py) ... done
 Requirement already satisfied: torch in c:\users\home\anaconda3\envs\codegenllama\lib\site-packages (from codellama==0.0.1) (2.0.1+cu117)
 Collecting fairscale (from codellama==0.0.1)
 Downloading fairscale-0.4.13.tar.gz (266 kB)
 ---------------------------------------- 266.3/266.3 kB 5.4 MB/s eta 0:00:00
 Installing build dependencies ... done
 Getting requirements to build wheel ... done
 Installing backend dependencies ... done
 Preparing metadata (pyproject.toml) ... done
 Collecting fire (from codellama==0.0.1)
 Downloading fire-0.5.0.tar.gz (88 kB)
 ---------------------------------------- 88.3/88.3 kB 5.2 MB/s eta 0:00:00
 Preparing metadata (setup.py) ... done
 Collecting sentencepiece (from codellama==0.0.1)
 Downloading sentencepiece-0.1.99-cp310-cp310-win_amd64.whl (977 kB)
 ---------------------------------------- 977.5/977.5 kB 10.3 MB/s eta 0:00:00
 Requirement already satisfied: numpy>=1.22.0 in c:\users\home\anaconda3\envs\codegenllama\lib\site-packages (from fairscale->codellama==0.0.1) (1.24.1)
 Requirement already satisfied: filelock in c:\users\home\anaconda3\envs\codegenllama\lib\site-packages (from torch->codellama==0.0.1) (3.9.0)
 Requirement already satisfied: typing-extensions in c:\users\home\anaconda3\envs\codegenllama\lib\site-packages (from torch->codellama==0.0.1) (4.4.0)
 Requirement already satisfied: sympy in c:\users\home\anaconda3\envs\codegenllama\lib\site-packages (from torch->codellama==0.0.1) (1.11.1)
 Requirement already satisfied: networkx in c:\users\home\anaconda3\envs\codegenllama\lib\site-packages (from torch->codellama==0.0.1) (3.0)
 Requirement already satisfied: jinja2 in c:\users\home\anaconda3\envs\codegenllama\lib\site-packages (from torch->codellama==0.0.1) (3.1.2)
 Collecting six (from fire->codellama==0.0.1)
 Using cached six-1.16.0-py2.py3-none-any.whl (11 kB)
 Collecting termcolor (from fire->codellama==0.0.1)
 Downloading termcolor-2.3.0-py3-none-any.whl (6.9 kB)
 Requirement already satisfied: MarkupSafe>=2.0 in c:\users\home\anaconda3\envs\codegenllama\lib\site-packages (from jinja2->torch->codellama==0.0.1) (2.1.2)
 Requirement already satisfied: mpmath>=0.19 in c:\users\home\anaconda3\envs\codegenllama\lib\site-packages (from sympy->torch->codellama==0.0.1) (1.2.1)
 Building wheels for collected packages: fairscale, fire
 Building wheel for fairscale (pyproject.toml) ... done
 Created wheel for fairscale: filename=fairscale-0.4.13-py3-none-any.whl size=332117 sha256=0945c01902555985db3ce29135a6b2eef4448b83305b47dbb614485d15188a24
 Stored in directory: c:\users\home\appdata\local\pip\cache\wheels\78\a4\c0\fb0a7ef03cff161611c3fa40c6cf898f76e58ec421b88e8cb3
 Building wheel for fire (setup.py) ... done
 Created wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116947 sha256=d2a6f24d8bb7ed4b42b780a7fcdbae383514bf5ef87b37c07ab76410ca582514
 Stored in directory: c:\users\home\appdata\local\pip\cache\wheels\90\d4\f7\9404e5db0116bd4d43e5666eaa3e70ab53723e1e3ea40c9a95
 Successfully built fairscale fire
 Installing collected packages: sentencepiece, termcolor, six, fire, fairscale, codellama
 Running setup.py develop for codellama
 Successfully installed codellama-0.0.1 fairscale-0.4.13 fire-0.5.0 sentencepiece-0.1.99 six-1.16.0 termcolor-2.3.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/11","Run code llama on mac?","2023-08-24T21:47:30Z","Open issue","compability,model-usage","Hi,
on mac I got the following error:
 RuntimeError: Distributed package doesn't have NCCL built in
 raise RuntimeError(""Distributed package doesn't have NCCL "" ""built in"")
 RuntimeError: Distributed package doesn't have NCCL built in
 ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 80731) of binary: /opt/dev/miniconda3/envs/llama/bin/python3.10
Guess this is because of the missing CUDA. Is there an option to run it with CPU?
 The text was updated successfully, but these errors were encountered: 
👍6
lostmygithubaccount, MauroLG, tuanha1305, changeling, kccheung, and binary-grabtaxi reacted with thumbs up emoji👀1
godpeny reacted with eyes emoji
All reactions
👍6 reactions
👀1 reaction"
"https://github.com/meta-llama/codellama/issues/10","Could not parse check file 'checklist.chk'","2023-08-24T20:57:34Z","Closed issue","No label","Getting this on MacOS when attempting to download
HTTP request sent, awaiting response... 200 OK
Length: 6489 (6.3K) [text/html]
Saving to: ‘./CodeLlama-13b-Instruct/checklist.chk’

./CodeLlama-13b-Instruct/checkl 100%[=====================================================>]   6.34K  --.-KB/s    in 0s

2023-08-24 13:51:33 (35.0 MB/s) - ‘./CodeLlama-13b-Instruct/checklist.chk’ saved [6489/6489]

Checking checksums
Could not parse check file 'checklist.chk' (2)

 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/9","Any plans to put this on Replicate?","2023-09-12T08:16:51Z","Closed issue","integrations","Would love to be able to use this on Replicate – are there any plans for Facebook Research to upload this there?
Thank you!
 The text was updated successfully, but these errors were encountered: 
😄1
GaganHonor reacted with laugh emoji
All reactions
😄1 reaction"
"https://github.com/meta-llama/codellama/issues/8","Unable to locally verify the issuer's authority","2023-09-12T08:33:39Z","Closed issue","download-install","Enter the list of models to download without spaces (7b,13b,34b,7b-Python,13b-Python,34b-Python,7b-Instruct,13b-Instruct,34b-Instruct), or press Enter for all: 7b
Downloading LICENSE and Acceptable Usage Policy
--2023-08-24 17:02:44--  {codellama url}
Download-Request-ID=254298234165953
Resolving download2.llamameta.net (download2.llamameta.net)... 54.230.31.124, 54.230.31.3, 54.230.31.14, ...
Connecting to download2.llamameta.net (download2.llamameta.net)|54.230.31.124|:443... connected.
ERROR: cannot verify download2.llamameta.net's certificate, issued by ‘emailAddress=support@fortinet.com,CN=FGT3KDT418800895,OU=Certificate Authority,O=Fortinet,L=Sunnyvale,ST=California,C=US’:
  Unable to locally verify the issuer's authority.
To connect to download2.llamameta.net insecurely, use `--no-check-certificate'.


I've requested the link 3 times now and the download.sh script still throws this error when trying to download the model.
 Thought it was a configuration issue on my side, but I tried again with normal llama-2 and it worked.
Confusingly, the download.sh script for llama-2 only works for some models. 70B-chat worked, 7B and 13B did not.
I also tried editing the download.sh script to include the --no-check-certificate option on all wget commands, but that then gives me the following error:
  Unable to locally verify the issuer's authority.
HTTP request sent, awaiting response... 403 Forbidden
2023-08-24 16:45:15 ERROR 403: Forbidden.

 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/7","Where is the 70B Model?","2023-08-28T13:18:11Z","Closed issue","No label","Just a question: Are you planning to release the 70B model in the future or what is the plan?
 The text was updated successfully, but these errors were encountered: 
👍9
Nondzu, electricalgorithm, S-Yacer, endomorphosis, catid, codelion, leng-yue, sirus20x6, and dalvarez06 reacted with thumbs up emoji
All reactions
👍9 reactions"
"https://github.com/meta-llama/codellama/issues/6","Size of the model info is missing","2023-09-08T14:23:09Z","Closed issue","documentation,question","It would be helpful to mention the size of disk space the pre-trained models will take so as to determine if this can be easily done on laptop or not.
 The text was updated successfully, but these errors were encountered: 
👍3
juliocspires, bwhitesell, and univanxx reacted with thumbs up emoji
All reactions
👍3 reactions"
"https://github.com/meta-llama/codellama/issues/5","access through HuggingFace?","2023-08-28T13:19:42Z","Closed issue","No label","Thanks so much for your work and for opening up access to Code Llama.
We received access for Llama2 previously and access the models via HF. Is it possible to do the same with Code Llama?
 (We just requested access for Code Llama via your submission form).
 The text was updated successfully, but these errors were encountered: 
👍22
Nondzu, iMvijay23, zch-cc, nicobasile, owenwastaken, carolinefrasca, Shas3011, basujindal, bwhitesell, arabacibahadir, and 12 more reacted with thumbs up emoji🎉5
Shas3011, bwhitesell, semanser, jackkwok, and ghishadow reacted with hooray emoji🚀5
Shas3011, semanser, SeungyounShin, michabbb, and ghishadow reacted with rocket emoji👀5
Shas3011, nicobasile, SeungyounShin, ghishadow, and dennis-gonzales reacted with eyes emoji
All reactions
👍22 reactions
🎉5 reactions
🚀5 reactions
👀5 reactions"
"https://github.com/meta-llama/codellama/issues/4","Scheme missing","2023-08-24T16:52:57Z","Closed issue","No label","Followed the instructions as per the blog and got my email download code.
Ater running download.sh, download link and choosing the models, I get a Scheme missing error and the script gets stuck on Checking checksums
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/meta-llama/codellama/issues/2","Does it support FIM (fill-in-the-middle)?","2023-08-29T12:37:12Z","Closed issue","No label","This is useful for code completion tasks. Starcoder, for instance, is trained with FIM — how does Code LLama compare?
 The text was updated successfully, but these errors were encountered: 
All reactions"
