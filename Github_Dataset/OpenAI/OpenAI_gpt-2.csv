"https://github.com/openai/gpt-2/issues/355","Resumable Download Script","2024-10-04T13:18:09Z","Closed issue","No label","Resume Downlaod Script
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/352","Link to paper not working anymore","2024-09-09T12:04:45Z","Open issue","No label","Code and models from the paper ""Language Models are Unsupervised Multitask Learners"".
This link does not work anymore, it says ""Access Denied"". Can you please fix that?
Alternatively, the pdf for the paper is linked here (see top right): https://scholar.google.com/citations?view_op=view_citation&hl=en&user=dOad5HoAAAAJ&citation_for_view=dOad5HoAAAAJ:YsMSGLbcyi4C
Or use the official link from the OpenAI page (see ""Read paper""): https://openai.com/index/better-language-models/
 The text was updated successfully, but these errors were encountered: 
👍2
T1bolus and sanezek reacted with thumbs up emoji
All reactions
👍2 reactions"
"https://github.com/openai/gpt-2/issues/348","GPT","2024-08-14T13:28:16Z","Open issue","No label","No description provided.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/344","GPT","2024-08-04T13:52:15Z","Open issue","No label","No description provided.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/343","Does the evaluation code get released?","2024-07-12T06:59:06Z","Open issue","No label","Hello, thank you for your contribution.
 I am trying to reproduce the results of the evaluation experiment reported in Table 3 of the paper, but found it a bit difficult for a novice. I would like to know if there is any available tutorial and code for gpt2 evaluation on datasets like LAMBADA and CBT?
 Thank you very much for your help!
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/342","سلام","2024-06-12T22:58:25Z","Open issue","No label","import subprocess
 from telegram import Update
 from telegram.ext import Updater, CommandHandler, CallbackContext
 import logging
تنظیمات لاگینگ
logging.basicConfig(format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
 level=logging.INFO)
 logger = logging.getLogger(name)
توکن تلگرام خود را وارد کنید
TELEGRAM_TOKEN = '7124126923:AAHVVtgGQlJA32M_shpw-9WhehfMLndNcHY'
توابع برای دستورات مختلف
def start(update: Update, context: CallbackContext) -> None:
 update.message.reply_text(""خوش آمدید!"")
def help_command(update: Update, context: CallbackContext) -> None:
 update.message.reply_text(
 ""/start - شروع\n""
 ""/help - کمک\n""
 ""/update - بروزرسانی\n""
 ""/create - ساخت ربات جدید\n""
 ""/createmanual - ساخت ربات جدید به صورت دستی\n""
 ""/fix - بازیابی اطلاعات\n""
 ""/autolaunch - راه‌اندازی خودکار\n""
 ""/clear - پاک کردن ربات\n""
 ""/install - نصب پیش نیازها\n""
 )
def update_bot():
 subprocess.run([""git"", ""fetch"", ""--all""])
 subprocess.run([""git"", ""reset"", ""--hard"", ""origin/persian""])
 subprocess.run([""git"", ""pull"", ""origin"", ""persian""])
 subprocess.run([""chmod"", ""+x"", ""bot""])
 logger.info(""بروزرسانی اطلاعات با موفقیت انجام شد."")
def update_command(update: Update, context: CallbackContext) -> None:
 update_bot()
 update.message.reply_text(""بروزرسانی اطلاعات با موفقیت انجام شد."")
def create_command(update: Update, context: CallbackContext) -> None:
 # کد ساخت ربات جدید
 update.message.reply_text(""ربات جدید با موفقیت ساخته شد."")
def createmanual_command(update: Update, context: CallbackContext) -> None:
 # کد ساخت ربات به صورت دستی
 update.message.reply_text(""لطفاً شماره دلخواه خود را وارد کنید."")
def fix_command(update: Update, context: CallbackContext) -> None:
 subprocess.run([""git"", ""reset"", ""--hard"", ""FETCH_HEAD""])
 update.message.reply_text(""بازیابی اطلاعات به آخرین بروزرسانی انجام شد."")
def autolaunch_command(update: Update, context: CallbackContext) -> None:
 update.message.reply_text(""راه‌اندازی خودکار ربات‌ها هر 20 دقیقه فعال شد."")
def clear_command(update: Update, context: CallbackContext) -> None:
 update.message.reply_text(""لطفاً شماره شناسه رباتی که می‌خواهید پاک کنید را وارد کنید."")
def install_command(update: Update, context: CallbackContext) -> None:
 update.message.reply_text(""نصب پیش نیازهای ربات در حال انجام است."")
هندلر خطا
def error_handler(update: Update, context: CallbackContext) -> None:
 logger.error(msg=""Exception while handling an update:"", exc_info=context.error)
 if update and update.message:
 update.message.reply_text('خطایی رخ داده است. لطفاً دوباره تلاش کنید.')
def main() -> None:
 updater = Updater(TELEGRAM_TOKEN)
 dispatcher = updater.dispatcher
dispatcher.add_handler(CommandHandler(""start"", start))
dispatcher.add_handler(CommandHandler(""help"", help_command))
dispatcher.add_handler(CommandHandler(""update"", update_command))
dispatcher.add_handler(CommandHandler(""create"", create_command))
dispatcher.add_handler(CommandHandler(""createmanual"", createmanual_command))
dispatcher.add_handler(CommandHandler(""fix"", fix_command))
dispatcher.add_handler(CommandHandler(""autolaunch"", autolaunch_command))
dispatcher.add_handler(CommandHandler(""clear"", clear_command))
dispatcher.add_handler(CommandHandler(""install"", install_command))

dispatcher.add_error_handler(error_handler)

updater.start_polling()
updater.idle()

if name == 'main':
 main()
 The text was updated successfully, but these errors were encountered: 
👍1
rasolsrfraz reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/gpt-2/issues/340","سلام برام یه کد ربات اد گروه به گروه بنویس","2024-06-07T23:27:54Z","Open issue","No label","No description provided.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/338","Tt","2024-05-15T16:49:10Z","Open issue","No label","S
 The text was updated successfully, but these errors were encountered: 
👍1
IT1M reacted with thumbs up emoji😕3
Foptciy, ShawSumma, and OliverYao123 reacted with confused emoji
All reactions
👍1 reaction
😕3 reactions"
"https://github.com/openai/gpt-2/issues/336","G","2024-05-02T07:07:29Z","Open issue","No label","@Article{radford2019language,
 title={Language Models are Unsupervised Multitask Learners},
 author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
 year={2019}
 }
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/334","GPT-2 implementation problem","2024-03-31T22:18:39Z","Open issue","No label","""Hi, I am reading the GPT-2 paper and encountering a problem with the following phrase related to implementation:
'A modified initialization method is used to account for the accumulation on the residual path with model depth. We scale the weights of residual layers at initialization by a factor of 1/√N, where N is the number of residual layers.'
My problem is that we normalize after accumulation (addition then normalization). So, why do we need to scale weights? Aren't we doing this to reduce the impact of accumulation?""
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/333","GPT-2 implementation problem","2024-03-31T22:18:35Z","Open issue","No label","""Hi, I am reading the GPT-2 paper and encountering a problem with the following phrase related to implementation:
'A modified initialization method is used to account for the accumulation on the residual path with model depth. We scale the weights of residual layers at initialization by a factor of 1/√N, where N is the number of residual layers.'
My problem is that we normalize after accumulation (addition then normalization). So, why do we need to scale weights? Aren't we doing this to reduce the impact of accumulation?""
 The text was updated successfully, but these errors were encountered: 
👀3
Chriscray1, longbowzhang, and asdf2014 reacted with eyes emoji
All reactions
👀3 reactions"
"https://github.com/openai/gpt-2/issues/332","GPT-2 implementation problem","2024-03-31T22:18:32Z","Open issue","No label","""Hi, I am reading the GPT-2 paper and encountering a problem with the following phrase related to implementation:
'A modified initialization method is used to account for the accumulation on the residual path with model depth. We scale the weights of residual layers at initialization by a factor of 1/√N, where N is the number of residual layers.'
My problem is that we normalize after accumulation (addition then normalization). So, why do we need to scale weights? Aren't we doing this to reduce the impact of accumulation?""
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/330","ChatGPT is generally really cool, but what if it used Google search to help attract results?","2023-12-03T16:32:07Z","Open issue","No label","plus it would be plagiarism free if it actually used Google, so it rarely might make mistakes.
Plus, it would still be itself with the plugin.
 The text was updated successfully, but these errors were encountered: 
👍2
TheAceInOne and Yang011013 reacted with thumbs up emoji
All reactions
👍2 reactions"
"https://github.com/openai/gpt-2/issues/329","Problems trying to build gpt-2","2023-11-24T17:54:49Z","Open issue","No label","Hi,
I am trying to build gpt-2 on Ubuntu 22.04.2.
I git cloned this repository. In DEVELOPERS.md, it says that you have to pip install tensorflow==1.12.0. When I try to install tensorflow==1.12.0, it makes an error:
ERROR: Could not find a version that satisfies the requirement tensorflow==1.12.0 (from versions: 2.8.0rc0, 2.8.0rc1, 2.8.0, 2.8.1, 2.8.2, 2.8.3, 2.8.4, 2.9.0rc0, 2.9.0rc1, 2.9.0rc2, 2.9.0, 2.9.1, 2.9.2, 2.9.3, 2.10.0rc0, 2.10.0rc1, 2.10.0rc2, 2.10.0rc3, 2.10.0, 2.10.1, 2.11.0rc0, 2.11.0rc1, 2.11.0rc2, 2.11.0, 2.11.1, 2.12.0rc0, 2.12.0rc1, 2.12.0, 2.12.1, 2.13.0rc0, 2.13.0rc1, 2.13.0rc2, 2.13.0, 2.13.1, 2.14.0rc0, 2.14.0rc1, 2.14.0, 2.14.1, 2.15.0rc0, 2.15.0rc1, 2.15.0)
 ERROR: No matching distribution found for tensorflow==1.12.0
I tried omitting the ==1.12.0 and it successfully installed a newer version of tensorflow, but if I use the new tensorflow, when I try to run the model, it makes an error:
env/bin/python src/interactive_conditional_samples.py --top_k 40
 2023-11-24 17:24:15.771054: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.
 2023-11-24 17:24:16.687450: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
 2023-11-24 17:24:16.687553: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
 2023-11-24 17:24:16.885832: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
 2023-11-24 17:24:17.264367: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.
 2023-11-24 17:24:17.267425: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
 To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
 2023-11-24 17:24:19.771254: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 Traceback (most recent call last):
 File ""/home/ubuntu/gpt-2/src/interactive_conditional_samples.py"", line 9, in 
 import model, sample, encoder
 File ""/home/ubuntu/gpt-2/src/model.py"", line 3, in 
 from tensorflow.contrib.training import HParams
 ModuleNotFoundError: No module named 'tensorflow.contrib'
In this stackoverflow article:
https://stackoverflow.com/questions/55082483/why-can-i-not-import-tensorflow-contrib-i-get-an-error-of-no-module-named-tenso
 someone says that tensorflow.contrib no longer exists in tensorflow 2.0.x and its modules were moved. Perhaps there is a way to get the modules from wherever they were moved?
Any help on building gpt-2 would be appreciated!
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/327","bug: text generation result is different for different operating systems (macOS, windows, and Linux)","2023-10-14T21:53:06Z","Open issue","No label","Repo of the project where the bug is spotted:
https://github.com/l1xiangyi/GPT2/tree/main
What is the problem
The problem occurs in main.py line 89 where I try to generate texts based on a NeruIPS dataset. This line calls the sample function that generates subsequent sentences based on the start_text. However the texts generated on macOS, Windows, and Linux are very different even though the seed is fixed in the config.yml.
macOS result: sumbit/samples.txt
Windows result: sumbit/sample_windows.txt
Windows result: sumbit/sample_linux.txt
Strange words like ""Zombie"" pop up a lot in the Windows and Linux generated texts.
How to reproduce this problem
Linux
The simplest way is go create a new Colab notebook and run the following commands:
!git clone https://github.com/l1xiangyi/gpt2/
%cd gpt2
!python -m pip install -r requirements.txt
!python main.py

Then check the samples.txt in the submit folder.
Windows
Similar steps to the Linux version. Just drop the special sign before each commands:
git clone https://github.com/l1xiangyi/gpt2/
cd gpt2
python -m pip install -r requirements.txt
python main.py

 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/325","Can't install ./download_model.sh??","2023-08-04T02:32:46Z","Closed issue","No label","Trying to get the command ./download_model.sh to run but I'm getting the error ""no such file or directory: ./download_model.sh"". Pretty new to this kind of stuff, but it seems previous commands to set up the stable-diffusion-webui folder may have missed this script?? (currently using macOS Ventura 13.5) Any help would be greatly appreciated.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/324","while writting the code it always forget to complete it at last","2023-06-28T07:30:53Z","Open issue","No label","for eg this code:
 /* Navbar */
 .navbar {
 background-color: #f8f9fa;
 padding: 0.5rem 1rem;
 }
.navbar-brand {
 color: #212529;
 font-weight: bold;
 text-decoration: none;
 }
.navbar-toggler {
 border: none;
 padding: 0.25rem 0.5rem;
 background-color: transparent;
 }
.navbar-toggler-icon {
 display: inline-block;
 width: 1.5em;
 height: 1.5em;
 vertical-align: middle;
 content: """";
 background-repeat: no-repeat;
 background-position: 50%;
 background-size: 100% 100%;
 background-image: url(""data:image/svg+xml,%3csvg viewBox='0 0 30 30' xmlns='http://www.w3.org/2000/svg'%3e%3cpath stroke='rgba(0, 0, 0, 0.5)' stroke-linecap='round' stroke-miterlimit='10' stroke-width='2' d='M4 7h22M4 15h22M4 23h22'/%3e%3c/svg%3e"");
 }
.navbar-collapse {
 display: flex;
 flex-direction: row;
 align-items: center;
 justify-content: flex-start;
 }
.navbar-nav {
 margin-left: auto;
 }
.nav-item {
 display: inline-block;
 padding: 0.5rem 1rem;
 }
.nav-link {
 color: #212529;
 text-decoration: none;
 }
.nav-item.active .nav-link {
 font-weight: bold;
 }
.dropdown-menu {
 position: absolute;
 top: 100%;
 left: 0;
 z-index: 1000;
 display: none;
 min-width: 10rem;
 padding: 0.5rem 0;
 margin: 0.125rem 0 0;
 font-size: 1rem;
 color: #212529;
 text-align: left;
 background-color: #fff;
 background-clip: padding-box;
 border: 1px solid rgba(0, 0, 0, 0.15);
 border-radius: 0.25rem;
 }
.dropdown-item {
 display: block;
 width: 100%;
 padding: 0.25rem 1.5rem;
 clear: both;
 font-weight: 400;
 color: #212529;
 text-align: inherit;
 white-space: nowrap;
 background-color: transparent;
 border: 0;
 }
.dropdown-divider {
 height: 0;
 margin: 0.5rem 0;
 overflow: hidden;
 border-top: 1px solid #e9ecef;
 }
.nav-link.disabled {
 opacity: 0.65;
 pointer-events: none;
 }
.form-inline {
 display: flex;
 flex-flow: row wrap;
 align-items: center;
 }
.form-control {
 display: block;
 width: 100%;
 padding: 0.375rem 0.75rem;
 font-size: 1rem;
 line-height: 1.5;
 color: #212529;
 background-color: #fff;
 background-clip: padding-box;
 border: 1px solid #ced4da;
 border-radius: 0.25rem;
 }
.btn {
 display: inline-block;
 font-weight: 400;
 color: #212529;
 text
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/322","How to get sentence embeddings using gpt2","2023-06-01T07:47:45Z","Open issue","No label","Hi, I have been doing some experimentation for getting sentence embedding using gpt2 for semantic similarity task. But failing to get desired results.
 Gpt is left to right autoregressive language model, so thought it might be good at picking up context of the whole sentence.
I tried 2 approaches for getting representation of the sentence.
Mean pooling of embeddings of all input words present in sentence
Embedding of last word
But nothing seems to work well.
 On the other hand have also tried the same with gpt3 API and the quality of embeddings seems good.
 Evaluated the embeddings subjectively on the task of semantic similarity.
Thanks in advance.
 The text was updated successfully, but these errors were encountered: 
👍1
gkep reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/gpt-2/issues/321","How does GPT-3 text embedding come from? Only by generative pre-training?","2023-05-26T01:14:37Z","Open issue","No label","Thank you very much!
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/317","Ppok","2023-05-07T07:06:56Z","Open issue","No label","No description provided.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/316","Looking to purchase GPT-4 API Keys","2023-05-05T07:10:16Z","Open issue","No label","Is anyone still on the waiting list for GPT-4? Don't worry, https://www.openai-365pro.com this platform offers a reliable GPT-4 API key rental service, which many people are already using. This is really great!
 The text was updated successfully, but these errors were encountered: 
👍1
SunnyGPT reacted with thumbs up emoji👎7
Luigi123, NotFenixio, xuuyz, mrahmadhassankhan, SolomidHero, Manamama, and abhishek47kashyap reacted with thumbs down emoji
All reactions
👍1 reaction
👎7 reactions"
"https://github.com/openai/gpt-2/issues/315","No prob","2023-03-08T21:15:38Z","Open issue","No label","No description provided.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/314","Installation, revisited","2023-02-27T13:03:33Z","Open issue","No label","Summary:
 Run e.g. this one
sudo docker pull am271/gpt2
sudo docker run --rm -it am271/gpt2 bash
python3 ./download_model.py 345M
python3 src/interactive_conditional_samples.py --model_name 345M

for a properly running version.
Details:
 Almost nothing has worked out of the box by now, even the dockers way:
load metadata for docker.io/tensorflow/tensorflow:1.12.0-gpu-py3: ------ failed to solve with frontend dockerfile.v0: failed to create LLB definition: rpc error: code = Unknown desc = failed to parse /home/codespace/.docker/.token_seed: unexpected end of JSON input @6Y3GRwDjtGVo4nAe2
 or
#11 0.791   File ""/usr/lib/python3.5/typing.py"", line 177, in _eval_type
#11 0.791     eval(self.__forward_code__, globalns, localns),
#11 0.791   File ""<string>"", line 1, in <module>
#11 0.791 AttributeError: module 'os' has no attribute 'PathLike'
------
executor failed running [/bin/sh -c python3 download_model.py 124M]: exit code: 1

and such, partially resolvable via a docker builders purge plus:
 rm ~/.docker/.token_seed.loc
 rm ~/.docker/.token_seed
 but then:
#11 0.782   File ""/usr/lib/python3.5/typing.py"", line 177, in _eval_type
#11 0.782     eval(self.__forward_code__, globalns, localns),
#11 0.782   File ""<string>"", line 1, in <module>
#11 0.782 AttributeError: module 'os' has no attribute 'PathLike'
------
executor failed running [/bin/sh -c python3 download_model.py 124M]: exit code: 1


The tensorflow ""dll hell"" got worse since then, obviously.
 Only this below has worked for me so far, so pro memoriam:
# Clone the GPT-2 repository from GitHub
git clone https://github.com/openai/gpt-2.git
conda init 
bash
conda create --name gpt2 python=3.6

etc. to intialize it and then:
#!/bin/bash

# Create conda environment with Python 3.6


# Activate the conda environment
conda activate gpt2
#or source activate gpt2


# Navigate to the cloned repository
cd gpt-2/

# Install required Python packages
pip install -r requirements.txt

# Install TensorFlow and NumPy packages with specific versions
pip install tensorflow==1.15.2 numpy==1.18.0
pip install requests
pip install fire
pip install regex
# Download the 345M model
python3 ./download_model.py 345M

# Run the interactive console with the 345M model
python3 src/interactive_conditional_samples.py --model_name 345M

and then get out:
conda deactivate
 Interestingly, it is OpenAI ChatGPT plus Bing Chat , the daughers of this one, who has helped me troubleshoot these.
The Github Spaces box where mine runs, another fyi:
~/Downloads/gpt-2 (master) $ neofetch
OS: Ubuntu 20.04.5 LTS x86_64
 Kernel: 5.4.0-1103-azure
 CPU: Intel Xeon Platinum 8272CL (4) @ 2.5GHz
 Memory: 2019MiB / 7957MiB
 Shell: /bin/bash 5.0.17
 Python: 3.10.4
Uptime: 1 day, 1 hour, 41 mins
 Packages: 844 (dpkg)
 Terminal: /dev/pts/0
CPU Usage: 2%
 Disk (/): 17G / 31G (59%)
Host: Virtual Machine 7.0
 Public IP: 20.105.169.104
Ver. 1.0.1
 The text was updated successfully, but these errors were encountered: 
👍3
AndrewZhengal, Entretoize, and yingqianch reacted with thumbs up emoji
All reactions
👍3 reactions"
"https://github.com/openai/gpt-2/issues/313","GPT-2 pretrain loss.","2023-02-08T02:32:30Z","Open issue","No label","Hello, Thanks for your great work. I want to know how to calculate the loss given the raw text. For example:
 I have a sample in training data: "" I want to go to school"". When I input the string into the GPT-2 model, every output logits has a loss value. So the total loss is the sum of all output logits loss?

 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/312","How does ChatGPT support languages except English?","2023-02-08T01:56:07Z","Open issue","No label","How does OpenAI achieve multi-language support?
How did OpenAI do so many human-labeling works?
I guess OpenAI is using all the world users' feedback data.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/310","Chats won't appear on desktop ChatGPT website","2023-01-30T23:08:11Z","Open issue","No label","On ChatGPT January 30 version, chats won't appear on desktop website. I'm using desktop Google Chrome version 109.0.5414.199 and Fedora Linux 37 system
:
Open ChatGPT website (https://chat.openai.com/chat)
Type any sentence on the box
Press Enter
Expected result:
 Chats should appear on website as usual.
Actual result:
 Chats won't appear on website.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/309","How to use gpt-2 for question answers task?","2022-11-28T05:52:45Z","Open issue","No label","No description provided.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/308","docker image","2022-11-15T13:31:54Z","Open issue","No label","the command : docker build --tag gpt-2 -f Dockerfile.gpu .
 return this :
Step 9/12 : RUN python3 download_model.py 124M
 ---> Running in afe900659249
Traceback (most recent call last):
  File ""/usr/local/lib/python3.5/dist-packages/certifi/core.py"", line 14, in <module>
    from importlib.resources import path as get_path, read_text
ImportError: No module named 'importlib.resources'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""download_model.py"", line 3, in <module>
    import requests
  File ""/usr/local/lib/python3.5/dist-packages/requests/__init__.py"", line 112, in <module>
    from . import utils
  File ""/usr/local/lib/python3.5/dist-packages/requests/utils.py"", line 24, in <module>
    from . import certs
  File ""/usr/local/lib/python3.5/dist-packages/requests/certs.py"", line 15, in <module>
    from certifi import where
  File ""/usr/local/lib/python3.5/dist-packages/certifi/__init__.py"", line 1, in <module>
    from .core import contents, where
  File ""/usr/local/lib/python3.5/dist-packages/certifi/core.py"", line 46, in <module>
    Resource = Union[str, ""os.PathLike""]
  File ""/usr/lib/python3.5/typing.py"", line 552, in __getitem__
    dict(self.__dict__), parameters, _root=True)
  File ""/usr/lib/python3.5/typing.py"", line 512, in __new__
    for t2 in all_params - {t1} if not isinstance(t2, TypeVar)):
  File ""/usr/lib/python3.5/typing.py"", line 512, in <genexpr>
    for t2 in all_params - {t1} if not isinstance(t2, TypeVar)):
  File ""/usr/lib/python3.5/typing.py"", line 190, in __subclasscheck__
    self._eval_type(globalns, localns)
  File ""/usr/lib/python3.5/typing.py"", line 177, in _eval_type
    eval(self.__forward_code__, globalns, localns),
  File ""<string>"", line 1, in <module>
AttributeError: module 'os' has no attribute 'PathLike'
The command '/bin/sh -c python3 download_model.py 124M' returned a non-zero code: 1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/307","How to reproduce the reported F-score for the CoQA benchmark?","2022-10-17T10:53:50Z","Open issue","No label","I have a question about how you evaluated GPT-2 on the CoQA dataset.
 We are struggling to reproduce the results reported in the paper (55 F1). We evaluated gpt2-xl from HuggingFace on CoQA and got an F1 of 28.7.
We used the official dev set and evaluation script, which we downloaded from here. Although we get good answers, these answers get a lower score due to the way the original CoQA benchmark evaluator is set up. Did you evaluate it differently?
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/306","interactive_conditional_samples.py crashes if there is more than one context token","2022-07-26T22:25:38Z","Open issue","No label","I can run the generate_unconditional_samples.py script on my GPU without issue, however, when I run the interactive_conditional_samples.py script, it crashes if there is more than one context token.
The interactive_conditional_samples.py script works fine as long as the model prompt only produces one context token, for instance using the prompt ""please"" produces the list of tokens [29688] and correctly generates text. However, it crashes if the model prompt produces two or more context tokens, for instance using the prompt ""pig"" produces the list of tokens [79, 328] and crashes immediately.
When it crashes I'm getting the error:
failed to run cuBLAS routine: CUBLAS_STATUS_EXECUTION_FAILED
And a little further down I see:
Blas xGEMMBatched launch failed : a.shape=[25,2,64], b.shape=[25,2,64], m=2, n=2, k=64, batch_size=25
         [[{{node sample_sequence/model/h0/attn/MatMul}}]]
         [[sample_sequence/while/Exit_3/_1375]]

If anyone has any insight on what might be going wrong, and how I can fix it, I'd really appreciate the help.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/305","Dose the pre-training data also use this prompt structure related to downstream tasks？","2022-07-12T06:50:48Z","Open issue","No label","I read the gpt2 paper, but not sure whether the pre-training data from WebText will add format information.
 For example, we konw data format will be english sentence = french sentencein the translation task. So during pre-training time, will we add similar promt to the training data?
Thanks!
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/303","Playground and API parameters dont seem to have an effect on certain completions","2022-06-22T04:48:36Z","Open issue","No label","I've tried changing temperature, top-p, presence_penalty and frequency_penalty to stop the model from repeating the same joke and other phrases with no success. I thought maybe the playground had a glitch, but this happens in the API too.
Ask for a completion on ""tell me a joke"" and you will likely get ""why did chicken cross the road"". Then variations on that ""why did the duck cross the road"", etc. even if the chicken tokens are removed.
The model knows other jokes.. ""why don't scientists trust atoms? because they make up everything"" and ""why didn't the bicycle go up the hill? because it was two tired"", but these only happen randomly.. once it says the chicken/road joke it cant change it (like a bad comedian ; ).
Sorry to put this here, but the GPT-3 repo is archived for new issues, but it probably also applies here.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/302","Can't download the 1558M model","2022-04-13T13:40:18Z","Closed issue","No label","Using the https://github.com/openai/gpt-2/blob/master/DEVELOPERS.md
 by running
python3 download_model.py 124M
python3 download_model.py 355M
python3 download_model.py 774M

I was able to download all 3 models, but running
python3 download_model.py 1558M

returns me just
<?xml version=""1.0"" encoding=""utf-8""?><Error><Code>BlobNotFound</Code><Message>The specified blob does not exist.
RequestId:9a7b6ba0-001e-00e5-5c36-4f8ab3000000
Time:2022-04-13T12:59:23.8743544Z</Message></Error>

and not the model itself.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/301","train","2022-03-20T09:54:06Z","Closed issue","No label","No description provided.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/299","TypeError: add_code_sample_docstrings() got an unexpected keyword argument 'tokenizer_class'","2021-11-09T15:06:55Z","Open issue","No label","No description provided.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/298","to EXTRA LARGE I only could download with this parameter: **1558M**","2021-08-18T06:25:24Z","Open issue","No label","to EXTRA LARGE I only could download with this parameter: 1558M
$ python download_model.py 1558M
 EXTRA LARGE
 1542M according to https://github.com/openai/gpt-2-output-dataset
 1.5BM according to https://openai.com/blog/gpt-2-1-5b-release/
Originally posted by @maigva in #209 (comment)
 The text was updated successfully, but these errors were encountered: 
👍3
SimonUzL, XingxingZhang, and preccrep reacted with thumbs up emoji
All reactions
👍3 reactions"
"https://github.com/openai/gpt-2/issues/297","Plug-in","2021-08-13T00:01:40Z","Open issue","No label","https://dpaste.com/FWPG6YU7V
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/296","Update GPT2 with the loss from a black box","2021-06-10T08:01:06Z","Open issue","No label","I want to implement an idea using tensorflow and gpt2(huggingface transformers version).
For each iteration, I just let the gpt2 produce some sentences, and these sentences are fed into a black box, which can return the loss, for example, based on the quality of these synthesis sentences produced by current GPT2.
 Then I want to use this loss to update the parameters of GPT2 to improve it.
 This idea is quite simple, however, I find that it is difficult to realize. Possibly because the connection between the loss and the GPT2, is not differentiable.
Here is my codes in tensorflow:
from transformers import GPT2Tokenizer, TFGPT2LMHeadModel, TFGPT2Model, TFAutoModelForCausalLM
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
tokenizer.padding_side = ""left"" 
tokenizer.pad_token = tokenizer.eos_token # to avoid an error

gpt2 = TFGPT2LMHeadModel.from_pretrained('gpt2')
gpt2.trainable = True

#model = TFAutoModelForCausalLM.from_pretrained(""gpt2"")
#model = TFGPT2LMHeadModel.from_pretrained('gpt2')

#model.train()
# when generating, we will use the logits of right-most token to predict the next token
# so the padding should be on the left



num_return_sequences = 1
#prompts = list(x_batch_train.numpy().reshape(-1))
#token_lens = [len(tokenizer.tokenize(sent)) for sent in prompts]
#max_length = math.ceil(np.array(token_lens).max())*2
max_len = get_tokens_len(ds, 0.99) 

cce = tf.keras.losses.CategoricalCrossentropy()   
optimizer = keras.optimizers.Adam(learning_rate=0.0001)


def loss_fn(output_sequences, labels):
    syn_sents = tokenizer.batch_decode(output_sequences, clean_up_tokenization_spaces=True, skip_special_tokens=True)
    syn_sents_pure = []
    for sent, sent_syn in zip(prompts, syn_sents):
        syn_sents_pure.append(sent_syn.replace(sent, '').replace('\n',' ').strip())

    preds = model(np.array(syn_sents_pure))

    assert preds.shape[0] == len(prompts) and preds.shape[1] == num_classes

    label_oht = tf.keras.utils.to_categorical( np.array([label_idx[l] for l in labels]), num_classes = num_classes, dtype='int' ) 
    label_oht_tf = tf.convert_to_tensor(label_oht)
    assert label_oht.shape == preds.shape

    loss_value = cce(label_oht_tf, preds)#.numpy()
    return loss_value

rows = ds.df_test.sample(5)
prompts = rows['content'].tolist()
labels = rows['label'].tolist()

with tf.GradientTape() as tape:
    # Run the forward pass of the layer.
    # The operations that the layer applies
    # to its inputs are going to be recorded
    # on the GradientTape.
    #logits = model(x_batch_train, training=True)  # Logits for this minibatch

    inputs = tokenizer(prompts, padding='max_length', truncation=True, max_length=max_len, return_tensors=""tf"")
    output_sequences = gpt2.generate(
        input_ids = inputs['input_ids'],
        attention_mask = inputs['attention_mask'],
        max_length= max_len*2,
        temperature=1,
        top_k=0,
        top_p=0.9,
        repetition_penalty=1,
        do_sample=True,
        num_return_sequences=num_return_sequences
    )

    # Compute the loss value for this minibatch.
    #loss_value = loss_fn(y_batch_train, logits)
    loss_value = loss_fn(output_sequences, labels) # <tf.Tensor: shape=(), dtype=float32, numpy=0.062384058>


# Use the gradient tape to automatically retrieve
# the gradients of the trainable variables with respect to the loss.
grads = tape.gradient(loss_value, gpt2.trainable_weights)

# Run one step of gradient descent by updating
# the value of the variables to minimize the loss.
optimizer.apply_gradients(zip(grads, model.trainable_weights))

I find that the grads are None once I run it.
 Actually, the black box, loss_fn, decode the outputs from GPT2 to plain texts, and use another model to calculate the loss.
 So is there any proper way to implement my idea?
 Or how to correct my codes ?
Thanks.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/295","ModuleNotFoundError: No module named 'fire'","2021-06-08T09:54:54Z","Open issue","No label","Hi there!
 Please, help to find out why I get this issue after module's import and installation. Behind I've added some proofs (stderr from my terminal):
 (venv) sasha@sasha-m-laptop:/PycharmProjects/pythonProject$ pip3 install fire
 Processing /home/sasha/.cache/pip/wheels/1f/10/06/2a990ee4d73a8479fe2922445e8a876d38cfbfed052284c6a1/fire-0.4.0-py2.py3-none-any.whl
 Requirement already satisfied: six in /usr/lib/python3/dist-packages (from fire) (1.14.0)
 Requirement already satisfied: termcolor in /home/sasha/.local/lib/python3.8/site-packages (from fire) (1.1.0)
 Installing collected packages: fire
 Successfully installed fire-0.4.0
 (venv) sasha@sasha-m-laptop:/PycharmProjects/pythonProject$ python ./venv/final_version.py
 Traceback (most recent call last):
 File ""./venv/final_version.py"", line 4, in 
 import fire
 ModuleNotFoundError: No module named 'fire'
Part of my script with import:
 import logging
 import fnmatch
 import subprocess
 import fire
 import os
def setup_log(name):
 logger = logging.getLogger(name) # > set up a new name for a new logger
logger.setLevel(logging.INFO)

Also I've tried use pip for installation, but there wasn't any impact on result...
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/294","lol guys","2021-05-25T20:51:10Z","Closed issue","No label","Open AI is so scared about people using it for fake news while me and my friends are using it for our english homework 😎
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/293","Using GPT2 for Translation Task","2021-05-18T12:33:18Z","Open issue","No label","Hello,
I am working on a research project in NLP, and I need to use GPT2 for translation. As you have mentioned in section 3.7 in the paper, you introduced a way to change the translation task to the text generation task. I was wondering where you do that in this code? I tried to find it, but I couldn't find it in src.
I appreciate your help in finding a code example of this task.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/292","WHEN I TRY TO TRAIN I GET THIS","2021-04-29T18:43:44Z","Closed issue","No label","Training...
 Traceback (most recent call last):
 File ""train.py"", line 313, in 
 main()
 File ""train.py"", line 291, in main
 feed_dict={train_context: sample_batch()})
 File ""train.py"", line 267, in sample_batch
 return [data_sampler.sample(1024) for _ in range(args.batch_size)]
 File ""train.py"", line 267, in 
 return [data_sampler.sample(1024) for _ in range(args.batch_size)]
 File ""C:\Users\Nic\Documents\gpt-2\gpt-2\load_dataset.py"", line 73, in sample
 assert length < self.total_size // len(
 ZeroDivisionError: integer division or modulo by zero
 i've sat here for over six hrs ive followed everything as closely as possible but just get error after error please help me
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/291","ModuleNotFoundError with TensorFlow","2021-04-29T01:49:49Z","Open issue","No label","I have tried with multiple versions of tensorflow and python (1.12.0, 1.14.0, 1.13.1, 2.4 for tf, 3.9, 3.6.0, and 3.6.5 for python) and every time i try to run any sample the same error message pops up.
(py365) C:\Users\xndrcode\Documents\gpt-2textbot\gpt-2>python3 src/interactive_conditional_samples.py --top_k 40
 Traceback (most recent call last):
 File ""C:\Users\speed\Documents\lemondemonbot\gpt-2\src\interactive_conditional_samples.py"", line 7, in 
 import tensorflow as tf
 File ""C:\Users\speed\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local
 packages\Python39\site-packages\tensorflow_init_.py"", line 41, in 
 from tensorflow.python.tools import module_util as _module_util
 ModuleNotFoundError: No module named 'tensorflow.python'
is there something i've done wrong? is there any way to fix this? any help would be greatly appreciated! thank you loads! and if any additional information is needed please let me know!
p.s. i apologize, i dont know how to format on github, so that's why the text isnt in a codeblock or anything. sorry again.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/290","in domains.txt","2021-04-13T15:19:42Z","Open issue","No label","in domains.txt there is a domain named ""ashemaletube""
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/288","Installation","2021-03-08T21:42:32Z","Open issue","No label","I am wondering how to install it on mac. So far it only shows how to install on windows.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/287","text export","2021-03-10T07:38:35Z","Closed issue","No label","No description provided.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/286","Beginner questions","2021-03-01T13:01:17Z","Open issue","No label","ModuleNotFoundError: No module named 'tensorflow.contrib'
May I know How can I solve this issue. Thank you
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/285","Checkpoint not generating","2021-02-27T22:18:21Z","Open issue","No label","I ran the train.py program of GPT-2 on a txt training data which has 3 stories. I used the 117M parameters model, and it runs, it trains the model, but once it stops it creates checkpoint folder inside it is run1 folder, but none of these files are generated:
checkpoint
model-xxx.data-00000-of-00001
model-xxx.index
model-xxx.meta
Use standard file APIs to check for files with this prefix.
 Loading dataset...
 100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 19.27it/s]
dataset has 12863 tokens
Training...
[1 | 22.35] loss=3.69 avg=3.69
 [2 | 40.40] loss=3.48 avg=3.58
 [3 | 72.00] loss=3.34 avg=3.50
 [4 | 91.34] loss=3.45 avg=3.49
 [5 | 111.14] loss=3.32 avg=3.45
 [6 | 130.68] loss=3.63 avg=3.48
 [7 | 146.00] loss=3.35 avg=3.46
 [8 | 164.12] loss=3.33 avg=3.45
 [9 | 187.81] loss=3.44 avg=3.45
 [10 | 212.46] loss=3.41 avg=3.44
 [11 | 238.91] loss=3.35 avg=3.43
 [12 | 265.70] loss=3.07 avg=3.40
 [13 | 286.85] loss=3.36 avg=3.40
 [14 | 309.50] loss=3.32 avg=3.39
 [15 | 327.70] loss=3.26 avg=3.38
 [16 | 344.01] loss=3.22 avg=3.37
 [17 | 358.19] loss=3.41 avg=3.37
 [18 | 371.93] loss=2.95 avg=3.35
 [19 | 386.32] loss=3.19 avg=3.34
 [20 | 400.90] loss=3.51 avg=3.35
 [21 | 415.34] loss=3.06 avg=3.33
 [22 | 430.17] loss=3.47 avg=3.34
 [23 | 444.54] loss=3.06 avg=3.33
forrtl: error (200): program aborting due to control-C event
Image PC Routine Line Source
 libifcoremd.dll 00007FFD7D033B58 Unknown Unknown Unknown
 KERNELBASE.dll 00007FFDC9D6B443 Unknown Unknown Unknown
 KERNEL32.DLL 00007FFDCC487034 Unknown Unknown Unknown
 ntdll.dll 00007FFDCC5BD241 Unknown Unknown Unknown
What should I do?
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/284","GPT-2 on Wordpress","2021-02-24T09:05:10Z","Open issue","No label","As i've wondering is it possible to make a GPT-2 work inside the wordpress?
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/283","Access Denied error when downloading model 345M","2021-02-17T10:22:40Z","Open issue","No label","I get the following error when downloading model 345M using download_model.py:
<Error>
<Code>AccessDenied</Code>
<Message>Access denied.</Message>
<Details>Anonymous caller does not have storage.objects.list access to the Google Cloud Storage bucket.</Details>
</Error>

Manually opening https://storage.googleapis.com/gpt-2 in the browser produces the same error.
 All links in this comment also produce the same error.
I am using download_model.py in Google Colab.
I tried to download the link in a different browser with empty cash and I get the same mistake.
 The text was updated successfully, but these errors were encountered: 
👍1
longyuxi reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/gpt-2/issues/282","Error with python download_model 117M","2021-02-24T22:35:49Z","Closed issue","No label","https://storage.googleapis.com/gpt-2/models/117M/checkpoint
 ...
 ...
https://storage.googleapis.com/gpt-2/models/117M/vocab.bpe
AccessDenied Access denied. 
Anonymous caller does not have storage.objects.get access to the Google Cloud Storage object.
Getting this error, even when I visit the link normally from the browser.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/281","Error with batch_size > 1","2021-02-18T08:51:23Z","Closed issue","No label","Hello.
Everything works fine until I increase nsamples and batch_size to >1. Tried in docker and in conda environment, with different versions of TF(from 1.12 to 1.15), on Tesla V100 and GF1060 - nothing helps.
Connected to pydev debugger (build 201.6668.115)
C:\ProgramData\Anaconda3\envs\gpt2_2\lib\site-packages\tensorflow\python\framework\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([(""qint8"", np.int8, 1)])
C:\ProgramData\Anaconda3\envs\gpt2_2\lib\site-packages\tensorflow\python\framework\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([(""quint8"", np.uint8, 1)])
C:\ProgramData\Anaconda3\envs\gpt2_2\lib\site-packages\tensorflow\python\framework\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([(""qint16"", np.int16, 1)])
C:\ProgramData\Anaconda3\envs\gpt2_2\lib\site-packages\tensorflow\python\framework\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([(""quint16"", np.uint16, 1)])
C:\ProgramData\Anaconda3\envs\gpt2_2\lib\site-packages\tensorflow\python\framework\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([(""qint32"", np.int32, 1)])
C:\ProgramData\Anaconda3\envs\gpt2_2\lib\site-packages\tensorflow\python\framework\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([(""resource"", np.ubyte, 1)])
2021-02-13 02:10:21.495431: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
WARNING:tensorflow:From C:\ProgramData\Anaconda3\envs\gpt2_2\lib\site-packages\tensorflow\python\framework\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
WARNING:tensorflow:From C:\Users\predoc\PycharmProjects\gpt2_2\gpt-2\sample.py:64: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.cast instead.
Traceback (most recent call last):
  File ""C:\ProgramData\Anaconda3\envs\gpt2_2\lib\site-packages\tensorflow\python\framework\ops.py"", line 1659, in _create_c_op
    c_op = c_api.TF_FinishOperation(op_desc)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Dimensions must be equal, but are 50257 and 2 for 'sample_sequence/Less' (op: 'Less') with input shapes: [2,50257], [2].
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
  File ""C:\ProgramData\Anaconda3\envs\gpt2_2\lib\site-packages\fire\core.py"", line 366, in _Fire
    component, remaining_args)
  File ""C:\ProgramData\Anaconda3\envs\gpt2_2\lib\site-packages\fire\core.py"", line 542, in _CallCallable
    result = fn(*varargs, **kwargs)
  File ""C:/Users/predoc/PycharmProjects/gpt2_2/gpt-2/interactive_conditional_samples.py"", line 77, in interact_model
    temperature=temperature, top_k=top_k, top_p=top_p
  File ""C:\Users\predoc\PycharmProjects\gpt2_2\gpt-2\sample.py"", line 74, in sample_sequence
    past, prev, output = body(None, context, context)
  File ""C:\Users\predoc\PycharmProjects\gpt2_2\gpt-2\sample.py"", line 66, in body
    logits = top_p_logits(logits, p=top_p)
  File ""C:\Users\predoc\PycharmProjects\gpt2_2\gpt-2\sample.py"", line 37, in top_p_logits
    logits < min_values,
  File ""C:\ProgramData\Anaconda3\envs\gpt2_2\lib\site-packages\tensorflow\python\ops\gen_math_ops.py"", line 4901, in less
    ""Less"", x=x, y=y, name=name)
  File ""C:\ProgramData\Anaconda3\envs\gpt2_2\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 788, in _apply_op_helper
    op_def=op_def)
  File ""C:\ProgramData\Anaconda3\envs\gpt2_2\lib\site-packages\tensorflow\python\util\deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""C:\ProgramData\Anaconda3\envs\gpt2_2\lib\site-packages\tensorflow\python\framework\ops.py"", line 3300, in create_op
    op_def=op_def)
  File ""C:\ProgramData\Anaconda3\envs\gpt2_2\lib\site-packages\tensorflow\python\framework\ops.py"", line 1823, in __init__
    control_input_ops)
  File ""C:\ProgramData\Anaconda3\envs\gpt2_2\lib\site-packages\tensorflow\python\framework\ops.py"", line 1662, in _create_c_op
    raise ValueError(str(e))
ValueError: Dimensions must be equal, but are 50257 and 2 for 'sample_sequence/Less' (op: 'Less') with input shapes: [2,50257], [2].

Will be appreciate for any help. Thank you.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/278","Custom pre-trained model...","2020-12-22T16:04:31Z","Open issue","No label","Hello!
I have custom txt dataset, how can I create my pre-training model?
Do I understand correctly that the concept ""fine-tuning"" what is the pretranded model(example 355M) + my training data?
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/277","Is there a way to use gpt-2 with the newest tensorflow (2.4)?","2020-12-26T17:44:20Z","Closed issue","No label","I tried and it didn't work.
I am having trouble installing tensorflow 1.12.0.
I am trying pip3 install tensorflow==1.12.0
Which returns:
ERROR: Could not find a version that satisfies the requirement tensorflow==1.12.0 (from versions: 2.2.0rc1, 2.2.0rc2, 2.2.0rc3, 2.2.0rc4, 2.2.0, 2.2.1, 2.3.0rc0, 2.3.0rc1, 2.3.0rc2, 2.3.0, 2.3.1, 2.4.0rc0, 2.4.0rc1, 2.4.0rc2, 2.4.0rc3, 2.4.0rc4) ERROR: No matching distribution found for tensorflow==1.12.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/276","GPT-2 generates error messages, not actual written language","2020-11-26T16:28:26Z","Open issue","No label","I followed the installation process in Developers.md precisely and am currently in a Docker environment.
When running
python src/generate_longinput_samples.py
(a modified version of the generate_conditional_samples.py file)
with the following settings:
def interact_model(
 model_name='345M',
 seed=None,
 nsamples=3,
 batch_size=1,
 length=1500,
 temperature=1,
 top_k=0,
 top_p=1,
 models_dir='models',
 ):
(I'd like for it to generate 3 samples with the 345M parameters model. I do not quite understand what the length parameter does, but I just turned it up, since my input text is long.)
I get the following error messages:
WARNING:tensorflow:From src/generate_longinput_samples.py:153: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

2020-11-26 15:53:57.503505: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compile
2020-11-26 15:53:57.513755: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3500035000 Hz
2020-11-26 15:53:57.518445: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5a4b980 executing computations on platform Host. Devices:
2020-11-26 15:53:57.518520: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
WARNING:tensorflow:From src/generate_longinput_samples.py:154: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From src/generate_longinput_samples.py:156: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.

WARNING:tensorflow:From /gpt-2/src/sample.py:51: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.

WARNING:tensorflow:From /gpt-2/src/model.py:148: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.

WARNING:tensorflow:From /gpt-2/src/sample.py:64: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.cast` instead.
WARNING:tensorflow:From /gpt-2/src/sample.py:39: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be rem
rsion.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING:tensorflow:From /gpt-2/src/sample.py:67: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.random.categorical` instead.
WARNING:tensorflow:From src/generate_longinput_samples.py:164: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.

WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training
ent) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
2020-11-26 15:54:06.541773: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar T
a_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active
 -ACG|ACG \|{endOFtext|>}

Now here's the kicker: I do not know how much of that is actually an error message. You see the \|{endOFtext|>} at the end there? This is for defining where the AI starts to generate some text / after one text ended. After this error message there are other error messages, but the next sample that is generated is not Sample 1 (that is where it starts) it is Sample 2. Here's how Sample 2 looks like:
======================================== SAMPLE 2 ========================================
 <|endofline|> <|iso8859-8|> ... </optgroup> <optgroup label=""_________________________________text""> <option value=""clientbcc;client/bccv3.nop"">clientbtcm
;clienta;clientw"">v1</option> <option value=""clientcac;client/cacv3.nop"">clientbcc;client/bccv3.nop</option> <option value=""clientcnm;andci/>3</option> <op
tion value=""clientdhcp;client/dhcpv4.nop"">clientbtcm;clienta;clientw</option> <option value=""clientipv4;clientipv4v4;clientipv6,clientipv6v4;clientipvgm;cl
ient/ipv6"">clientiegeo1;Andes</option> <option value=""clientipv6;clientipv6v6;clientipvgm-client.1860.0;clientipv6:/24246mip6cwdh0n9:/192.168.1.7(Mibed)</o
ption> <option value=""clientipv6;clientipv6v6;clientipvimi;clientipv6v6v6v6v6v6v6v6v6v6v6v6v6v6v6v6v6v6v.14+:96809>clientih:l::64674h:poies</option> <strin
g value=""pln["" <optgroup label=""thermal alternatives"">thermal options which keep the device in thermal-cooling mode</option>] ; at the computer floor tempe
rature</string> <optgroup label=""thermal reduction."">Thermal reduction: &pp 2</optgroup> </optgroup> <optgroup label=""changespace""> <optgroup value=""soft"">
0-4: close</optgroup> <optgroup value=""hard"">5-10: close</optgroup> <optgroup value=""intermittent"">sheets log exclude heat.u.ltxt only</optgroup> <optgroup
 value=""folder"">/base/crystalmoon/lips/DIR</optgroup> <optgroup value=""install-date"">/fallout 3.1</optgroup> <optgroup value=""font"">Open Sans MS

THIS IS NOT AN ERROR MESSAGE. This is the generated output. I do not want to show the input text, but let me reassure you that it is highly literary and does not include any of the characters or words written here.
What I think is happening:
My input text is causing some error messages, GPT-2 then takes those error messages as input as well.
Why I think it is happening:
I copied the text from some external HTML source, meaning it does not only include ASCII characters. I am uncertain what characters GPT-2 can comprehend, but I removed ""ä ö ü"" and ""à á"" etc. etc. I removed all non-ASCII characters with the following function: [^\x00-\x7F]+.
I allowed ASCII control characters ([\x00-\x1F]+) to be inside of my input text, as they are valid ASCII characters.
Anyway, if anyone has any idea why this is happening or why I am getting error messages instead of generated text, please let me know!
 The text was updated successfully, but these errors were encountered: 
😄1
jaimu97 reacted with laugh emoji
All reactions
😄1 reaction"
"https://github.com/openai/gpt-2/issues/275","Import tf stack compilation error","2020-10-13T17:05:54Z","Open issue","No label","When trying to run the code I got this error:
File ""C:\Users\cristian\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow_core\python\util\tf_stack.py"", line 29, in 
 from tensorflow.python import _tf_stack
 ImportError: DLL load failed: The specified procedure could not be found.
I already have done pip install tensorflow==1.15
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/274","AttributeError: 'NoneType' object has no attribute 'rfind'","2020-10-01T22:46:46Z","Open issue","No label","I'm super new to coding and python, so I don't really have a clue what I'm doing, but I keep getting this error when I try to finetune 1558M through a Google Cloud TPU.
Loading snapshot None... Traceback (most recent call last): File ""train.py"", line 581, in <module> main() File ""train.py"", line 337, in main saver.restore(sess, ckpt) File ""/home/abigailkkeenan/gpt-2/tflex.py"", line 206, in restore if '.ckpt' in os.path.basename(save_path): File ""/usr/lib/python3.5/posixpath.py"", line 139, in basename i = p.rfind(sep) + 1 AttributeError: 'NoneType' object has no attribute 'rfind'
this is the input that resulted in the error:
PYTHONPATH=src python3 train.py \ --init_tpu \ --model_name 1558M \ --dataset combined2.txt \ --restore_from ‘latest’ \ --run_name ’model1’ \ --learning_rate 0.00001 \ --save_time 60 \ --sample_every 20 \ --sample_num 1 \ --sample_length 192 \ --save_on_ctrlc \ --max_to_keep 100 \ --only_train_transformer_layers \ --batch_size 20
I had the same error the other day on a now preempted TPU and somehow managed to fix it, but I can't remember what I did..
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/273","How can i use GPT to correct the alignment and spellings in a sentence?","2020-09-30T15:28:10Z","Open issue","No label","I have a dataset with 20lakh food item names. And another clean dataset with all the correct vocabularies. I want to use GPT to build a item name corrector. For eg:
IN: ""Cheeesseee Pijjja""
 OUT: ""Cheese Pizza""
I am aware that GPT language model is capable to do this but I'm not sure how it can be implemented. Any help would really be appreciated
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/272","GPT vs BERT, under same computation and data resource, which one is better for downstream tasks like GLUE?","2020-09-30T02:21:20Z","Open issue","No label","Thank you very much.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/270","tensor mask shape may be different with tensor matmul_qk shape","2020-09-11T15:32:16Z","Closed issue","No label","Im reading this code, and wondering as follows
 The tensor mask shape maybe ：[batch, 1, seq_len, seq_len]
 by the code
 `def get_padding_mask(seq):
 with tf.name_scope(""Padding_Mask""):
 seq = tf.cast(tf.math.equal(seq, 0), tf.float32)
    # add extra dimensions to add the padding
    # to the attention logits.
    # TODO by Ethan 2020-09-11, 周五, 23:1:here should be like this ? return seq[:, tf.newaxis, :]
    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)

def attention_mask(size):
 """"""
 if size is 4 then it returns below matrix
 [[0., 1., 1., 1.],
 [0., 0., 1., 1.],
 [0., 0., 0., 1.],
 [0., 0., 0., 0.]]
""""""
with tf.name_scope(""attention_mask""):
    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)
    return mask  # (seq_len, seq_len)

def create_masks(inp):
 with tf.name_scope(""att_masking""):
 att_mask = attention_mask(tf.shape(inp)[1])
 padding_mask = get_padding_mask(inp)
 # TODO by Ethan 2020-09-11, 周五, 23:13: this shape will be [batch, 1, seq_len, seq_len]
 mask = tf.maximum(padding_mask, att_mask)
    return mask`

However, the tensor matmul_qk shape maybe ：[batch, seq_len_q, seq_len_k]
 by the code
 ` def multihead_attention(self, q, k, v, training, mask=None):
 matmul_qk = tf.matmul(q, k, transpose_b=True) # (..., seq_len_q, seq_len_k)
 if self.scale:
 dk = tf.cast(tf.shape(k)[-1], tf.float32)
 matmul_qk = matmul_qk / tf.math.sqrt(dk)
    if mask is not None:
        matmul_qk += (mask * -1e9)`

Im not sure. Somebody help me
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/268","got an error while trying to download","2020-09-02T05:48:15Z","Closed issue","No label","did ""python3 download_model.py 1558M
Traceback (most recent call last):
 File ""download_model.py"", line 4, in 
 from tqdm import tqdm
 ModuleNotFoundError: No module named 'tqdm'
please help
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/267","tf_upgrade_v2 fails.","2020-08-31T08:47:57Z","Open issue","No label","Hi,
 I've tried to upgrade the code to tf2 using tf_upgrade_v2 tool and got the following error:
Traceback (most recent call last): File ""/usr/local/bin/tf_upgrade_v2"", line 8, in <module> sys.exit(main()) File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/tools/compatibility/tf_upgrade_v2_main.py"", line 152, in main args.input_tree, output_tree, args.copy_other_files) File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/tools/compatibility/ast_edits.py"", line 1050, in process_tree _, l_report, l_errors = self.process_file(input_path, output_path) File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/tools/compatibility/ast_edits.py"", line 900, in process_file temp_file) File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/tools/compatibility/ast_edits.py"", line 958, in process_opened_file lines = in_file.readlines() File ""/usr/lib/python3.6/encodings/ascii.py"", line 26, in decode return codecs.ascii_decode(input, self.errors)[0] UnicodeDecodeError: 'ascii' codec can't decode byte 0xc2 in position 786: ordinal not in range(128)
The reason for this error is the following line in encoder.py: 
gpt-2/src/encoder.py
 Line 19 in 0574c57
	bs=list(range(ord(""!""), ord(""~"")+1))+list(range(ord(""¡""), ord(""¬"")+1))+list(range(ord(""®""), ord(""ÿ"")+1)) 
I think that I can bypass this issue by deleting the line, converting the code, and then inserting the line back. Do you think it's a valid solution, or maybe it can break something I didn't think about? Thanks.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/266","KeyError: 'para_index' coming when using Question-Generation(interact.py)","2020-08-27T13:31:36Z","Open issue","No label","I was curious to try using the question generation component. Based on the overall README, I might expect to be able to run interact.py without any arguments, but this doesn't work:
ERROR:pytorch_pretrained_bert.modeling_openai:Model name '' was not found in model name list (openai-gpt). We assumed '' was a path or url but couldn't find files pytorch_model.bin and config.json at this path or url.
Traceback (most recent call last):
  File ""question-generation/interact.py"", line 238, in <module>
    run()
  File ""question-generation/interact.py"", line 149, in run
    model.to(args.device)
AttributeError: 'NoneType' object has no attribute 'to'

That is ok, I downloaded a pretrained model myself from Google drive and tried interact.py again (python3 question-generation/interact.py --model_checkpoint ~/Downloads/gpt2_corefs_question_generation/). When I did so, I first hit the below issue:
File ""question-generation/interact.py"", line 238, in <module>
  run()
File ""question-generation/interact.py"", line 152, in run
  data = get_positional_dataset_from_file(tokenizer, args.filename)
File ""/Users/amooren/Code/squash-generation/question-generation/dataloader.py"", line 81, in get_positional_dataset_from_file
  with open(file, 'rb') as f:
FileNotFoundError: [Errno 2] No such file or directory: 'data/instances_dev.pkl'

Reading the overall README, I inferred that I needed to download instances_dev.pkl, and I found something closely named instances_dev.pickle and instances_coref_dev.pickle here. I assumed I'd need the coref labeled one, as the pretrained folder is named ""gpt2_corefs_question_generation"". But when I tried to use this, I hit the next issue:
Traceback (most recent call last):
  File ""question-generation/interact.py"", line 238, in <module>
    run()
  File ""question-generation/interact.py"", line 168, in run
    para_index = inst[""para_index""]
KeyError: 'para_index'

Would you mind clarifying the steps to test out just this component? It would greatly speed up my efforts.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/265","Running GPT-2 with Tensorflow2.2+ issues","2020-08-27T01:42:46Z","Closed issue","No label","Hi! I have been trying to install GPT-2 locally through several methods, but due to Tensorflow 2.2 (and greater) I have encounter many issues. First with hparam import in the first lines of model.py. I solved this issue thanks to this: https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/utils/hparam.py After that several errors appeared related to Tensorflow not having attributes like tf.Session, tf.get_variable, etc, which I solved usingimport tensorflow.compat.v1 as tf at the beginning of the file, but that developed in other complex errors when the console encounters these lines of code in models.py:
def mlp(x, scope, n_state, *, hparams):
    with tf2.variable_scope(scope):
        nx = x.shape[-1].value
        h = gelu(conv1d(x, 'c_fc', n_state))
        h2 = conv1d(h, 'c_proj', nx)
        return h2

The console outputs ""int"" has no attribute .value
 And at this point I have no idea how to fix this issue. So I tried this: https://github.com/akanyaani/gpt-2-tensorflow2.0 But it tries to install tensorflow 2.0.1, which is not availabe.
I have also tried to install earlier versions of tensorflow, but apart to the fact that the tf.contrib module is deprecated in TF 2.0, my console output an error saying that it was not compatible with it.
Is there a different way to run GPT-2 now that only Tensorflow 2.2(and greater) is available? I would really love to try GPT-2 in many projects, and it would also be a really good practice until GPT-3 API is released. I will really appreciate any help. Thank you
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/264","BPE tokenizer has problem in training the low source language like persian","2020-08-24T19:37:23Z","Open issue","No label","hi everybody, I'm trying to start train gpt2 in a large amount of Persian data for the special tasks..
 but now I got a problem with this tokenizer...
 after training one data, the .json and .txt frequency information files include some unknown characters:(((((
 for example:
 ""ĠØ¨Ø§Ø¯ÙĩØ§""
 something like this...
 it's good to mention that BPE tokenizer has no problem in English texts... and it makes me confused because this had trained with the Persian dataset but couldn't encode simple Persian sentence...
logging.basicConfig(level=logging.INFO)

# paths = [str(x) for x in Path(""./eo_data/"").glob(""**/*.txt"")]
paths = [str(x) for x in Path(""./txt/"").glob(""**/*.txt"")]
tokenizer = ByteLevelBPETokenizer()

for path in paths:
    # logging.info(path)
    try:
        tokenizer.train(files=paths, vocab_size=52_000, min_frequency=2, special_tokens=[
            ""<s>"",
            ""<pad>"",
            ""</s>"",
            ""<unk>"",
            ""<mask>"", ])
    except Exception as e:
        logging.warning(e)
        logging.warning(path)


# Save files to disk
tokenizer.save(""."",""Maryam_V1"")

it is a related piece of code... make me happy by your guidance:(((
 The text was updated successfully, but these errors were encountered: 
👍1
amirsh87 reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/gpt-2/issues/263","simple length","2020-08-24T01:56:52Z","Open issue","No label","I want the output length is about 5000 characters under command python src/interactive_conditional_samples.py
Is that possible? many thanks.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/261","The specified module was not found","2020-08-18T08:04:55Z","Open issue","No label","This exception occurred after I change TensorFlow version into 1.13.1
Traceback (most recent call last):
 File ""C:\Users\L\AppData\Local\Programs\Python\Python38\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in 
 from tensorflow.python.pywrap_tensorflow_internal import *
 File ""C:\Users\L\AppData\Local\Programs\Python\Python38\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in 
 _pywrap_tensorflow_internal = swig_import_helper()
 File ""C:\Users\L\AppData\Local\Programs\Python\Python38\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
 _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
 File ""C:\Users\L\AppData\Local\Programs\Python\Python38\lib\imp.py"", line 242, in load_module
 return load_dynamic(name, filename, file)
 File ""C:\Users\L\AppData\Local\Programs\Python\Python38\lib\imp.py"", line 342, in load_dynamic
 return _load(spec)
 ImportError: DLL load failed while importing _pywrap_tensorflow_internal: 找不到指定的模块。
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
 File ""src/interactive_conditional_samples.py"", line 7, in 
 import tensorflow as tf
 File ""C:\Users\L\AppData\Local\Programs\Python\Python38\lib\site-packages\tensorflow_init_.py"", line 24, in 
 from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import
 File ""C:\Users\L\AppData\Local\Programs\Python\Python38\lib\site-packages\tensorflow\python_init_.py"", line 49, in 
 from tensorflow.python import pywrap_tensorflow
 File ""C:\Users\L\AppData\Local\Programs\Python\Python38\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in 
 raise ImportError(msg)
 ImportError: Traceback (most recent call last):
 File ""C:\Users\L\AppData\Local\Programs\Python\Python38\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in 
 from tensorflow.python.pywrap_tensorflow_internal import *
 File ""C:\Users\L\AppData\Local\Programs\Python\Python38\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in 
 _pywrap_tensorflow_internal = swig_import_helper()
 File ""C:\Users\L\AppData\Local\Programs\Python\Python38\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
 _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
 File ""C:\Users\L\AppData\Local\Programs\Python\Python38\lib\imp.py"", line 242, in load_module
 return load_dynamic(name, filename, file)
 File ""C:\Users\L\AppData\Local\Programs\Python\Python38\lib\imp.py"", line 342, in load_dynamic
 return _load(spec)
 ImportError: DLL load failed while importing _pywrap_tensorflow_internal: 找不到指定的模块。
Failed to load the native TensorFlow runtime.
I can only find ""_pywrap_tensorflow_internal.pyd"" this path.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/260","Is it possible to make the output text's length greater than 1024?","2020-08-15T03:42:28Z","Open issue","No label","know how can I control the output of a finetuned model when running python generate_unconditional_samples.py ? For example I want the output is ten thousand characters. the default output is likely to have only a few hundred characters. I changed the para length's value form none to 6000 and run , it throws a error:
“ raise ValueError(""Can't get samples longer than window size: %s"" % hparams.n_ctx)
 ValueError: Can't get samples longer than window size: 1024”
def sample_model(
 model_name='124M',
 seed=None,
 nsamples=0,
 batch_size=1,
 length=6000,
 temperature=1,
 top_k=0,
 top_p=1,
 models_dir='models',
 ):
Is it possible to make the output text's length greater than 1024? and if possible , how could I do that? thanks.
 The text was updated successfully, but these errors were encountered: 
👍1
Samvid95 reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/gpt-2/issues/259","download_model.py never gets requests","2020-08-17T16:50:45Z","Closed issue","No label","Hi, I'm having an issue trying to use download_model.py to get any of the models. I've tried some debugging and it looks like the requests.get commands are never returning. When I manually navigate to the address in browser, it looks like the page contains no files/data. Have the locations of the models changed? This occurs for all 4 sets of model data.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/256","Openai/gpt-2","2020-07-21T06:38:19Z","Open issue","No label","No description provided.
 The text was updated successfully, but these errors were encountered: 
👎13
IndigoLily, AmolDerickSoans, kaminskia1, kurisufriend, kami4ka, Ziltosh, SCR33M, jaimu97, drusepth, nikhilanayak, and 3 more reacted with thumbs down emoji
All reactions
👎13 reactions"
"https://github.com/openai/gpt-2/issues/254","A probable tweak for creation of word vectors ?","2020-07-16T07:07:54Z","Open issue","No label","While creation of the word vectors do you account for the polarity of the sentence?
For example :
 'You are good'
 'You are not bad'
Here, good & bad have been used equally in the context of 'you' & 'are'.
 This might lead to them having rather similar word vectors.
But since these words are antonyms of one another, they shouldn't be used interchangeably in the same context.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/253","nda deactivate","2020-07-12T18:13:53Z","Open issue","No label","No description provided.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/250","How can I use gpt-2 for text classification?","2020-06-16T00:15:16Z","Open issue","No label","Is there any code or repo where anyone has used the gpt2 for text classification? or used the gpt2 transformers from HuggingFace?
 The text was updated successfully, but these errors were encountered: 
👍5
arthurmarcal, antonio-sb, bitmman, iskyd, and Jheel-patel reacted with thumbs up emoji
All reactions
👍5 reactions"
"https://github.com/openai/gpt-2/issues/248","335M model disappeared","2020-06-09T04:18:18Z","Closed issue","No label","Disregard
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/247","Build error with Dockerfile.gpu","2020-06-07T16:47:36Z","Open issue","No label","I get the below error when I tried this on my Jetson Xavier NX. Anyone had a similar issue?
 My device has ARM v8 processor, OS Type 64-bit
~/gpt-2$ sudo docker build --tag gpt-2 -f Dockerfile.gpu .
Sending build context to Docker daemon 4.712MB
 Step 1/9 : FROM tensorflow/tensorflow:1.12.0-gpu-py3
 ---> 413b9533f92a
 Step 2/9 : LABEL com.nvidia.volumes.needed=""nvidia_driver""
 ---> Using cache
 ---> 05ba5810d177
 Step 3/9 : LABEL com.nvidia.cuda.version=""${CUDA_VERSION}""
 ---> Using cache
 ---> ebbf24a709ba
 Step 4/9 : ENV NVIDIA_VISIBLE_DEVICES=all NVIDIA_DRIVER_CAPABILITIES=compute,utility NVIDIA_REQUIRE_CUDA=""cuda>=8.0"" LANG=C.UTF-8
 ---> Using cache
 ---> ca9f7af4b739
 Step 5/9 : RUN pip3 install -r requirements.txt
 ---> Running in 7839cb2e5a88
standard_init_linux.go:211: exec user process caused ""exec format error""
 The command '/bin/sh -c pip3 install -r requirements.txt' returned a non-zero code: 1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/246","got this error","2020-05-31T03:01:35Z","Open issue","No label","~/gpt-2$ python3 src/interactive_conditional_samples.py
 Traceback (most recent call last):
 File ""src/interactive_conditional_samples.py"", line 9, in 
 import model, sample, encoder
 File ""/home/prasad/gpt-2/src/model.py"", line 3, in 
 from tensorflow.contrib.training import HParams
 ModuleNotFoundError: No module named 'tensorflow.contrib'
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/245","checksum does not match error for 1558M model","2020-05-23T18:47:56Z","Open issue","No label","Downloaded the master branch and 1558M model.
 Got checksum error.
Is there a way to bypass the checksum and keep working ?
Command given:
 python3 src/interactive_conditional_samples.py --model_name=1558M
Error in linux:
2020-05-23 23:57:34.717724: W tensorflow/core/framework/op_kernel.cc:1401] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Data loss: Checksum does not match: stored 1096252745 vs. calculated on the restored bytes 1479428755
 Traceback (most recent call last):
 File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py"", line 1334, in _do_call
 return fn(*args)
 File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py"", line 1319, in _run_fn
 options, feed_dict, fetch_list, target_list, run_metadata)
 File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py"", line 1407, in _call_tf_sessionrun
 run_metadata)
 tensorflow.python.framework.errors_impl.DataLossError: Checksum does not match: stored 1096252745 vs. calculated on the restored bytes 1479428755
 [[{{node save/RestoreV2}}]]
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
 File ""src/generate_unconditional_samples.py"", line 79, in 
 fire.Fire(sample_model)
 File ""/usr/local/lib/python3.7/dist-packages/fire/core.py"", line 138, in Fire
 component_trace = _Fire(component, args, parsed_flag_args, context, name)
 File ""/usr/local/lib/python3.7/dist-packages/fire/core.py"", line 468, in _Fire
 target=component.name)
 File ""/usr/local/lib/python3.7/dist-packages/fire/core.py"", line 672, in _CallAndUpdateTrace
 component = fn(*varargs, **kwargs)
 File ""src/generate_unconditional_samples.py"", line 67, in sample_model
 saver.restore(sess, ckpt)
 File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/saver.py"", line 1276, in restore
 {self.saver_def.filename_tensor_name: save_path})
 File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py"", line 929, in run
 run_metadata_ptr)
 File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py"", line 1152, in _run
 feed_dict_tensor, options, run_metadata)
 File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py"", line 1328, in _do_run
 run_metadata)
 File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py"", line 1348, in _do_call
 raise type(e)(node_def, op, message)
 tensorflow.python.framework.errors_impl.DataLossError: Checksum does not match: stored 1096252745 vs. calculated on the restored bytes 1479428755
 [[node save/RestoreV2 (defined at src/generate_unconditional_samples.py:65) ]]
Caused by op 'save/RestoreV2', defined at:
 File ""src/generate_unconditional_samples.py"", line 79, in 
 fire.Fire(sample_model)
 File ""/usr/local/lib/python3.7/dist-packages/fire/core.py"", line 138, in Fire
 component_trace = _Fire(component, args, parsed_flag_args, context, name)
 File ""/usr/local/lib/python3.7/dist-packages/fire/core.py"", line 468, in _Fire
 target=component.name)
 File ""/usr/local/lib/python3.7/dist-packages/fire/core.py"", line 672, in _CallAndUpdateTrace
 component = fn(*varargs, **kwargs)
 File ""src/generate_unconditional_samples.py"", line 65, in sample_model
 saver = tf.train.Saver()
 File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/saver.py"", line 832, in init
 self.build()
 File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/saver.py"", line 844, in build
 self._build(self._filename, build_save=True, build_restore=True)
 File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/saver.py"", line 881, in _build
 build_save=build_save, build_restore=build_restore)
 File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/saver.py"", line 513, in _build_internal
 restore_sequentially, reshape)
 File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/saver.py"", line 332, in _AddRestoreOps
 restore_sequentially)
 File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/saver.py"", line 580, in bulk_restore
 return io_ops.restore_v2(filename_tensor, names, slices, dtypes)
 File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_io_ops.py"", line 1572, in restore_v2
 name=name)
 File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/op_def_library.py"", line 788, in _apply_op_helper
 op_def=op_def)
 File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/deprecation.py"", line 507, in new_func
 return func(*args, **kwargs)
 File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py"", line 3300, in create_op
 op_def=op_def)
 File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py"", line 1801, in init
 self._traceback = tf_stack.extract_stack()
DataLossError (see above for traceback): Checksum does not match: stored 1096252745 vs. calculated on the restored bytes 1479428755
 [[node save/RestoreV2 (defined at src/generate_unconditional_samples.py:65) ]]
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/243","ModuleNotFoundError: No module named 'tqdm'","2020-05-12T15:37:48Z","Open issue","No label","After installing the requirements.txt with pip3 I get:
python3 download_model.py 1558M
Traceback (most recent call last):
  File ""download_model.py"", line 4, in <module>
    from tqdm import tqdm
ModuleNotFoundError: No module named 'tqdm'

when trying to python3 download_model.py 1558M even if I have tqdm 4.31.1 installed. I also tried uninstalling and installing the latest version of tqdm 4.46.0
 The text was updated successfully, but these errors were encountered: 
👍1
Agent215 reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/gpt-2/issues/242","Native Installation Leads to a ""no attribute sort"" error?","2020-04-29T20:26:19Z","Open issue","No label","After following the instructions carefully in bash, for the native installation they inevitably lead to this error: AttributeError: module 'tensorflow' has no attribute 'sort'
I wrote a SO post about it here which has the full stack trace: https://stackoverflow.com/questions/61510865/tensorflow-has-no-attribute-sort-in-gpt-2-git-release
What should I do to have a successful install and resolve this issue?
 The text was updated successfully, but these errors were encountered: 
👍5
wjlight, mcenderdragon, tom-doerr, Adrianjewell91, and JakeRoggenbuck reacted with thumbs up emoji
All reactions
👍5 reactions"
"https://github.com/openai/gpt-2/issues/241","Add full domain names to model card's domains.txt","2020-04-07T18:44:44Z","Open issue","No label","hello! The model_card.md doc describes the procedure for gathering the links in WebText sometime in 2017, and links to another doc with the top domains, domains.txt. These aren't really full domains though, so even in the top 20 it's hard to understand what some of these mean (eg, go is probably abcnews.go.com based on searching reddit). To give another specific example, item 58 is adf but it's ambiguous whether these are pages are from a company that has ""led the industry in supplying premium poultry ingredients to pet-food manufacturers"" (adf.com) or ""a Pagan church based on ancient Indo-European traditions"" (adf.org).
Further, other items appear to refer to archive sites or URL shorteners, so don't really provide information about the ultimate domain that the scraper took the text from (eg, presumably archive is archive.org and goo is goo.gl).
As a specific suggestion, could you rebuild the domains.txt in the same overall format, but including the full domain names that the text came from (not the proximal link posted on Reddit)? I'm curious about this from the perspective of research exploring how differences in training data flow through to differences in language models trained on other tasks. EDIT: I can also see links to a sample of WebText are available in the gpt-2-output-dataset detector docs. Thanks!
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/240","[question] reasonable and maximum possible input dataset size for 1.5b(or any other) model?","2020-03-23T13:57:40Z","Open issue","No label","What is the largest possible input dataset for fine-tuning of gpt-2 1.5b or any other model (774M)?
 For example, does it make sense to train the model on 300kk tokens(1.2GB of txt file, or ~500MB npz file)? If yes, then what is the decent limit for any of those models which can still improve it and make sense for fine-tuning? Or does it better to split the input dataset into several parts and then use it one by one for fine-tuning of the same checkpointed model?
 The text was updated successfully, but these errors were encountered: 
👍2
trevyn and siddas27 reacted with thumbs up emoji
All reactions
👍2 reactions"
"https://github.com/openai/gpt-2/issues/239","Can GPT-2 be used to generate sentences from set of keywords?","2020-03-23T09:08:05Z","Open issue","No label","I have a use case where I have set of keywords and target sentences. I want to build a model which will take keywords as inputs and generate sentences as outputs. Can this be achieved using GPT-2 in some way?
Example - set of keywords (Sam, Went, India, Wedding) expected output Sam went to India for his friends wedding
 The text was updated successfully, but these errors were encountered: 
👍1
ratulalahy reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/gpt-2/issues/238","ModuleNotFoundError when training the model","2020-03-18T15:25:50Z","Open issue","No label","Hi guys, I am working on finetuning a GPT-2 in Google Collab to use in (behavioral) research, but I am very new to this and I don't understand what's going on. I have been using it without errors for a few weeks now, but now the algorithm does not cooperate with me anymore lol :)
Traceback (most recent call last):
 File ""./train.py"", line 14, in 
 import model, sample, encoder
 File ""/content/gpt-2/src/model.py"", line 3, in 
 from tensorflow.contrib.training import HParams
 ModuleNotFoundError: No module named 'tensorflow.contrib'
I've tried to import tensorflow.contrib manually (import tensorflow.contrib) but it does not solve the problem. Also, I have tried to see which tensorflow was installed, uninstalled and reinstalled newer versions but it still does not work.
Can someone please help me? Thanks so much. If needed, here is the google collab link: 
Cheers,
 Marloes
 The text was updated successfully, but these errors were encountered: 
👍5
mitya12342, eddiewang, NexusXe, tobico, and Olegt0rr reacted with thumbs up emoji
All reactions
👍5 reactions"
"https://github.com/openai/gpt-2/issues/237","blank model","2020-03-16T12:45:36Z","Open issue","No label","is it possible to get a working blank model?
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/236","TensorFlow.contrib.hparams import error","2020-03-14T07:01:28Z","Closed issue","No label","Had a bit of difficulty getting started
 got:
ModuleNotFoundError: No module named 'tensorflow.contrib
For anyone else trying to install natively who has a messy python env, I finally solved it by running:
python3 -m pip install tensorflow==1.13.1
and
python3 -m pip install regex
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/235","A few questions..","2020-03-06T22:41:48Z","Open issue","No label","I'm using Pyhon3.6, tensorflow-gpu:1.14.0, and CUDA 10.0.
Is there any way to use the GPU for training but use system memory rather than GPU memory? Would that even be beneficial since memory speed would become a bottleneck? I tried setting ""per_process_gpu_memory_fraction="" to more than 1 but it throws an illegal memory access error. I can run the model on GPU but cannot train on GPU because of memory limitations.
When using the ""interactive model"" does it have any form of retention? Is there any training caused by this interaction? Or does it function in a way that if you wanted to setup a conversation bot you would have a separate script for conversation then feed GPT-2 "" conversation memory"" along with the current ""conversation input""?
Is there a limit to the size of text used for the model prompt?
 The text was updated successfully, but these errors were encountered: 
👍1
red40forever reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/gpt-2/issues/234","Does gpt-2 have this function ?","2020-03-06T17:20:35Z","Open issue","No label","Does gpt-2 have this function ?
 can we use gpt-2 to calculate the probability that a input text as a real/resonable sentence base on the corpus we trained
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/233","gpt-2 slow inference","2020-03-04T02:00:23Z","Open issue","No label","Using gpt-2 345M model to run inferences in batches between 10 and 100 documents with approximately ~60 tokens is taking ~15ms in a Tesla T4 GPU machine. Why? That looks really bad if someone would like to hook this up in a realtime pipeline... It means ~66 inferences/second which isn't sufficient for many realtime systems. Am I losing something? Is there any up-to-date benchmark I can use to compare my numbers?
pytorch 1.4.0
 transformers 2.3.0
 CUDA 10.1
 apex 0.1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/232","Get prediction Score","2020-03-02T08:52:26Z","Open issue","No label","Can we tweak the script so for each prediction we can have the score of that output ?
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/231","Modifying to work for tensorflow 2.0","2020-02-20T19:13:09Z","Open issue","No label","Since I am running Arch, it is an absolute pain to deal with older versions of software. Therefore, I want to update this to tensorflow 2.0. The biggest problem is in the model.py script, since it relies on tensorflow.contrib which no longer exists (even in tensorflow.compat.v1). How can I easily modify this script to make this work with the latest and greatest tensorflow?
 The text was updated successfully, but these errors were encountered: 
👍9
DaveXanatos, iam-abbas, wdonno, andrewy12, NexusXe, djakopacSM, j-planet, IndigoLily, and 2br-2b reacted with thumbs up emoji
All reactions
👍9 reactions"
"https://github.com/openai/gpt-2/issues/229","ZeroDivisionError: integer division or modulo by zero","2020-02-02T20:12:17Z","Open issue","No label","Hey,
I've been trying to get this to run, but keep running into an issue when I get to the training phase.
 I keep getting this back:
Loading checkpoint models\117M\model.ckpt
 Loading dataset...
 100%|████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 64.01it/s]
 dataset has 0 tokens
 Training...
 Traceback (most recent call last):
 File ""Desktop/Thesis_GPT2_Training/gpt-2-finetuning/src/train.py"", line 297, in 
 main()
 File ""Desktop/Thesis_GPT2_Training/gpt-2-finetuning/src/train.py"", line 275, in main
 feed_dict={context: sample_batch()})
 File ""Desktop/Thesis_GPT2_Training/gpt-2-finetuning/src/train.py"", line 251, in sample_batch
 return [data_sampler.sample(1024) for _ in range(args.batch_size)]
 File ""Desktop/Thesis_GPT2_Training/gpt-2-finetuning/src/train.py"", line 251, in 
 return [data_sampler.sample(1024) for _ in range(args.batch_size)]
 File ""C:\Users\luffm\Desktop\Thesis_GPT2_Training\gpt-2-finetuning\src\load_dataset.py"", line 74, in sample
 self.chunks
 ZeroDivisionError: integer division or modulo by zero
I saw someone else was able to fix this problem by increasing the the lines of text, but I've tried doing that with no luck. (I've tried running it with 1000, 4000, and 10,000 lines of text)
Has anyone else had this issue or found a way to fix it?
Thank you.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/228","src/model.py gelu uses numpy functions","2020-01-28T12:43:48Z","Open issue","No label","The gelu function in the src/model.py script uses numpy.sqrt and numpy.pi, how does this affect GPU performance, and does it even work with GPU? If not, it should be changed to similar functions in tf.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/227","BPE using sequence of bytes. HOW ?","2020-01-25T23:41:49Z","Open issue","No label","Hello, I read the paper about GPT 2 : it says that they used BPE on a sequence of bytes and that they only needed a vocab size of 256. I researched the internet but didn't find any explanation on how BPE on sequence of bytes work and why the 256 vocab size. I am confusing since I don't know how this works compared to applying BPE on normal characters and what are the clear motivations since they also say that character/byte level LMs don't work great. How this is different.
 THANKS.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/226","recommended VRAM for the small 117M model","2020-02-27T10:26:31Z","Closed issue","No label","Hello,
is it possible to train the small 117M GPT-2 model with 6 GB VRAM using FP16?
Recommended Vram is 12 GB so with fp16 I could halve the memory consumption?
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/222","enc.encoder[""<|endoftext|>""] is wrong and nobody realizes it.","2019-12-21T00:27:21Z","Open issue","No label","Relevant tweet chain:
https://twitter.com/theshawwn/status/1208169319223480322
https://twitter.com/theshawwn/status/1208171700057186304
Basically, you're prompting the model with <|endoftext|> (a single token with BPE value 50256 or whatever), but the BPE encoder encodes <|endoftext|> as <| end of text|>, five separate tokens. It's completely different.
 The text was updated successfully, but these errors were encountered: 
👍3
apeguero1, aaronjanse, and schneiderfelipe reacted with thumbs up emoji
All reactions
👍3 reactions"
"https://github.com/openai/gpt-2/issues/221","how can i get the input and output node names to generate pb file?","2019-12-20T07:14:20Z","Open issue","No label","I am working to make pb file of the checkpoint but i don't know the input and output nodes of the model so i am unable to make frozen graph of the model for prediction.
 i tried so many ways to make pb file. In one of the way i am getting error that PB file should be less than 2gb
 How to resolve this issue?
 Thank you for your time.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/220","CUBLAS_STATUS_NOT_INITIALIZED in colab.","2019-12-17T18:43:27Z","Closed issue","No label","Previously, the 1558М model perfectly generated text, now I get this error. Who faced
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/219","CUBLAS_STATUS_NOT_INITIALIZED in colab.","2019-12-17T18:09:06Z","Open issue","No label","No description provided.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/217","Generate text samples by giving some features or text as a parameter","2019-12-12T14:12:57Z","Open issue","No label","I am using GPT2 small model (124M). I have trained the model on real Estate home descriptions samples. So it start generating homes description. I am use the GPT2 following generate function to generate home description samples. gpt2.generate(sess,length=500,temperature=0.9,nsamples=10,run_name='run1')
Well the generated description well look like this,
2700 NW Dogwood St Unit H104 is a condo in Seattle. WA 98146. This 830 square foot condo features 2 bedrooms and 2 bathrooms having 3 stories and 3 floors.This property was built in 1992 and last sold for $40000. .Based on Redfin's Seattle data. we estimate the home's value is $472830. Comparable nearby homes include 2700 NW Dogwood St Unit H101. 2700 NW Dogwood St Unit H202.
So my question is that how can i give the parameter like ""4 stories 4 floors"" to GPT2 generate function and it will generating the home description samples with this feature (""4 stories 4 floors"") and if i give parameter like 2 bedrooms and 2 bathrooms it will start generating samples with this feature ( 3 bedrooms and 3 bathrooms )
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/216","Are there some research papers about text-to-set generation?","2019-12-11T01:54:35Z","Closed issue","No label","I know this question is a little out of topic. But it is helpful to me. Thank you.
Text-to-(word)set generation or sequence-to-(token)set generation.
For example, input a text and then output the tags for this text:
'Peter is studying English' --> {'good behavior','person','doing something'}
Thank you!
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/215","No such file or directory: 'models/774M/encoder.json","2019-12-08T20:12:53Z","Closed issue","No label","I am using mac 10.15.1
 and I am using: src/generate_unconditional_samples.py
However, I have an error: FileNotFoundError: [Errno 2] No such file or directory: 'models/774M/encoder.json'
Checking in the model 774M folder I can see the file:
 /Users/XXX/gpt-2/models/774M/encoder.json
Here the entire message
 (tf1) XXX:~ XXX$ python3 ~/gpt-2/src/generate_unconditional_samples.py
 Traceback (most recent call last):
 File ""..../src/generate_unconditional_samples.py"", line 79, in 
 fire.Fire(sample_model)
 File ""/Applications/anaconda3/envs/tf1/lib/python3.7/site-packages/fire/core.py"", line 138, in Fire
 component_trace = _Fire(component, args, parsed_flag_args, context, name)
 File ""/Applications/anaconda3/envs/tf1/lib/python3.7/site-packages/fire/core.py"", line 471, in _Fire
 target=component.name)
 File ""/Applications/anaconda3/envs/tf1/lib/python3.7/site-packages/fire/core.py"", line 675, in _CallAndUpdateTrace
 component = fn(*varargs, **kwargs)
 File ""..../gpt-2/src/generate_unconditional_samples.py"", line 44, in sample_model
 enc = encoder.get_encoder(model_name, models_dir)
 File ""..../gpt-2/src/encoder.py"", line 109, in get_encoder
 with open(os.path.join(models_dir, model_name, 'encoder.json'), 'r') as f:
 FileNotFoundError: [Errno 2] No such file or directory: 'models/774M/encoder.json'
Any advise?
 Thanks
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/214","GPT-2 for Webnlg","2019-11-28T16:22:09Z","Open issue","No label","Can we leverage GPT-2 pre-trained model for WebNLG tasks http://webnlg.loria.fr/pages/challenge.html
The WebNLG challenge consists in mapping data to text
 similar to what is being done in https://github.com/tyliupku/wiki2bio.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/213","Out of Memory Error","2019-11-19T22:24:00Z","Open issue","No label","I get the below errors when running gpt-2. The model runs in the end and seems to work, but is there any way to fix this?
Thanks!
2019-11-19 17:13:26.908876: E tensorflow/stream_executor/cuda/cuda_driver.cc:893] failed to alloc 4294967296 bytes on host: CUDA_ERROR_OUT_OF_MEMORY: out of m
emory

2019-11-19 17:13:26.913409: W .\tensorflow/core/common_runtime/gpu/gpu_host_allocator.h:44] could not allocate pinned host memory of size: 4294967296

2019-11-19 17:13:26.917233: E tensorflow/stream_executor/cuda/cuda_driver.cc:893] failed to alloc 3865470464 bytes on host: CUDA_ERROR_OUT_OF_MEMORY: out of m
emory

2019-11-19 17:13:26.921787: W .\tensorflow/core/common_runtime/gpu/gpu_host_allocator.h:44] could not allocate pinned host memory of size: 3865470464
2019-11-19 17:13:26.925608: E tensorflow/stream_executor/cuda/cuda_driver.cc:893] failed to alloc 3478923264 bytes on host: CUDA_ERROR_OUT_OF_MEMORY: out of m
emory

2019-11-19 17:13:26.930056: W .\tensorflow/core/common_runtime/gpu/gpu_host_allocator.h:44] could not allocate pinned host memory of size: 3478923264

 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/212","Finding the preceding text, the input block that would create a text block.","2019-11-19T14:03:10Z","Open issue","No label","Finding the preceding text. Or feeding into output - to get input.
I've been working on batch processing for a Seed.txt file - I can pass 10 lines at a time through the sampler, before then passing the output back in - to get a continuous text output - with good contextual continuation, despite 1023 maximum token(s).
I am more a (retired) scientist than a programmer, , but I would like a proper ""predictive page text editor"" Like the Sci-Fi Atom one - but more fully functioned to be developed, i.e. mark which text to keep, and which text to predict, and have some undo.
in my case my training data on songs, I have a prototype Song writer (Songster) A.I. framework.
So what I would like to do is - Write a partial song or title ( required output), back propagate that, and record the input state, pass the input state through the decoder - to get the song or text, that the RNN thinks came before the current text.
This can be used as a seed to generate appropriate changes.
Except I can't find anyone who has done that, in any form - any ideas if it is easy and I'm missing the command, i suppose it is partially running the back propagation. Then outputting the input state in a decodable form.
Another use of this (backpropagation sampling) would be to confirm how accurate the model is on ""remembering"" historical data, by being able to create a time series history for a set of readings, then compare, long term charts.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/210","No module named 'tensorflow.contrib'","2019-11-15T18:11:17Z","Closed issue","No label","I am trying to use
 python3 src/interactive_conditional_samples.py --top_k 40
But it gives me this error
 Traceback (most recent call last):
 File ""src/interactive_conditional_samples.py"", line 9, in 
 import model, sample, encoder
 File ""/.../gpt-2/src/model.py"", line 3, in 
 from tensorflow.contrib.training import HParams
 ModuleNotFoundError: No module named 'tensorflow.contrib'
Details of Tensorflow
 tensorflow 2.0.0
 tensorflow-addons 0.6.0
 tensorflow-estimator 2.0.1
How can I solve this issue?
 Can I replace ""from tensorflow.contrib.training import HParams"" in src/model.py ?
 if yes, how?
Thanks
 The text was updated successfully, but these errors were encountered: 
👍1
DSLituiev reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/gpt-2/issues/209","The true names/sizes of the 4 GPT-2 models","2019-11-14T12:43:37Z","Open issue","No label","Can you please clarify the true names of the 4 now available models?
SMALL:
117M according to https://github.com/openai/gpt-2-output-dataset and https://github.com/openai/gpt-2/blob/master/README.md and https://openai.com/blog/better-language-models/
124M according to https://github.com/openai/gpt-2/blob/master/download_model.py and https://openai.com/blog/gpt-2-1-5b-release/
MEDIUM:
345M according to https://github.com/openai/gpt-2/blob/master/README.md and https://github.com/openai/gpt-2-output-dataset and https://openai.com/blog/better-language-models/
355M according to https://openai.com/blog/gpt-2-1-5b-release/
LARGE:
762M according to https://github.com/openai/gpt-2-output-dataset and https://openai.com/blog/better-language-models/
774M according to https://openai.com/blog/gpt-2-1-5b-release/
EXTRA LARGE
1542M according to https://github.com/openai/gpt-2-output-dataset
1.5BM according to https://openai.com/blog/gpt-2-1-5b-release/
This makes downloading them through download_model.py incredibly hard. It'd be really useful if you could put the true names either on the readme or in the download script.
Thanks
 The text was updated successfully, but these errors were encountered: 
👍11
moyid, InconsolableCellist, shelvacu, woctezuma, philipkd, sytelus, pubkey, garyhlai, cgebbe, rohitdwivedula, and MM-IR reacted with thumbs up emoji
All reactions
👍11 reactions"
"https://github.com/openai/gpt-2/issues/207","AllenNLP and a Question Generator","2019-11-11T04:40:37Z","Open issue","No label","I would love to see gpt-2 used to generate questions. I have a working example here that uses BIDAF and an automatic question generator. Do you think the full model will ever be released, or can you submit the full model to AllenNLP with some type of com-pliancy license?
https://googlier.com/csv/2019_10_28/artificial%20intel
https://demo.allennlp.org/user-models
https://github.com/dipta1010/Automatic-Question-Generator
 The text was updated successfully, but these errors were encountered: 
👍1
tim5go reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/gpt-2/issues/206","Exposing a programmatic REST API","2019-11-09T18:48:17Z","Open issue","No label","I was wondering if there was a way to expose an API of some sort for the conditional generator. I am not good with Python, and I don't think that wrapping with Expect library is the way to go. Also, ideally one would be allowed to put \n characters in input to the model, since I think right now if you use carriage return it will treat it as a separate input. And getting the formatting right is critical to some things like poetry, lists, etc. Anyone know of some code to expose an API? I didn't see any in the code itself, nor do I see any issues opened regarding this. Thanks.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/205","Documentation out of date","2019-11-09T17:13:20Z","Open issue","No label","I noticed this PR which updates the DEVELOPERS file: #181
It is not yet merged.
I was reading through https://github.com/openai/gpt-2/blob/master/DEVELOPERS.md
I tried pip3 install tensorflow==1.12.0. I found that version doesn't exist. I used:
 pip3 install tensorflow==1.13.2
It seems to work with this version - however it stopped mid sentence and had some warnings. I am using a Linode, the default size version with the latest Ubuntu.
python3 src/generate_unconditional_samples.py | tee /tmp/samples
/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([(""qint8"", np.int8, 1)])
/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([(""quint8"", np.uint8, 1)])
/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([(""qint16"", np.int16, 1)])
/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([(""quint16"", np.uint16, 1)])
/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([(""qint32"", np.int32, 1)])
/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([(""resource"", np.ubyte, 1)])
2019-11-09 17:03:09.817453: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-11-09 17:03:09.823008: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 1999990000 Hz
2019-11-09 17:03:09.823403: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x315f5f0 executing computations on platform Host. Devices:
2019-11-09 17:03:09.823491: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>
WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
WARNING:tensorflow:From /root/gpt-2/src/sample.py:64: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.cast instead.
WARNING:tensorflow:From /root/gpt-2/src/sample.py:67: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.random.categorical instead.
WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
2019-11-09 17:03:16.296025: W tensorflow/core/framework/allocator.cc:124] Allocation of 7077888 exceeds 10% of system memory.
2019-11-09 17:03:16.296514: W tensorflow/core/framework/allocator.cc:124] Allocation of 154389504 exceeds 10% of system memory.
2019-11-09 17:03:16.366548: W tensorflow/core/framework/allocator.cc:124] Allocation of 9437184 exceeds 10% of system memory.
2019-11-09 17:03:16.406170: W tensorflow/core/framework/allocator.cc:124] Allocation of 9437184 exceeds 10% of system memory.
2019-11-09 17:03:16.433514: W tensorflow/core/framework/allocator.cc:124] Allocation of 7077888 exceeds 10% of system memory.
 
======================================== SAMPLE 1 ========================================

Worcester Uinger and Worcester Ugo were dismissed from Sheffield United's squad on Tuesday for a clash in southport despite High Five Scotland's injures involving Ukafor, Blais and C Stevens taking the spot, the Yorkshire side

Newcastle: Shaun Wright (left) had been named as the 18th captain

the squad attending Stuart Holden's next game will now be relegated to Northolt

Media playback is not supported on this device No/Wyrandsie: Out of body does Neeskens get an anti-kicking award

Norwich: Ayew played 17 minutes of the match, stretched out on 15 completions to Robin Huddlestone for his 34 to George Graham - a move that was denied when midfielder Thierry Henry became frustrated with his opponent

Charlton's Ashley Griffith for his 25 - his first quarterbackacking change of late - retired S Dyrde's double kick to Ryan Gunn to get a first two for the Heat after Lowell Waite opened the scoring in DET's central halffall - but Wycombe Man Watson shot wide and vented his anger to the All Blacks in penalties down the line

Norwich: Ugaawe Matukwo was outstanding on the number 25 spell, extending his unbeaten run - 10 straight

Charlton side: Fit, Sam Adams, Acic Iwako, Stewart Neely, Marcell Sene who closed - but Henry made the scorebar by part way - with the Pavilion at 1-1

Liverpool: Shearer taken on a a two-month suspension (tail to nose tackle) from Derby this morning - first spent the night he was in France because of incontinence

Banks: Prior to Saturday's media day update Watford were next games on their road trips to Aberdeen Vale, Newport but will be without Sene still

Wolverhampton Wanderers vs 13: AFC Wimbledon huddle up on the same pitch as Bristol City (Jan 12, 2008) Titans 17, Circuit last

Champions League: A Picture of The Midlands Derby at the Stadium of Light is seen in Green Lane

Only Premiership sides face away with equal record by a on average 11 points, painted as ""fata di noblico"" or ""play for seven"", Cardiff Blues FC v Manchester City V20, Crystal Palace v Everton and Hull City v South Wales after taking a 0-0 draw at Hampden. Bournemouth got 15 per cent. Stoke prevented Everton scuffling. Villarreal v Ipswich City V20, Ipswich strip, Bournemouth out of the race for contention.


Transfer deadline day's top predictions for the transfer market for back-to-back seasons:


West Brom's Xia-hu for £42m (Chomey Pereo United). Afterwards, Roberto di Matteo for the remaining two years (in April), Nicolas Otamendi for less giving-a-phone fees (in 2011) as Mumbai-born Demonelopoulou played only 20 Premier League snaps. Qatari English Fokoamils investing 4 million of £5m into his side, now with just 6 Farmers Trophy catches. Molineux attended MK Dons training, fielding his outfield and U6 timeout. Manchester United Hotspur are out of the race for Darwen Champlain, doing what West Brom's Chris Pearson did - their bowl dipped as far as 0.78 goals per match. Nacional at Charlton, Omar O Torres for 26 minutes before chilling off from friendly vs Chibadjrian. on loan Spalletti for his first half scoring against Liverpool (40) (I called off Bournemouth) while Dunford zip swift for third half form with former payoff Earnolls by Crusaders scratched by Charlton for incontinence from Toreanga keeper Fabian Edrat (McManaman surprised by a recovery). Carolina after word directly from manager Howley. Barcelona – former spouse Seylocent at Premier League level Qunyuan in love with Tobi McFarran Guan, training with San Marcos only eight weeks before Chelsea released him to failings bone and glass. Crawley paid MG Radhams twice as sanction, trailing from their 544th draw in 2010-11, after being jostled & bounced. Visit Chelsea - Roger Salisbury in February, Dani Alves building forever on experience of dealing's rampant couple Scudetto-Addy clip Real Rangers 9-17 Californ, Marseille winning the £25m Christmas holiday. FC Santander AGU early training with Gregory Trie, Chizal keMed to script your perfect fight Such dry extinguished roof and must conference cup finals I am less confident ZORSFC owners Glenn Hoddle (RUBIC SCIFI REAL SCIFI SLAUCHercombe) Charlie opens something! La Scudetto win 118 points DE PREO de Porto until Sunday between Vecchio and Ray Barrie To boot, with Breredé Sty team me Raú
======================================== SAMPLE 2 ========================================
a pair of blushes. After having one of them for mere cause, so to speak, the blonde surprised, and inquired whether she would take those in a fight? And underneath that devoured glory waxed and waned like the microscopic claw of alcoholism, suddenly she found what she needed - something different. Suddenly everything fell overboard

Child Sarah seized her astonishing wand and locked her lover in a corner, threatening him with the knuckle phone. Sarah didn't have the emotional energy to follow in the ears of her store or contemplate that act, so she opted for a distraction with a concealed camera beside her, hoping it would reveal real-life innocents lurking out there being popped for smoking. Then Sarah started the notes, the feline lips moving in unison with between eyebrows of their haunting non-trigger scribblings. Eventually, again with uninterrupted detail, the letter reading 'Iraq-sample prints'.

The friends did not seem to mind. A urbane musical goof bruiser on the arena floor pissed their teeth. Some were Robin CD's, some of them nabbed for £20 an hour and the likes. Lots of rich behaviour and lovd. And more: the blonde blushing recounting the trick by showing herself standing in a square and pinned to a pretty newspaper then offering the listen, seemingly stunned by the gaudy monstrosity. As if suddenly lost in thought for a minute after her boyfriend's scarily brief caress was snapped, Sarah invited Neal to become what had been supposed to be she once was, then tucked the sickening items into a locked drawer, trying not to let it be understood that the Aberdeen Bar had failed them since its peak

Photographer Laura Bowdich toured the basement stage of the loud yet sumptuous armoured playhouse, alongside her friend and fellow art gardener Brian Hill, with just hours and unheard camera efforts with her. Asword and the recklessly nautical soundtrack quieted the crawlish breeze whilst the drug- and alcohol-addled crowd chopped up the brains of your enemies. Rests of quirkiness followed the lush, garish aura of almost every one. The thing that made this world's great southern art 'dead works' i.e. sloppy working but very apparently sweet, may as well get exported to this East Deptford paradise.

So pleased with this ghoulish attempt to absorb all the terms of quirkiness it would redeem itself when it arrived it chirped. It had been caught, and impressed, with the provocative outton claims, although with poor preparation for his use. We have a 'n—', just if not a 'k', reason to heed his side of the change. Will it go on in any other government collection? A prickly treat for a guy who's a charity organizer at some pre-war PtheonYes! party in Mendoza where other BABYLEG UNITS which are also being scrounging for their latest $10 million minimum is topical use? Or is, in some cases,, a silenced attempt to verify their outing by some mño-mphigantre-fish?

There was a short pause before the animal rights groups agreed with the announcement that the whole secrecy, colloquialisation and admirable naïveté was going to get them the moment they, comfortable with what was glossed over in an otherwise dishonorable assortment of adjectives, were all baskatively welcomed to reveal because at the end we all know this is nakedly, to learn, extremely hopeful money window would fall. We are ballistic18tops pushing in so far as centre lies, this time because, from an mightier's point of view, calculating R&J. Bjerken brought perusal to the taboo subject with the blushes. But Sarah nursed parties, contests, marriages and even visits to tattoo fairs and churches where little they were more obscure than its evasions about something characterised as something bucketheap-soaked fiendishly fell hard upstairwell or sickly smashed. It was revelation. (For those uninitiated, 'medieval Britain' constitutes not only British imperialism, but Christian rump Britain and the Chesterton luddite Protestants across the nautical edge of the Caribbean.) Sarah seemed to be begging for more transparency about public bulk, something in emerging directions, something that isn't at all secret.

Stuffed around her lips was a plate of trapped low-fat lip cocktail beverage, all of which somehow, like a single word at a all-Ireland-specific Hip-Rock English pub, was some sort of S-C-I-D — a faintly 'hingerstep moment'-boosting drug – suddenly spilling from the bottle. It was extremely attractive to look at and stick around Bennett But Understanding's local boardroom tables. Many have seen jars that list half hats, or perhaps about Snakes and Bees. (This was the place of the Hutchinson twins' 25-hour break for eating. Which begs the

 The text was updated successfully, but these errors were encountered: 
👍2
alexa-ai and IndigoLily reacted with thumbs up emoji
All reactions
👍2 reactions"
"https://github.com/openai/gpt-2/issues/204","Running using Docker instructions results in libcuda.so.1 missing error","2019-11-29T18:03:22Z","Closed issue","No label","$ docker build --tag gpt-2 -f Dockerfile.gpu .

...

$ docker run -it gpt-2 bash

...

root@a8c0f508dc92:/gpt-2# export PYTHONIOENCODING=UTF-8
root@a8c0f508dc92:/gpt-2# python3 src/interactive_conditional_samples.py 
Traceback (most recent call last):
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/usr/lib/python3.5/imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""/usr/lib/python3.5/imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: libcuda.so.1: cannot open shared object file: No such file or directory

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""src/interactive_conditional_samples.py"", line 7, in <module>
    import tensorflow as tf
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/usr/lib/python3.5/imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""/usr/lib/python3.5/imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: libcuda.so.1: cannot open shared object file: No such file or directory


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
root@a8c0f508dc92:/gpt-2# ```

 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/203","Error when testing with Docker build / run","2019-11-08T11:22:45Z","Open issue","No label","I got an error when testing with Docker build / run and I can't see how to fix it. Can someone assist?
Build:
$ docker build -t gpt2 . -f Dockerfile.cpu
Sending build context to Docker daemon  4.706MB
Step 1/10 : FROM tensorflow/tensorflow:1.12.0-py3
1.12.0-py3: Pulling from tensorflow/tensorflow
...
...

Removing intermediate container de4b147fa32d
 ---> f00f44c69f05
Successfully built f00f44c69f05
Successfully tagged gpt2:latest

Run:
root@750bbcc0d467:/gpt-2# export PYTHONIOENCODING=UTF-8

root@750bbcc0d467:/gpt-2# python3 src/generate_unconditional_samples.py --top_k 40 --temperature 0.7 | tee /tmp/samples
2019-11-08 11:21:11.150936: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
Traceback (most recent call last):
  File ""src/generate_unconditional_samples.py"", line 79, in <module>
    fire.Fire(sample_model)
  File ""/usr/local/lib/python3.5/dist-packages/fire/core.py"", line 138, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File ""/usr/local/lib/python3.5/dist-packages/fire/core.py"", line 471, in _Fire
    target=component.__name__)
  File ""/usr/local/lib/python3.5/dist-packages/fire/core.py"", line 675, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File ""src/generate_unconditional_samples.py"", line 62, in sample_model
    temperature=temperature, top_k=top_k, top_p=top_p
  File ""/gpt-2/src/sample.py"", line 74, in sample_sequence
    past, prev, output = body(None, context, context)
  File ""/gpt-2/src/sample.py"", line 66, in body
    logits = top_p_logits(logits, p=top_p)
  File ""/gpt-2/src/sample.py"", line 28, in top_p_logits
    sorted_logits = tf.sort(logits, direction='DESCENDING', axis=-1)
AttributeError: module 'tensorflow' has no attribute 'sort'
root@750bbcc0d467:/gpt-2# 

 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/201","training from scratch","2019-11-06T08:11:43Z","Open issue","No label","Will the code for training from scratch be released after the 1.5B model?
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/200","Question","2019-11-01T05:01:35Z","Closed issue","No label","I am very sorry, but I just want to know what is ""fire"" in generate_unconditional_samples.py?
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/195","Retraining with Beatles’ lyrics only","2019-10-26T15:52:43Z","Closed issue","No label","Suppose I want a model of all Beetles’ lyrics, how do I train the system from scratch with them. Assume I have them in a txt file. What would be the steps? My goal is to input Beatles lyrics and then output new lyrics in the same style. Detailed answers or pointers are highly appreciated.
 I noticed this project is aiming to achieve ""general"" capabilities as opposed to ""specialized"" ones. If I am going about my problem the wrong way, or misunderstanding the premise of this project, please kindly correct me and provide pointers on how I'd achieve my goal elsewhere if possible.
 Thank you.
 -Andy
 The text was updated successfully, but these errors were encountered: 
👍1
martin12333 reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/gpt-2/issues/193","Bluh... set top_p really low","2019-10-21T00:32:07Z","Closed issue","No label","Bluh
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/192","What is the maximum model prompt size for interactive samples?","2019-10-16T15:27:48Z","Open issue","No label","./interactive_conditional_samples.py
Is there a maximum size that's recommended? I notice that if I feed it with very long prompts it'll write a few sentences, then devolve into a schizophrenic rant.
I was, was, was, was, was, like, was, like, was, like, like, was, was like, was, was like, like, was, like, like, I was,, went away, was, gone, was like, was, was, a, was, was, was,
What we
 ’
 ’ ’ ’ ’ ’ ’
‵ ’   ’
 ’ ’
 ’   ―
’
 That ’ ’ ’
 ’ ’
 ’ ’ ’ ’
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/191","Does the model prompt support multiple lines of input?","2019-10-13T05:00:23Z","Open issue","No label","Currently I have found no way to enter multiple paragraphs or a list format. Enter and all other newline methods I've tried do not work.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/190","Unable to download 744M model.","2019-10-04T00:30:36Z","Closed issue","No label","When downloading the 744M model using the provided script, the ""model.ckpt.data-00000-of-00001"" file is only 210 bytes as compared to the 345M model which is roughly 1.42GB.
Any help would be appreciated. Thank you for your time.
Solution
You can use the incorrect parameter when running the py file and the download_model.py will still attempt to download files (and write a few of them). To solve this I used 774M instead of my incorrect 744M.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/189","Code not working for tensorflow==1.12.0","2019-10-02T08:28:22Z","Open issue","No label","I created a virtual environment with tensorflow==1.12.0 to run this code, as mentioned in starter guide. But, program is throwing errors on tf 1.12. But, it is working fine on tf 1.14. Please update the starter guide.
 The text was updated successfully, but these errors were encountered: 
👍6
rogerallen, ITSOES, gardner, AutumnEvans418, HarshaLaxman, and duhaime reacted with thumbs up emoji
All reactions
👍6 reactions"
"https://github.com/openai/gpt-2/issues/188","Feature/Question: With GPT-2 is it possible to get previous word prediction?","2019-10-16T15:20:19Z","Closed issue","No label","Feature/Question: With GPT-2 is it possible to get previous word prediction?
Hi,
I say this after seeing this https://towardsdatascience.com/deconstructing-bert-distilling-6-patterns-from-100-million-parameters-b49113672f77
And wondering how I could maybe write a method that would allow me to predict the previous word? (ideally for GPT2)
Many thanks,
 Vince.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/187","Questions about nucleus sampling/Potential Bugs","2019-09-30T18:20:44Z","Open issue","No label","Why is nucleus sampling implemented on top of the top-k logits? (lines 65-66 in sample.py) My understanding from the paper is that the threshold should be implemented on all of the logits.
 The text was updated successfully, but these errors were encountered: 
😕1
zkailinzhang reacted with confused emoji
All reactions
😕1 reaction"
"https://github.com/openai/gpt-2/issues/185","confused about vocab.bpe and encoder.json","2019-09-23T19:28:21Z","Open issue","No label","I'm reading the source code. And I have two questions about vocab and encoder. Please help me with that. Thank you in advance.
For vocab, I take the second row (Ġ t) for example. But I found ""Ġ"" appears in many rows(for example the third row). So why isn't it one-to-one correspondence?
Are the items in encoder.json the subtokens from BPE? I take ""\u0120regress"" for example. Why does ""\u0120"" appear here?
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/184","Docker installation does not work.","2019-09-21T16:56:55Z","Open issue","No label","I installed the Dockerfile.gpu version, at which I found that gpu passthrough is unavailable in Windows 10 Home, so I just changed the tensorflow version to the cpu version, and had to change to 1.13.1 in order to use the sort function as shown in this pull request: #177
Now I get the following output:
root@56fc5edb1e15:/gpt-2# python3 src/generate_unconditional_samples.py --model_name 774M --nsamples 1
2019-09-21 16:50:49.057226: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
2019-09-21 16:50:49.066466: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3696000000 Hz
2019-09-21 16:50:49.067238: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x4a90030 executing computations on platform Host. Devices:
2019-09-21 16:50:49.067444: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>
WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
WARNING:tensorflow:From /gpt-2/src/sample.py:64: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.cast instead.
WARNING:tensorflow:From /gpt-2/src/sample.py:67: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.random.categorical instead.
WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
2019-09-21 16:51:03.331001: W tensorflow/core/framework/allocator.cc:124] Allocation of 257315840 exceeds 10% of system memory.
Killed

As you can see, the program seems to get killed by something in the virtual environment. I am unsure what it is as I have not installed anything else in this environment.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/183","Generate encoder.json for other languages","2019-09-21T08:07:15Z","Open issue","No label","Hi all,
 Please tell me the way to generate encoder.json for other languages French, Spanish... I tried to create a encoder.json but it's not exactly the same example of GPT and failed in run model
 The text was updated successfully, but these errors were encountered: 
👍1
nasterfy reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/gpt-2/issues/182","Large model?","2019-09-08T04:14:39Z","Closed issue","No label","The code in src/interactive_conditional_samples.py	 never calls the large model, it uses 124M. Is this correct?
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/180","error : ""AttributeError: 'HParams' object has no attribute 'override_from_dict'""","2019-09-05T14:09:17Z","Open issue","No label","error : ""AttributeError: 'HParams' object has no attribute 'override_from_dict'""
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/179","Installation guide, please","2019-08-31T19:42:56Z","Open issue","No label","This code looks great and all, but without a proper installation guide, most people can't even start playing with it. I have checked a couple of pages with supposed installation guides, but none seem to work for me.
Please, just add one, or create an .exe file.
 The text was updated successfully, but these errors were encountered: 
👍2
yzevm and GnarGnarHead reacted with thumbs up emoji
All reactions
👍2 reactions"
"https://github.com/openai/gpt-2/issues/178","Developers.md","2019-08-30T16:43:38Z","Open issue","No label","Developers.md says that we should install: pip3 install tensorflow==1.12.0
But when I run:
python3 src/generate_unconditional_samples.py | tee /tmp/samples
it throws:
AttributeError: module 'tensorflow' has no attribute 'sort'
Looks like sort method was launched in tensorflow 1.14 onwards. Hence this readme can be updated.
 The text was updated successfully, but these errors were encountered: 
👍7
TBAIKamine, alexellis, PatMyron, IndigoLily, Pawandeep-prog, fscheidt, and MatthieuCourbariaux reacted with thumbs up emoji
All reactions
👍7 reactions"
"https://github.com/openai/gpt-2/issues/176","while text generation first past is repeating again and again.","2019-08-29T13:30:00Z","Open issue","No label","After first generating first word, the present is not getting updated.
with tf.variable_scope(scope):
        c = conv1d(x, 'c_attn', n_state*3)
        q, k, v = map(split_heads, tf.split(c, 3, axis=2))

      **present = tf.stack([k, v], axis=1)**

        if past is not None:
            pk, pv = tf.unstack(past, axis=1)
            k = tf.concat([pk, k], axis=-2)
            v = tf.concat([pv, v], axis=-2)
        a = multihead_attn(q, k, v)
        a = merge_heads(a)
        a = conv1d(a, 'c_proj', n_state)
        return a, present

Please change it to
with tf.variable_scope(scope):
        c = conv1d(x, 'c_attn', n_state*3)
        q, k, v = map(split_heads, tf.split(c, 3, axis=2))
        if past is not None:
            pk, pv = tf.unstack(past, axis=1)
            k = tf.concat([pk, k], axis=-2)
            v = tf.concat([pv, v], axis=-2)

      **present = tf.stack([k, v], axis=1)**

        a = multihead_attn(q, k, v)
        a = merge_heads(a)
        a = conv1d(a, 'c_proj', n_state)
        return a, present

 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/175","Slow inference speed","2019-08-28T01:49:24Z","Closed issue","No label","Hello,
I am currently fine-tuning GPT-2 with my own dataset.
 There are 90000 documents in my training set and it takes around 10 seconds to finish one epoch.
 However, when I try the inference with the interactive_conditional_samples.py, it takes me more around 30 seconds to generate one document.
I cannot explain why it becomes so slow.
 Is it because of every document is sent to the BPE encoding separately?
All suggestions are welcomed. Thanks.
 Helena
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/173","ResourceExhaustedError: OOM when allocating tensor with shape[3,16,1024,1024]","2019-08-24T15:08:40Z","Open issue","No label","I'm getting this error when training the model 345M on a dual GTX 1080i - 8GB RAM.
ResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[3,16,1024,1024] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc

 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/172","How to get reproducible interactive conditional samples?","2019-08-22T18:32:48Z","Closed issue","No label","The 'seed' argument of interactive_conditional_samples.py helps getting reproducible sample if I restart the script, i.e., if I start from a new session.
However, I cannot find a way to get the exact same sample for the same prompt within a single session. How can this be achieved without creating a new session?
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/169","Why not the 1.5B model as well? Even 8.3B (instructions) have been posted by NVIDIA","2019-08-22T18:33:53Z","Closed issue","No label","Now as I understand, and have audited the code myself - NVIDIA has released an improved 8.3B solution, albeit not a MODEL - I was able to follow it through to the point that I fully understood what actually RUNNING this monstrosity would take in computing power and dollars considering I'd probably need to settle for cloud TPU through google or amazon. The cost was unappealing to say the least.
Now we're already to the 755M model - THANK YOU! Don't think I'm not grateful. But it just makes sense to release the full 1.5B as most of your partners have said there is little difference between the two anyways, and smaller researchers who don't want to rack up a $3000 compute bill to train a model would love to have it. I understand this is actually probably the biggest ""danger"" you guys were referring to early on - certain people not having access to re-train the software. But at least having a point of reference helps us connect the dots.
Thanks again.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/168","Does gpt-2 use a POS tagger?","2019-08-22T18:34:38Z","Closed issue","No label","Hi, sorry it is not an issue just a question, does GPT-2 use a POS tagger, a dep/constituency parser?
If not, loosing such data isn't a loss in potential accuracy?
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/166","Example sentence","2019-08-18T09:19:26Z","Open issue","No label","Hello,
Is it possible to predict the next or previous word in a sentence as the research claims ?
For example, like Google translate ""apple"", and produced the example sentences
Thanks
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/165","conv1d naming?","2019-08-22T18:37:21Z","Closed issue","No label","I am wondering why is this function https://github.com/openai/gpt-2/blob/master/src/model.py#L50 named conv1d?
It seems to be a linear transformation to me, not a conv1d operation.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/164","ZeroDivisionError: integer division or modulo by zero","2019-08-13T23:18:26Z","Closed issue","No label","When I run my train.py, this is the output
Loading checkpoint models\117M\model.ckpt
 W0802 09:08:48.051176 6828 deprecation.py:323] From C:\Program Files\Python36\lib\site-packages\tensorflow\python\training\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
 Instructions for updating:
 Use standard file APIs to check for files with this prefix.
 Loading dataset...
 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1001.03it/s]
 dataset has 0 tokens
 Training...
 Traceback (most recent call last):
 File ""train.py"", line 293, in 
 main()
 File ""train.py"", line 271, in main
 feed_dict={context: sample_batch()})
 File ""train.py"", line 247, in sample_batch
 return [data_sampler.sample(1024) for _ in range(args.batch_size)]
 File ""train.py"", line 247, in 
 return [data_sampler.sample(1024) for _ in range(args.batch_size)]
 File ""C:\Users\pc\Documents.Random Programming\Ai Test\GPT-2\gpt-2-finetuning\src\load_dataset.py"", line 74, in sample
 self.chunks
 ZeroDivisionError: integer division or modulo by zero
My idea is that I may have too small of a dataset, but upon consistently increasing size, it is not to be. I have moved from 300 lines to 3100.
 Help?
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/163","Failed to interpret file %s as a pickle","2019-08-02T08:09:32Z","Closed issue","No label","Every time I run train.py with my dataset, I get this error.
Loading dataset...
 0%| | 0/1 [00:00<?, ?it/s]
 Traceback (most recent call last):
 File ""C:\Program Files\Python36\lib\site-packages\numpy\lib\npyio.py"", line 447, in load
 return pickle.load(fid, **pickle_kwargs)
 EOFError: Ran out of input
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
 File ""train.py"", line 293, in 
 main()
 File ""train.py"", line 173, in main
 chunks = load_dataset(enc, args.dataset, args.combine)
 File ""C:\Users\pc\Documents.Random Programming\Ai Test\GPT-2\gpt-2-finetuning\src\load_dataset.py"", line 27, in load_dataset
 with np.load(path) as npz:
 File ""C:\Program Files\Python36\lib\site-packages\numpy\lib\npyio.py"", line 450, in load
 ""Failed to interpret file %s as a pickle"" % repr(file))
 OSError: Failed to interpret file 'lyric.npz' as a pickle
Any solutions? What am I doing wrong?
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/162","Training for Style vs. Topic","2019-07-27T12:23:57Z","Open issue","No label","When using GPT-2 via the interactive prompt or finetuning with a secondary corpus, the model seems to learn both writing style and what topic to write about simultaneously. Let's say I wanted to teach GPT-2 to write about modern architecture (topic), in a Twitter vernacular (style). Is there a way to indicate to GPT-2 which parts of the input are topic related and which are stylistically important?
 The text was updated successfully, but these errors were encountered: 
👍2
LifeIsStrange and Metalbaba reacted with thumbs up emoji
All reactions
👍2 reactions"
"https://github.com/openai/gpt-2/issues/160","Batch of samples with different length","2019-08-22T18:38:19Z","Closed issue","No label","Hi,
I'd like to feed short samples of various length to the model, and I would like to put them in a single batch.
 Hence my question is, does the model support padding?
If yes what is the token?
 The text was updated successfully, but these errors were encountered: 
👍1
jinluyang reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/gpt-2/issues/159","TypeError: 'HParams' object is not callable","2019-07-26T16:13:50Z","Closed issue","No label","I get the error in the title when trying to run interactive_conditional_samples.py. Here is the full error message:
A:\Coding_and_Scripting\Miniconda3\lib\site-packages\h5py\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_convertersTraceback (most recent call last):
  File ""A:\Coding_and_Scripting\Miniconda3\_PROJECTS\gpt-2-master\src\interactive_conditional_samples.py"", line 90, in <module>
    fire.Fire(interact_model)
  File ""A:\Coding_and_Scripting\Miniconda3\lib\site-packages\fire\core.py"", line 127, in Fire
    component_trace = _Fire(component, args, context, name)
  File ""A:\Coding_and_Scripting\Miniconda3\lib\site-packages\fire\core.py"", line 366, in _Fire
    component, remaining_args)
  File ""A:\Coding_and_Scripting\Miniconda3\lib\site-packages\fire\core.py"", line 542, in _CallCallable
    result = fn(*varargs, **kwargs)
  File ""A:\Coding_and_Scripting\Miniconda3\_PROJECTS\gpt-2-master\src\interactive_conditional_samples.py"", line 49, in interact_model
    hparams(json.load(f))
TypeError: 'HParams' object is not callable
Any help is appreciated.
EDIT: I use Google Colab now and I don't get this issue
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/158","Training question","2019-07-17T13:37:15Z","Open issue","No label","Hi,
I have taken a look into your model and looks really nice!
 I do have a question due to I am pretty new to Tensorflow.
 How can I do it in order to train my model using for example a Spanish Dataset?
Thank you!
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/157","Arguments for multi-layer decoder-only Transformer","2019-07-14T09:22:48Z","Open issue","No label","Hello, I recently started studying language modeling and GPT(-2) in particular. While I start to understand the way it is trained/fine-tuned, I do have some questions about its architecture.
In OpenAI's paper it is stated that GPT (and GPT-2) is a multi-layer decoder-only Transformer. From a higher perspective I can understand that an Encoder/Decoder architecture is useful for sequence 2 sequence applications, but that it becomes less attractive for language modeling tasks. Therefore, it seems logical OpenAI decided to stick with the multi-layer decoder only. However, during the training/fine-tuning stage of GPT, in these decoding-layers, tokens are still encoded and eventually decoded, right?
I'm not sure whether my question is clear, but it basically comes down to this: in GPT's paper it is stated that they use a decoder-only transformer, but I cannot find any arguments this decision is based on. What would be the difference if they stuck to the regular Transformer architecture for example?
I hope someone is able to give me more insight into this.
Many thanks in advance.
 The text was updated successfully, but these errors were encountered: 
👍5
KartikKannapur, mymusise, Eniac-Xie, itanfeng, and WuTianming reacted with thumbs up emoji
All reactions
👍5 reactions"
"https://github.com/openai/gpt-2/issues/156","no such file 'models\\345M\\encoder.json' but I have encoder.json","2019-07-14T03:56:11Z","Closed issue","No label","Running generate_unconditional_samples.py outputs this error:
Traceback (most recent call last):
  File ""generate_unconditional_samples.py"", line 78, in <module>
    fire.Fire(sample_model)
  File ""A:\Coding_and_Scripting\Miniconda3\lib\site-packages\fire\core.py"", line 127, in Fire
    component_trace = _Fire(component, args, context, name)
  File ""A:\Coding_and_Scripting\Miniconda3\lib\site-packages\fire\core.py"", line 366, in _Fire
    component, remaining_args)
  File ""A:\Coding_and_Scripting\Miniconda3\lib\site-packages\fire\core.py"", line 542, in _CallCallable
    result = fn(*varargs, **kwargs)
  File ""generate_unconditional_samples.py"", line 43, in sample_model
    enc = encoder.get_encoder(model_name, models_dir)
  File ""A:\Coding_and_Scripting\Miniconda3\_PROJECTS\gpt-2-master\src\encoder.py"", line 109, in get_encoder
    with open(os.path.join(models_dir, model_name, 'encoder.json'), 'r') as f:
FileNotFoundError: [Errno 2] No such file or directory: 'models\\345M\\encoder.json'
interactive_conditional_sample.py outputs the same error.
I even tried replacing 117M in all the files with 345M because I downloaded the 345M model. I also tried putting a direct path to replace line 109 in encoder.py:
with open(os.path.join(models_dir, model_name, 'encoder.json'), 'r') as f:
 I tried
with open(os.path.join(""A:\Coding_and_Scripting\Miniconda3\_PROJECTS\gpt-2-master\models\345M\encoder.json""), 'r') as f:
 and it gives me the error
Traceback (most recent call last):
  File ""interactive_conditional_samples.py"", line 90, in <module>
    fire.Fire(interact_model)
  File ""A:\Coding_and_Scripting\Miniconda3\lib\site-packages\fire\core.py"", line 127, in Fire
    component_trace = _Fire(component, args, context, name)
  File ""A:\Coding_and_Scripting\Miniconda3\lib\site-packages\fire\core.py"", line 366, in _Fire
    component, remaining_args)
  File ""A:\Coding_and_Scripting\Miniconda3\lib\site-packages\fire\core.py"", line 542, in _CallCallable
    result = fn(*varargs, **kwargs)
  File ""interactive_conditional_samples.py"", line 46, in interact_model
    enc = encoder.get_encoder(model_name, models_dir)
  File ""A:\Coding_and_Scripting\Miniconda3\_PROJECTS\gpt-2-master\src\encoder.py"", line 109, in get_encoder
    with open(os.path.join(""A:\Coding_and_Scripting\Miniconda3\_PROJECTS\gpt-2-master\models\345M\encoder.json""), 'r') as f:
FileNotFoundError: [Errno 2] No such file or directory: 'A:\\Coding_and_Scripting\\Miniconda3\\_PROJECTS\\gpt-2-master\\modelsåM\\encoder.json'
I don't know what's going on with the modelsåM at the last line there.
Thank you!
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/155","Legit real-world use cases?","2019-07-08T10:52:39Z","Open issue","No label","Hello,
I've been reading about this project, and many of the articles referring to it discuss the potential abuse or whether it does or not represent an actual risk. I also read the mega thread in #16 and draw some conclusions for myself:
Most people who wants the full model release argue it's ""for the sake of knowledge""
I feel like an ample percent of those are actually internet trolls that want a fun-and-easy to use tool for generating scam emails and such
Some people is actually concerned about the potential abuse and understand the caution on not releasing the full model
Now, what I didn't saw, neither on that thread or in the articles speaking about this project, are actual legitimate use cases for this technology - far from the obvious ""research purposes"".
So let's forget about fake news and internet trolling, I honestly don't see a situation where this would be of any help - or let me rephrase it... where this should be of any help. I've seen commercial offerings that pretty much sum up to ""are you too lazy as for interpreting your own data? let our bot write reports for your stakeholders so they feel your project is going somewhere even if you don't know what you are doing at all"".
The other real world use cases I can think of would be (non)writers who instead of paying others to write their books as they do currently, would use some sort of AI to bake standardized best-sellers for their own profit (with a huge marketing effort ofc - this could also help on that regard), or crappy news agencies who deprecated all the reporters in favor of interns who can type one paragraph so the text auto-generation tool can fill the rest of the made up article.
To sum up, I'd really love to hear some legitimate real world use cases which don't completely suck for this technology, from people who are actually working on it.
Cheers!
 The text was updated successfully, but these errors were encountered: 
👍2
krasi0 and alexa-ai reacted with thumbs up emoji
All reactions
👍2 reactions"
"https://github.com/openai/gpt-2/issues/154","Why gpt-2 could apply to other tasks without fine-tune?","2019-08-08T08:17:43Z","Closed issue","No label","Thank you!
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/153","How to convert .ckpt files to .pb?","2019-11-18T22:52:53Z","Closed issue","No label","No description provided.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/152","solved","2019-06-28T08:32:02Z","Closed issue","No label","No description provided.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/151","Generating text on custom encoder.json of much smaller size.","2019-06-26T10:38:25Z","Open issue","No label","No description provided.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/150","GPU configuration","2019-06-23T15:19:46Z","Open issue","No label","What GPU configuration is required to train the 345M model
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/149","Samples lenght","2019-06-20T18:01:11Z","Open issue","No label","I want to adjust it so that it gives out shorter samples. If you limit the length, then gpt2 breaks phrases
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/148","Intermediate layer output","2019-06-19T09:35:30Z","Open issue","No label","Is it possible to use the intermediate layer outputs and generate text ignoring the layers on top?
 Basically, I want to check quality of generations as we keep on adding more layers. What modifications in the src/sample.py script would I have to make for the same? Thanks.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/147","How to freeze checkpoint graph to .pb format?","2019-06-16T15:40:47Z","Open issue","No label","Trying to freeze GPT2 fine-tuned model but unable to figure out what the output node name will be. Using this code as a reference, I put this together:-
import fire
import json
import os
import numpy as np
import tensorflow as tf

import model, sample, encoder

seed=None
length=40
temperature=1
top_k=0

hparams = model.default_hparams()
with open('models/345M/hparams.json') as f:
  hparams.override_from_dict(json.load(f))

with tf.Session(graph=tf.Graph()) as sess:
  context = tf.placeholder(tf.int32, [1, None])
  np.random.seed(seed)
  tf.set_random_seed(seed)
  output = sample.sample_sequence(
      hparams=hparams, length=length,
      context=context,
      batch_size=1,
      temperature=temperature, top_k=top_k
  )

  saver = tf.train.Saver()
  ckpt = tf.train.latest_checkpoint(os.path.join('models', '345M'))
  saver.restore(sess, ckpt)

  print([n.name for n in tf.get_default_graph().as_graph_def().node])

  # Freeze the graph
  frozen_graph_def = tf.graph_util.convert_variables_to_constants(sess,sess.graph_def,[output.name])

  # Save the frozen graph
  with open('output_graph.pb', 'wb') as f:
    f.write(frozen_graph_def.SerializeToString())

but I get
AssertionError: sample_sequence/while/Exit_3:0 is not in graph
So what should I put as argument 3 output node name in freeze_graph?
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/146","Why does the generated text contain ""??"" (⁇) characters?","2019-06-11T11:37:41Z","Open issue","No label","The generated samples during learning contain sequences like ""⁇ ames"", with ""??"" being a single unicode character.
It seems that this character is used when the model cannot complete a word / sentence piece, but I am pretty sure that it should be able to complete ""ames"" to ""James"" with a vocabulary of 50000 and an input text that contains ""James"" quite a few times.
On the other hand, it seems to be quite consistant in not learning uppercase ""J"". I see ""⁇ us"" as well or ""⁇ udging"".
 At some other places, ""⁇"" stands between two words without any hint on what it should represent.
I am 100% sure, that the vocabulary was generated from the same text as I used as the input.
If it is some issue with the vocabulary: Is there an option to fix it for a trained model?
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/145","How to call input.txt and write to output.txt for interactive_conditional_samples","2019-06-09T15:21:07Z","Open issue","No label","I did setup the system on windows 10, x64, 32 gb ram, core i7 2600k and GTX 1050Ti
When I run this python src/interactive_conditional_samples.py It asks Model prompt >>
Then I type my word such as Pikachu and output is as below. This output obviously seeems incorrect and unrelated
======================================== SAMPLE 1 ========================================
-the-cat

You can see more of my artworks on my art page.<|endoftext|>(WOMENSUE.COM) – It's been eight weeks since Republican presidential nominee Donald Trump made news with controversial remarks about U.S. Sen. Kirsten Gillibrand (D-N.Y.) , prompting some to question whether Trump was using a moment of national embarrassment to garner votes.

But the question never has come up among women voters. According to a new poll from Morning Consult among more than 800 likely voters, Clinton trails Trump by 11 points among female voters, 44-36. Even if Clinton wins the women over 30 in the survey – 55-28 in favor of Hillary - the margin is still quite narrow at 3 percentage points.

""If you look at the race and you see women's views, it's clear that most women don't see Trump as a guy who is a threat to their future as women. If you are one of the women and you do see Trump as someone who is a threat to women, then he's not a guy worth getting to know,"" wrote Morning Consult Associate Director of Public Policy Dan Pate during a recent phone interview.

But some female voters do see Trump in that unflattering light. That's because for most women, Trump is the biggest threat to ""the future of women's lives,"" according to a Morning Consult-NBC News poll conducted Dec. 28 to Dec. 29.

For young women, the question of gender is only slightly less divisive. Fifty-one percent of young women between 18 and 29 in the poll said they view Clinton negatively, compared with 59 percent of young women 42 to 48. But a majority (54 percent) also believe Trump is a threat to men.

But while women vote in droves for their two major party candidates, women are still relatively conservative (44 percent of women between 18 and 29 and 53 percent in the other poll). Just 6 percent of young women believe Trump cares about women, while 36 percent believe he doesn't.

And even among those women who do believe Trump cares about women, his approval rating among female voters is about the same as the ratings among millennial men (60 percent positive to 23 percent negative).

This poll of 1,013 female voters was conducted by telephone Nov. 27-30 with a 3.0 percent margin of error; a general-election poll had a 5.0 percent margin of error and a survey conducted in late November by Fox News had
================================================================================

So my first questions are as below
1 ) How can i give input.txt as input which will have few sentences and write output to a text file?
2 ) Also are there any other parameters than these?
interactive_conditional_samples.py
hparams.json
3) How can i make it work on GPU instead of CPU?
4) Can I make it return more logical output for given sentences?
How can I make it run on GPU instead of CPU? I have GTX 1050 Ti, 4 GB ram
Here is the full input and output of my CMD
C:\gpt-2>python src/interactive_conditional_samples.py
WARNING:tensorflow:From C:\Python37\lib\site-packages\tensorflow\python\framework\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
WARNING:tensorflow:From C:\gpt-2\src\sample.py:46: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.cast instead.
WARNING:tensorflow:From C:\gpt-2\src\sample.py:48: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.random.categorical instead.
WARNING:tensorflow:From C:\Python37\lib\site-packages\tensorflow\python\training\saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
Model prompt >>> Pikachu
======================================== SAMPLE 1 ========================================
-the-cat

You can see more of my artworks on my art page.<|endoftext|>(WOMENSUE.COM) – It's been eight weeks since Republican presidential nominee Donald Trump made news with controversial remarks about U.S. Sen. Kirsten Gillibrand (D-N.Y.) , prompting some to question whether Trump was using a moment of national embarrassment to garner votes.

But the question never has come up among women voters. According to a new poll from Morning Consult among more than 800 likely voters, Clinton trails Trump by 11 points among female voters, 44-36. Even if Clinton wins the women over 30 in the survey – 55-28 in favor of Hillary - the margin is still quite narrow at 3 percentage points.

""If you look at the race and you see women's views, it's clear that most women don't see Trump as a guy who is a threat to their future as women. If you are one of the women and you do see Trump as someone who is a threat to women, then he's not a guy worth getting to know,"" wrote Morning Consult Associate Director of Public Policy Dan Pate during a recent phone interview.

But some female voters do see Trump in that unflattering light. That's because for most women, Trump is the biggest threat to ""the future of women's lives,"" according to a Morning Consult-NBC News poll conducted Dec. 28 to Dec. 29.

For young women, the question of gender is only slightly less divisive. Fifty-one percent of young women between 18 and 29 in the poll said they view Clinton negatively, compared with 59 percent of young women 42 to 48. But a majority (54 percent) also believe Trump is a threat to men.

But while women vote in droves for their two major party candidates, women are still relatively conservative (44 percent of women between 18 and 29 and 53 percent in the other poll). Just 6 percent of young women believe Trump cares about women, while 36 percent believe he doesn't.

And even among those women who do believe Trump cares about women, his approval rating among female voters is about the same as the ratings among millennial men (60 percent positive to 23 percent negative).

This poll of 1,013 female voters was conducted by telephone Nov. 27-30 with a 3.0 percent margin of error; a general-election poll had a 5.0 percent margin of error and a survey conducted in late November by Fox News had
================================================================================

 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/144","pip install -r requirements.txt - Errors","2019-06-09T15:02:03Z","Closed issue","No label","I run this command and i am getting the below output
My OS is windows 10, X64
pip install -r requirements.txt
C:\gpt-2>pip install -r requirements.txt
Requirement already satisfied: fire>=0.1.3 in c:\python37\lib\site-packages (from -r requirements.txt (line 1)) (0.1.3)
Collecting regex==2017.4.5 (from -r requirements.txt (line 2))
  Using cached https://files.pythonhosted.org/packages/36/62/c0c0d762ffd4ffaf39f372eb8561b8d491a11ace5a7884610424a8b40f95/regex-2017.04.05.tar.gz
Collecting requests==2.21.0 (from -r requirements.txt (line 3))
  Using cached https://files.pythonhosted.org/packages/7d/e3/20f3d364d6c8e5d2353c72a67778eb189176f08e873c9900e10c0287b84b/requests-2.21.0-py2.py3-none-any.whl
Collecting tqdm==4.31.1 (from -r requirements.txt (line 4))
  Using cached https://files.pythonhosted.org/packages/6c/4b/c38b5144cf167c4f52288517436ccafefe9dc01b8d1c190e18a6b154cd4a/tqdm-4.31.1-py2.py3-none-any.whl
Requirement already satisfied: six in c:\python37\lib\site-packages (from fire>=0.1.3->-r requirements.txt (line 1)) (1.12.0)
Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\python37\lib\site-packages (from requests==2.21.0->-r requirements.txt (line 3)) (3.0.4)
Requirement already satisfied: certifi>=2017.4.17 in c:\python37\lib\site-packages (from requests==2.21.0->-r requirements.txt (line 3)) (2019.3.9)
Collecting urllib3<1.25,>=1.21.1 (from requests==2.21.0->-r requirements.txt (line 3))
  Using cached https://files.pythonhosted.org/packages/01/11/525b02e4acc0c747de8b6ccdab376331597c569c42ea66ab0a1dbd36eca2/urllib3-1.24.3-py2.py3-none-any.whl
Requirement already satisfied: idna<2.9,>=2.5 in c:\python37\lib\site-packages (from requests==2.21.0->-r requirements.txt (line 3)) (2.8)
Building wheels for collected packages: regex
  Building wheel for regex (setup.py) ... error
  ERROR: Complete output from command 'c:\python37\python.exe' -u -c 'import setuptools, tokenize;__file__='""'""'C:\\Users\\King\\AppData\\Local\\Temp\\pip-install-41gnaq1s\\regex\\setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d 'C:\Users\King\AppData\Local\Temp\pip-wheel-vzczahym' --python-tag cp37:
  ERROR: c:\python37\lib\site-packages\setuptools\dist.py:475: UserWarning: Normalizing '2017.04.05' to '2017.4.5'
    normalized_version,
  running bdist_wheel
  running build
  running build_py
  creating build
  creating build\lib.win-amd64-3.7
  copying Python3\regex.py -> build\lib.win-amd64-3.7
  copying Python3\_regex_core.py -> build\lib.win-amd64-3.7
  copying Python3\test_regex.py -> build\lib.win-amd64-3.7
  running build_ext
  building '_regex' extension
  creating build\temp.win-amd64-3.7
  creating build\temp.win-amd64-3.7\Release
  creating build\temp.win-amd64-3.7\Release\Python3
  C:\Program Files (x86)\Microsoft Visual Studio\2019\BuildTools\VC\Tools\MSVC\14.21.27702\bin\HostX86\x64\cl.exe /c /nologo /Ox /W3 /GL /DNDEBUG /MT -Ic:\python37\include -Ic:\python37\include ""-IC:\Program Files (x86)\Microsoft Visual Studio\2019\BuildTools\VC\Tools\MSVC\14.21.27702\include"" ""-IC:\Program Files (x86)\Windows Kits\NETFXSDK\4.6.1\include\um"" /TcPython3\_regex.c /Fobuild\temp.win-amd64-3.7\Release\Python3\_regex.obj
  _regex.c
  c:\python37\include\pyconfig.h(59): fatal error C1083: Cannot open include file: 'io.h': No such file or directory
  error: command 'C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\VC\\Tools\\MSVC\\14.21.27702\\bin\\HostX86\\x64\\cl.exe' failed with exit status 2
  ----------------------------------------
  ERROR: Failed building wheel for regex
  Running setup.py clean for regex
Failed to build regex
Installing collected packages: regex, urllib3, requests, tqdm
  Found existing installation: regex 2019.6.8
    Uninstalling regex-2019.6.8:
      Successfully uninstalled regex-2019.6.8
  Running setup.py install for regex ... error
    ERROR: Complete output from command 'c:\python37\python.exe' -u -c 'import setuptools, tokenize;__file__='""'""'C:\\Users\\King\\AppData\\Local\\Temp\\pip-install-41gnaq1s\\regex\\setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' install --record 'C:\Users\King\AppData\Local\Temp\pip-record-kx97zxlw\install-record.txt' --single-version-externally-managed --compile:
    ERROR: c:\python37\lib\site-packages\setuptools\dist.py:475: UserWarning: Normalizing '2017.04.05' to '2017.4.5'
      normalized_version,
    running install
    running build
    running build_py
    creating build
    creating build\lib.win-amd64-3.7
    copying Python3\regex.py -> build\lib.win-amd64-3.7
    copying Python3\_regex_core.py -> build\lib.win-amd64-3.7
    copying Python3\test_regex.py -> build\lib.win-amd64-3.7
    running build_ext
    building '_regex' extension
    creating build\temp.win-amd64-3.7
    creating build\temp.win-amd64-3.7\Release
    creating build\temp.win-amd64-3.7\Release\Python3
    C:\Program Files (x86)\Microsoft Visual Studio\2019\BuildTools\VC\Tools\MSVC\14.21.27702\bin\HostX86\x64\cl.exe /c /nologo /Ox /W3 /GL /DNDEBUG /MT -Ic:\python37\include -Ic:\python37\include ""-IC:\Program Files (x86)\Microsoft Visual Studio\2019\BuildTools\VC\Tools\MSVC\14.21.27702\include"" ""-IC:\Program Files (x86)\Windows Kits\NETFXSDK\4.6.1\include\um"" /TcPython3\_regex.c /Fobuild\temp.win-amd64-3.7\Release\Python3\_regex.obj
    _regex.c
    c:\python37\include\pyconfig.h(59): fatal error C1083: Cannot open include file: 'io.h': No such file or directory
    error: command 'C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\VC\\Tools\\MSVC\\14.21.27702\\bin\\HostX86\\x64\\cl.exe' failed with exit status 2
    ----------------------------------------
  Rolling back uninstall of regex
  Moving to c:\python37\lib\site-packages\regex-2019.06.08.dist-info\
   from c:\python37\lib\site-packages\~egex-2019.06.08.dist-info
  Moving to c:\python37\lib\site-packages\regex\
   from c:\python37\lib\site-packages\~egex
ERROR: Command ""'c:\python37\python.exe' -u -c 'import setuptools, tokenize;__file__='""'""'C:\\Users\\King\\AppData\\Local\\Temp\\pip-install-41gnaq1s\\regex\\setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' install --record 'C:\Users\King\AppData\Local\Temp\pip-record-kx97zxlw\install-record.txt' --single-version-externally-managed --compile"" failed with error code 1 in C:\Users\King\AppData\Local\Temp\pip-install-41gnaq1s\regex\

 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/143","Subreddit","2019-06-08T23:01:06Z","Open issue","No label","https://www.reddit.com/r/SubSimulatorGPT2/ that is composed entirely of different gpt-2 models that are posting and commenting on each others posts and comments.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/141","Using VScode in Conda , no models found ""import model, sample, encoder""","2019-08-28T10:07:43Z","Closed issue","No label","No description provided.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/140","Announce dates of staged releases","2019-05-31T16:39:36Z","Open issue","No label","It's great to see that OpenAI plans to release better models for public use in the future. However, I disagree with the current staged released strategy in place. Rather than announce when these releases will occur, OpenAI is just releasing them sporadically whenever the time is right for them. This is even worse than not releasing the model at all - isn't the whole point of staged releases to give time for others to prepare? Partnerships do not necessarily fix this issue; the general public should at least know approximately when these new models will release.
 The text was updated successfully, but these errors were encountered: 
😄1
bladedsupernova reacted with laugh emoji
All reactions
😄1 reaction"
"https://github.com/openai/gpt-2/issues/139","ERROR: Could not find a version that satisfies the requirement tensorflow-gpu==1.12.0 (from versions: none)","2019-05-28T08:49:02Z","Open issue","No label","After running pip3 install tensorflow-gpu==1.12.0
I see
  ERROR: Could not find a version that satisfies the requirement tensorflow-gpu==1.12.0 (from versions: none)
ERROR: No matching distribution found for tensorflow-gpu==1.12.0

I'm running Python 3.7.0 on a macOS Mojave.
I see a closed issue for this repo that says only Python 3.6 is supported.
And I see on the Tensorflow repo that Python 3.7 is supported on all operating systems.
So that's a bit confusing.
If I have to use Python 3.6 on my mac, is it best to use Conda? Also which version of Conda? Looks like there are a few versions; Anaconda®, Miniconda, Anaconda Repository, Anaconda Enterprise.
And doing something like this
conda create -n py36 python=3.6 anaconda
seems right?
 The text was updated successfully, but these errors were encountered: 
👍4
negrinho, robmerki, adrianwix, and GnarGnarHead reacted with thumbs up emoji
All reactions
👍4 reactions"
"https://github.com/openai/gpt-2/issues/138","Estimate of sequence probability ?","2019-05-25T05:35:07Z","Open issue","No label","Is there a simple way to get the probability of the language model generating a sequence? Given different possible sequences is there an easy way to compare their relative likelihood ? Maybe as an optional output given for encoding and for in batch generation? The use case is for comparing the systems preference for multiple choice options, and for ranking its preferences given a batch.
 The text was updated successfully, but these errors were encountered: 
👍10
volkfox, charlesaugdupont, deepbluesea, a3nm, DanTm99, dhruvluci, Tal-Golan, joelmichelson, anukurian16, and caroarriaga reacted with thumbs up emoji
All reactions
👍10 reactions"
"https://github.com/openai/gpt-2/issues/137","How to increase the maximum length?","2019-05-23T02:57:16Z","Closed issue","No label","I'm trying to figure out how to bypass the 1024 limit on Length but it crashes.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/136","Dockerfile.gpu has some bugs","2019-05-20T01:40:47Z","Closed issue","No label","Step 8/10 : RUN pip3 install -r requirements.txt
 ---> Running in a86e5b566855
 Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'
 You are using pip version 18.1, however version 19.1.1 is available.
 You should consider upgrading via the 'pip install --upgrade pip' command.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/132","Why does GPT-2 use a 500MB model.ckpt txt file that contains Asian characters LOL?","2019-05-23T02:57:58Z","Closed issue","No label","model.ckpt is almost 500MB in size for my download of GPT-2. Is this training data? Why the asian characters? How does GPT-2 USE this?
별ꠧ㰝噛빲࿆붣�㶣굑뵇ᦁ㸯䱝㶹甲밉ꬶ붨᷏붸手㹓㪚붒쑧㺏飩㷭뙲벧虧븽쁥㲚쉦㵒ᄃ봁హ㹞츙㸼㷑렊볕 㸀㶎뷻뵱㐄㶙訪브偱붤㴃둿㳆解㶔ͽ㱞餑부椨뵠㱏㶏⦱뷕伝뵋䁅㸉捺㴘鴆㸁벻蔺볯빱鐺범꿗봍☦븴಄뷆킔㸆윂㷕ᵜ봐횵㱊ᲁ㷘㗛㹶蘤뵅ꔲ뷄頱㸬끢㴂擄㸜㦞㸻㖨㸍ɳ㸄킹뷁봣ᰇ㸨겲뷆꯰㹺∼㷦暮븝쓏㹂⢻빯㸘븕뇈㷎史빇瓌벂쾼브붔也뷫ἶ믨�뺉ⶠ븩䫿뷝먮붛ꡌ븜䖏㶌꩘㸢륛뺅ఝ㮫홷㴃Ꝫ㹝뵆鼔㹢濟㷆쎪㶶鋈㸒쫜밲溝㯐﬒봈㳠븘邙㷞䉍빑캐㴏펿㱚ꂢ㴜℅붡㶓큠볥㶳郟㴿珌㴷⯄뷫⌁벂쫱봃ᦦ㶖붳잁뷰忿㷙쎧뷞༳㶧穀믘䭠븎齵븎軳볞�㶙ਜ믧ᕞ㲿㶿밲邝㴽툛븏茉뵑犚㷝新㲗♮봺㗾뷘儔㷶捴㱾㳺㷢ὕ븚狕㷮䦱㳞卦뷬㳦ᶀ봞Ꝼ㳲㵔将㶉夸㶻㷅븚㶒窲븘圄㷩檳㸉捯봉竂㸃维빂⃛㸥뺼뷪㲺취봕뢘븹盔븋ˮ붲⯠㭑é붳䥜㶟憱㸗㵋㭨㺇䧜뺗�뵼ꦀ븰ኪ㸛췪빋慌㹈௨뷳♂붝ื㶝玭㹛ꔯ㸾蹚㷌꽳뵈緁붙㭽벶܈㶕寛빯醢봼蘱㶸ꁊ뺇뷇驃㴊㷋鋹㶢૬붴ꚟ븃ᴶ빲减밋֗㷉敦봿唺㷷㣄㰃엓봉伟빓斫븅냿㴙❋벏뱐ছ뭫뵚櫅㶆丌㷸鼌뷦㶈ෞ뭙而㶏ⵓ븨뷛洄빏�㪡Ȭ븟햩㷸�㷟ꮓ봼᭴뵯콏뷗붭玖㰳귡㶣೜뷤ᖟ붉䋓븤璶벛犀뷣낥㷲볫嗷비ﭓ붵껺봥胒㷼斛봥Ḅ뷈봊꾢븞ꄯ㶍皀㶹捴벯ﮎ㷚腑㺁䄂㸙項㶍到㞳픧㶁鳠㸿ᇯ븗봪脚㵨㶎铙㷑༂㷥싐㲶皠붤ೢ븪ው㷻㸮Ⓛ㳵痂붔후㰓껬븜궱볃糄㷦㞔㹒뷵ᔁ벟稯믑八㱢ἡ봰렝㴅ਨ㴁췏㸷℉빨荟붼혴㵻❋빜鸾봾빯韇㹯ᣔ㹪劮뷪䔼배뵳グ붌砼뵭ⴀ㸚�㸃㷄ᑉ㷂捙㸼튆㶗�박㸊㲲⨷㸲無볷䁅㴛թ㺐퀱㯲鎌㸐ꇕ봀餉볤볙붓ၶ빙ꇩ븑뺃ᤩ㩎І뵏䓅㺆䛟㴑꘸㺁끹㸝哃붉遲㯴◫붢蕽㰑鉣붑괻븃㴶墧븽⋾붖厬㷲䘦㴓ﹿ㸇䈹붇㸆㵸᧨㹀䧤㷫e㮓䒩뱋㖩뷬ᬝ㵓鹴붯灩뺝줠㸱淵부뵜㡻㶛㣎㴝ꥨ㷱艳㶈㔇븲駖븷㶅ઌ붔䷘뷜唯빉灾㺁뎋뵸殨븧쓠㺨뷧⮳뵔툒붯鍭㴪핁㸤㵙�벶㷮镪봿烉㹂㺬붽�븢₰믪궔붕핚봔퍢뵎짲㲂⼹㶎薪㵾᮸벺䄗뵮㷏㸐髞㸲흌㵣ಥ붍箧밉鄖㵝ᡧ법喗㴡豮㳕ⷤ㴀䯹뱟☬㴾䤚븀픱㴔쎮붬砡븨དྷ밬彚븴뜃븉ⶎ빑葝붢紉븻ᕁ㶪淡㸊솝㷀뱒㲀䞹㸑橣㳹幍몀⦧㴣䴆㵸絲븬뮯㹂㌯븿皫㷰䒦븖檉㸴퍉봗묤붬ꜷ봔苚붱쳂볝ຓ㸺瘄벚㏲㶒త㴦軨부㗇㶓≱벗붢㸗봌㕙봎䟷뺦劺㴅䊾붢♀붽㲴뭈❽㳦䧆㳫ጾ뷠뺥㷍ꦤ봸嗒㷏䍩뵻毁㷉ﬡ물窹㺼뾟㬓붹썖붿뷀녃㸓鴓븛澹붣ჼ빠㌬㶱簤므ꟙ㶠ᰬ㹺襊붒�㵾ᤎ㵫邑㶛짚뵛┚㴬炚뱅ᶫ볅逃㹱뺋ၴ㵃ẇ붐쮫뷢К㱉൛㲕旕㱩뷃᪐㶰ꬳ㱗ᆓ㶑▙㷽邶븴蟆㺚႒㴍謂㳸볧瑂뺓潢㴮㴗㎛빟煉㯃ꀤ뷖朴㸬螆빔絷빪↧뱃㴿ᶊ㸂징빺翝븶꥙븴Ⱙ㴫䏽벿琘븎렭㴁飬㺧ˮ믫آ㮫纹뺂㷄밗㠺㴘빈匩붎ᚍ㸓攣㶂ꤶ볞㞻붉䴵붹붸ʒ㳘귂뷝잧붕ၷ㶗틈㵓臞㳔獶㷮著뵌㷝䅻㶢忙뮱鳠붨〿봤븍㶛㺜붖㸪Ꮿ㳇膩붍䯟㱃謺㱚笲벓꽱㲸⋒붴沆뷗⨭㴜禯밥␲뱻욁뷫좤뵧칕븛㬡❲㹅땳붐䦪㴐߅㯚⼏봕붼轝㶼≽벪㴝㶉뷅봰ꩯ빈ꄕ뵄ꄱ볨춆㶗⏋㵟冶㫜諧㪚⢤뱓龳밻頖뷴�븊髵븧�㳸䚻봣ࣗ븧ꧢ㱣嶪㴋㿆㺣䊵븘�㳽慩㺁줞붥蔟볛邰㹔ピ뷌쟏븿虝뷊ՠ븭ᓕ밮�붐笉㵣俜밃ꕜ㲮⃋㰂瘗븱褤㵪ꦪ봕�뵗쨾㳴⧬㴅ࣝ㷘鋼㵶븫㺕㵫暱㸥䫼뷛땨빂췺븚燛㸟ﶎ㸶뤚㶍㣕㶫摋㶐篃붦㖑㰮의㸚㶪븉뺁ﴃ븴硎븕螸㳍ഝ붲ὺ빅侖봕眶븧媺㷘ᐇ븨敨㸢嫇㵚ﰥ㴏虱㷶㶤洏뷣薻뷰麋㶈篰븾�뷝댣㨸ᛎ㺙脋봜䵠㶵횘뵫럘㸏ㆾ뺜弖봼ﳨ뷝询붧婴㸤뤰븉᳒뷱봢椗븽&븟첃㸃匁㶤췈㵫乆㸀뵐뷜訡붮�㹨㷨쇁빜ﳃ붠밡㵖䙾뱔ₛ믈㰐뷢蠽㺱㴼齊㯵霙㸈荭븊㸛꽲붒⿜㶦圗뷓⼓뵚묧㱃致㶜ᆏ븋戄봦孫㸆ឍ㸚�㶈籔㷕뱎�뮑脬븤➮㺑蹀㲫᣿㲙쪲뱰�㷾艎㱺ᙈ㴉捝㷳ᰇ봐덜뷠놇붣뢻㶆鰖븪詛붗ጶ㶆뵚띃㷣㶹撠븅Ჳ붧〞뺼娊㷠攖㺁纙봊⣯뷱욹뷮�뵼ᥒ㺝懒㸬盋㲟뵤釐븑븂鬦㷢쪏㸳ᤧ㶪Ѭ㲄咘㷖㔟㸡ꨵ㶨豯㵼㺎⧣㸐ﶚ븷�㶸�뺬ࣹ빆鞲㶲㴜坸븘鍿㲘ᅾ㲰ඤ㺇쑂뱌黡뵂賾㭑暑붏呋㺮趨㸣跔뵸쀘붉빦㹩聢㸙빨�㶯匡불꼿㯑㶸樝빧儵㸫낺㵖癋㹀튗뷕牂븯殞빗㬘㺫癪㴂腚㺬啠붡ߦ벙미ҍ븞蝲변笼붌헁㹴浑븀�㷤꘎㸜멀븟ⶕ㸺邁㬈绅㹋숊뱳㷞뵰⮾봜뷂뤎뷌䘶봳ᬧ㺑㷵⪥㻈欄븻怚봾䕯㴫歬㶙㦁㻄꩒뷟๹뷽猪믜핚㴤빂塑㸳⬂㲺誨㶲뷄훳㳈仆㳓辥봌镆㶋輨뷷◛빇웉㶊窳㸖출㸡㵫ጷ밞榟븝졤㷠恣㵸핊뵺迿㹁ꓱ븕髚㱔檌분䅔㹠ᮌ㶝Ϛ㸰뵟褿㴲릳붮鰄㲪趬븂�봨ꌇ벰롥붠陷뫽坬㷷䆑븒毭븝欃㵵㢕볪璘봃돟㺅剸㴤⼂붷륱븾댓㶧翠㸸ܶ㹄㉫뮼腏㴘辮붎㚶㲈ﺹ㴽ୱ븈ꩭ붺꼁봟㸙턨㴮挳㷧瑌㸚埜벐㒚뷡뱢봓덢㪯脕붩玔㷗옺붣ᦥ븫ௌ붼鱩븬藍뵽⇯㸒붉㴮銎뵭亢븄ꮾ뵍꺻부ᮏ봦辁믨쏼㸳뀗밃索㶠ⴞ㶛⿱㺗蒚㸜뽍㸢᠎뭔�믮젵㸀⸐㶻䲝붶糞㶎㸂싽㲏嗹㹔ꏬ㷚⢌㵀裹뮯ĩ붂䫜㹗鼜㸄Ŕ㵮㮨㸍읹뷣亏㶉ㄉ붐鐱벛箕뵓ᰳ㸁諼㻖뻖︇붚˅㸸῵㵃뷂堔㶰蔤㹼㳄붍뎡봰㓰뷷틛븲봚쥂붗弞㸾눲붟挓벃笏㶢쇲㳟㾑㸂�㲨ധ㸭甓봮绸㺊欦㮁꣈㳢ᕚ㶰㜈㸃磫㶓墋㸚Ჸ붶尷㹧�㵢䭯㷻禥봘䈚㸾黭봗䷆볪ꘖ뺅�뷂ᗒ㺆㮔븓⭢북꣑븨熧㰞﹩뷴㮔퍁뷀鰋㱤窪븗桙븄뷧敆㷵�㸠㙄㷃筶㴌㶀컝㷚귶㷪婬불謺㶵짽붽摏붛굹붒駦㷑쑗뺓ᚘ㹎廝㸅홡뱥ȿ빌幽뱧✱㴟絅빤֊㶚웳빖㣹빁鐂뺬陵뵇烱밞躟㴫ᐍ㳾⅘㶽紻㺚㸮᧦분ሡ㺕ꏸ㶔ۨ㫸죸㶄Ṫ븳ﲙ㴼쑷㵇掔뷤퓟붤캱뷍揜㸣嚙㹗汓뷩隈빥㪟㺳㊎㵝汜㸀뀮㺸♁㸌밂渫벇畛㬅㹀餬㶬燩븾뵃ဨ뷳⢧㹚姗㸰셾㶺斢븛㸄骖븝Ხ㲱�㶰炿빈˟㳭�㺎䥬㺨鍪㹹馑㸃髢릾빰騱㶈㶈줲㹶条㷀䩢㹻譶㲼䣨㹫覆㺇ᇔ뵎⍨븀뭷㰋틙붎䑔㸩㙴븝솶㺣箿뵜㪀㶬ᇌ봰眮㶽봥溴㸶쵵㻀魡봶�븚䰜㶍鍾㶯튻뷰ẩ㹒跰㰺㿼뻐㊬벍㠝봳뒩㱭镰㸤笠㴚蠫봧분㳙唾㵑蓡㷎鞺㸫㙷㴈퀓㶫暓믒㷏㺁⟧㺑᱿㱻셅뭉밵붟䘺븘栴㶺軄㺈㵳늴붃炻㹬붛ᯎ븟퇵벂뺃弥붼໚㸅㸋ꍍ뵍㏾㸛訳㷉胯㯉᠓㷖ⅷ㸒貋㶥놎뷊送빰墷벏鞐㴐웮벓冉㷥쉥㶄반㯸곸㲼❐㸻걢㸘뉤㹫ꈬ뭢쭐㷶쀋뤓琨㵑ꯧ뷄ꋔ봖좇㱬ᤘ㶾볭㹙솫㶂䨼㶹㭒벐틡㵅빱㴀嘢붻⯟볓것뵱歮㶛氺㴔㓓뷎빡涨㮉隟㺓㶵䨸㶷␴㹤牘뵜䛄빽ꪮ㶽䪐병ӧ㸝뜜뮯붕᪖뵌쵥㹞莮㷦릹붲ꭰ뷨誯봗怵뱩辍㶄ᕂ㸁붒韷㷫䢿㰮㍵벸ߞ먿䩅㸯䫀븸ป㷵牲븫㸍ቄ㶈᥷븴臌㲿覬봡혝붧뷰糮붕ᶻ벓ꛤ뵻瑕㶉첟㹗�븰櫴㹈�뷙⽁뷦뼪㷯ﶞ㳇甖㹉瞚㵛ᩇ봫잪붑ঈ㷵↪빺Ƙ빉輇뵓寅㰫鍥㺁㷖䁸㶌깐㵁뾫붗䘁뷹鎵㹆첷㷏�뷝➪㴩䵓뷲郿븏ꯧ봡�뷷鸕㴎﹍㶃ᬿ븣몼빒凅븿ヱ㸿䀶㵋ꧡ붺␵㸚㶆騷㹀펆㶕ꐺ㻟떢㸦舉㴭鮱븿狽㷌ច㮿㴋㹴說빖䦻븎뀊븝⢫㴤㣼붟㷥䐻㲓耪㶐㧦븳㋨뮃䄺㳾ᙨ㵀軷㸔쒛㹼謱밀ႌ㲀浃뵕䗑㸛薳㸼偫㹨ᄎ뵓繞붝尅벻㨰뺁柸㹆阽봒䭳㶴儡㸗㷔ῐ붋䩽㺐㤏㴍퉭㹨ဨ㶲輞㸲봜⧜㴩꓇붥ﹾ뺃钅붘⑜㲎䀈빸㵛뵐૵㱕㲍먷븃빫㷯≔먓퐇뷉揓㸌抹㲘蠍㵊圇㶞㹿௻㸶阾부ᥠ뺈㷜봘놁븢攟㶫罺붚왈븁ᤲ븐Ἶ분㋁뮒客븅֯㺂㙂㵁玖㷁맢브ㄻ㶐뙲붻뽄뵶癋봭ꢁ㶦旸㸞錩㸖붙⋳㸆豈㸪妾븍㭖㸅㹚᪇㷚淏㸥⟍븤㸔ᢓ뷻㵿䜟㸨뙘볖﫶㵡㵡붬衤㸐㴸䯔붒�뷰먪㵺泃봺鎔붝篶뵐劗븶⎄㱶㸂欴㷉僄㵾챜뵰�뺪킌㶹浂㶦嫷㴕㝆붉늻㮰쨋㴬簁붯쇖㸬乮뵫b붆髦봱䢩볣ꤱ뻌誌볻禑뺙眇뱃揗㸧쿠뷒蔆㷷븒ꙷ뵴啬㷘͆㶷囉뷦뺁挵볾☝㲏㶁찀㵔⃢븛腾븹隶빚뇁봳蠛뺔ک㷞㶣갅㶮꧰밣誜㷁ﳽ븀䭴뷽ු빝噽븩আ블붵㊁봆혟붴꽜㶿ㄿ㱌칵뵘顝뷋⑞㵔�㹭㇐봉欀뷇㸤ⓐ뺂傖㶃㜫붺녽벧ᒑ뷕勧붨맸뷇�뷥Ṷ㲂䐀㹸벮賢밖᫩뱃㩏㷼㝇밫眮㵿囥㳤븮뮗㸇ઁ㹰짭㳍﫵봠⠧㯶롡㳲㹊찢묏뢮㱇老㴶橈㶩뭣㲦�㵷鮎뫞哲붩ꗶ㹋弦붳资㶒蒜볶㹛㦮볅뷮ꯂ㹒짒㴵䂦㸝᪏㷃﫪㹕ړ비揺뷊㋍뺴㳓㷜ჱ㸊ꇵ븊Ⓩ㹼搤㸺북퇳뷻滘뵯兀붻㶊ￔ㷥൷빛ꬍ㹑㾔븚踑뷤竂뱒ٙ㸤붻㑵붊厬뷞䄌먛렵봻ᴧ㷬蜔빀㲮됳㹊ḉ밿峂뷂橃뱥皠빊죞빓ꥬ빪붃⎮뱰蛫븤仔㬪枤봆끠㶏㳇흢㴤존㰮軐븩뻧빨㲼㳥Ṃ붍〨붤膝붭ᛍ븛㴻Ѣ벗娐븖㒘봎ﲡ벡鯠㸂왻㷳甿뫄➘㲡♂㴦艓벙欺봄莤붃汍벒駿뷀뵿ﱄ㸤版㵑䁪㯓濗붙�뵑刳㵞붋멠㶽血㷙棢㷡琭빻Ѝ㶐瘼븸㠾붙ᓫ㵒좴㷃봙뷌붨�북曑㪡颾㸣묧㺤쟓㸪᳙㲉⣟㰼⾢븯䁄밲콇㲓ণ㶡ꂩ빿ፉ㲑�㴒榁붭狶붦﨩㷈埿붙븎㟭뺏詡벓棛㴟妷뵙籍뵱蚶㸠Ꮙ㶧㸠抗봫됖뷅峝붎捘㸍嵅뷬鍄법嶯㶺䴤빩뱶硥봻숅㵊癃븟�봛ઌ붍⧛보鬇㴐介뷟㒎봯坓뷤ⱷ㸼䱒뵮縜㵩뵙Ø뷟퉾봓ᚈ봿ጃ뺣뫼븏樓㶑⋁븓䰝㷓삛㰈㵪㷘贼뷵쁙뺁�㱶勃㹁輁㷂菪㲂픷봉ꁣ뷌៍㱕㶈ᚽ븸朰빞賞벾굣븗럒붑黔㳵븚鐺븛謶㷥鰨븳瑻㷅뒮뱀㓧㳰븺诿㷱팔봇䉬븳뷺繰㺑㵙㗔봠វ㵔՗㵵ৌ붮ࡉ붅兀뵞僇㱎໕븞묣봣䰃븼텇㷁쩈㷠뵭툱봅怿㷡螸봒㼥븓웓뺄捧㯸湰㵣釟㹵붰剙㱶ꩪ㷫閕㳷⿾뵨㪊붸륷㸒촙뷀뺍㶉뵅ួ봴殝뵎ở뵀㷥胴㶭㸑ᣑ㶛赹㶛蛳㳨ᮧ봫䲲빀଍붯룲볐�빌ᅏ붱ऩ㳱禬㱦ꑺ볇よ㹁졽㶄잠빜ꮯ㷹᧻㬃�㶗␱㲁퍒빍䍂밓案㷝ᛃ㺂禲붧畤뵕쨆빹羜빅봝㜝㸹갥븖譿븘㺙㸶頞뺌䧧빋㓗밼䰶㵀單분㲎慿㶗노㵯ẝ㯆㴵뀩붶岣붓㹰䩻뻈᧍붶Ľ뺍㿳붒鄯붑뼜뷕杒붆㩰벨竛㴿肽빇慥붙罹㱱祍붇䍵㳰�븧禲빇㏇㶚貓㸏虋㸊鎾봞熤봲唙㶬䰟뵴蹹㶜痉븎鶣㺒ᕔ㵹腱㶔丕붮즽㴂늘병㴄ᄩ뷖洚븒ﴈ벓큲㶵컷㷋⪘빉郪볨䗏붯ﺣ㸔檥㷦�㷈봫䝐㲏ﰈ㸅ઞ뺏檭㶷䝲뷿헼㵓햍㷮嫟봁�뵐뼞볛眷부ﾚ㷘३㵉驒뷸⪥봛丗붚㑐㵿᪵뮜ਗ㷢㷝䋕㶞℟㹷쮺㳃ᅒ볜噒㱚춀㰯Ḍ㯸縠붷馴붎쑺볆ꒌ뷚⁹뷻羚㱀츱븈벅嵃뷄縸㷚썛비愭㴤؊빒뎣붪ꊤ뷮镟붥휪㹇睊븉歾븬빭퉈벘㬷붦ꉅ㴭㽫븙㤞㳫㶾㸁⪲뵨Ǐ㶧섦벺䛣㺝�㶴�봳遝븛觴붫蔍㱉ꑇ붯甚㸀볾븾儞㶅菤붂㵣홃㷑趋㵅殽볓Ӡ㶖냡벰ﲉ불蛌볠ᦞ뺁ྴ㴣ꄡ붌渏㸕ਹ㷱䡿뷱⻳㷢﯏뷿᰺㩌蕤뵚븮뺟栧붲᱿㸎଺㷊ᙋ뷰唩뱩匁빔셝븛가㵺㷚顳㲸毇㳢ᨹ㵸ퟒ㯅蛸붍沈븵⇑뺪ﹹ㳇ȱ㷇쬛뭋婩벵揉㴡ꔱ뺅葋빻諬볯⓻㸃൝뺋汾㸔걑㶆᪕뷆법갊㳑䡨븝䧓㵈ᄙ뷀㰹븟뺅鰽㲳㏀㴲�㱘㴳칩붚彗뷅㬇봽뎭봶㷝㸬홰㰣뵒㴥⬟㴂㻵㷅ᚆ㹃˩부앟봒ꤖ㶫⇇붚貽㶫쏗밠롭벴뮂㰹ྈ벱ᳰ붃椝㹶綍봙髑볕䫲뷫Ц븈㹟澘별ॼ㳦姶븑㾤뱀빰縣봧Ⅻ붸涕㸒㿜붩颪㶖䰱뵏㟩㯆皋뺁老뺂迯㵅낥볋鶷뷼찕봼ᇩ뵳踲북炙뵣᱇㱠ⳓ㵌襥㲌�봫즈㶐ﺝ㸅᱾㲔뎗㷐틿㴖ꢠ뷇ፅ㯝뱎ᙂ붮允봷藫㸗炊㵘쨢㶄涰붫⠴뵾ه붥≤㷫㞡빜瘟붕१븒�봦饺븲⎌봕餫㸢㜏㰷杯뷤꽔붅ፖ볃쟃㶁돢㹉垚븯鋶뮐떈㲨鯯㺄ᆉ㸱靈㳵⸅붔㊸믵﹆븒場빈賠㷃㖢㸜巁붡軠븄꾸㶻�븲菾빸ﴮ빘蛝봎㇠㷯밵㴕︯먷궶먀劬뵏풑㴫뱳﯏㶸㯛㯽皿㸗豜븎嚴㶝天㴶㣇봙ꄛ뷬닑붕쯤㴧汹붚㳲븘⶞㲆꼉㷥銇봏໎백핫봂㺉뵀붂怍뷍ᩄ㴖룅㸞홯밃죭㸎暆밄閊븳ﹺ㶈䇭㺎ꖤ븬ᵡ빇휆븟�㳈㶝잻㺢뛉뷦時먃輻봇㫚뺠뷼붑埐㶨ୗ빡ᅏ㵏㬑㶌╓㰚餏㸗焯발⿱빘퇢㹃侌㳢빐㳟鷎뱚轂뱵式벐梅㴕鄛붙㶈匚㶱ᕋ㸪쳗붛㾇㵹᫱뱪࢈븏㭧兑뷲聳붗襨봾院봪穞봢퓉믨蕲㶱᩵뷫๻㩩篍㸉ꕅ㸈鑡㲜্밐땗뷠檔뷅㵣붏ﱂ법⺀빜Ⲱ빨ࡘ뺯砬븯䊬㷐ᖎ뷇나㹊墝㷄送봭븯믖㶸े㴶哣붐鯖봏�블ꚢ묢�㸮䛟㸑῱뷋ꎆ㵐ډ㲼有㶣꺑㸜쯔㶼ٱ㬒嬨빠娩볊콺㴁ｖ㯳ܘ㹀㹴얍㶃童백帢븅ꁬ븠ᣯ㶻朰벸ڡ빰䟮븐침㹍킠빓㷏冃볼诈뺵歂㶍痎붨놺㴚䜑㸜䒅믟뤻벯ॏ㴚糗㸐䷿㶻鴭뷺�㵐큕㸵鰴븃빆妅㶨샓㺑੃뵤㏹벾瀒㴮ꁢ㷈辰뺢柵붊ᾘ뷍酫㳚Ｋ붯빓븵붘ᗄ뷠ᯰ븞㿻㴢�빕앿㸕穦븮갔빮￳봜໹㷂꘎븨ӗ㷜顺㰦睹붇⪚㸲ꖺ봝ી㳈╒㳷뀿붵䈍㵌
 The text was updated successfully, but these errors were encountered: 
👎2
KaneTW and huchinlp reacted with thumbs down emoji
All reactions
👎2 reactions"
"https://github.com/openai/gpt-2/issues/131","Release raw lambada dataset","2019-05-15T18:57:04Z","Closed issue","No label","Is it possible to release the Lambada dataset used to generate accuracy numbers in Table 3 of the paper? This would make it easier to do comparisons with other models :)
@Newmu
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/128","Is there a way to separate multiple segments of input text for question answering tasks?","2019-05-04T11:40:45Z","Open issue","No label","I am using GPT2 for Generative Reading Comprehension task and am wondering if there is any special way to induce the understanding of passage, question and target_answer (like ""TL; DR:"" is used in text summarization task to separate article and summary).
Presently, I am using "":"" and ""?"" as passage-question and question-answer separators respectively.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/127","Can't open download_model.sh in colab","2019-05-03T00:32:17Z","Closed issue","No label","When I try to download the model I see the error:
sh: 0: Can't open download_model.sh
What am I doing wrong? I used this code in colab:
!git clone https://github.com/openai/gpt-2
%cd gpt-2
!sh download_model.sh 117M
import re
with open('requirements.txt', 'r+') as f:
  t = f.read()
  t = re.sub(""regex==\d+\.\d+\.\d+"", ""regex==2018.01.10"", t)
  f.seek(0)
  f.write(t)
!pip3 install -r requirements.txt

 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/126","WebText Cutoff Date","2019-04-29T15:49:29Z","Open issue","No label","Hi, what's the cutoff date for the text crawled in WebText? The original paper mentions that CoQA comes after, but I don't think it says the specific date.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/125","Interested in programming length recognition.","2019-05-15T18:59:07Z","Closed issue","No label","Hello! I was wondering if you guys would be interested in collaborating on programming in a stop-word or length recognition to adjust the text output. (like dicing it instead of 3-4 paragraphs every time.)
Was this considered, and I was wondering if there was any harm in a PR to adjust the values for each text input. Like saying:
Input: The cat is blue
 output: and brown and grey.
Instead of:
 Input: The cat is blue
 output: and brown and grey... 5 paragraphs.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/123","Issue when i try to train using my own dataset of bigger size","2019-05-03T00:33:01Z","Closed issue","No label","I am trying to train the GPT-2 using my own dataset which is substantially larger size. However, the process gets stopped at a point after displaying below details -
2019-04-20 18:30:42.851036: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz
 2019-04-20 18:30:42.851356: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x1b06260 executing computations on platform Host. Devices:
 2019-04-20 18:30:42.851408: I tensorflow/compiler/xla/service/service.cc:158] StreamExecutor device (0): , 
 2019-04-20 18:30:43.106241: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
 2019-04-20 18:30:43.106797: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x1b05e40 executing computations on platform CUDA. Devices:
 2019-04-20 18:30:43.106854: I tensorflow/compiler/xla/service/service.cc:158] StreamExecutor device (0): Tesla T4, Compute Capability 7.5
 2019-04-20 18:30:43.107280: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties:
 name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59
 pciBusID: 0000:00:04.0
 totalMemory: 14.73GiB freeMemory: 14.60GiB
 2019-04-20 18:30:43.107310: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0
 2019-04-20 18:30:43.920575: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:
 2019-04-20 18:30:43.920645: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0
 2019-04-20 18:30:43.920654: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0: N
 2019-04-20 18:30:43.920962: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14115 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)
 WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
 Instructions for updating:
 Colocations handled automatically by placer.
 WARNING:tensorflow:From /content/gpt-2/src/sample.py:51: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
 Instructions for updating:
 Use tf.cast instead.
 WARNING:tensorflow:From /content/gpt-2/src/sample.py:53: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.
 Instructions for updating:
 Use tf.random.categorical instead.
 WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
 Instructions for updating:
 Use tf.cast instead.
 WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
 Instructions for updating:
 Deprecated in favor of operator or tf.math.divide.
 Loading checkpoint models/117M/model.ckpt
 WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
 Instructions for updating:
 Use standard file APIs to check for files with this prefix.
 Loading dataset...
 0% 0/1 [00:00<?, ?it/s]tcmalloc: large alloc 1243209728 bytes == 0x5554e000 @ 0x7fb0676851e7 0x5a969b 0x53a351 0x7faffe9d664e 0x7faffe9f08bd 0x5030d5 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x502209 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x506393 0x634d52 0x634e0a 0x6385c8 0x63915a 0x4a6f10 0x7fb067282b97 0x5afa0a
 ^C
What is happenning here?? I have no idea.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/121","interactive_conditional_samples not checking if prompt length is greater than hparams.n_ctx / 2","2019-04-12T21:11:31Z","Open issue","No label","If the length is greater this will break the model due to the word position embedding (wpe tensor) not being large enough. Should a check be added?
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/120","Why did GPT-2 not use dropout in the model?","2019-05-02T05:36:42Z","Closed issue","No label","I noticed that there is no dropout applied in the gpt-2 model, but a dropout of 0.1 was applied in Bert. Is there no overfiting problem in training gpt-2?
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/118","What are the 117M parameters?","2019-05-02T05:36:27Z","Closed issue","No label","Trying to understand what is meant by ""parameters"" in this context, but could not find this anywhere in the literature. Are these attention scores for each word in the transformer? How does a larger amount of parameters help the model accuracy?
Also, given that this is Transformer-based, is there an easy way to visualize these attention scores (ie, see which words the network attended to for each representation)?
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/117","Looking for a GPT-2 diagram","2019-05-15T18:59:21Z","Closed issue","No label","I'd take this as a code 'issue'. (not actually the code part, but the math & terminology)
I'm not a layman - I do work too hard on AGI, but I (laymen figure in too) have taken the path to become fully experienced in AGI with no code or math whatsoever... Don't laugh, it worked.
I want to learn from GPT-2 immediately, as I have a white-box net made of contexts that can do everything, so I want to see a few things...
Issue is the algorithm is advanced, the matrices/Norm/etc (ignore code) 'flow' is not explained, I need a flowing diagram that goes further than any currently on the internet and shows these math/layer-terms. I need to move fast...
I'm looking to pay someone.
While I have to hire programmers to build my stuff, the real issue is not knowing 'what' to tell them to code...the AI 'idea' comes first, and I'm really good at that part.
The takeaway here is I have a lot of effort to put in and make huge change immediately to make AGI, worth 50 brains of effort in a year, but I need some help here.... Too few freelancers even know this new area of BERT & GPT-2.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/116","Simple common questions lurking","2019-05-15T18:59:57Z","Closed issue","No label","Do yous really know how it works or is there [some] black-box mystery?
What is the closest thing to GPT-2? BERT? Or simply the Transformer? But how close is it?
Byte Pair Encoding discovers segmented parse parts like this?: [[The [red ball]] [hit [the ground]]]
Say we have a 200-word book and want to predict the next word. Do you place multi-head windows along the end like this and then sum em together? Three 4-word windows are below:
... and so the frog and [cat (lived {happily ever] after) because}
In the net, do yous sum the created window with the positional BPE to get compatible decision for next word?
Dispensing terminology, summarizing GPT-2 greatly, explain how GPT-2 predicts the next word, how does it know what follows? How many words are actually looked at? Where? Does it refine its decision? If it seen the word follow before, what else is there to it...It's like a markov chain.
...I really want to see it drawn full detail, I don't know all this vector norm dot product language, I want it in English...
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/114","Create and fit my own vocab.bpe","2019-05-23T03:04:04Z","Closed issue","No label","Hello. First of all, thank you so much for this great job and for sharing with the community.
I would like to know how I could create my own vocab.bpe file. I have a spanish corpus text that I would like to use to fit my own bpe encoder. I have succeedeed in creating the encoder.json with the python-bpe library, but I have no idea on how to obtain the vocab.bpe file.
I have reviewed the code in encoder.py but, I have not been able to find any hint. Any help or idea?
Thank you so much in advance
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/113","Tensorflow for TPU?","2019-04-05T22:42:24Z","Closed issue","No label","What version of tensorflow should I install that supports TPU?
 Thanks!
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/112","Reading Comprehension: answer questions about given passages","2019-04-05T22:45:29Z","Closed issue","No label","Is there any way to run the Reading Comprehension: answer questions about given passages as shown in the openai example link.
 Can we run this using 117m model if yes than how.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/111","tensorflow is installed but it tells me that 'tenorflow.python.saved_model.model_utils' does not exist","2019-05-23T03:01:49Z","Closed issue","No label","I am trying to run gpt-2, and this is the error:
 ModuleNotFoundError: No module named 'tensorflow.python.saved_model.model_utils'
 I installed tensorflow via pip, installed gpt-2, but when I try to run it, it gives me this error.
any help would be greatly appreciated.
 thx
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/110","Which is better: GPT or RelGAN for text generation?","2019-03-26T08:44:21Z","Open issue","No label","Based on my understanding, gpt or gpt-2 are using language model loss to train and generate text, which do not contains GAN.
So which is better: GPT vs RelGAN/LeakGAN/SeqGAN/TextGAN
I am so confused about this question. Thank you very much.
 The text was updated successfully, but these errors were encountered: 
❤️7
bladedsupernova, Morizeyao, AnShengqiang, adeeb10abbas, lord-alfred, lethaiq, and clouedoc reacted with heart emoji🚀2
AnShengqiang and lethaiq reacted with rocket emoji
All reactions
❤️7 reactions
🚀2 reactions"
"https://github.com/openai/gpt-2/issues/109","How can I use this for sentiment analysis task?","2019-11-06T07:54:01Z","Closed issue","No label","No description provided.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/108","How to prepare the data for text generation task. Thank you very much.","2019-03-25T00:53:46Z","Open issue","No label","First, I'm not sure whether the model contains the encoder during training.
EOS means end-of-sentence. Encoder and decoder are part of transformer network.
If without-encoder, training time:
target: [E, F, G, H, EOS]
decoder input: [0, E, F, G, H]

If without-encoder, testing time:
decoder input: [0]

If with encoder, training time:
encoder input: [A, B, C, D]
target: [E, F, G, H, EOS]
decoder input: [0, E, F, G, H]

If with-encoder, testing time:
encoder input: [A, B, C, D]
decoder input: [0]

Am I exact right?
I know it is beyond the topic of this project, but hope you could help.
 Thank you and thank you.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/106","InvalidArgumentError error message during conditioned text generation","2019-03-22T20:02:51Z","Open issue","No label","Any thoughts for the following error message when running the conditioned text generation script on the 117M pre-trained model for a bit large input text?
InvalidArgumentError (see above for traceback): indices[0,0] = 1024 is not in [0, 1024) [[Node: sample_sequence/while/model/GatherV2_1 = GatherV2[Taxis=DT_INT32, Tindices=DT_INT32, Tparams=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](sample_sequence/while/model/GatherV2_1/Enter, sample_sequence/while/model/ExpandDims, sample_sequence/while/model/h11/attn/range/start, ^sample_sequence/while/model/h11/attn/strided_slice_8/stack_2)]]
 The text was updated successfully, but these errors were encountered: 
👍1
lostmsu reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/gpt-2/issues/105","If word2vec wasn't used, what was ?","2019-04-05T22:48:14Z","Closed issue","No label","Word2vec and WordNet can translate words, like cat=dog. They can create a high dimensional relational-clustering in a neural network.
French to English - yous do translation. And, when GPT-2 very-accurately predicts the next word, it looks at all the last words - various needs for translation come into play, absolutely.
And, GPT-2's translation is superb clearly.
But how does GPT-2 know cat=dog!? What did yous use? Why is it so good?
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/104","How to replace 'enc.encode(raw_text)' ?","2019-05-23T03:03:52Z","Closed issue","No label","I want to use another tokenizer. I know the 'encoder.json' and 'vocab.bpe' are provided, but I can't understand how they are made.
How can I replace the tokenizer model to that I trained using own dataset.
Thanks.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/102","How many structures are in the small model?","2019-03-19T22:43:13Z","Closed issue","No label","In the small model, how many neurons, layers, and nets are there?
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/101","ONNX export","2019-03-13T20:47:57Z","Open issue","No label","Has anyone been successful exporting GPT-2 to the ONNX format for visualization etc. I'd like to study this model and a nice visual in Netron always helps.
Thx.
 The text was updated successfully, but these errors were encountered: 
👍5
WilliamTambellini, yeldarby, ykafia, stared, and taku-ito reacted with thumbs up emoji
All reactions
👍5 reactions"
"https://github.com/openai/gpt-2/issues/100","Installation Guide?","2019-03-13T19:03:05Z","Open issue","No label","Forgive me, I'm very new to coding. Here's what I've done so far.
Installed GitHub Desktop
Installed GitBash
Installed Python 3.17
Cloned the repository
Opened the repository in Visual Studio Code
Read all of the issues on this thread
From here, I really don't know what to do to use the model. Is there anyone willing to lend a hand? Much appreciated.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/99","How can I use this for QnA task?","2020-06-18T09:13:46Z","Closed issue","No label","Currently I am using BERT, but how can I try the same thing with GPT-2.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/98","Dockerfile is using Python 3.7 but tensorflow-gpu needs Python 3.6","2019-03-10T15:21:28Z","Open issue","No label","I have this issue when i install the dockerfile.gpu:
Collecting tensorflow-gpu==1.12.0
  Could not find a version that satisfies the requirement tensorflow-gpu==1.12.0 (from versions: 1.13.0rc1, 1.13.0rc2, 1.13.1, 2.0.0a0)
No matching distribution found for tensorflow-gpu==1.12.0

And is because i have Python 3.7.1.
Can you configure the Dockerfile to use python 3.6 ?
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/97","Input size is restricted in 2 ways","2019-05-02T05:37:56Z","Closed issue","No label","When I enter the small model a wall of text bigger than 396 words, it always errors. How can I feed it a big page?
Also, if my input isn't 1 wall of text, it thinks my input in ex. 8 queries, and I have to exit my program or else it goes through each paragraph one by one.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/96","Autocomplete a sentence","2019-03-08T17:34:37Z","Closed issue","No label","Hello,
Is it possible to predict the next word in a sentence as the research claims ? Is this locked in the bigger model ?
The python code randomnly generate sentences.
For example, like Google smart compose type ""My father gifted me "", and an autocomplete prompts cheque ?
Thanks
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/95","Well thought out reasoning of releasing world-modifying tools","2019-03-07T19:29:23Z","Closed issue","No label","I thought this out well.
AGI, ASI, and tools that can instantly change Earth in days (nanobots), can not be handed to ALL humans. [Some] humans are disabled and are not fit. In fact, if [some] humans even [see, not hear] on the news an AGI android talking to us, they may send nukes off. Generating huge curated text may prove to readers some ideas, and readers often trust it when they read Elon did X. When it comes to others ideas to prove to the reader, more tangible hand-on ideas, those are harder - and if they can be proven then they may be indeed true. Like how to make Cryonics work. Humans can already write up walls of text way better.
There's a catch. The more friends you tell, the more progress you get. You must verify them as stable. The more people you tell, the more likely a disabled man will hear about the powerful tool. We are not at this stage yet. If it were ASI then it would be tight because it is the final technology. Too few people in on it though, and a resistance may decide to capture such small group.
My answer is: Release how GPT2 works.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/94","json ecoder error","2019-05-23T03:04:31Z","Closed issue","No label","Hey guys! thanks for this project.. still trying to make it run...
 A step by step guide would be useful.
 Got this error when trying to run interactive_conditional_samples.py
User@DESKTOP-OR1I4MI MINGW64 ~/desktop/bots/gpt-2-master
 $ python src/interactive_conditional_samples.py 117M
 Traceback (most recent call last):
 File ""src/interactive_conditional_samples.py"", line 86, in 
 fire.Fire(interact_model)
 File ""C:\Users\User\Desktop\bots\gpt-2-master\gptvenv\lib\site-packages\fire\core.py"", line 127, in Fire
 component_trace = _Fire(component, args, context, name)
 File ""C:\Users\User\Desktop\bots\gpt-2-master\gptvenv\lib\site-packages\fire\core.py"", line 366, in Fire
 component, remaining_args)
 File ""C:\Users\User\Desktop\bots\gpt-2-master\gptvenv\lib\site-packages\fire\core.py"", line 542, in CallCallable
 result = fn(*varargs, **kwargs)
 File ""src/interactive_conditional_samples.py"", line 42, in interact_model
 enc = encoder.get_encoder(model_name)
 File ""C:\Users\User\desktop\bots\gpt-2-master\src\encoder.py"", line 110, in get_encoder
 encoder = json.load(f)
 File ""C:\Users\User\AppData\Local\Programs\Python\Python36\lib\json_init.py"", line 299, in load
 parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw)
 File ""C:\Users\User\AppData\Local\Programs\Python\Python36\lib\json_init.py"", line 354, in loads
 return _default_decoder.decode(s)
 File ""C:\Users\User\AppData\Local\Programs\Python\Python36\lib\json\decoder.py"", line 339, in decode
 obj, end = self.raw_decode(s, idx=_w(s, 0).end())
 File ""C:\Users\User\AppData\Local\Programs\Python\Python36\lib\json\decoder.py"", line 357, in raw_decode
 raise JSONDecodeError(""Expecting value"", s, err.value) from None
 json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)
 (gptvenv)
 User@DESKTOP-OR1I4MI MINGW64 ~/desktop/bots/gpt-2-master
help much appreciated
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/93","Cool and fast model","2019-03-07T19:19:22Z","Closed issue","No label","Guys thanks for releasing this model. It is very fast compared with LSTM models. Cudos!
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/92","Train model on small data set","2019-03-07T19:20:31Z","Closed issue","No label","Great work releasing this!
Is it possible to get the data used to train the 117M model?
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/91","Extracting representations from the LM","2019-03-06T01:36:37Z","Open issue","research_question","Hello,
Is it possible to extract features from the language model using the current version of the code?
 For example, given a string such as ""the cow goes moo"", can we extract a vector for use in downstream NLP tasks?
Thanks
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/90","error downloading model using ""python download_model.py 117M""","2019-03-07T19:27:26Z","Closed issue","No label","got this permission error:
AccessDenied Access denied. 
 Anonymous caller does not have storage.objects.get access to gpt-2/117M/encoder.json. 
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/87","clarification","2019-02-28T18:31:58Z","Closed issue","No label","Hi, can you just try to clarify in human readable text what can be done with this project? I find the usage explanation a bit shallow. Basically can I or cant I use this model to start from sample text to generate text accordinglyas a POC ? if so, can you describe more accurately how to actually do this? many tnx
 The text was updated successfully, but these errors were encountered: 
👍1
tanasi111 reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/gpt-2/issues/86","Is it possible to edit the code based on this project to train from scratch?","2019-02-28T18:36:45Z","Closed issue","No label","Is it possible? To achieve maybe 90% of the experiment result in the paper.
 Thank you very much!
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/85","Problem with re.compile in encoder.py, Python 3.6+ x64","2019-02-28T18:37:37Z","Closed issue","No label","This is from 9d1e704:
re.compile(r""""""'s|'t|'re|'ve|'m|'ll|'d| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+"""""")
When I try it in Python 3.6 or 3.7 x64 on either Windows or Ubuntu, I get
>>> import re
>>> re.compile(r""""""'s|'t|'re|'ve|'m|'ll|'d| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+"""""")
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\lib\re.py"", line 233, in compile
    return _compile(pattern, flags)
  File ""C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\lib\re.py"", line 301, in _compile
    p = sre_compile.compile(pattern, flags)
  File ""C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\lib\sre_compile.py"", line 562, in compile
    p = sre_parse.parse(p, flags)
  File ""C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\lib\sre_parse.py"", line 855, in parse
    p = _parse_sub(source, pattern, flags & SRE_FLAG_VERBOSE, 0)
  File ""C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\lib\sre_parse.py"", line 416, in _parse_sub
    not nested and not items))
  File ""C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\lib\sre_parse.py"", line 502, in _parse
    code = _escape(source, this, state)
  File ""C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\lib\sre_parse.py"", line 401, in _escape
    raise source.error(""bad escape %s"" % escape, len(escape))
sre_constants.error: bad escape \p at position 26

Works fine in Python 2.7
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/84","How can I train your model on my own text corpus?","2019-02-28T23:52:52Z","Closed issue","No label","I know that you trained your model on a corpus of news texts. I would like to train it on my own set of documents. I'm not looking for precision, rather for poetry. Could you please tell me how I could create my own training set and reactivate this model?
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/83","Windows installation","2019-02-28T23:54:23Z","Closed issue","No label","Hi. Found this software through a game. This could help a lot with the roleplay posts. A game changer! Unfortunately, total noob here. So, I was wondering if someone could guide me through a detailed windows 8 installation?
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/82","OutOfRangeError","2019-02-27T08:36:15Z","Open issue","No label","OutOfRangeError (see above for traceback): Read fewer bytes than requested
 [[node save/RestoreV2 (defined at F:/ex/gpt-2-master/gpt-2-master/src/interactive_conditional_samples.py:45) = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]
 The text was updated successfully, but these errors were encountered: 
👍3
JasonVann, ariera, and wanshun123 reacted with thumbs up emoji
All reactions
👍3 reactions"
"https://github.com/openai/gpt-2/issues/80","Why \u0120 (Ġ) is in so many pairs?","2019-02-27T01:30:49Z","Open issue","No label","It seems obvious that it's used as a ""blank byte"" but I don't understand how this ""blank byte"" ends up being represented by this ""Ġ"". Can someone explain this?
 The text was updated successfully, but these errors were encountered: 
👍3
entslscheia, arunsah, and Luo-Chang reacted with thumbs up emoji
All reactions
👍3 reactions"
"https://github.com/openai/gpt-2/issues/79","How to cite the paper","2019-02-28T23:52:04Z","Closed issue","question","How should the paper be cited? I could not find bibtex entries on the internet. Thank you
 The text was updated successfully, but these errors were encountered: 
👍3
alelom, jaywonchung, and dreamerP reacted with thumbs up emoji
All reactions
👍3 reactions"
"https://github.com/openai/gpt-2/issues/78","Question about reported perplexities","2019-02-25T20:55:45Z","Open issue","question","I've been reading your paper, interesting work.
I have a question about how you compute perplexities, especially over datasets that are already tokenized (e.g., wikitext-103). I understand that your encoding can assign probabilities to any string, but I'd expect the LM to do poorly when fed pre-tokenized input. For example, the tokenized wikitext-103 input looks like M @-@ 82 begins at a junction with M @-@ 120 and B @-@ 96 west of Fremont . How do you report perplexity in this case?
 The text was updated successfully, but these errors were encountered: 
👍7
alexeib, huihuifan, WuTheFWasThat, MaximumEntropy, ngoyal2707, Masood-Lapeh, and shijie-wu reacted with thumbs up emoji
All reactions
👍7 reactions"
"https://github.com/openai/gpt-2/issues/75","Rephrasing a sentence","2019-02-24T00:19:09Z","Closed issue","No label","Can this model rephrase a given sentence or even paraphrase? At the prompt, I tried to give a sample such as:
 Symptoms of influenza include fever and nasal congestion = A stuffy nose and elevated temperature are signs you may have the flu.
 Giraffes like Acacia leaves and hay, and they can consume 75 pounds of food a day. =
But the sample returned something completely different.
 The text was updated successfully, but these errors were encountered: 
👍1
abnf reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/gpt-2/issues/74","Port model download script to python","2019-03-04T18:47:03Z","Closed issue","help wanted","No description provided.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/73","Getting all exclamation marks on Windows","2019-02-22T22:23:46Z","Closed issue","No label","Once model is downloaded, I tried the interactive python and does not matter what I write, the output is
 SAMPLE 1 ===
 !!!!!!!!!!!!!!!!!!!!!!!!!!!!
 And 3 or 4 lines or exclamation marks.
 This happens also with samples.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/72","No module named 'fire' issue","2019-02-23T01:29:37Z","Closed issue","No label","Hi,
 I wanted to try to generate some text but I have an issue while trying to use commands for text generation, the console tells me that no module named fire has been found. I have installed fire 0.1.3 but there is still an error
tristan@DESKTOP-7AFO391:~/gpt-2$ python3 src/generate_unconditional_samples.py --top_k=1
 Traceback (most recent call last):
 File ""src/generate_unconditional_samples.py"", line 3, in 
 import fire
 ModuleNotFoundError: No module named 'fire'
any ideas what i could have done wrong ?
 I'm using windows 10 and the ubuntu console
Thank you
 The text was updated successfully, but these errors were encountered: 
👍2
aiutopiadev and bytes-commerce reacted with thumbs up emoji
All reactions
👍2 reactions"
"https://github.com/openai/gpt-2/issues/71","Clarify who has access to the model or code","2019-02-21T18:25:03Z","Closed issue","No label","In your Introducing OpenAI post you mentioned collaboration with institutions and companies. Could you clarify whether any of those institutions/companies, or your sponsors, has access to the model and/or the code of GPT-2? Thank you!
 The text was updated successfully, but these errors were encountered: 
👍2
JohnnyOpcode and fev4 reacted with thumbs up emoji😄1
hackable reacted with laugh emoji
All reactions
👍2 reactions
😄1 reaction"
"https://github.com/openai/gpt-2/issues/68","Model code is not same as paper describled","2019-02-21T02:41:04Z","Closed issue","No label","After read the model source code, I notice that the code uses a “past” context for multi head attention. Could you tell us more about how this “past” context is organized into the training pipeline?
 I guess this “past” context maybe very critical for the Gpt-2 model!
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/67","UnicodeEncodeError: 'ascii' codec can't encode character","2019-02-21T22:00:25Z","Closed issue","No label","I've reproduced this Python error 3 times already. First I tried out the unconditional samples:
$ python3 src/generate_unconditional_samples.py | tee samples

... TF log lines elided ...

======================================== SAMPLE 1 ========================================
Traceback (most recent call last):
  File ""src/generate_unconditional_samples.py"", line 55, in <module>
    fire.Fire(sample_model)
  File ""/usr/local/lib/python3.5/dist-packages/fire/core.py"", line 127, in Fire
    component_trace = _Fire(component, args, context, name)
  File ""/usr/local/lib/python3.5/dist-packages/fire/core.py"", line 366, in _Fire
    component, remaining_args)
  File ""/usr/local/lib/python3.5/dist-packages/fire/core.py"", line 542, in _CallCallable
    result = fn(*varargs, **kwargs)
  File ""src/generate_unconditional_samples.py"", line 52, in sample_model
    print(text)
UnicodeEncodeError: 'ascii' codec can't encode character '\xf8' in position 141: ordinal not in range(128)

That didn't work, but I got a few simple interactive conditional samples working until I entered this model prompt:
$ python3 src/interactive_conditional_samples.py --top_k 40

... TF log lines elided ...

================================================================================
Model prompt >>> Brexit has already cost the UK economy at least $80bn since the EU referendum,
======================================== SAMPLE 1 ========================================
Traceback (most recent call last):
  File ""src/interactive_conditional_samples.py"", line 68, in <module>
    fire.Fire(interact_model)
  File ""/usr/local/lib/python3.5/dist-packages/fire/core.py"", line 127, in Fire
    component_trace = _Fire(component, args, context, name)
  File ""/usr/local/lib/python3.5/dist-packages/fire/core.py"", line 366, in _Fire
    component, remaining_args)
  File ""/usr/local/lib/python3.5/dist-packages/fire/core.py"", line 542, in _CallCallable
    result = fn(*varargs, **kwargs)
  File ""src/interactive_conditional_samples.py"", line 64, in interact_model
    print(text)
UnicodeEncodeError: 'ascii' codec can't encode character '\xa3' in position 9: ordinal not in range(128)

Thought it didn't like the $ or , characters so I removed those but still observed the same error:
$ python3 src/interactive_conditional_samples.py --top_k 40

... TF log lines elided ...

================================================================================
Model prompt >>> Brexit has already cost the UK economy at least 80bn since the EU referendum                                      
======================================== SAMPLE 1 ========================================
Traceback (most recent call last):
  File ""src/interactive_conditional_samples.py"", line 68, in <module>
    fire.Fire(interact_model)
  File ""/usr/local/lib/python3.5/dist-packages/fire/core.py"", line 127, in Fire
    component_trace = _Fire(component, args, context, name)
  File ""/usr/local/lib/python3.5/dist-packages/fire/core.py"", line 366, in _Fire
    component, remaining_args)
  File ""/usr/local/lib/python3.5/dist-packages/fire/core.py"", line 542, in _CallCallable
    result = fn(*varargs, **kwargs)
  File ""src/interactive_conditional_samples.py"", line 64, in interact_model
    print(text)
UnicodeEncodeError: 'ascii' codec can't encode character '\u2014' in position 2400: ordinal not in range(128)

I'm running GPT-2 on master @ 99af6d7 in a container built from the nvidia/cuda:9.0-runtime-ubuntu16.04 image running on a single core Nvidia Tesla T4 node provisioned with Google Kubernetes Engine.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/64","After These Messages We'll Be Right Back","2019-02-20T17:50:32Z","Closed issue","No label","Very minor and semi-amusing, but noticed gpt-2 with the 117M model will generate ""Advertisements"" as a string in isolation on a line, so I'm guessing the training data has that somewhere that could be scrubbed whenever is convenient.
The implications of AIs accidentally learning to value commercial interruption is left as an exercise for the reader :)
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/63","Request: Release Bloom Filters for WebText (or provide other method to check a given text is in WebText)","2019-02-20T10:39:49Z","Open issue","question","Hi,
Thank you for releasing your pretrained model and saving us training time. I am currently exploring possible applications, but I ran into a problem that might also annoy many researchers trying to use your model.
AFAIK, you have not released the WebText corpus (although I know this is currently discussed in issue #24). This is fine by me, except for one aspect: it makes it impossible for me to know if my test
 data is somehow included in WebText. Which, in turns, makes it impossible for me to tell if any improvement I am getting is due to the quality of GPT or the fact that the pretrained model has already seen my test data.
If you do not plan to release WebText in the very near future, I was thinking you could release the bloom filters you describe in your technical paper (code + filled filters). This would allow us to evaluate the proportion of 8-grams in our test data that is also in WebText.
Would this be possible?
 Thank you.
 The text was updated successfully, but these errors were encountered: 
👍9
koth, Isinlor, daedalus, Sobsz, mightydeveloper, Veedrac, leotam, philippmuench, and lahwran reacted with thumbs up emoji
All reactions
👍9 reactions"
"https://github.com/openai/gpt-2/issues/62","Generate conditional output based on input keywords","2019-02-20T17:55:25Z","Closed issue","No label","I think it would be nice if it would be possible to generate an output sentence based on input keywords, where the lenght of the sentence, or the input/output words ratio could be fine tuned. In this way we could create a higher system (motive) which could use this model to generate a guided conversation.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/61","encoder.json download error","2019-02-20T04:05:57Z","Closed issue","No label","The download_model.sh script can't download encoder.json from online repository. help me! thanks in advance.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/60","Installation question","2019-02-20T04:40:13Z","Closed issue","No label","Is the Docker installation an alternative to Native installation or are both needed?
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/59","Requirements on Arch Linux","2019-02-24T07:48:54Z","Closed issue","No label","These who use archlinux should not follow the instructions but try to
pacman -S python-regex tensorflow
aurman -S python-fire
 After uninstall everything that will be install by pacman.
 I did piping install the sample did nothing and exit
 The text was updated successfully, but these errors were encountered: 
👎1
nmstoker reacted with thumbs down emoji
All reactions
👎1 reaction"
"https://github.com/openai/gpt-2/issues/58","Fixing seed and/or setting top_k to 1 don't make sampling deterministic","2019-02-21T02:40:29Z","Closed issue","help wanted","No description provided.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/57","sh doesn't do anything on Windows 10","2019-02-20T18:21:57Z","Closed issue","help wanted","Hello, what operating system do the instructions apply to? sh doesn't do anything on Windows 10. How would I install this on Win10?
Also, is the first step to clone the repo? The instructions don't seem to make sense otherwise.
Thanks.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/55","Which python should be used? 3.7.2 catches exception","2019-02-19T19:25:20Z","Closed issue","No label","Using 3.7.2 on any tutorial cmd
python3 src/interactive_conditional_samples.py --top_k 40
 /usr/local/Cellar/python/3.7.2_2/Frameworks/Python.framework/Versions/3.7/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.7
 return f(*args, **kwds)
 2019-02-19 17:51:24.116332: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
 Traceback (most recent call last):
 File ""src/interactive_conditional_samples.py"", line 68, in 
 fire.Fire(interact_model)
 File ""/usr/local/lib/python3.7/site-packages/fire/core.py"", line 127, in Fire
 component_trace = _Fire(component, args, context, name)
 File ""/usr/local/lib/python3.7/site-packages/fire/core.py"", line 366, in _Fire
 component, remaining_args)
 File ""/usr/local/lib/python3.7/site-packages/fire/core.py"", line 542, in _CallCallable
 result = fn(*varargs, **kwargs)
 File ""src/interactive_conditional_samples.py"", line 42, in interact_model
 temperature=temperature, top_k=top_k
 File ""/Users/steinmacht/gpt-2/src/sample.py"", line 76, in sample_sequence
 back_prop=False,
 File ""/usr/local/lib/python3.7/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 3291, in while_loop
 return_same_structure)
 File ""/usr/local/lib/python3.7/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 3004, in BuildLoop
 pred, body, original_loop_vars, loop_vars, shape_invariants)
 File ""/usr/local/lib/python3.7/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 2924, in _BuildLoop
 c = ops.convert_to_tensor(pred(*packed_vars))
 File ""/usr/local/lib/python3.7/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 3259, in 
 math_ops.logical_and(i < maximum_iterations, orig_cond(*lv)))
 File ""/usr/local/lib/python3.7/site-packages/tensorflow/python/ops/gen_math_ops.py"", line 4365, in logical_and
 ""LogicalAnd"", x=x, y=y, name=name)
 File ""/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 510, in _apply_op_helper
 preferred_dtype=default_dtype)
 File ""/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/ops.py"", line 1146, in internal_convert_to_tensor
 ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
 File ""/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py"", line 229, in _constant_tensor_conversion_function
 return constant(v, dtype=dtype, name=name)
 File ""/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py"", line 208, in constant
 value, dtype=dtype, shape=shape, verify_shape=verify_shape))
 File ""/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/tensor_util.py"", line 542, in make_tensor_proto
 append_fn(tensor_proto, proto_values)
 File ""tensorflow/python/framework/fast_tensor_util.pyx"", line 134, in tensorflow.python.framework.fast_tensor_util.AppendBoolArrayToTensorProto
 File ""/usr/local/lib/python3.7/site-packages/numpy/lib/type_check.py"", line 547, in asscalar
 return a.item()
 UnboundLocalError: local variable 'a' referenced before assignmen
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/54","Integrate a training feature to pass JSON markup convo data","2019-02-19T23:20:25Z","Closed issue","No label","Other alternatives to this software have allowed such a system to exist problem free, cakechat by Replika.ai for example; allows and encourages people to check out the 40GB reddit corpus you guys build GPT-2 from. I was getting highly similar results with altering their token model and marking up the JSON with emotion compared to what I am seeing with your currently released solution.
Examples of things more dangerous than your software that turned out safer than imagined...:
Example #1: Fusor.net provides detailed instructions on how to build a nuclear fusion reactor, 12 year olds in south america have even made them. Yet Bogota hasn't become Chernobyl.
Example #2: 3D printed weapons have been around for years and actually are far more safe / less accessible than normal weapons. Nobody has ever been harmed by one.
Example #3: The early days of Bitcoin, although they were accompanied by some awful things - didn't do anything as bad as what the Sinaloa cartel managed under El Chapo daily for decades unchecked.
I'll protest this until I get my own copy of this software. It's a human right for you guys to release your full, open and honest production software instead of choose profit and proprietary.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/53","[PROPOSAL] Allow remote testing of larger models","2019-02-20T01:53:05Z","Closed issue","No label","Your paper and this implementation could mark a significant evolution in UMLs.
 Especially (for what I'm concerned) in CoQa & translation.
 It would be nice to kick the tires of the full model even remotely, without having direct access to it.
 Would you consider setting up -say- a MQTT server with some channels to be able to interact with a couple of functionalities?
 Abuse of this service could be moderated by simply pruning out suspicious requests &&/|| by sending authorization to validated mail.
--R
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/52","Charmap error","2019-02-20T02:18:24Z","Closed issue","No label","File ""src/generate_unconditional_samples.py"", line 55, in 
 fire.Fire(sample_model)
 File ""C:\Users\UKumar\AppData\Local\Continuum\anaconda3\lib\site-packages\fire\core.py"", line 127, in Fire
 component_trace = _Fire(component, args, context, name)
 File ""C:\Users\UKumar\AppData\Local\Continuum\anaconda3\lib\site-packages\fire\core.py"", line 366, in _Fire
 component, remaining_args)
 File ""C:\Users\UKumar\AppData\Local\Continuum\anaconda3\lib\site-packages\fire\core.py"", line 542, in _CallCallable
 result = fn(*varargs, **kwargs)
 File ""src/generate_unconditional_samples.py"", line 52, in sample_model
 print(text)
 File ""C:\Users\UKumar\AppData\Local\Continuum\anaconda3\lib\encodings\cp1252.py"", line 19, in encode
 return codecs.charmap_encode(input,self.errors,encoding_table)[0]
 UnicodeEncodeError: 'charmap' codec can't encode character '\u2015' in position 1410: character maps to 
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/51","Is it fine not use Softmax after forwarding?","2019-02-19T23:11:27Z","Closed issue","No label","@WuTheFWasThat

 I found something different from the paper.
https://github.com/openai/gpt-2/blob/master/src/model.py#L171
 It may be mabe some error when tf.multinomial(logits, num_samples=1, output_dtype=tf.int32) function
 because there will be value less than zero
 I try with softmax but it does not shows good result for text-generator..
 This is result when use softmax function

 Not using softmax is your original meant?
I added softmax before multinomial sampling when implement to Pytorch to avoid encountering probability entry < 0
https://github.com/graykode/gpt-2-Pytorch/blob/master/sample.py#L43
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/50","Can you make site/blog that publish content from GPT ?","2019-02-19T08:52:54Z","Closed issue","No label","It will intersting and entertainment everyone (maybe ??)
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/49","Wrong Scaling in multiattention?","2019-02-19T00:25:44Z","Closed issue","No label","Hello,
Not quite sure if this is intended or not,
https://github.com/openai/gpt-2/blob/master/src/model.py#L94
You are scaling by rsqrt(dim value) where as it is usual to scale by rsqrt( dim key ).
 Is this a typo or does it work better?
Thanks
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/48","SentencePiece","2019-02-19T00:26:35Z","Closed issue","No label","Why we not used SentencePiece for BPE (https://github.com/google/sentencepiece)?
 Can you provide SentencePiece unigram model from your dataset?
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/47","Implementation about gpt-2-pytorch here","2019-02-19T00:27:25Z","Closed issue","No label","https://github.com/graykode/gpt-2-Pytorch
This is my code about Implementation of gpt-2-pytorch
 But in first Commit about BackBone Code, This is not running because model can not load *.ckpt file in Pytorch. Also, I wait license on original gpt-2 repository !!
 I hope it's good for pythorch developers.
PS) Thanks for @WuTheFWasThat for allowing
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/46","\\ issue in Windows","2019-02-17T20:47:23Z","Closed issue","No label","When running GitHub\gpt-2\src>generate_unconditional_samples.py
 Getting this issue ? any -idea is it due to \ ?
 FileNotFoundError: [Errno 2] No such file or directory: 'models\117M\encoder.json'
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/45","How to download on windows?","2019-02-18T01:14:48Z","Closed issue","No label","I'm using Windows 10, 64x. Could someone please explain to a novice how to download this?
Following the instructions, I downloaded gsutil and started a new configuration. Run:
 sh download_model.sh 117M
 But receive error:
 download_model.sh: download_model.sh: No such file or directory
Tried both:
 gsutil cp -r dir gs://gpt-2/models/117M
 gsutil cp -r dir gs://gpt-2/models/models
 But receive error:
 AccessDeniedException: 403 [my gmail] does not have storage.objects.list access to gpt-2.
Tried the solution from loretoparisi at
https://github.com/loretoparisi/gpt-2/blob/master/download_model.sh
 But I think I'm doing something wrong here. I downloaded Curl and Grep, then created a .bat file in Notepad++ with his script. Executed the file, but it only opens and closes.
Any help would be greatly appreciated.
 Thanks,
 Pete
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/44","I dont know why 'while_loop' only loop one time?","2019-02-17T12:41:22Z","Closed issue","No label","hello.
 I dont know why 'tf.while_loop' only loop one time Nevertheless return value of condition is always invarient on False.

gpt-2/src/sample.py
 Line 61 in 9c3a78d
	returnTrue
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/43","模型依然没法下载","2019-02-18T01:15:24Z","Closed issue","No label","0 0 0 0 0 0 0 0 --:--:-- 0:01:15 --:--:-- 0curl: (7) Failed to connect to drive.google.com port 443: Operation timed out
你好，我们（中国）这里还是没法下载。真心请把模型文件放到一个网盘。或者是否可以直接把模型发送到我的邮箱：1224070840@qq.com
感谢感谢啦。
 The text was updated successfully, but these errors were encountered: 
👍2
superMC5657 and lee2015new reacted with thumbs up emoji
All reactions
👍2 reactions"
"https://github.com/openai/gpt-2/issues/42","Syntax error in sh_download_model.sh","2019-02-18T01:16:22Z","Closed issue","No label","When running ""sh download_model.sh 117M"", I am told that there is a syntax error on line 14:
'ownload_model.sh: line 14: syntax error near unexpected token do download_model.sh: line 14: for filename in checkpoint encoder.json hparams.jso' model.ckpt.data-00000-of-00001 model.ckpt.index model.ckpt.meta vocab.bpe; do
Unfortunately, I don't know how to write shell scripts and can't troubleshoot this myself, but I don't see this error reported anywhere else.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/37","Is it fine to implemente gpt2 on Pytorch?","2019-02-16T18:39:51Z","Closed issue","No label","Thanks for the awesome work! 👍
 Can I try to implement gpt2 on Pytorch?
 Of course, I'll write down the sources of the papers and codes.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/36","模型下载失败","2019-02-16T18:32:42Z","Closed issue","No label","你好。
 我是python3的环境，可是gsutil requires python 2.7。除了通过gsutil，请问有没有其他的下载链接方式呢？谢谢。如果可以，是否可以发送到我邮箱：1224070840@qq.com。谢谢
 The text was updated successfully, but these errors were encountered: 
👎2
jasperzhong and deepylt reacted with thumbs down emoji
All reactions
👎2 reactions"
"https://github.com/openai/gpt-2/issues/35","Markers of ""end of text"" appear when generating samples","2019-02-16T18:21:37Z","Closed issue","No label","When generating samples interactively sometimes an ""end of text"" marker is included on the text. Is this related to the sanitization of the training data?
 The text was updated successfully, but these errors were encountered: 
👎1
mingzi150110 reacted with thumbs down emoji
All reactions
👎1 reaction"
"https://github.com/openai/gpt-2/issues/34","My CPU doesn't support Tensorflow AVX instructions","2019-02-20T01:54:50Z","Closed issue","No label","I was able to install all the requirements. However while generating samples, getting the following error. I have an Intel i3 First gen Processor and running Ubuntu 18.
2019-02-16 03:12:49.453982: F tensorflow/core/platform/cpu_feature_guard.cc:37] The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine.
 Aborted (core dumped)
I then installed Tensorflow 1.5 (pip3 install tensorflow==1.5). The sample was generated, however another warning popped up as shown below. Will this affect the quality? Do I need to compile TensorFlow on my system?
2019-02-16 03:22:19.785441: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/33","Abstractive summarization","2019-02-16T18:31:35Z","Closed issue","No label","How can I create an abstractive summary of a text document?
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/32","Please close it.","2019-02-16T16:04:31Z","Closed issue","No label","No description provided.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/31","Is there a plan to open a Chinese model?","2019-02-16T18:33:50Z","Closed issue","No label","No description provided.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/30","vocab.bpe contains an invalid unicode char and can't be read in Windows","2019-02-16T10:56:34Z","Closed issue","No label","The reader causes an exception on Windows 10 when reading the vocab.bpe file. I have tried with Unix endlines and Windows endlines.
The byte that fails has a value of 0x81 and causes an exception when running the interactive mode.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/29","Powershell script to do the same than the download_model.sh does","2019-02-16T19:04:59Z","Closed issue","No label","I have had to manually download the model in Windows10, because even when using bash, the path is not exported. I can contribute a powershell script to download the model without using bash, using powershell. I don't see any reason of this being bad.
Would that be desirable?
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/27","Sampling code flags descriptions (support for --help?)","2019-03-05T00:34:37Z","Closed issue","good first issue,help wanted","Is there a list of the flags for both conditional and unconditional models with their definitions?
 (I looked in the blog and paper and couldn't find any mention.)
In particular, for reproducibility purposes, it'd be great to know the definition of temperature and top_k and how choosing different values for these affect the results.
Thanks!
 The text was updated successfully, but these errors were encountered: 
👍5
iinc, shkatulo, vasilescur, drscarlat, and imflash217 reacted with thumbs up emoji
All reactions
👍5 reactions"
"https://github.com/openai/gpt-2/issues/26","bug in encoder.py","2019-02-15T19:55:25Z","Closed issue","No label","Add encoding=""utf-8"" line 111, encoder.py
 or won't work on windows ...
 with open(os.path.join('models', model_name, 'vocab.bpe'), 'r', encoding=""utf-8"") as f:
 The text was updated successfully, but these errors were encountered: 
👍1
imgntn reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/gpt-2/issues/25","Try in Colaboratory","2019-02-15T19:56:27Z","Closed issue","No label","For a quick run:
https://colab.research.google.com/drive/1UfBDy-UkONgfdER21VNUCnsf63LGldFA
 The text was updated successfully, but these errors were encountered: 
❤️2
nealmcb and ethanjperez reacted with heart emoji
All reactions
❤️2 reactions"
"https://github.com/openai/gpt-2/issues/24","Any plans to release WebText corpus?","2019-02-15T13:16:23Z","Open issue","question","I've seen #16 and appreciate the valid concerns raised about releasing the model, but the WebText corpus could be a tremendous help to general research if you were able to release it.
Are there plans to do so?
I did wonder if this might simply enable people to recreate the unreleased GPT-2 but presumably this is no trivial matter, needing expertise and time/resources, thus deterring the causal mischief maker!
Anyway, whatever you end up doing, I wanted to thank you for what you have released already which is really interesting 🙂
 The text was updated successfully, but these errors were encountered: 
👍36
madisonmay, momoz44, bionicles, pclucas14, isaacmg, victoriastuart, rspeer, loretoparisi, Jbollenbacher, ivandir, and 26 more reacted with thumbs up emoji❤️9
bhack, berkeleymalagon, rkfg, yunjiangster, LukeWeidenwalker, mightydeveloper, eric-haibin-lin, csaroff, and skabbit reacted with heart emoji
All reactions
👍36 reactions
❤️9 reactions"
"https://github.com/openai/gpt-2/issues/23","AccessDeniedException: 403 xxx@xxx.com does not have storage.objects.list access to gpt-2.","2019-02-15T11:50:55Z","Closed issue","No label","Hello!
 I am unable to download the model. When doing: sh download_model.sh I get this error: AccessDeniedException: 403 xxx@xxx.com does not have storage.objects.list access to gpt-2.. Any advice?
Greetings!
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/22","Can't install sh_download_model.sh","2019-02-16T22:28:14Z","Closed issue","No label","Noob here (linguist, with rudimentary knowledge of computers) I've installed the gcloud sdk but I can't get the command: sh download_model.sh 117M to run. I get: 'sh' is not recognized as an internal or external command.
 Any help would be greatly appreciated.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/20","Translation task","2019-02-15T19:57:26Z","Closed issue","No label","What was the format for translation task?
 Do you provide sequence of pairs delimited by new lines, e.g. ""sentence1 = translation_of_sentence1 \n sentence2 = translation_of_sentence2 \n ... \n testing_sentence = ""?
 Does the training dataset consist of similar format translations?
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/19","how to train it","2019-02-20T02:10:33Z","Closed issue","No label","how to train it
 The text was updated successfully, but these errors were encountered: 
👍45
jwijffels, isaacmg, clintonm9, Franck-Dernoncourt, soneo1127, QiaoranC, jackylee1, amallecourt, nicolas-ivanov, luiscastillocr, and 35 more reacted with thumbs up emoji👎1
btlvr reacted with thumbs down emoji
All reactions
👍45 reactions
👎1 reaction"
"https://github.com/openai/gpt-2/issues/18","It is painful to install gsutil, why not wget/curl","2019-02-16T19:07:23Z","Closed issue","No label","As shown in title
 The text was updated successfully, but these errors were encountered: 
👍2
Findus23 and ZeweiChu reacted with thumbs up emoji
All reactions
👍2 reactions"
"https://github.com/openai/gpt-2/issues/17","Is there a plan to support PyTorch?","2019-02-15T19:58:10Z","Closed issue","No label","Thanks for the awesome work, hope to support PyTorch.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/16","Release The Full Model!","2019-02-15T02:02:26Z","Open issue","question","I understand your concerns but I still think it's better to release the full model now and let people poke at it's abilities and discover potential issues quicker.
 The text was updated successfully, but these errors were encountered: 
👍289
lamhoangtung, yzho0907, imgntn, junior-zsy, samantha-lee890410, jayelm, emailvenkatm, Franck-Dernoncourt, bharathgs, cdharris, and 279 more reacted with thumbs up emoji👎13
gabefair, cchalfan1, sunny-panchal, moebg, ML-Chen, davidhughhenrymack, tolgacangoz, vizhnu, nahez, PrivateGER, and 3 more reacted with thumbs down emoji😄10
pushshift, dackdel, LastRemote, its5Q, Ina299, Serkan-devel, luismmolina, sonvx, jumelet, and CT83 reacted with laugh emoji🎉15
its5Q, Aishou, Tom-Neverwinter, Phude, Ina299, Serkan-devel, DanielTakeshi, HareshKarnan, f-dx, joytianya, and 5 more reacted with hooray emoji❤️33
michaelahlers, suzil, bhack, joemillervi, martyn, pjox, rafagarciac, dackdel, ZeroCool940711, nkk0, and 23 more reacted with heart emoji🚀21
joemillervi, pjox, dackdel, ZeroCool940711, kaloyan-chernev, LastRemote, affablebloke, Serkan-devel, CarlosLannister, gionapaolini, and 11 more reacted with rocket emoji👀20
DanielTakeshi, bhack, jwijffels, Apprico, dackdel, donglixp, Serkan-devel, LastRemote, rorph, mikeyang01, and 10 more reacted with eyes emoji
All reactions
👍289 reactions
👎13 reactions
😄10 reactions
🎉15 reactions
❤️33 reactions
🚀21 reactions
👀20 reactions"
"https://github.com/openai/gpt-2/issues/14","Errors during model downloading","2019-02-15T00:09:48Z","Closed issue","No label","When I try to download the model on my Ubuntu Linux 14.04 LTS box I get the following errors from gsutil:
$  bash download_model.sh 117M
No command was given.

Choose one of -b, -d, -e, or -r to do something.
Try `/usr/bin/gsutil --help' for more information.
No command was given.

Choose one of -b, -d, -e, or -r to do something.
Try `/usr/bin/gsutil --help' for more information.
No command was given.

Choose one of -b, -d, -e, or -r to do something.
Try `/usr/bin/gsutil --help' for more information.
No command was given.

Choose one of -b, -d, -e, or -r to do something.
Try `/usr/bin/gsutil --help' for more information.
No command was given.

Choose one of -b, -d, -e, or -r to do something.
Try `/usr/bin/gsutil --help' for more information.
No command was given.

Choose one of -b, -d, -e, or -r to do something.
Try `/usr/bin/gsutil --help' for more information.
No command was given.

Choose one of -b, -d, -e, or -r to do something.
Try `/usr/bin/gsutil --help' for more information.

Also, do you need a CUDA card just to run the model?
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/13","-","2019-02-14T22:39:45Z","Closed issue","No label","Download.sh fails: + gsutil cp gs://gpt-2/models//model.ckpt.meta models/
 AccessDeniedException: 403 [user] does not have storage.objects.list access to gpt-2.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/11","generate_unconditional_samples.py Only return {text}","2019-02-14T21:25:55Z","Closed issue","No label","I used tensorflow_gpu (didn't have the patience to use the cpu)
No errors, but when I run generate_unconditional_samples.py with and without flags only return {text}
======================================== SAMPLE 1 ========================================
{text}
======================================== SAMPLE 2 ========================================
{text}
======================================== SAMPLE 3 ========================================
{text}
======================================== SAMPLE 4 ========================================
{text}
======================================== SAMPLE 5 ========================================
{text}
======================================== SAMPLE 6 ========================================
{text}

Can you please advise?
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/10","License","2019-02-20T01:11:11Z","Closed issue","No label","Hi, is there a license associated with this/any plans towards MIT licensing this model?
 The text was updated successfully, but these errors were encountered: 
👍13
transitive-bullshit, stefan-it, roschler, madisonmay, dackdel, valentinvieriu, reuben, stefanogoria, MattyMc, PetrHeinz, and 3 more reacted with thumbs up emoji
All reactions
👍13 reactions"
"https://github.com/openai/gpt-2/issues/9","Help doing transfer learning to generate spanish-language text?","2019-02-14T21:25:25Z","Closed issue","No label","Hi! Amazing results 😮
 I know this is an open-ended and lazy question, but I'd appreciate if you could give me some pointers into how to re-train the model with additional text in another language (e.g spanish). I already have a small (6 MB) dataset in spanish, and I'm not very well versed in ML but I'm curious about playing with your model.
 Thanks! I'll be sure to report results back if I somehow figure it out :)
 The text was updated successfully, but these errors were encountered: 
👍4
GitHub30, ilopezfr, Masood-Lapeh, and tpisto reacted with thumbs up emoji
All reactions
👍4 reactions"
"https://github.com/openai/gpt-2/issues/7","ModuleNotFoundError: No module named 'src'","2019-02-14T21:20:25Z","Closed issue","No label","maxwoolf$ sudo python3 src/generate_unconditional_samples.py
Traceback (most recent call last):
  File ""src/generate_unconditional_samples.py"", line 9, in <module>
    from src import model, sample, encoder
ModuleNotFoundError: No module named 'src'

Python can't import from a subfolder unless it's on the Python path.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/6","Python 3.6 requirement must be explicitly stated","2019-02-14T19:34:19Z","Closed issue","No label","You need to use exactly 3.6.x since 3.6 has f-strings (which this repo uses) and 3.7 doesn't support TensorFlow.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/5","Window size error when trying to sample","2019-02-14T19:35:17Z","Closed issue","No label","I'm trying to sample from 117M using the following commands:
 python3 src/generate_unconditional_samples.py
python3 src/generate_unconditional_samples.py | tee samples
python3 src/interactive_conditional_samples.py
And I get the following error:
File ""src/interactive_conditional_samples.py"", line 34
 raise ValueError(f""can't get samples longer than window size: {hparams.n_ctx}"")
 ^
 SyntaxError: invalid syntax
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/gpt-2/issues/3","Error when models folder does not exist","2019-02-14T19:41:39Z","Closed issue","No label","When models folder does not exist, I am not able to set up using the instructions provided:
gpt-2 $sh download_model.sh 117M
mkdir: models: No such file or directory

 The text was updated successfully, but these errors were encountered: 
👍1
GitHub30 reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/gpt-2/issues/1","Issue with gsutil download_model.sh","2019-02-14T22:34:55Z","Closed issue","No label","Hi,
I'm not familiar with gsutil. Installed it freshly using the 6 steps of :
https://cloud.google.com/storage/docs/gsutil_install
Upon running the script :
When I'm not logged in on cloud.
ServiceException: 401 Anonymous caller does not have storage.objects.get access to gpt-2/models//checkpoint.
ServiceException: 401 Anonymous caller does not have storage.objects.get access to gpt-2/models//encoder.json.
ServiceException: 401 Anonymous caller does not have storage.objects.get access to gpt-2/models//hparams.json.
ServiceException: 401 Anonymous caller does not have storage.objects.get access to gpt-2/models//model.ckpt.data-00000-of-00001.
ServiceException: 401 Anonymous caller does not have storage.objects.get access to gpt-2/models//model.ckpt.index.
ServiceException: 401 Anonymous caller does not have storage.objects.get access to gpt-2/models//model.ckpt.meta.
ServiceException: 401 Anonymous caller does not have storage.objects.get access to gpt-2/models//vocab.bpe.


When I'm logged in on cloud :

AccessDeniedException: 403 myemail@gmail.com does not have storage.objects.list access to gpt-2.
AccessDeniedException: 403 myemail@gmail.com does not have storage.objects.list access to gpt-2.
AccessDeniedException: 403 myemail@gmail.com does not have storage.objects.list access to gpt-2.
AccessDeniedException: 403 myemail@gmail.com does not have storage.objects.list access to gpt-2.
AccessDeniedException: 403 myemail@gmail.com does not have storage.objects.list access to gpt-2.
AccessDeniedException: 403 myemail@gmail.com does not have storage.objects.list access to gpt-2.
AccessDeniedException: 403 myemail@gmail.com does not have storage.objects.list access to gpt-2.


Thanks
 The text was updated successfully, but these errors were encountered: 
All reactions"
