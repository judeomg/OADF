"https://github.com/openai/openai-dotnet/issues/260","GPT Vision request not working (Error 500 from API)","2024-10-18T11:07:42Z","Open issue","bug","Service
OpenAI
Describe the bug
Receiving a 500 error when using the OpenAI API to create image descriptions (Vision) in C#. The error occurs on the line making the API call using model ""gpt-4o-mini"". The code fails when sending a request with an image part, leading to an internal server error.
Error message:
The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error
Steps to reproduce
Where the text is bold, the error will occur.
Load an image from the local drive and convert it to a byte array.
Create a list of chat messages, including system instructions and a user message containing both text and an image.
Utilize the GPT-4o-mini Model and submit a request to the OpenAI API to request a response based on the provided messages.
Output the assistant's response to the console.
Code snippets
async Task TestGPT(){
    //Convert image to byte array
    var imageJpg = ""C:\\Users\\<myPath>"";
    byte[] image = File.ReadAllBytes(imageJpg);

    List<ChatMessage> messages = new List<ChatMessage>() 
    {
        new SystemChatMessage(""Describe, briefly, the image.""),
        new UserChatMessage(ChatMessageContentPart.CreateTextPart(""Hello, what can you see on the image""), ChatMessageContentPart.CreateImagePart(new BinaryData(image),""image/jpeg"", ChatImageDetailLevel.Low))
    };

    ChatCompletion res = await _client.GetChatClient(""gpt-4o-mini"").CompleteChatAsync(messages , null);

    Console.WriteLine($""Asystent: {res.Content[0].Text}"");

    Console.ReadKey();}
OS
windows 11
.NET version
.NET 8
Library version
2.0.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-dotnet/issues/259","InternalsVisibleTo for Azure.AI.OpenAI","2024-10-14T21:54:15Z","Open issue","bug","Service
OpenAI
Describe the bug
openai-dotnet/src/OpenAI.csproj
 Lines 54 to 57 in ef201cd
	 <InternalsVisibleToInclude=""Azure.AI.OpenAI""Condition=""'$(Configuration)' != 'Unsigned'""> 
	 <PublicKey>0024000004800000940000000602000000240000525341310004000001000100097ad52abbeaa2e1a1982747cc0106534f65cfea6707eaed696a3a63daea80de2512746801a7e47f88e7781e71af960d89ba2e25561f70b0e2dbc93319e0af1961a719ccf5a4d28709b2b57a5d29b7c09dc8d269a490ebe2651c4b6e6738c27c5fb2c02469fe9757f0a3479ac310d6588a50a28d7dd431b907fd325e18b9e8ed</PublicKey> 
	 </InternalsVisibleTo> 
	 <InternalsVisibleToInclude=""Azure.AI.OpenAI""Condition=""'$(Configuration)' == 'Unsigned'"" /> 
The OpenAI library is tracking all received properties and storing them into a SerializedAdditionalRawData dictionary. It doesn't publicly expose this data, except via InternalsVisibleTo to Azure.AI.OpenAI.
Why is Azure.AI.OpenAI special here, such that this data is only available to that specific library?
Steps to reproduce
Try to access additional properties sent down as part of an OpenAI-compatible response. You can't.
Code snippets
No response
OS
Any
.NET version
Any
Library version
2.1.0-beta
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-dotnet/issues/258","Assistant Vector Store results are not retrieved","2024-10-11T23:38:16Z","Open issue","bug","Service
Azure OpenAI
Describe the bug
as described in the OpenAI documentation here, you can retrieve the search result by adding include[]=step_details.tool_calls[*].file_search.results[*].content query string to a GetRunStep call.
Steps to reproduce
add the following code for initialization of the httpClient:
var httpClient = new HttpClient();
httpClient.BaseAddress = OpenAIHelpers.AzureOAIEndpoint;var token = await new DefaultAzureCredential().GetTokenAsync(new Azure.Core.TokenRequestContext([""https://cognitiveservices.azure.com/.default""]));
httpClient.DefaultRequestHeaders.Authorization = new AuthenticationHeaderValue(""Bearer"", token.Token);
this code to get the retreived chunks from the vector db:
private static async Task GetSearchInfo(RunStepUpdate runStepUpdate){
    if (runStepUpdate.UpdateKind != StreamingUpdateReason.RunStepCompleted
        || !runStepUpdate.Value.Details.ToolCalls.Any(t => t.ToolKind == RunStepToolCallKind.FileSearch))
        return; // skip if not completed or doesn't use file search

    var SearchInfoQueryString = ""include[]=step_details.tool_calls[*].file_search.results[*].content"";

    string endpoint = $""/openai/threads/{runStepUpdate.Value.ThreadId}/runs/{runStepUpdate.Value.RunId}/steps/{runStepUpdate.Value.Id}"";
    var response = await httpClient.GetAsync($""{endpoint}?{SearchInfoQueryString}&api-version=2024-09-01-preview"");
    if (response.IsSuccessStatusCode)
    {
        var json = await response.Content.ReadAsStringAsync();
        var rootNode = JsonNode.Parse(json);

        var searchResults = rootNode?[""step_details""]?[""tool_calls""]?[0]?[""file_search""]?[""results""]?
            .AsArray().Select(r => JsonSerializer.Deserialize<string>(r?[""content""]?[0]?[""text""]));

        Debug.WriteLine(""Search Results:"");
        Debug.WriteLine(string.Join(""\n\n"", searchResults ?? []));
    }}
and this code to run the streaming response:
private static async Task OutputCompletionStreaming(AssistantThread thread, Assistant assistant, RunCreationOptions runOptions){
    var asyncUpdates = OpenAIHelpers.OpenAIAssistantClient.CreateRunStreamingAsync(thread.Id, assistant.Id, runOptions);

    ThreadRun? currentRun;
    do
    {
        currentRun = null;

        List<ToolOutput> outputsToSubmit = [];

        await foreach (var update in asyncUpdates ?? AsyncEnumerable.Empty<StreamingUpdate>())
        {
            switch (update)
            {
                case RequiredActionUpdate requiredActionUpdate:
                    // handle tool calls
                    break;

                case MessageStatusUpdate messageStatus:
                    //Debug.WriteLine($"" {messageStatus.UpdateKind} {messageStatus.Value.Id}"");
                    break;

                case MessageContentUpdate contentUpdate:
                    Console.Write(contentUpdate.Text);

                    if (contentUpdate.TextAnnotation != null)
                    {
                        // Reference to RAG results
                        Debug.WriteLine($""  --> From file: {contentUpdate.TextAnnotation.InputFileId}, replacement: {contentUpdate.TextAnnotation.TextToReplace}"");
                    }
                    break;

                case RunUpdate runUpdate:
                    currentRun = runUpdate;
                    Debug.WriteLine($""{runUpdate.UpdateKind} {runUpdate.Value.Id}"");
                    if (runUpdate.UpdateKind == StreamingUpdateReason.RunFailed)
                    {
                        Console.WriteLine($""{ConsoleColors.Red}Error: {runUpdate.Value.LastError.Message}"");
                    }
                    break;

                case RunStepUpdate runStepUpdate:
                    Debug.WriteLine(
                        $"" {runStepUpdate.UpdateKind} {runStepUpdate.Value.Id} "" +
                        $""Tools: [ {runStepUpdate.Value.Details.ToolCalls.Select(t => t.ToolKind.ToString()).StringJoin(""; "")} ] "");

                    await GetSearchInfo(runStepUpdate);
                    break;
            }
        }
    }
    while (currentRun?.Status.IsTerminal == false);
    Console.WriteLine();}
You'll see that whenever the file search tool is called, the results are always empty.
 The same exact code (except few changes in how httpClient is initialized) works with vanilla OpenAI SDK.
Code snippets
No response
OS
Windows
.NET version
8
Library version
2.1.0-beta.1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-dotnet/issues/257","AzureExtensionsOptions and AzureExtensionsContext are gone after upgrading to stable version of the library","2024-10-15T15:32:13Z","Closed issue","bug","Service
Azure OpenAI
Describe the bug
With the prerelease version I used to use AzureExtensionsOptions inside ChatCompletionsOptions to set my Azure AI Search configuration.
 Then, in the response after making the call I used to use message.AzureExtensionsContext.Citations.
After upgrading to latest stable version of Azure.AI.OpenAI library all those options are gone.
 I cannot find any details about it but it seems to me like a breaking change?
Any help with it, please? Maybe there's a different way to do it now.
 Cheers.
Steps to reproduce
Upgrade Azure.AI.OpenAI from 2.0.0-beta.1 to 2.0.0.
Try to use this piece of code:

Code snippets
var chatCompletionsOptions = new ChatCompletionsOptions
        {            
            AzureExtensionsOptions = new AzureChatExtensionsOptions()
            {
                Extensions =
                {
                    new AzureSearchChatExtensionConfiguration()
                    {
                        SearchEndpoint = new Uri(_searchSettings.SearchUri),
                        Authentication = new OnYourDataApiKeyAuthenticationOptions(searchKey),
                        IndexName = _indexName,
                        QueryType = AzureSearchQueryType.Semantic,
                        SemanticConfiguration = $""{_indexName}-semantic-config""
                    },
                },
            }
        };

...

var citations = message.AzureExtensionsContext.Citations;
OS
Windows 11
.NET version
8
Library version
2.0.0
 The text was updated successfully, but these errors were encountered: 
👍1
dciborow reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/openai-dotnet/issues/256","Stream mode function parameters parse incorrect.","2024-10-11T14:34:43Z","Closed as not planned issue","bug","Service
OpenAI
Describe the bug
openai-dotnet/src/Generated/Models/InternalChatCompletionMessageToolCallChunkFunction.Serialization.cs
 Line 99 in ef201cd
	returnnew InternalChatCompletionMessageToolCallChunkFunction(name, arguments, serializedAdditionalRawData);
This code to parse sse data will add null parameters (string name = null;BinaryData arguments = null;) to InternalChatCompletionMessageToolCallChunkFunction
 When to call StreamingChatToolCallUpdate.FunctionArgumentsUpdate.ToString() will throw null exception.
It may be cause by this SSE data. The delta is empty
data: {""id"":""xx"",...,""choices"":[{""index"":0,""delta"":{},""logprobs"":null,""finish_reason"":""tool_calls""}]}

Steps to reproduce
See error
Log
Trace: [2024/10/11 21:25:51] Request: POST https://xxx.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-08-01-preview
Accept: application/json
User-Agent: azsdk-net-AI.OpenAI/2.1.0-beta.1, (.NET 8.0.8; Microsoft Windows 10.0.22631)
x-ms-client-request-id: 0ae8cafc-d86d-48bb-a39c-b0ce1c7f9bc0
api-key: ***
Content-Type: application/json

{""messages"":[{""role"":""system"",""content"":""You are a smart weather assistant""},{""role"":""user"",""content"":""Guangzhou weather""}],""model"":""gpt-4o"",""frequency_penalty"":0,""presence_penalty"":0,""stream"":true,""temperature"":0.7,""top_p"":1,""tools"":[{""type"":""function"",""function"":{""description"":""Get city weather based on area code"",""name"":""WeatherPlugin-GetWeather"",""parameters"":{""type"":""object"",""required"":[""addressCode""],""properties"":{""addressCode"":{""type"":""string"",""description"":""City area address code. e.g. 110000, 440100...""}}}}}],""tool_choice"":""auto"",""max_tokens"":2048}

Trace: [2024/10/11 21:25:53] /openai/deployments/gpt-4o/chat/completions OK
Transfer-Encoding: chunked
...
Content-Type: text/event-stream; charset=utf-8

Trace: [2024/10/11 21:25:53] 
data: {""id"":""xxx"",""object"":""chat.completion.chunk"",""created"":1728653152,""model"":""gpt-4o-2024-05-13"",""system_fingerprint"":""fp_67802d9a6d"",""choices"":[{""index"":0,""delta"":{""role"":""assistant"",""content"":null,""tool_calls"":[{""index"":0,""id"":""call_yXkmOQ1GvYheU9tVolJngmLa"",""type"":""function"",""function"":{""name"":""WeatherPlugin-GetWeather"",""arguments"":""""}}]},""logprobs"":null,""finish_reason"":null}]}

data: {""id"":""xxx"",""object"":""chat.completion.chunk"",""created"":1728653152,""model"":""gpt-4o-2024-05-13"",""system_fingerprint"":""fp_67802d9a6d"",""choices"":[{""index"":0,""delta"":{""tool_calls"":[{""index"":0,""function"":{""arguments"":""{\n""}}]},""logprobs"":null,""finish_reason"":null}]}

data: {""id"":""xxx"",""object"":""chat.completion.chunk"",""created"":1728653152,""model"":""gpt-4o-2024-05-13"",""system_fingerprint"":""fp_67802d9a6d"",""choices"":[{""index"":0,""delta"":{""tool_calls"":[{""index"":0,""function"":{""arguments"":"" ""}}]},""logprobs"":null,""finish_reason"":null}]}

data: {""id"":""xxx"",""object"":""chat.completion.chunk"",""created"":1728653152,""model"":""gpt-4o-2024-05-13"",""system_fingerprint"":""fp_67802d9a6d"",""choices"":[{""index"":0,""delta"":{""tool_calls"":[{""index"":0,""function"":{""arguments"":"" \""""}}]},""logprobs"":null,""finish_reason"":null}]}

data: {""id"":""xxx"",""object"":""chat.completion.chunk"",""created"":1728653152,""model"":""gpt-4o-2024-05-13"",""system_fingerprint"":""fp_67802d9a6d"",""choices"":[{""index"":0,""delta"":{""tool_calls"":[{""index"":0,""function"":{""arguments"":""address""}}]},""logprobs"":null,""finish_reason"":null}]}

data: {""id"":""xxx"",""object"":""chat.completion.chunk"",""created"":1728653152,""model"":""gpt-4o-2024-05-13"",""system_fingerprint"":""fp_67802d9a6d"",""choices"":[{""index"":0,""delta"":{""tool_calls"":[{""index"":0,""function"":{""arguments"":""Code""}}]},""logprobs"":null,""finish_reason"":null}]}

data: {""id"":""xxx"",""object"":""chat.completion.chunk"",""created"":1728653152,""model"":""gpt-4o-2024-05-13"",""system_fingerprint"":""fp_67802d9a6d"",""choices"":[{""index"":0,""delta"":{""tool_calls"":[{""index"":0,""function"":{""arguments"":""\"":""}}]},""logprobs"":null,""finish_reason"":null}]}

data: {""id"":""xxx"",""object"":""chat.completion.chunk"",""created"":1728653152,""model"":""gpt-4o-2024-05-13"",""system_fingerprint"":""fp_67802d9a6d"",""choices"":[{""index"":0,""delta"":{""tool_calls"":[{""index"":0,""function"":{""arguments"":"" \""""}}]},""logprobs"":null,""finish_reason"":null}]}

data: {""id"":""xxx"",""object"":""chat.completion.chunk"",""created"":1728653152,""model"":""gpt-4o-2024-05-13"",""system_fingerprint"":""fp_67802d9a6d"",""choices"":[{""index"":0,""delta"":{""tool_calls"":[{""index"":0,""function"":{""arguments"":""440""}}]},""logprobs"":null,""finish_reason"":null}]}

data: {""id"":""xxx"",""object"":""chat.completion.chunk"",""created"":1728653152,""model"":""gpt-4o-2024-05-13"",""system_fingerprint"":""fp_67802d9a6d"",""choices"":[{""index"":0,""delta"":{""tool_calls"":[{""index"":0,""function"":{""arguments"":""100""}}]},""logprobs"":null,""finish_reason"":null}]}

data: {""id"":""xxx"",""object"":""chat.completion.chunk"",""created"":1728653152,""model"":""gpt-4o-2024-05-13"",""system_fingerprint"":""fp_67802d9a6d"",""choices"":[{""index"":0,""delta"":{""tool_calls"":[{""index"":0,""function"":{""arguments"":""\""\n""}}]},""logprobs"":null,""finish_reason"":null}]}

data: {""id"":""xxx"",""object"":""chat.completion.chunk"",""created"":1728653152,""model"":""gpt-4o-2024-05-13"",""system_fingerprint"":""fp_67802d9a6d"",""choices"":[{""index"":0,""delta"":{""tool_calls"":[{""index"":0,""function"":{""arguments"":""}""}}]},""logprobs"":null,""finish_reason"":null}]}

data: {""id"":""xxx"",""object"":""chat.completion.chunk"",""created"":1728653152,""model"":""gpt-4o-2024-05-13"",""system_fingerprint"":""fp_67802d9a6d"",""choices"":[{""index"":0,""delta"":{},""logprobs"":null,""finish_reason"":""tool_calls""}]}

data: [DONE]



Error: [2024/10/11 21:25:53]
System.ArgumentNullException: Value cannot be null. (Parameter 'bytes')
   at System.ArgumentNullException.Throw(String paramName)
   at System.Text.Encoding.GetString(Byte* bytes, Int32 byteCount)
   at System.BinaryData.ToString()
   at Microsoft.SemanticKernel.Connectors.OpenAI.OpenAIFunctionToolCall.TrackStreamingToolingUpdate(IReadOnlyList`1 updates, Dictionary`2& toolCallIdsByIndex, Dictionary`2& functionNamesByIndex, Dictionary`2& functionArgumentBuildersByIndex)
   at Microsoft.SemanticKernel.Connectors.OpenAI.ClientCore.GetStreamingChatMessageContentsAsync(String targetModel, ChatHistory chatHistory, PromptExecutionSettings executionSettings, Kernel kernel, CancellationToken cancellationToken)+MoveNext()
   at Microsoft.SemanticKernel.Connectors.OpenAI.ClientCore.GetStreamingChatMessageContentsAsync(String targetModel, ChatHistory chatHistory, PromptExecutionSettings executionSettings, Kernel kernel, CancellationToken cancellationToken)+MoveNext()
   at Microsoft.SemanticKernel.Connectors.OpenAI.ClientCore.GetStreamingChatMessageContentsAsync(String targetModel, ChatHistory chatHistory, PromptExecutionSettings executionSettings, Kernel kernel, CancellationToken cancellationToken)+System.Threading.Tasks.Sources.IValueTaskSource<System.Boolean>.GetResult()
...

Code snippets
No response
OS
winos
.NET version
.net 8.0
Library version
2.1.0-beta.1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-dotnet/issues/250","GenerateImageEdit not working","2024-10-09T21:04:11Z","Open issue","bug","Service
OpenAI
Describe the bug
GenerateImageEdit is not working and force you to use ""dall-e-2""
Steps to reproduce
Send any image to function
Code snippets
//Retunr the same image.ClientResult<GeneratedImage> response = imageClient.GenerateImageEdit(imagePath, prompt);
OS
windows
.NET version
core 8
Library version
2.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-dotnet/issues/248","Tool choice serializes erroneously causing error in Realtime API","2024-10-09T05:59:52Z","Open issue","bug","Service
OpenAI
Describe the bug
when setting tool choice in the session options of the realtime api in 2.1.0-beta.1 like this:
conversationSessionOptions.ToolChoice = ConversationToolChoice.CreateNoneToolChoice();
it doesn't work and the API seems to not like the serialization of the value to pascal case.
Error: Invalid value: 'None'. Supported values are: 'auto', 'none', and 'required'., Code: invalid_value, Details: session.tool_choice
Steps to reproduce
set tool choice in the Realtime API session, like
conversationSessionOptions.ToolChoice = ConversationToolChoice.CreateNoneToolChoice();
listen to events
start a new session
observe error event being reported back
Code snippets
No response
OS
MacOS
.NET version
8
Library version
2.1.0-beta.1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-dotnet/issues/246","CreateImagePart overloads + implicit casts from string lead to confusion","2024-10-07T19:22:23Z","Open issue","bug","Service
OpenAI
Describe the bug
CreateImagePart has two overloads:
public static ChatMessageContentPart CreateImagePart(BinaryData imageBytes, string imageBytesMediaType, ChatImageDetailLevel? imageDetailLevel = null);
public static ChatMessageContentPart CreateImagePart(Uri imageUri, ChatImageDetailLevel? imageDetailLevel = null);

In general for .NET APIs we try to avoid situations where the same type of argument in the same position leads to different behaviors, however here that happens because of an implicit cast from string to ChatImageDetailLevel. I had a call to CreateImagePart(imageBytes, ""image/jpeg"") and I then changed it to CreateImagePart(imageUrl, ""image/jpeg""). It compiled fine, but then it threw an exception at run-time, because the ""image/jpeg"" was no longer considered to be a media type and instead considered to be an image detail level.
Steps to reproduce
See description above.
These APIs are now stable, so I'm not sure much can be done about it.
Code snippets
No response
OS
Windows 11
.NET version
.NET 8
Library version
2.0.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-dotnet/issues/245","Difficulty Using Function Calling in OpenAI Batch API","2024-10-07T14:02:04Z","Open issue","No label","Confirm this is a feature request for the .NET library and not the underlying OpenAI API
 This is a feature request for the .NET library
Describe the feature or improvement you are requesting
I am trying to implement function calling using the OpenAI Batch API as mentioned in the documentation here. The documentation states that function calling is supported in both the Chat Completions API, Assistants API, and the Batch API. However, I am having trouble getting it to work with the Batch API.
Despite following the guidelines provided, I cannot seem to find a way to utilize function calling effectively within the Batch API. If anyone has experience or insights regarding this functionality, I would greatly appreciate your assistance.
Additional context
I have reviewed the examples provided for function calling in the Chat Completions API and Assistants API, but I haven't found similar examples for the Batch API. My goal is to implement a conversational assistant that can process multiple requests in a batch while utilizing function calling to retrieve and respond with real-time data.
I am particularly interested in understanding the specific requirements or limitations when using function calling with the Batch API. If there are any differences in implementation compared to the other APIs, that information would be very helpful.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-dotnet/issues/244","(Beta) Websocket closing unexpectedly on ReceiveUpdatesAsync","2024-10-04T17:19:40Z","Open issue","bug","Service
OpenAI
Describe the bug
I was setting up the realtime API for voice chatting, but I found after my first question and reply from the model, the web socket would close and terminate the conversation. (This was while using CreateServerVoiceActivityTurnDetectionOptions)
When calling ReceiveUpdatesAsync to receive server messages, I didn't expect this to close the _clientWebSocket, but it did.
It took me a while to figure out that the AsyncWebsocketMessageResultEnumerator is automatically disposing of the _clientWebSocket when it finishes iterating, causing ReceiveUpdatesAsync to terminate the WebSocket. Commenting out the disposing call within AsyncWebsocketMessageResultEnumerator resulted in the expected behavior for me:
public ValueTask DisposeAsync(){
    //_clientWebSocket?.Dispose();
    return new ValueTask(Task.CompletedTask);}
This allowed me to have my expected 2-way conversation. If there's another intended method to receive server events without terminating the socket, or to keep the socket alive for more than one request-response, please let me know.
Steps to reproduce
Initialize a RealtimeConversationSession with server voice activity turn detection:
var client = new RealtimeConversationClient(model: ""gpt-4o-realtime-preview-2024-10-01"", new(apiKey));


CancellationTokenSource cts = new();

var session = await client.StartConversationSessionAsync(cts.Token);

var options = new ConversationSessionOptions(){
    Instructions = ""<system prompt>"",
    TurnDetectionOptions = ConversationTurnDetectionOptions.CreateServerVoiceActivityTurnDetectionOptions(0.5f, TimeSpan.FromMilliseconds(300), TimeSpan.FromMilliseconds(200)),
    Voice = ConversationVoice.Alloy,
    OutputAudioFormat = ConversationAudioFormat.Pcm16,
    InputTranscriptionOptions = new ConversationInputTranscriptionOptions()
    {
        Model = ""whisper-1""
    }};

await session.ConfigureSessionAsync(options);
Begin sending audio through SendAudioAsync (in my case with NAudio Wave):
waveIn.DataAvailable += (s, a) =>{    using var memoryStream = new MemoryStream();    memoryStream.Write(a.Buffer, 0, a.BytesRecorded);    memoryStream.Position = 0;    session.SendAudioAsync(memoryStream, token).Wait();};
Begin handling server responses with ReceiveUpdatesAsync in a loop:
while (true){
    await foreach (var update in session.ReceiveUpdatesAsync(token))
    {
        //Handle received updates
    }}
Make an audible request to the AI, and wait for its response to complete. On the second loop, ReceiveUpdatesAsync will throw a System.ObjectDisposedException:
Cannot access a disposed object.
Object name: 'System.Net.WebSockets.ClientWebSocket'.
Code snippets
No response
OS
winOS
.NET version
.NET 8 Core
Library version
2.1.0-beta.1
 The text was updated successfully, but these errors were encountered: 
👍2
lentrodev and isaac-j-miller reacted with thumbs up emoji
All reactions
👍2 reactions"
"https://github.com/openai/openai-dotnet/issues/243","No compatible with NativeAOT.","2024-10-04T07:54:41Z","Open issue","bug","Service
OpenAI
Describe the bug
Starting from version OpenAI_2.0.0-beta.8, the library is no longer compatible with NativeAOT.
Steps to reproduce
Create sample console application
Insert example code from Example01_SimpleChat.cs
Enable Native Compilation in project settings.
<PropertyGroup>
        <OutputType>Exe</OutputType>
       ...
        <PublishAot>true</PublishAot>
       ...
</PropertyGroup>

Launch project, get error
System.InvalidOperationException: Reflection-based serialization has been disabled for this application. Either use the source generator APIs or explicitly configure the 'JsonSerializerOptions.TypeInfoResolver' property.
 at System.Text.Json.ThrowHelper.ThrowInvalidOperationException_JsonSerializerIsReflectionDisabled()
 at System.Text.Json.JsonSerializerOptions.ConfigureForJsonSerializer()
 at System.Text.Json.JsonSerializer.GetTypeInfo(JsonSerializerOptions options, Type inputType)
 at System.Text.Json.JsonSerializer.SerializeToUtf8Bytes(Object value, Type inputType, JsonSerializerOptions options)
 at System.BinaryData.FromObjectAsJson[T](T jsonSerializable, JsonSerializerOptions options)
 at OpenAI.ModelSerializationExtensions..cctor() in /home/nab0y/Projects/openai-dotnet/src/Generated/Internal/ModelSerializationExtensions.cs:line 18
Code snippets
No response
OS
linux, win, macOs
.NET version
8.0
Library version
Starting from version OpenAI_2.0.0-beta.8,
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-dotnet/issues/242","Consider a SendAudio overload that allows streaming through something other than Stream","2024-10-03T13:02:57Z","Open issue","No label","Confirm this is a feature request for the .NET library and not the underlying OpenAI API
 This is a feature request for the .NET library
Describe the feature or improvement you are requesting
I'm experimenting with the new c# bindings for the realtime voice api. So far so good, except the sending of audio is not great. My audio understandably comes from a microphone. I get a bunch of samples at a time. Right now the only way to get this to the c# binding is by SendAudioAsync(Stream). people in this common scenario have no stream though. the api is forcing people to put the samples in a MemoryStream, which is inefficient, as it will just grow and grow.
The other overload SendAudioAsync(BinaryData) looked promising, but it only supports sending a complete recording, which is a very rare scenario for a realtime api.
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-dotnet/issues/240","GetVectorStoreAsync is now internal rather than public in V2.0,0","2024-10-02T22:37:58Z","Open issue","bug","Service
OpenAI
Describe the bug
Should this method have been public rather than internal in V2.0.0?
    /// <summary>
    /// Gets an instance representing an existing <see cref=""VectorStore""/> based on its ID.
    /// </summary>
    /// <param name=""vectorStoreId""> The ID of the vector store to retrieve. </param>
    /// <param name=""cancellationToken""> A token that can be used to cancel this method call. </param>
    /// <returns> A representation of an existing <see cref=""VectorStore""/>. </returns>
    internal virtual async Task<ClientResult<VectorStore>> GetVectorStoreAsync(string vectorStoreId, CancellationToken cancellationToken = default)
    {
        Argument.AssertNotNullOrEmpty(vectorStoreId, nameof(vectorStoreId));

        ClientResult result
            = await GetVectorStoreAsync(vectorStoreId, cancellationToken.ToRequestOptions()).ConfigureAwait(false);
        return ClientResult.FromValue(
            VectorStore.FromResponse(result.GetRawResponse()), result.GetRawResponse());
    }
    
    ```

### Steps to reproduce

There is no GetVectorStoreAsync available, only GetVectorStore, Im wondering if this was intentional or not?

Im updating from 2.0.0-beta.7 to 2.0.0

### Code snippets

_No response_

### OS

winOS

### .NET version

.net 8

### Library version

2.0.0

 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-dotnet/issues/237","AssistantClient","2024-10-01T19:41:27Z","Closed issue","bug","Service
OpenAI
Describe the bug
Im trying to use the Assistat and i keep getting this error in the console
 Error (active)	OPENAI001	'OpenAI.Assistants.AssistantClient' is for evaluation purposes only and is subject to change or removal in future updates. Suppress this diagnostic to proceed.
Steps to reproduce
All you have to do is add AssistantClient assistantClient = openAIClient.GetAssistantClient(); to any .cs file in a blazor app
Code snippets
OpenAIClient openAIClient = new(Environment.GetEnvironmentVariable(""OPENAI_API_KEY""));OpenAIFileClient fileClient = openAIClient.GetOpenAIFileClient();AssistantClient assistantClient = openAIClient.GetAssistantClient();
OS
11
.NET version
.NET 8
Library version
2.0.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-dotnet/issues/232","How to stop generation midway with CancellationTokenSource ?","2024-09-29T11:38:25Z","Closed issue","bug","Service
Vllm
Describe the bug
When communicating with the Vllm service, there is no way to stop the build midway like with python.
Steps to reproduce
When I execute stream.response.close() in Python code, vllm will output ""Aborted request chat-xxxx."".
 In C#, when I call cancelSource.Cancel(), the streaming thread in C# does stop. However, the vllm server will continue to generate until all inferences are completed and end with the output ""Finished request chat-xxxx"".
Code snippets
BackgroundWorker bw = new BackgroundWorker();
 CancellationTokenSource cancelSource;
 public TestForm()
 {
     InitializeComponent();

     bw.WorkerReportsProgress = true;
     bw.WorkerSupportsCancellation = true;
     bw.DoWork += (sender, e) =>     {         OpenAIClientOptions option = new OpenAIClientOptions();         option.Endpoint = new Uri(""http://172.18.130.1:8000/v1"");         OpenAIClient client = new("""", option);         cancelSource = new CancellationTokenSource();         ChatClient cc = client.GetChatClient(""chatllm"");         CollectionResult<StreamingChatCompletionUpdate> updates = cc.CompleteChatStreaming([             new SystemChatMessage(sysp),             new UserChatMessage(userp)             ]             ,             cancellationToken: cancelSource.Token);         foreach (StreamingChatCompletionUpdate update in updates)         {             foreach (ChatMessageContentPart updatePart in update.ContentUpdate)             {                 bw.ReportProgress(0, updatePart.Text);             }         }     };

     bw.ProgressChanged += (sender, e) =>     {         richTextBox1.AppendText(e.UserState.ToString());     };

     bw.RunWorkerCompleted += (sender, e) =>     {         button1.Enabled = true;     };
 }

 private void button1_Click(object sender, EventArgs e)
 {
     richTextBox1.Clear();
     bw.RunWorkerAsync();
     button1.Enabled = false;
 }

 private void button3_Click(object sender, EventArgs e)
 {
     if (bw.IsBusy)
     {
         cancelSource.Cancel();
     }
 }
OS
Windwos 11
.NET version
8.0
Library version
Prepare 2.0.0-beta.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-dotnet/issues/228","Maintain support for legacy 'max_tokens' property (for non-OpenAI APIs, like Groq, Mistral)","2024-09-27T00:34:19Z","Open issue","No label","Confirm this is a feature request for the .NET library and not the underlying OpenAI API
 This is a feature request for the .NET library
Describe the feature or improvement you are requesting
LLM providers like Groq, Mistral and Cerebras have OpenAI compatible APIs, but these still use the legacy max_tokens field.
It would be great if this library could be backward compatible with these legacy APIs. Right now, we can't use the latest SDK with those APIs since they error out if you sent max_completion_tokens.
 > ChatCompletionOptions will automatically apply its MaxOutputTokenCount value (renamed from MaxTokens) to the new max_completion_tokens request body property
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
👍1
JadynWong reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/openai-dotnet/issues/226","NU1900: Warning As Error: Error occurred while getting package vulnerability data","2024-09-26T00:47:15Z","Open issue","bug","Service
OpenAI
Describe the bug
When trying to do dotnet restore inside docker package targeting arm64 the openai project generates this warning:
NU1900: Warning As Error: Error occurred while getting package vulnerability data: Object reference not set to an instance of an object.
As warnings as errors are set in csproj it breaks my build pipeline.
Could you add NU1900 to the nowarn list in .csproj please?
Steps to reproduce
create docker file that attempts to do a dotnet restore on the csproj file and try to build for arm64 platform.
Code snippets
No response
OS
Ubuntu 22.04 Arm64
.NET version
8
Library version
main branch
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-dotnet/issues/219","StreamingChatCompletionUpdate.ContentUpdate throw NullReferenceException","2024-09-20T08:10:48Z","Closed issue","bug","Confirm this is not an issue with the OpenAI Python Library
 This is not an issue with the OpenAI Python Library
Confirm this is not an issue with the underlying OpenAI API
 This is not an issue with the OpenAI API
Confirm this is not an issue with Azure OpenAI
 This is not an issue with Azure OpenAI
Describe the bug
Accessing StreamingChatCompletionUpdate.ContentUpdate throws a NullReferenceException
To Reproduce
When get this result from OpenAI. Calling the property ContentUpdate throws a NullReferenceException. bcz Choices[0].Delta is null.
 file: openai-dotnet/src/Custom/Chat/StreamingChatCompletionUpdate.cs
 line: 82
data: {""choices"":[{""delta"":{},""finish_reason"":""stop"",""index"":0,""logprobs"":null}],""created"":1726817324,""id"":""chatcmpl-A9SMGfev90VrXlgs4WflvoE2XHmVP"",""model"":""gpt-4o-2024-05-13"",""object"":""chat.completion.chunk"",""system_fingerprint"":""fp_80a1bad4c7""}

data: {""choices"":[{""content_filter_offsets"":{""check_offset"":2275,""start_offset"":2185,""end_offset"":2420},""content_filter_results"":{""hate"":{""filtered"":false,""severity"":""safe""},""self_harm"":{""filtered"":false,""severity"":""safe""},""sexual"":{""filtered"":false,""severity"":""safe""},""violence"":{""filtered"":false,""severity"":""safe""}},""finish_reason"":null,""index"":0}],""created"":0,""id"":"""",""model"":"""",""object"":""""}

data: [DONE]


public IReadOnlyList<ChatMessageContentPart> ContentUpdate => (Choices.Count > 0)
    ? Choices[0].Delta.Content
    : _contentUpdate ??= new ChangeTrackingList<ChatMessageContentPart>();

Code snippets
var chatUpdate = client.CompleteChatStreamingAsync(Messages, options, cancellationToken);await foreach (var item in chatUpdate){
	if (item.ContentUpdate.Count > 0)
	{
		sb.Append(item.ContentUpdate[0].Text);
		onStreaming?.Invoke(item.ContentUpdate[0].Text);
	}}
OS
win os
.NET version
.net 8.0
Library version
2.0.0-beta.11
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-dotnet/issues/218","Cannot call CompleteChatAsync with options after ToolChatMessage in messages of type List<ChatMessage>","2024-09-19T14:15:50Z","Open issue","bug","Confirm this is not an issue with the OpenAI Python Library
 This is not an issue with the OpenAI Python Library
Confirm this is not an issue with the underlying OpenAI API
 This is not an issue with the OpenAI API
Confirm this is not an issue with Azure OpenAI
 This is not an issue with Azure OpenAI
Describe the bug
I really like the idea of this codesnip, but it seems like the client.CompleteChat(messages, options); throws exception from server without any specific errors (only 400 Bad Request has been returned), if we call this method after the ToolCall were processed in the previous loop. If we remove options and only keep messages without options CompleteChat(messages), and again after ToolCall, it would work fine.
do
{
    requiresAction = false;
    ChatCompletion chatCompletion = client.CompleteChat(messages, options);

    switch (chatCompletion.FinishReason)
    ...
while (requiresAction)

in some forums, I see that they suggest to use CompleteChat with options for the first time, and then go through the tool before call the CompleteChat for the last time without options. In this case how we can include the whole history? For example when the bot need a parameter from user and ask a question before the bot can run function further?
 On other words, how can we process tool calls as chain (not parallel), output from the first tool can be input for the next tool.
To Reproduce
Just run the codesnip and make sure that the model runs at least one tool. You will see that it throws exception.
do
{
    requiresAction = false;
    ChatCompletion chatCompletion = client.CompleteChat(messages, options);

    switch (chatCompletion.FinishReason)
    ...
while (requiresAction)

The error I have gotten:
System.ClientModel.ClientResultException: Service request failed.
      Status: 400 (Bad Request)
      
         at Azure.AI.OpenAI.ClientPipelineExtensions.ProcessMessageAsync(ClientPipeline pipeline, PipelineMessage message, RequestOptions options)
         at Azure.AI.OpenAI.Chat.AzureChatClient.CompleteChatAsync(BinaryContent content, RequestOptions options)
         at OpenAI.Chat.ChatClient.CompleteChatAsync(IEnumerable`1 messages, ChatCompletionOptions options, CancellationToken cancellationToken)

Code snippets
No response
OS
Windows
.NET version
8.0.6
Library version
2.0.0-beta.2
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-dotnet/issues/215","Expose additional metadata that's currently internal/private","2024-09-17T20:43:06Z","Open issue","No label","Confirm this is not a feature request for the underlying OpenAI API.
 This is not a feature request for the underlying OpenAI API
Confirm this is not a feature request for Azure OpenAI.
 This is not a feature request for Azure OpenAI
Describe the feature or improvement you're requesting
OpenAIClient should expose Url Endpoint { get; }
ChatClient should expose string Model { get; }
Clients like ChatClient should expose a way to navigate back to the owning OpenAIClient so that if you have access to the ChatClient, you can access its Endpoint, or alternatively the Endpoint should also be exposed on ChatClient.
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-dotnet/issues/214","Error ""Thread ... already has an active run"" when using assistants with function calling and streaming","2024-09-17T06:00:16Z","Open issue","bug","Confirm this is not an issue with the OpenAI Python Library
 This is not an issue with the OpenAI Python Library
Confirm this is not an issue with the underlying OpenAI API
 This is not an issue with the OpenAI API
Confirm this is not an issue with Azure OpenAI
 This is not an issue with Azure OpenAI
Describe the bug
When using the C# OpenAI SDK with assistants, function calling, and streaming, an error is thrown when calling CreateRunStreamingAsync. I described the issue already in https://community.openai.com/t/error-400-already-has-an-active-run/930753/6?u=rainer1, but got no feedback. Other users are reporting this problem, too.
To Reproduce
I copied the corresponding example from this repository and ran it with GPT-4o, GPT-4o-2024-08-06, and GPT-4-turbo. All models show the same behavior.
Code snippets
Here is the exact code that I used (it is an exact copy of the example from the OpenAI repo, except it is no longer a unit test):
using System.ClientModel;using System.ClientModel.Primitives;using System.Text.Json;using dotenv.net;using OpenAI.Assistants;
#pragma warning disable OPENAI001

var env = DotEnv.Read(options: new DotEnvOptions(probeForEnv: true, probeLevelsToSearch: 7));

// This example parallels the content at the following location:// https://platform.openai.com/docs/assistants/tools/function-calling/function-calling-beta
#region Step 1 - Define Functions

// First, define the functions that the assistant will use in its defined tools.

FunctionToolDefinition getTemperatureTool = new(){
    FunctionName = ""get_current_temperature"",
    Description = ""Gets the current temperature at a specific location."",
    Parameters = BinaryData.FromString(""""""
    {
        ""type"": ""object"",
        ""properties"": {
        ""location"": {
            ""type"": ""string"",
            ""description"": ""The city and state, e.g., San Francisco, CA""
        },
        ""unit"": {
            ""type"": ""string"",
            ""enum"": [""Celsius"", ""Fahrenheit""],
            ""description"": ""The temperature unit to use. Infer this from the user's location.""
        }
        }
    }
    """"""),};

FunctionToolDefinition getRainProbabilityTool = new(){
    FunctionName = ""get_current_rain_probability"",
    Description = ""Gets the current forecasted probability of rain at a specific location,""
        + "" represented as a percent chance in the range of 0 to 100."",
    Parameters = BinaryData.FromString(""""""
    {
        ""type"": ""object"",
        ""properties"": {
        ""location"": {
            ""type"": ""string"",
            ""description"": ""The city and state, e.g., San Francisco, CA""
        }
        },
        ""required"": [""location""]
    }
    """"""),};

#endregion

// Assistants is a beta API and subject to change; acknowledge its experimental status by suppressing the matching warning.AssistantClient client = new(env[""OPENAI_KEY""]!);

#region Create a new assistant with function tools
// Create an assistant that can call the function tools.AssistantCreationOptions assistantOptions = new(){
    Name = ""Example: Function Calling"",
    Instructions =
        ""Don't make assumptions about what values to plug into functions.""
        + "" Ask for clarification if a user request is ambiguous."",
    Tools = { getTemperatureTool, getRainProbabilityTool },};

Assistant assistant = await client.CreateAssistantAsync(env[""OPENAI_MODEL""]!, assistantOptions);
#endregion

#region Step 2 - Create a thread and add messages
AssistantThread thread = await client.CreateThreadAsync();ThreadMessage message = await client.CreateMessageAsync(
    thread,
    MessageRole.User,
    [
        ""What's the weather in San Francisco today and the likelihood it'll rain?""
    ]);
#endregion

#region Step 3 - Initiate a streaming run
AsyncCollectionResult<StreamingUpdate> asyncUpdates
    = client.CreateRunStreamingAsync(thread, assistant);

ThreadRun? currentRun = null;do{
    currentRun = null;
    List<ToolOutput> outputsToSubmit = [];
    await foreach (StreamingUpdate update in asyncUpdates)
    {
        if (update is RunUpdate runUpdate)
        {
            currentRun = runUpdate;
        }
        else if (update is RequiredActionUpdate requiredActionUpdate)
        {
            if (requiredActionUpdate.FunctionName == getTemperatureTool.FunctionName)
            {
                outputsToSubmit.Add(new ToolOutput(requiredActionUpdate.ToolCallId, ""57""));
            }
            else if (requiredActionUpdate.FunctionName == getRainProbabilityTool.FunctionName)
            {
                outputsToSubmit.Add(new ToolOutput(requiredActionUpdate.ToolCallId, ""25%""));
            }
        }
        else if (update is MessageContentUpdate contentUpdate)
        {
            Console.Write(contentUpdate.Text);
        }
    }
    if (outputsToSubmit.Count > 0)
    {
        asyncUpdates = client.SubmitToolOutputsToRunStreamingAsync(currentRun, outputsToSubmit);
    }}while (currentRun?.Status.IsTerminal == false);

#endregion

// Optionally, delete the resources for tidiness if no longer needed.RequestOptions noThrowOptions = new() { ErrorOptions = ClientErrorBehaviors.NoThrow };_ = await client.DeleteThreadAsync(thread.Id, noThrowOptions);_ = await client.DeleteAssistantAsync(assistant.Id, noThrowOptions);
OS
Ubuntu
.NET version
8
Library version
2.0.0-beta.11
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-dotnet/issues/213","CancellationToken, not cancelling after start of streaming.","2024-09-21T19:12:31Z","Closed issue","bug","Confirm this is not an issue with the OpenAI Python Library
 This is not an issue with the OpenAI Python Library
Confirm this is not an issue with the underlying OpenAI API
 This is not an issue with the OpenAI API
Confirm this is not an issue with Azure OpenAI
 This is not an issue with Azure OpenAI
Describe the bug
We got an issue raised at Semantic Kernel that ended up also being a potential bug on the SDK implementation.
Bug: The latest version of the C # SDK cannot use CancellationToken properly microsoft/semantic-kernel#8718
After starting the streaming of the first chunk and issuing Cancel on the token, the stream is not cancelled and continues to give the remaining chunks.
To Reproduce
Important
This example uses the AzureOpenAIClient but the CancellationToken behavior is inherited from the OpenAIClient.
Failing xUnit Reproduction Code (should Pass)
  [Fact]
  public async Task ItCancellationWorksAsExpectedAfterFirstChunkSuccessful2Async()
  {
      // Arrange
      using var streamText = new MemoryStream(Encoding.UTF8.GetBytes(
          """"""
          data: {""id"":""Eoo"",""object"":""chat.completion.chunk"",""created"":1711377846,""model"":""gpt-4-0125-preview"",""system_fingerprint"":""fp_a7daf7c51e"",""choices"":[{""index"":0,""delta"":{""content"":""Test chat streaming response""},""logprobs"":null,""finish_reason"":null}]}

          data: {""id"":""Eoo"",""object"":""chat.completion.chunk"",""created"":1711377846,""model"":""gpt-4-0125-preview"",""system_fingerprint"":""fp_a7daf7c51e"",""choices"":[{""index"":0,""delta"":{},""logprobs"":null,""finish_reason"":""stop""}]}

          data: [DONE]
          """"""
          ));

      using var response = new HttpResponseMessage(HttpStatusCode.OK) { Content = new StreamContent(streamText) };
      this._messageHandlerStub.ResponsesToReturn.Add(response);

      var azureClient = new AzureOpenAIClient(new Uri(""http://localhost""), ""api-key"", new AzureOpenAIClientOptions { Transport = new HttpClientPipelineTransport(this._httpClient) });

      using var cancellationTokenSource = new CancellationTokenSource();
      cancellationTokenSource.CancelAfter(1000);

      var sut = azureClient.GetChatClient(""mock-model"");

      // Act & Assert
      var enumerator = sut.CompleteChatStreamingAsync([""Hello!""], cancellationToken: cancellationTokenSource.Token).GetAsyncEnumerator();
      await enumerator.MoveNextAsync();

      var firstChunk = enumerator.Current;
      Assert.False(cancellationTokenSource.IsCancellationRequested);
      await Task.Delay(1000);

      await Assert.ThrowsAsync<TaskCanceledException>(async () =>      {          // Should throw for the second chunk          Assert.True(cancellationTokenSource.IsCancellationRequested);                   Assert.True(cancellationTokenSource.Token.IsCancellationRequested);          await enumerator.MoveNextAsync();          await enumerator.MoveNextAsync();      });
  }
Code snippets
No response
OS
Windows 11
.NET version
.net 8
Library version
beta.11
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-dotnet/issues/212","api.md missing","2024-09-06T05:21:10Z","Closed issue","bug","Confirm this is not an issue with the OpenAI Python Library
 This is not an issue with the OpenAI Python Library
Confirm this is not an issue with the underlying OpenAI API
 This is not an issue with the OpenAI API
Confirm this is not an issue with Azure OpenAI
 This is not an issue with Azure OpenAI
Describe the bug
I want to see the full description for the APIs but seems like the api.md is not existing.
To Reproduce
N/A
Code snippets
No response
OS
macOS
.NET version
.NET CORE
Library version
8.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-dotnet/issues/211","Is there a way to set stopSequences? I can see the option in the Typescript and Python APIs but not .NET","2024-09-04T22:40:53Z","Closed issue","No label","Confirm this is not a feature request for the underlying OpenAI API.
 This is not a feature request for the underlying OpenAI API
Confirm this is not a feature request for Azure OpenAI.
 This is not a feature request for Azure OpenAI
Describe the feature or improvement you're requesting
I would like to set stopSequences but it appears to be read only
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-dotnet/issues/210","Assistant streaming run throwing exception.","2024-09-06T05:54:59Z","Closed issue","bug","Confirm this is not an issue with the OpenAI Python Library
 This is not an issue with the OpenAI Python Library
Confirm this is not an issue with the underlying OpenAI API
 This is not an issue with the OpenAI API
Confirm this is not an issue with Azure OpenAI
 This is not an issue with Azure OpenAI
Describe the bug
Assistant streaming run is throwing exception when trying to access a function call in RequiredActionUpdate.
Exception is:
 System.ClientModel.ClientResultException: 'HTTP 400 (invalid_request_error: )
 Thread THREAD_ID already has an active run RUN_ID
To Reproduce
Run example Example02b_FunctionCallingStreaming
Code snippets
No response
OS
winOS
.NET version
.NET 8.0
Library version
2.0.0-beta.11
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-dotnet/issues/207","stream_options is included in every request even though it is optional","2024-09-21T19:15:27Z","Closed issue","bug","Confirm this is not an issue with the OpenAI Python Library
 This is not an issue with the OpenAI Python Library
Confirm this is not an issue with the underlying OpenAI API
 This is not an issue with the OpenAI API
Confirm this is not an issue with Azure OpenAI
 This is not an issue with Azure OpenAI
Describe the bug
In the OpenAI document stream_options is optional and defaults to null. In the library it has a default value and is always sent when streaming is enabled.
The ask here is to update the implementation to match the API specification so that unnecessary properties are not being sent by default.
To Reproduce
Make a streaming chat completion requests
Inspect the request being generated and you will see stream_options is included.
{
  ""messages"": [
    {
      ""role"": ""user"",
      ""content"": ""Can you help me tell the time in Seattle right now?""
    }
  ],
  ""model"": ""gpt-4o"",
  ""stream"": true,
  ""stream_options"": {
    ""include_usage"": true
  }
}

Code snippets
No response
OS
winOS
.NET version
8.0
Library version
2.0.0-beta.10
 The text was updated successfully, but these errors were encountered: 
👍1
JadynWong reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/openai-dotnet/issues/205","missing_required_parameter response_format.json_schema using example code in Unity (C# 9.0)","2024-09-03T22:44:39Z","Closed issue","bug","Confirm this is not an issue with the OpenAI Python Library
 This is not an issue with the OpenAI Python Library
Confirm this is not an issue with the underlying OpenAI API
 This is not an issue with the OpenAI API
Confirm this is not an issue with Azure OpenAI
 This is not an issue with Azure OpenAI
Describe the bug
I'm always getting the same error message as soon as I try to use any variation of ChatResponseFormat.CreateJsonSchemaFormat
Couldn't get response from ChatGPT: System.ClientModel.ClientResultException: HTTP 400 (invalid_request_error: missing_required_parameter)
Parameter: response_format.json_schema
Missing required parameter: 'response_format.json_schema'.
  at OpenAI.ClientPipelineExtensions.ProcessMessageAsync (System.ClientModel.Primitives.ClientPipeline pipeline, System.ClientModel.Primitives.PipelineMessage message, System.ClientModel.Primitives.RequestOptions options) [0x0015e] in <92cde0f04c3a467caa9bf3e15fe4f0c8>:0 
  at System.Threading.Tasks.ValueTask`1[TResult].get_Result () [0x0001b] in <27124aa0e30a41659b903b822b959bc7>:0 
  at OpenAI.Chat.ChatClient.CompleteChatAsync (System.ClientModel.BinaryContent content, System.ClientModel.Primitives.RequestOptions options) [0x000ad] in <92cde0f04c3a467caa9bf3e15fe4f0c8>:0 
  at OpenAI.Chat.ChatClient.CompleteChatAsync (System.Collections.Generic.IEnumerable`1[T] messages, OpenAI.Chat.ChatCompletionOptions options, System.Threading.CancellationToken cancellationToken) [0x00152] in <92cde0f04c3a467caa9bf3e15fe4f0c8>:0 

In the code below, if I use chatOptions.ResponseFormat = ChatResponseFormat.JsonObject, I don't get an error message, but then I of course don't get to specify the schema.
This user was also having issues in Unity, but the error message was unclear so it's hard to know if it's the same issue. Plus whatever fixed it should be documented.
To Reproduce
Run the code below in Unity 2022.3.22f1
 Library installed using NuGet for Unity
Code snippets
var client = new OpenAI.Chat.ChatClient(model: model.Or(DEFAULT_CHAT_GPT_MODEL), apiKey);var chatOptions = new ChatCompletionOptions();
chatOptions.ResponseFormat = ChatResponseFormat.CreateJsonSchemaFormat(
    name: ""math_reasoning"",
    jsonSchema: BinaryData.FromString(
        ""{\n"" +
        ""    \""type\"": \""object\"",\n"" +
        ""    \""properties\"": {\n"" +
        ""        \""steps\"": {\n"" +
        ""            \""type\"": \""array\"",\n"" +
        ""            \""items\"": {\n"" +
        ""                \""type\"": \""object\"",\n"" +
        ""                \""properties\"": {\n"" +
        ""                    \""explanation\"": { \""type\"": \""string\"" },\n"" +
        ""                    \""output\"": { \""type\"": \""string\"" }\n"" +
        ""                },\n"" +
        ""                \""required\"": [\""explanation\"", \""output\""],\n"" +
        ""                \""additionalProperties\"": false\n"" +
        ""            }\n"" +
        ""        },\n"" +
        ""        \""final_answer\"": { \""type\"": \""string\"" }\n"" +
        ""    },\n"" +
        ""    \""required\"": [\""steps\"", \""final_answer\""],\n"" +
        ""    \""additionalProperties\"": false\n"" +
        ""}""),
    strictSchemaEnabled: true);var chatMessage = new UserChatMessage(query);return client.CompleteChatAsync(new List<UserChatMessage> { chatMessage }, chatOptions)
OS
winOS
.NET version
.NET Standard 2.1
Library version
2.0.0-beta.10
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-dotnet/issues/203","InvalidOperationException related to GetRunStepsAsync","2024-09-03T22:46:07Z","Closed issue","bug","Confirm this is not an issue with the underlying OpenAI API
 This is an issue with the Python library
Confirm this is not an issue with Azure OpenAI
 This is not an issue with Azure OpenAI
Describe the bug
I am retrieving the RunSteps from an Assistant. I go through the standard sequence: GetRunStepsAsync, GetAllValuesAsync, await foreach for each step. If there was a file_search step, the foreach generates this exception:
 System.InvalidOperationException: 'The requested operation requires an element of type 'String', but the target element has type 'Object''.
This occurs with the latest beta-10 and with the older beta-6 that we have in the field. This is new behavior within the last couple of days, which makes me think that there is a change in the base API that is now triggering this exception.
I don't know if this is an issue with the Python library, but I had to check that to enable the Submit button.
To Reproduce
Make a request via the Assistants API that will use the file_search feature.
Get the RunSteps.
 This generates System.InvalidOperationException
Code snippets
AsyncPageCollection<RunStep> res = _assistantClient.GetRunStepsAsync(threadId, runId, null, ct);
            IAsyncEnumerable<RunStep> values = res.GetAllValuesAsync(ct);

            await foreach(RunStep step in values)
            {// Whatever, code does not execute because the foreach throws an exception
            }
OS
Windows 11
.NET version
Framework 4.8
Library version
2.0.0-beta.10
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-dotnet/issues/201","Document how the code is generated from the OpenAPI spec","2024-08-30T13:04:01Z","Open issue","No label","Confirm this is not a feature request for the underlying OpenAI API.
 This is not a feature request for the underlying OpenAI API
Confirm this is not a feature request for Azure OpenAI.
 This is not a feature request for Azure OpenAI
Describe the feature or improvement you're requesting
In the README, it is stated that the client is generated from the OpenAPI specification.
 Please document the process for doing this.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-dotnet/issues/200","De-serialization error using AssistantClient streaming","2024-09-03T22:47:51Z","Closed issue","bug","Confirm this is not an issue with the underlying OpenAI API
 This is an issue with the Python library
Confirm this is not an issue with Azure OpenAI
 This is not an issue with Azure OpenAI
Describe the bug
Using the AssistantClient with file search enabled and streaming gives the error: The requested operation requires an element of type 'String', but the target element has type 'Object'.'
The exception is raised here: 
openai-dotnet/src/Generated/Models/InternalRunStepDeltaStepDetailsToolCallsFileSearchObject.Serialization.cs
 Line 115 in 79014ab
	 dictionary.Add(property0.Name, property0.Value.GetString());
To Reproduce
Create and streaming enumerate a Run using an assistant with file search enabled.
Code snippets
var response = assistantClient.CreateRunStreamingAsync(
    threadId: ""thread_id"",
    assistantId: ""assistant_id"",
    options: new RunCreationOptions {},
    cancellationToken: cToken);

await foreach (var update in response.WithCancellation(cToken)){
     //after a couple of updates, the error ""InvalidOperationException The requested operation requires an element of type 'String', but the target element has type 'Object'."" is raised.}
OS
Windows 11
.NET version
net8.0
Library version
2.0.0-beta.10
 The text was updated successfully, but these errors were encountered: 
👍5
span, DanielHMortensen, EmilMyrup, citron8000, and Pauluz26 reacted with thumbs up emoji
All reactions
👍5 reactions"
"https://github.com/openai/openai-dotnet/issues/198","NullReferenceException when retrieving properties of a streaming chat chunk using Asynchronous Filter","2024-08-29T09:19:56Z","Open issue","bug","Confirm this is not an issue with the underlying OpenAI API
 This is an issue with the Python library
Confirm this is not an issue with Azure OpenAI
 This is not an issue with Azure OpenAI
Describe the bug
When using StreamingChatCompletionUpdate with Asynchronous Filter enabled in azure, this class breaks.
openai-dotnet/src/Custom/Chat/StreamingChatCompletionUpdate.cs
 Line 83 in 583e9f6
	? Choices[0].Delta.Content 
When a SSE data: doesn't have a delta (null) the Choice[0].Delta line throws NullReferenceException.
Final 3 streaming chunks of an Asynchornous Filter response.
data: {""choices"":[{""delta"":{},""finish_reason"":""stop"",""index"":0,""logprobs"":null}],""created"":1724860848,""id"":""chatcmpl-A1FOCU4vamBSxxqjaKcvADYMlwlaC"",""model"":""gpt-4o-2024-05-13"",""object"":""chat.completion.chunk"",""system_fingerprint"":""fp_abc28019ad""}

data: {""choices"":[{""content_filter_offsets"":{""check_offset"":1576,""start_offset"":1576,""end_offset"":2318},""content_filter_results"":{""hate"":{""filtered"":false,""severity"":""safe""},""self_harm"":{""filtered"":false,""severity"":""safe""},""sexual"":{""filtered"":false,""severity"":""safe""},""violence"":{""filtered"":false,""severity"":""safe""}},""finish_reason"":null,""index"":0}],""created"":0,""id"":"""",""model"":"""",""object"":""""}

data: {""choices"":[{""content_filter_offsets"":{""check_offset"":1576,""start_offset"":1576,""end_offset"":2318},""content_filter_results"":{""protected_material_code"":{""filtered"":false,""detected"":false},""protected_material_text"":{""filtered"":false,""detected"":false}},""finish_reason"":null,""index"":0}],""created"":0,""id"":"""",""model"":"""",""object"":""""}

This issue was reported in our Semantic Kernel Issues Here but actually seems to be a problem when Azure tries to use the OpenAI SDK for those chunks, this potentially can be handled by Azure SDK library in a non-ideal (try catch block) or in the back-end (providing always an empty delta) but I wonder would be quicker/simple getting a fix in here.
The problem also happen for other properties of the class as below:
To Reproduce
Enable Asynchronous Filter in Azure or OpenAI (if possible)
Make a streaming request and try to check the Role property of the last 2 chunks
Since the property is a pointer, we are unable to test it for null.
if (lastChunk.Role.HasValue) { } // throwsif (lastChunk.Role is null) { } // throws
Code snippets
No response
OS
Windows 11
.NET version
.net 8
Library version
beta.10
 The text was updated successfully, but these errors were encountered: 
👍2
FrankyCTY and sandrohanea reacted with thumbs up emoji
All reactions
👍2 reactions"
"https://github.com/openai/openai-dotnet/issues/193","ChatResponseFormat.CreateJsonSchemaFormat() is not working correctly.","2024-08-31T05:49:16Z","Closed issue","bug","Confirm this is not an issue with the underlying OpenAI API
 This is an issue with the Python library
Confirm this is not an issue with Azure OpenAI
 This is not an issue with Azure OpenAI
Describe the bug
When I try to set response_format by ChatResponseFormat.CreateJsonSchemaFormat, but the variable's base is not created correctly. The fact that 'Schema' and 'Name' is input correctly can be checked with below capture.

To Reproduce
Use the code below with c# 9.0 and .NET 4.7.1 and use Debug mod to check it.
Code snippets
[System.Serializable]public class ExploererFormat{
    public string name { get; set; }
    public int age { get; set; }

    [Description(""The character's background story."")]
    public string background { get; set; }}

public class RandomExploerer : MonoBehaviour{
    private ChatClient chatClient = new(model: ""gpt-4o-2024-08-06"", Environment.GetEnvironmentVariable(""OPENAI_API_KEY""));

    private static string schemaString = null;

    // Start is called before the first frame update
    void Start()
    {
        CreateSchema();
        
        CreateRandom();
    }

    public void CreateSchema()
    {
        JSchemaGenerator generator = new JSchemaGenerator();
        generator.DefaultRequired = Required.Always; // required by OpenAI

        JSchema schema = generator.Generate(typeof(ExploererFormat));
        schema.AllowAdditionalProperties = false; // required by OpenAI

        schemaString = schema.ToString();

        Debug.Log(schemaString);
    }

    public async void CreateRandom()
    {
        BinaryData data = BinaryData.FromString(schemaString);
        if (data == null ) { Debug.Log(""Why this is empty?""); }

        ChatResponseFormat format = ChatResponseFormat.CreateJsonSchemaFormat(                
            name: ""Character_Sheet"",
            jsonSchema: BinaryData.FromString(schemaString),
            strictSchemaEnabled: true);
        

        var superMessages = new List<ChatMessage>
        {
            ChatMessage.CreateUserMessage(""As a novel writer, create a random novel character with the following instructions: 1. All outputs must be written in Korean. 2. Genre of the novel is Ocean Adventure."")
        };

        ChatCompletionOptions jsonOptions = ModelReaderWriter.Read<ChatCompletionOptions>(BinaryData.FromString(""{ \""messages\"": [{\""role\"": \""user\"", \""content\"": \""hello world\""}] }""));
        jsonOptions.ResponseFormat = format;
        Debug.Log(ModelReaderWriter.Write(jsonOptions).ToString());
        
        ChatCompletion chatCompletion = await chatClient.CompleteChatAsync(        
            superMessages
            ,jsonOptions);}
OS
winOS
.NET version
4.7.1
Library version
2.0.0-beta.9
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-dotnet/issues/191","Special custom endpoint not available","2024-08-28T00:49:07Z","Closed issue","bug","Confirm this is not an issue with the underlying OpenAI API
 This is an issue with the Python library
Confirm this is not an issue with Azure OpenAI
 This is not an issue with Azure OpenAI
Describe the bug
After the #186 merged (2.0.0-beta.10), special custom endpoints are no longer available. All clients have a '/v1' path prefix. This is different from the behavior of the Python version.
Such as https://open.bigmodel.cn/api/paas/v4 and other special custom endpoints.
To Reproduce
Execute chat completions by a special endpoint.
var client = new OpenAIClient(""api-key"", new()
        {
            Endpoint = new Uri(""https://open.bigmodel.cn/api/paas/v4"")
        });
var chatClient = client.GetChatClient(""glm-4"");
ChatCompletion completion = client.CompleteChat(""Say 'this is a test.'"");

Got 404 exception. The chat completions link is build as ""https://open.bigmodel.cn/api/paas/v4/v1/chat/completions"". It should be ""https://open.bigmodel.cn/api/paas/v4/chat/completions"".
Code snippets
No response
OS
windows
.NET version
8.0
Library version
2.0.0-beta.10
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-dotnet/issues/184","OpenAIClient Constructor not persisting _options internally, fails for custom options (+endpoints).","2024-08-26T22:57:18Z","Closed issue","bug","Confirm this is not an issue with the underlying OpenAI API
 This is an issue with the Python library
Confirm this is not an issue with Azure OpenAI
 This is not an issue with Azure OpenAI
Describe the bug
On OpenAIClientoptions, constructor is missing set of internal _options = options`.
Here:

openai-dotnet/src/Custom/OpenAIClient.cs
 Line 101 in 4897e23
	}
The impact of those changes is big as any specialized clients created from OpenAIClient lose specific options, including custom endpoints.
Suggest adding UnitTests to ensure the behavior.
To Reproduce
When running the test below, it will never reach localhost, instead will go for public OpenAI API with a 401.
private sealed class HttpMessageHandlerStub : DelegatingHandler{
    public HttpRequestHeaders RequestHeaders { get; private set; }

    public HttpContentHeaders ContentHeaders { get; private set; }

    public byte[] RequestContent { get; private set; }

    public Uri RequestUri { get; private set; }

    public HttpMethod Method { get; private set; }

    public HttpResponseMessage ResponseToReturn { get; set; }

    public HttpMessageHandlerStub()
    {
        InnerHandler = new HttpClientHandler();
        this.ResponseToReturn = new HttpResponseMessage(HttpStatusCode.OK)
        {
            Content = new StringContent(""{}"", Encoding.UTF8, ""application/json"")
        };
    }

    protected override async Task<HttpResponseMessage> SendAsync(HttpRequestMessage request, CancellationToken cancellationToken)
    {
        this.Method = request.Method;
        this.RequestUri = request.RequestUri;
        this.RequestHeaders = request.Headers;
        if (request.Content is not null)
        {
            this.RequestContent = await request.Content.ReadAsByteArrayAsync(cancellationToken);
        }

        this.ContentHeaders = request.Content?.Headers;
        return await Task.FromResult(this.ResponseToReturn);
    }}

[Test]public void CustomEndpoint(){
    using var handler = new HttpMessageHandlerStub();
    using var httpClient = new HttpClient(handler, false);
    Uri fakeUri = new(""https://localhost:1234"");
    OpenAIClient client = new(""sk-not-a-real-key"", new OpenAIClientOptions()
    {
        Endpoint = fakeUri,
        ApplicationId = ""my-app-id"",
        NetworkTimeout = TimeSpan.FromSeconds(10),
        RetryPolicy = new ClientRetryPolicy(),
        Transport = new HttpClientPipelineTransport(httpClient)
    });

    client.GetChatClient(""gpt-4o"").CompleteChat([""Hello, world!""]);

    Assert.That(handler.RequestUri, Is.EqualTo(fakeUri));}
Code snippets
No response
OS
Windows 11
.NET version
.net 8
Library version
beta.9
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-dotnet/issues/182","Support structured output in chat completion","2024-08-26T01:20:17Z","Closed issue","No label","Confirm this is not a feature request for the underlying OpenAI API.
 This is not a feature request for the underlying OpenAI API
Confirm this is not a feature request for Azure OpenAI.
 This is not a feature request for Azure OpenAI
Describe the feature or improvement you're requesting
We need InternalCreateChatCompletionRequestResponseFormatType to have a schema type
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-dotnet/issues/179","No citations and intents from OpenAI library","2024-08-22T17:41:07Z","Open issue","bug","Confirm this is not an issue with the underlying OpenAI API
 This is an issue with the Python library
Confirm this is not an issue with Azure OpenAI
 This is not an issue with Azure OpenAI
Describe the bug
Hello, we recently updated our libraries to Azure.AI.OpenAI 2.0.0-beta.2. and underlying OpenAI 2.0.0-beta.5. And now we are not receiving StreamingChatCompletionUpdate context (which contains Citations and intents which we need). If you look into Azure library, the call for context is pretty straightforward call into OpenAI library and I am pretty sure the bug is not there.
I apologize if the context, citations & intents are not received from OpenAI API, but i didn't get as far as there, just here :-) I also did a lot of emptyhanded search for this and found nothing, so i am asking here :-)
Thank you
To Reproduce
Use the code below with Azure.AI.OpenAI 2.0.0-beta.2 and OpenAI 2.0.0-beta.5. Then it is never received. I tried multiple questions and no context.
Code snippets
AzureOpenAIClient client = new AzureOpenAIClient(new Uri(openAiApiBaseUrl), new AzureKeyCredential(openAiApiKey));

var ccChatClient = client.GetChatClient(""gpt-4o"");

await foreach (var chatUpdate in

               ccChatClient.CompleteChatStreamingAsync(new List<ChatMessage>()
               {
                   new SystemChatMessage(""You are helpful assistant""),
                   new UserChatMessage(""Tell me something about dolphins"")
               })){
#pragma warning disable AOAI001
    var context = chatUpdate.GetAzureMessageContext();
#pragma warning restore AOAI001

    if (context != null)
    {
        Console.WriteLine(""Yeah!!!"");
    }}
OS
windows
.NET version
8.0
Library version
2.0.0-beta.5
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-dotnet/issues/178","Fully support the ""limit"" query parameter in GetMessages and GetMessagesAsync for Assistants","2024-08-24T04:43:43Z","Closed issue","No label","Confirm this is not a feature request for the underlying OpenAI API.
 This is not a feature request for the underlying OpenAI API
Confirm this is not a feature request for Azure OpenAI.
 This is not a feature request for Azure OpenAI
Describe the feature or improvement you're requesting
Getting messages has support for a limit query parameter (https://platform.openai.com/docs/api-reference/messages/listMessages) but it's not available in the main methods of the GetMessages and GetMessagesAsync.
An extension method exists for both, with the limit param, but it's implemented a bit strangely and doesn't have the same return type. This should probably be removed.
Ideal usage:
PageableCollection<ThreadMessage> messages = assistantClient.GetMessages(threadId, limit: 20);
This should return the 20 newest messages in the active thread.
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-dotnet/issues/176","How to mock AssistantClient and FileClient responses","2024-08-21T11:49:35Z","Open issue","No label","Confirm this is not a feature request for the underlying OpenAI API.
 This is not a feature request for the underlying OpenAI API
Confirm this is not a feature request for Azure OpenAI.
 This is not a feature request for Azure OpenAI
Describe the feature or improvement you're requesting
The responses provided by both FileClient and AssistantClient in the following functions:
 FileClient:
UploadFileAsync()
DeleteFileAsync()
AssistantClinet:
CreateAssistant()
CreateThreadAndRun()
GetRun()
GetMessages()
Cannot be mocked or instantiated thus disabling proper unit testing.
 How can we write unit tests to check proper flow?
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-dotnet/issues/169","Assistant Streaming: Incorrect MessageRole for MessageContentUpdate.Role","2024-08-12T21:53:31Z","Open issue","bug","Confirm this is not an issue with the underlying OpenAI API
 This is not an issue with the Python library
Confirm this is not an issue with Azure OpenAI
 This is not an issue with Azure OpenAI
Describe the bug
MessageContentUpdate.Role is User for an assistant response.
Turns out role is not present in the response payload, but perhaps the default should be Assistant instead of MessageRole role = default;:
https://github.com/openai/openai-dotnet/blob/main/src/Generated/Models/InternalMessageDeltaObjectDelta.Serialization.cs#L81
https://community.openai.com/t/streaming-message-delta-missing-role/904647
To Reproduce
Using the provided code:
Set breakpoint on line:
Console.Write(contentUpdate.Text); // ISSUE: contentUpdate.Role == MessageRole.User
Run
Expect: contentUpdate.Role == MessageRole.Assistant
Actual: contentUpdate.Role == MessageRole.User
Code snippets
AssistantClient client = new(Environment.GetEnvironmentVariable(""OPENAI_API_KEY""));AssistantCreationOptions assistantOptions = new();Assistant assistant = await client.CreateAssistantAsync(""gpt-4-turbo"", assistantOptions);

AssistantThread thread = await client.CreateThreadAsync();ThreadMessage message = await client.CreateMessageAsync(
    thread,
    MessageRole.User,
    [
        ""Is 191 a prime number?""
    ]);

AsyncCollectionResult<StreamingUpdate> asyncUpdates
    = client.CreateRunStreamingAsync(thread, assistant);

ThreadRun currentRun = null;do{
    currentRun = null;
    List<ToolOutput> outputsToSubmit = [];
    await foreach (StreamingUpdate update in asyncUpdates)
    {
        if (update is RunUpdate runUpdate)
        {
            currentRun = runUpdate;
        }
        else if (update is MessageContentUpdate contentUpdate)
        {
            Console.Write(contentUpdate.Text); // ISSUE: contentUpdate.Role == MessageRole.User
        }
    }}while (currentRun?.Status.IsTerminal == false);
OS
Windows
.NET version
8.0
Library version
2.0.0-beta.8
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-dotnet/issues/166","Get the usage when generating embeddings.","2024-08-10T18:18:00Z","Closed issue","question","Confirm this is not a feature request for the underlying OpenAI API.
 This is not a feature request for the underlying OpenAI API
Confirm this is not a feature request for Azure OpenAI.
 This is not a feature request for Azure OpenAI
Describe the feature or improvement you're requesting
Token usage is not present when generating embeds.
 This feature seems to be already implemented in Azure OpenAI SDK Implementation:
is this planned to be implemented, is it possible to use the Azure SDK as a drop-in replacement?
Azure/azure-sdk-for-net#44599
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-dotnet/issues/162","CompleteChat requests with the tools and the name field set for the assistant messages results in ""The model produced invalid content"" response","2024-09-03T01:10:19Z","Closed issue","bug","Confirm this is not an issue with the underlying OpenAI API
 This is an issue with the Python library
Confirm this is not an issue with Azure OpenAI
 This is not an issue with Azure OpenAI
Describe the bug
The .Net CompleteChat requests that include tools and have name field set for the assistant messages produce ""The model produced invalid content"" response in 20-30% of the cases.
To Reproduce
Start with official example https://github.com/openai/openai-dotnet?tab=readme-ov-file#how-to-use-chat-completions-with-tools-and-function-calling . Ensure it works as expected for you.
 Modify the example by setting ParticipantName for each AssistantChatMessage before adding it to the messages list.
 Ask for the weather 10-20 times. In 20-30% of the cases, the API call fails with the following message:
 ""The model produced invalid content. Consider modifying your prompt if you are seeing this error persistently.""
Code snippets
Instead:
messages.Add(new AssistantChatMessage(chatCompletion));
...use...
var acm = new AssistantChatMessage(chatCompletion);
acm.ParticipantName = ""Debbie"";
messages.Add(acm);
OS
winOs
.NET version
.Net 8.0
Library version
2.0.0.-beta.8
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-dotnet/issues/160","Add Support for OpenAI new Structured Outputs.","2024-08-24T04:37:25Z","Closed issue","enhancement","Add Support for Structured Outputs
As mentioned in the recent blog post from OpenAI.
Now is possible to specify the response_format as json_schema.
Additional context
 This is not a feature request for the underlying OpenAI API
 This is not a feature request for Azure OpenAI
 The text was updated successfully, but these errors were encountered: 
👍30
rodion-m, HavenDV, alperenbelgic, LinkinNg, gbryer, KanaHayama, alienwareone, lundgrenmattias, Swah, shearos, and 20 more reacted with thumbs up emoji❤️7
sagos95, ferryferry, benhunterandrewrobertson, gsears, jordan-orionfinity, danielwinkler, and Xavier-Mobius reacted with heart emoji
All reactions
👍30 reactions
❤️7 reactions"
"https://github.com/openai/openai-dotnet/issues/155","Move service methods for paginated endpoints to use PageCollection pattern","2024-08-27T00:26:38Z","Closed issue","No label","Confirm this is not a feature request for the underlying OpenAI API.
 This is not a feature request for the underlying OpenAI API
Confirm this is not a feature request for Azure OpenAI.
 This is not a feature request for Azure OpenAI
Describe the feature or improvement you're requesting
The following service endpoints return paginated lists of results and should move to the SCM-based client PageCollection pattern:
List fine-tuning jobs
List fine-tuning events
List fine-tuning checkpoints
List batch
The pattern is illustrated for the list assistants and list vector stores endpoints in this PR.
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-dotnet/issues/152","Unsafe overridden ToString on ChatCompletion","2024-07-31T14:09:23Z","Open issue","No label","This override crashes as soon as there is no content returned e.g. in the case of a FunctionTool response.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-dotnet/issues/151","Initialize ChatClient with azure open ai key and endpoint values","2024-07-31T09:02:01Z","Closed issue","No label","I have ""AzureOpenAIApiKey"", ""AzureOpenAIEndpoint"" and azure model . can I initialize ChatClient using those values something like below ?
 ChatClient client = new(model: ""gpt-4o"", ""AZURE-OPENAI_API_KEY"" , ""AZURE-OPENAI_ENDPOINT"". ),);
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-dotnet/issues/143","ChatCompletionOptions property N is internal - unable to set number of chat completion choices","2024-07-29T17:44:51Z","Open issue","No label","Hi, with internal property N for ChatCompletionOptions I'm unable to specify number of chat completion choices to generate for input message. Is this intended? If yes, why? If no, would you change it, or how could I specify multiple chat completion choices to generate?
I have: 2.0.0-beta.5
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-dotnet/issues/138","Unable to send an image from local file","2024-07-27T01:40:52Z","Closed issue","No label","Hello,
 Im trying to prompt an image from a local file, but each time I keep getting error ""image uri too long"",
 i've tried:
Converted the image to a base64 string.
Created a data URL (data:image/jpeg;base64,{base64String}).
Attempted to pass the data URL using CreateImageMessageContentPart wrapped in a Uri object.
Directly passed the base64 string without wrapping it.
Using (new uri()) inside image message content part.
the image is inside a folder in my project, under the path:
 ""Assets/1.jpg""
any idea what am I doing wrong ?
Im also trying to use it as a service then use it in a controller so it will be easier for me to maintain when I'll need to. here is my code:
 service
using OpenAI.Chat;


namespace backend.Services
{
    public class OpenAiService
    {
        private readonly ChatClient _chatClient;
        private readonly ChatCompletionOptions _options;

        public OpenAiService(IConfiguration configuration)
        {
            var apiKey = configuration.GetValue<string>(""OpenAI:Key"");
            _chatClient = new ChatClient(""gpt-4"", apiKey);
            _options = new ChatCompletionOptions()
            {
                MaxTokens = 300,
            };
        }

        public async Task<string> ExtractListOfItems()
        {
            var imagePath = Path.Combine(Directory.GetCurrentDirectory(), ""Assets"", ""1.jpg"");
            var base64Image = ConvertImageToBase64(imagePath);
            var dataUrl = $""data:image/jpeg;base64,{base64Image}"";

            var messages = new List<ChatMessage>
            {
                new UserChatMessage(new List<ChatMessageContentPart>
                {
                    ChatMessageContentPart.CreateTextMessageContentPart(""Extract the items from the following image and return a list of items including prices and amount.""),
                    ChatMessageContentPart.CreateImageMessageContentPart(CreateDataUri(dataUrl))
                })
            };

            var completion = await _chatClient.CompleteChatAsync(messages, _options);
            return completion.Value.ToString();
        }

        private static string ConvertImageToBase64(string imagePath)
        {
            var imageBytes = File.ReadAllBytes(imagePath);
            return Convert.ToBase64String(imageBytes);
        }

        private static Uri CreateDataUri(string dataUrl)
        {
            // Wrapping the base64 data URL into a Uri object
            return new Uri(dataUrl);
        }
    }
}

using Microsoft.AspNetCore.Mvc;
using System.Threading.Tasks;
using backend.Services;
using OpenAI;
using OpenAI.Chat;

namespace backend.Controllers;

[ApiController]
[Route(""[controller]"")]
public class OpenAiDemoController : ControllerBase
{
    private readonly OpenAiService _openAiService;

    public OpenAiDemoController(OpenAiService openAiService)
    {
        _openAiService = openAiService;
    }
    
    [HttpPost]
    [Route(""extract-items"")]
    public async Task<IActionResult> CompleteSentence()
    {
        var completion = await _openAiService.ExtractListOfItems();
        return Ok(completion);
    }
    
}

 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-dotnet/issues/136","Address perf issues in current page collection implementation","2024-07-23T00:27:14Z","Open issue","No label","Address the linked comments:
Update paging APIs to use updated SCM types #105 (comment)
Update paging APIs to use updated SCM types #105 (comment)
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-dotnet/issues/135","BUG: MessageContent.FromImageUrl(Uri) incompatible with ""data url""","2024-07-22T22:50:46Z","Open issue","No label","Open uses a custom URL format for providing an image message where the base64 encoded image bytes are provided:
data:image/jpeg;base64,{base64_image}
The .Net Uri constructor has a length limitation.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-dotnet/issues/134","Chat Completion Options Can't Set Tools","2024-07-23T15:50:38Z","Closed issue","No label","Goal
I want to use arrays of tools based on user query parameters.
Working Implementation
Failing (due to lack of setter)
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-dotnet/issues/132","Add 'extra_body' Parameter to ChatCompletionOptions in .NET SDK","2024-07-22T02:10:37Z","Open issue","enhancement","In the OpenAI Python SDK, there is an extra_body parameter available in the Completions.create method, which allows users to add additional JSON properties to the request body. This feature can be very useful in various scenarios, especially when needing to send extra data that is not covered by the predefined parameters.
However, this parameter does not exist in the .NET SDK. Including this parameter would help maintain consistency across SDKs and provide .NET users with the same level of flexibility that Python users have.
Reference to Python SDK Code:
 Here is the implementation in the Python SDK for reference:
https://github.com/openai/openai-python/blob/af8f606b3eef1af99f98929847f1b5baf19171ae/src/openai/resources/chat/completions.py#L76
Suggested Change:
 Add an extra_body property to the ChatCompletionOptions class in the .NET SDK that allows additional JSON properties to be serialized into the request body.
This could look something like:
public class ChatCompletionOptions{
    // Existing properties...

    // New property for additional body parameters
    [JsonProperty]
    public Dictionary<string, object>? ExtraBody { get; set; }

    // Method to include this in the serialized output...}
Please consider adding this feature to improve the SDK's flexibility and usability.
The content you are editing has changed. Please copy your edits and refresh the page.
Tasks
BetaGive feedback
No tasks being tracked yet.
Options
Convert to issue
Toggle completion
Rename
Remove
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-dotnet/issues/131","Unable to upload files, error saying invalid 'purpose' value","2024-08-01T18:59:52Z","Closed issue","No label","openai-dotnet/src/Custom/Files/Internal/InternalFileUploadOptions.cs
 Line 38 in 3e12a62
	 content.Add(Purpose.ToString(),""purpose"");
when uploading files, it doesn't matter what upload purpose I choose, I get an ""invalid purpose"" HTTP 400 error. The OpenAI specs show these values in all lower-case, but the code here simply takes the Enum Value and ToString's it which would result in case like Assistants and not assistants
I'm hypothesizing this is the reason none of my uploads work.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-dotnet/issues/130","StopSequence read only - ChatCompletionOptions","2024-07-18T20:28:58Z","Closed issue","No label","ignore
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-dotnet/issues/126","How can I download a file from OpenAI api","2024-07-17T14:46:44Z","Closed issue","question","How can I enable file downloads from the OpenAI API using the OpenAI NuGet package? I’m currently using version 2.0.0-beta.1.
While the platform indicates this isn’t feasible, I’ve come across code snippets for making HTTP requests to the OpenAI API to retrieve files.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-dotnet/issues/125","I cannot set MaxToken in a .Net framework project (C# language version problem)","2024-08-07T17:13:29Z","Closed issue","No label","Hi there,
Thank you for creating such an amazing library! However, I encountered an issue when using it in a .NET Framework project. I'm unable to set the MaxToken value. This limitation prevents me from configuring it as needed within the .NET Framework environment.
Error (active)	CS8370	Feature 'init-only setters' is not available in C# 7.3. Please use language version 9.0 or greater.
ChatCompletionOptions options = new ChatCompletionOptions(){
    MaxTokens = 2000,
    //Temperature = (float) 0.7};
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-dotnet/issues/123","How can I use openai-dotnet to access local Ollama models?","2024-07-12T11:30:30Z","Open issue","No label","I've tried the access a local ollama instance via openai-dotnet.
 I set the Endpoint in OpenAIClientOptions to http://localhost:11434/
 and set the model name to an existing model.
 But I Always a 404 error.
 Does somebody successfully called a local Ollama model?
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-dotnet/issues/121","Add support for mocking via the OpenAIModelFactory","2024-10-03T01:24:18Z","Closed issue","enhancement","As of version 2.0.0-beta.7, there is no simple way for users to mock service outputs for testing. Consequently, they currently rely on workarounds such as using ModelReaderWriter.Read<T> to create a mock from JSON as illustrated below:
ChatCompletion completion = ModelReaderWriter.Read<ChatCompletion>(BinaryData.FromObjectAsJson(new {
  id = ""1234"",
  choices = new object[]
  {
      new {
          finish_reason = ""stop"",
          index = 0,
          message = new {
              content = ""It's a nice day today!"",
              role = ""assistant""
          }
      }
  },
  created = DateTimeOffset.Now.ToUnixTimeSeconds(),
  model = ""model"",
  system_fingerprint = ""N/A"",
  @object = ""N/A"",
  usage = (object)null}));
To improve the experience around mocking, we plan to fully implement and expose the OpenAIModelFactory static class
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-dotnet/issues/115","AzureOpenAIClient with DefaultOpenAIClient() throws intermittent 401s","2024-07-09T16:19:18Z","Closed issue","No label","I have the following setup as a Singleton using dependency injection:
new AzureOpenAIClient(
    new Uri(endpoint),
    new DefaultAzureCredential(),
    new AzureOpenAIClientOptions(ServiceVersion.V2024_06_01)
);

I then inject this into my application and use this code to chat:
var chatClient = _openAIClient.GetChatClient(deploymentName);
var response = await chatClient.CompleteChatAsync(messages, options);

Intermittently, this call returns a 401. It seems to happen every 30 seconds or so and lasts for 30 seconds, then it goes away. Is there an issue with token management? FYI, this code worked fine before v2.0.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-dotnet/issues/109","Azure openai client","2024-07-03T18:28:36Z","Closed issue","No label","Hi,
 I'm encountering the following error:

 When trying to run the following line of code:
ChatCompletion completion = await _chatClient.CompleteChatAsync(
            [
                new SystemChatMessage(request.SystemMessage),
                new UserChatMessage(request.Query)
            ],
            chatCompletionsOptions
        );

chatCompletionsOptions is built with this datasource:
 var chatExtensionConfiguration = new AzureSearchChatDataSource()
            {
                Endpoint = _config.SearchServiceEndpoint,
                IndexName = _config.SearchIndex,
                Filter = request.Filter,
                FieldMappings = new DataSourceFieldMappings()
                {
                    ContentFieldNames = { ""content_with_image_summary"" },
                    FilepathFieldName = ""filepath"",
                    TitleFieldName = ""title"",
                    ContentFieldSeparator = ContentFieldSeperator,
                    UrlFieldName = ""url"",
                    VectorFieldNames = { ""content_with_image_summary_vector"" }
                },
                Authentication = DataSourceAuthentication.FromSystemManagedIdentity(),
                VectorizationSource =
                    DataSourceVectorizer.FromEndpoint(_config.EmbeddingEndpoint,
                        DataSourceAuthentication.FromSystemManagedIdentity()),
                InScope = true,
                RoleInformation = request.SystemMessage,
                QueryType = dataSourceQueryType,
                SemanticConfiguration = SemanticConfiguration
            };

Creating options like so:
      ChatCompletionOptions chatCompletionsOptions = new ChatCompletionOptions
      {
          Temperature = 0,
          MaxTokens = 800,
      };
      chatCompletionsOptions.AddDataSource(chatExtensionConfiguration);

These are the packages' versions I'm working with:
  ```

 ``` The issues started after updating the Azure.Identity package which prompted us to update the Azure.AI.OpenAI to the new major version. The issue is not consistent and sometimes fails or passes for the same inputs, it even succeeds for the same input that failed within the same run. I'd appreciate any help you may provide! 
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-dotnet/issues/108","OpenAIFilePurpose ToString() Equality","2024-07-03T09:39:38Z","Open issue","No label","When using the structs, i.e.: OpenAIFilePurpose and comparing them, the comparison is not invariant or ignoring case, this creates a problem when trying to use them in common switch case scenarios.
Tests bellow Fail when attempted.
[Test][TestCase(""AssIsTants"", ""assistants"")][TestCase(""Assistants"", ""assistants"")][TestCase(""assistants"", ""assistants"")]public void FilePurposeToStringShouldInvariantiblyMatch(string purposeInstance, string purposeCompare){
    var filePurpose = new OpenAIFilePurpose(purposeInstance);

    Assert.That(filePurpose.ToString(), Is.EqualTo(purposeCompare));}

[Test]public void FilePurposeShouldWorkInSwitchCases(){
    var filePurpose = OpenAIFilePurpose.Assistants.ToString();

    switch (filePurpose)
    {
        case nameof(OpenAIFilePurpose.Assistants):
            Assert.Pass();
            break;
        default:
            Assert.Fail();
            break;
    }}
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-dotnet/issues/104","Allow multiple granularities in AudioTranscriptionOptions","2024-07-01T20:58:16Z","Closed issue","No label","As per API definition, the granularities are multiple, but in current design we can only have one or another.
https://platform.openai.com/docs/api-reference/audio/createTranscription#audio-createtranscription-timestamp_granularities
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-dotnet/issues/103","BUG: include_usage implementation doesn't seem to work.","2024-07-01T23:42:43Z","Closed issue","No label","This feature seems to be switched on to true by default in the OpenAI client (correct?). However, the StreamingChatCompletionUpdate.Usage property is always null. This appears to be a bug.
Before CompleteChatStreamingAsync(messages, options) is called when I inspect the options object, Stream=null and StreamOptions.IncludeUsage=true. However, immediately after CompleteChatStreamingAsync(messages, options) is called when I inspect the options object, Stream=true and SteamOptions is null.
Client version: Azure OpenAI
 Azure client version: 2.0.0 Beta 2
 OpenAI Client version: implicit (>= 2.0.0-beta.5)
 OpenAI API version option: AzureOpenAIClientOptions.ServiceVersion.V2024_06_01
 OpenAI API provider : Azure OpenAI
 Model name: GPT-4o
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-dotnet/issues/102","Can't get the Audio Transcription to work in Unity C#","2024-07-27T01:43:59Z","Closed issue","question","Hello dear openai-dotnet Community.
 Im currently working on a OpenAI integration in my VR-Unity Project.
 I was previously working with Azure SST & TTS and want to change it all to the OpenAI-API.
Even trying out the simple Example:
 `public async Task Example02_SimpleTranscriptionAsync()
 {
 AudioClient client = new(""whisper-1"", Environment.GetEnvironmentVariable(""OPENAI_API_KEY""));
    string audioFilePath = Path.Combine(""Assets"", ""audio_houseplant_care.mp3"");

    AudioTranscription transcription = await client.TranscribeAudioAsync(audioFilePath);

    Console.WriteLine($""{transcription.Text}"");
}`

it will give me this error:
Error: boundary
 UnityEngine.Debug:Log (object)
 SpeechToTextManager/<Example02_SimpleTranscriptionAsync>d__14:MoveNext () (at Assets/02-KiChatGPT/SpeechToTextManager.cs:70)
 System.Runtime.CompilerServices.AsyncTaskMethodBuilder:Start<SpeechToTextManager/<Example02_SimpleTranscriptionAsync>d__14> (SpeechToTextManager/<Example02_SimpleTranscriptionAsync>d__14&)
 SpeechToTextManager:Example02_SimpleTranscriptionAsync ()
 SpeechToTextManager:StartSpeechToText () (at Assets/02-KiChatGPT/SpeechToTextManager.cs:47)
 NewOpenAIController:Start () (at Assets/02-KiChatGPT/NewOpenAIController.cs:57)
I would be very grateful if somebody could help me with it.
 Greetings,
 Benjamin
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-dotnet/issues/101","Does the Library currently support Audio and Video conversation APIs with direct streaming?","2024-07-01T23:50:23Z","Closed issue","No label","Does the Library currently support Audio and Video conversation APIs with direct Input / Output streaming for Audio & Video prompts
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-dotnet/issues/99","Parsing the OpenAI messages json","2024-09-22T06:40:53Z","Closed issue","documentation","Hi,
I would like to use this SDK to Parse the messages Json and then iterate it and access the different fields
For example, can I do this?
 var messages = """"""
     [
         {
               ""role"": ""system"",
               ""content"": ""You are an AI assistant...""
         },
         {
               ""role"": ""user"",
               ""content"": ""some user question"",
         }
     ]
     """""";

List<ChatMessage> p = ParseChatRequest(messages);
foreach (var message in p)
{
    Print($""{p.role}, {p.content}"").
}

 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-dotnet/issues/96","FileUpload and chunk strategy on the Vector store","2024-06-28T20:14:31Z","Open issue","No label","Hi, I'm uploading a File using the FileClient (all works fine), which creates a temporary vector store in the background.
 Is it possible to set the chunk size and overlap on the temporary vector store?.
OR do you have to create a VectorStore and upload your file into it?
                     FileClient fileClient = openAIClient.GetFileClient();
                    byte[] fileBytes = Convert.FromBase64String(base64Content);

                    using (MemoryStream document = new MemoryStream(fileBytes))
                    {
                        document.Seek(0, SeekOrigin.Begin);
                        
                        OpenAIFileInfo pdfFile = await fileClient.UploadFileAsync(
                            document,
                            fileIdentifier,
                            
                            FileUploadPurpose.Assistants);

                        var result = FileUploadResult.SuccessResult<FileUploadResult>();
                        result.FileId = pdfFile.Id;
                        return result;
                    }


 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-dotnet/issues/95","Exceeding rate limits results in a retry which ends with 401 Unauthorized","2024-06-29T07:55:11Z","Closed issue","No label","We have a rather big prompt, and a small rate limit of 1000 tokens/minute.
 Due to that combination we can invoke the OpenAPI endpoint only once every minute.
If we exceed that we get back such an exception:
""message"": ""Service request failed.\nStatus: 401 (Unauthorized)\n"",
""stackTrace"": ""   at Azure.AI.OpenAI.ClientPipelineExtensions.ProcessMessageAsync(ClientPipeline pipeline, PipelineMessage message, RequestOptions options)\n   at Azure.AI.OpenAI.Chat.AzureChatClient.CompleteChatAsync(BinaryContent content, RequestOptions options)\n   at OpenAI.Chat.ChatClient.<>c__DisplayClass8_0.<<CompleteChatStreamingAsync>g__getResultAsync|0>d.MoveNext()\n--- End of stack trace from previous location ---\n   at OpenAI.Chat.AsyncStreamingChatCompletionUpdateCollection.AsyncStreamingChatUpdateEnumerator.CreateEventEnumeratorAsync()\n   at OpenAI.Chat.AsyncStreamingChatCompletionUpdateCollection.AsyncStreamingChatUpdateEnumerator.System.Collections.Generic.IAsyncEnumerator<OpenAI.Chat.StreamingChatCompletionUpdate>.MoveNextAsync()\n   at Sofia.Common.DigitalAssistantModule.Clients.OpenAi.OpenAiClient.GetChatCompletionsStream(String text, Language language, OpenApiUseCaseOption options, CancellationToken cancellationToken, String callerMemberName)+MoveNext() in /src/Sofia.Common.DigitalAssistantModule/Clients/OpenAi/OpenAiClient.cs:line 125\n   at Sofia.Common.DigitalAssistantModule.Clients.OpenAi.OpenAiClient.GetChatCompletionsStream(String text, Language language, OpenApiUseCaseOption options, CancellationToken cancellationToken, String callerMemberName)+MoveNext() in /src/Sofia.Common.DigitalAssistantModule/Clients/OpenAi/OpenAiClient.cs:line 125\n   at Sofia.Common.DigitalAssistantModule.Clients.OpenAi.OpenAiClient.GetChatCompletionsStream(String text, Language language, OpenApiUseCaseOption options, CancellationToken cancellationToken, String callerMemberName)

We investigated deeper and the truth is that at first there is a 429 (due to rate limits), then I assume the client retries, which results in a 401, and then this exception.
It leads to very wrong investigations, as you think you have a 401, when it's actually a rate limiting issue.
 Now the big question would be why upon retrying it becomes unauthorized.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-dotnet/issues/94",".CompleteChatAsync freezes without any exceptions. non-async version of the method works fine","2024-06-28T09:31:05Z","Open issue","No label","Hi. When our application is deployed on aws, the .CompleteChatAsync method stops working. Locally everything works fine. The timeout is configured
    public class OpenAiService
    {
        private readonly string apiKey = ConfigurationManager.AppSettings[""openai.apikey""];
        private readonly string model = ConfigurationManager.AppSettings[""openai.model""];
        private readonly float temperature = Convert.ToSingle(ConfigurationManager.AppSettings[""openai.temperature""]);
        private readonly int timeout = Convert.ToInt32(ConfigurationManager.AppSettings[""openai.timeout""]);
        private ChatClient client;

        public OpenAiService()
        {
            var options = new OpenAIClientOptions()
            {
                NetworkTimeout = TimeSpan.FromSeconds(timeout),
            };
            client = new ChatClient(model, apiKey, options);

            ServicePointManager.SecurityProtocol |= SecurityProtocolType.Tls11;
            ServicePointManager.SecurityProtocol |= SecurityProtocolType.Tls12;
        }

        public async Task<string> SendRequestToOpenAiAsync(OpenAI.Chat.ChatMessage[] messages)
        {
            try
            {
                var completion = await client.CompleteChatAsync(messages);

                return completion.Value.Content[0].Text;
            }
            catch (Exception ex)
            {
                Log.SaveLog($""OPENAI request error occurred:"");
                Log.SaveLog(ex.Message + ex.StackTrace + ex.Source);
                
                return null;
            }
        }
    }


but still no exceptions at all. The application just freezes.
When switched to .CompleteChat, everything starts working when deployed.
2024/06/28 09:21:19   Running start turn for turn 145163
2024/06/28 09:21:21   1
2024/06/28 09:21:26   2
2024/06/28 09:21:26   Finished running start turn for turn 145163
2024/06/28 09:21:27   Successfully sent email ***
after changing CompleteChat to CompleteChatAsync
2024/06/28 09:23:47   Running start turn for turn 145163
2024/06/28 09:23:49   1

 The text was updated successfully, but these errors were encountered: 
👍1
maryamariyan reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/openai-dotnet/issues/90","Allow ApiKey to be optional in the OpenAIClient constructor.","2024-09-03T01:21:20Z","Closed issue","No label","This will allow the OpenAIClient be used against custom OpenAI Gateways that intercept and inject the apikey, in the pipeline without having to provide a fake key on the caller code.
This also allows the client to be used against other OpenAI compatible endpoints that don't require ApiKey to be sent in the header.
openai-dotnet/src/Custom/OpenAIClient.cs
 Line 58 in 0b97311
	publicOpenAIClient(ApiKeyCredentialcredential,OpenAIClientOptionsoptions=null)
Consider not having an empty check or allow us to use a NullCredential.Instance in the System.ClientModel package.
https://github.com/Azure/azure-sdk-for-net/blob/d1cac0ba5673ddcb999846d560c13f71f7b259fe/sdk/core/System.ClientModel/src/Convenience/ApiKeyCredential.cs#L64

 The text was updated successfully, but these errors were encountered: 
👍2
JadynWong and SongPing reacted with thumbs up emoji
All reactions
👍2 reactions"
"https://github.com/openai/openai-dotnet/issues/89","Unable to mock CompleteChatAsync method","2024-07-12T06:32:55Z","Closed issue","No label","How can I mock response from CompleteChatAsync method? using 2.0.0-beta.3
 Content property in ChatCompletion cannot be overriden
 is that's even possible, can you please provide an example
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-dotnet/issues/88","Access to all public properties from a service point of view","2024-06-25T01:30:12Z","Open issue","No label","Our .NET service wants to receive OpenAI REST payloads and then hub the request to different specific service implementations. We were using the Azure OpenAI API (which is now deprecated for this project).
 That library allows us to process every property on the chat request and also construct chat responses.
 For some reason the code generation on this project wants to hide everything possible targeting only the client scenario.
 We want to use this project but its almost impossible, one example:
public partial class ChatCompletionOptions
 {
 // CUSTOM:
 // - Made internal. This value comes from a parameter on the client method.
 // - Added setter.
 /// 

 /// A list of messages comprising the conversation so far. Example Python code.
 /// Please note is the base class. According to the scenario, a derived class of the base class might need to be assigned here, or this property needs to be casted to one of the possible derived classes.
 /// The available derived classes include , , , and .
 /// 

 [CodeGenMember(""Messages"")]
 internal IList Messages { get; set; }
Our service can't access the Messages property, but in theory we can deserialize the payload.
Bottom line, we are forced to use the Betalgo OpenAI.
Any plan to support a service scenario?
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-dotnet/issues/87","Assistant API how to properly handle hanging run?","2024-07-27T01:43:00Z","Closed issue","question","I develop an app which utilizes assistant API with custom function call.
The problem I am having is that if user has two clients opened with the same thread. One of the clients create a run, which hangs in function call. Then the next client starts constantly failing at
var message = assistantClient.CreateMessage(threadInfo, MessageRole.User, new[] { MessageContent.FromText(newText) },
with
ClientResultException: HTTP 400 (invalid_request_error: )

Can't add messages to thread_4yYjTyrns5kqKcM4XZNw75wz while a run run_kdnQuv6LVnk1dzoHpTynhnFR is active.
OpenAI.ClientPipelineExtensions.ProcessMessage (System.ClientModel.Primitives.ClientPipeline pipeline, System.ClientModel.Primitives.PipelineMessage message, System.ClientModel.Primitives.RequestOptions options) (at <9ae513c2b2b94747af8c7391523e6f61>:0)
OpenAI.Assistants.InternalAssistantMessageClient.CreateMessage (System.String threadId, System.ClientModel.BinaryContent content, System.ClientModel.Primitives.RequestOptions options) (at <9ae513c2b2b94747af8c7391523e6f61>:0)
OpenAI.Assistants.AssistantClient.CreateMessage (System.String threadId, System.ClientModel.BinaryContent content, System.ClientModel.Primitives.RequestOptions options) (at <9ae513c2b2b94747af8c7391523e6f61>:0)
OpenAI.Assistants.AssistantClient.CreateMessage (System.String threadId, OpenAI.Assistants.MessageRole role, System.Collections.Generic.IEnumerable`1[T] content, OpenAI.Assistants.MessageCreationOptions options, System.Threading.CancellationToken cancellationToken) (at <9ae513c2b2b94747af8c7391523e6f61>:0)
OpenAI.Assistants.AssistantClient.CreateMessage (OpenAI.Assistants.AssistantThread thread, OpenAI.Assistants.MessageRole role, System.Collections.Generic.IEnumerable`1[T] content, OpenAI.Assistants.MessageCreationOptions options) (at <9ae513c2b2b94747af8c7391523e6f61>:0)
...

And the other instance might hang forever on waiting for function call return. IDK why.
Opening the previous thread is important for the app. Thus I probably have to handle running runs somehow? There are many ways how to actually do this, but how to correctly handle it without disturbing the other running instance work?
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-dotnet/issues/86","Stream a PDF into a File","2024-06-24T16:03:17Z","Closed issue","bug,duplicate","Hi, I'm trying to upload a file and then use it in an Assistant, the UploadFileAsync() throws a Object reference exception,
 document is loading ok, size in stream looks correct.
I tried with or without seeking the stream back to the beginning, just wondering if I'm doing this correctly, or off track.
                    FileStream document = File.Open(@""c:\temp\sample.pdf"", FileMode.Open);
                    document.Seek(0, SeekOrigin.Begin);
                    OpenAIClient openAIClient = new(""12345"");
                    
                    FileClient fileClient = openAIClient.GetFileClient();
                    OpenAIFileInfo pdfFile = await fileClient.UploadFileAsync(
                        document,
                        ""sample.pdf"",
                        FileUploadPurpose.Assistants);

   at OpenAI.OpenAIClient.<>c__DisplayClass21_0.<CreateAddCustomHeadersPolicy>b__0(PipelineMessage message)
   at OpenAI.GenericActionPipelinePolicy.<ProcessAsync>d__3.MoveNext()
   at OpenAI.GenericActionPipelinePolicy.<ProcessAsync>d__3.MoveNext()
   at System.ClientModel.Primitives.ClientPipeline.<SendAsync>d__14.MoveNext()
   at OpenAI.ClientPipelineExtensions.<ProcessMessageAsync>d__0.MoveNext()
   at System.Threading.Tasks.ValueTask`1.get_Result()
   at OpenAI.Files.FileClient.<UploadFileAsync>d__21.MoveNext()
   at OpenAI.Files.FileClient.<UploadFileAsync>d__3.MoveNext()

 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-dotnet/issues/84","Generating embedding with EmbeddingClient causes throw reference exception for 2.0.0-beta.6","2024-06-24T16:03:40Z","Closed issue","bug","There is a crash when trying to generate an embedding, both synchronous and async methods. This only occurs for 2.0.0-beta.6. Downgrading to 2.0.0-beta.5 is a workaround.
Minimal code repro:
using OpenAI.Embeddings;

EmbeddingClient client = new(model: ""text-embedding-3-small"", Environment.GetEnvironmentVariable(""OPENAI_API_KEY""));
string description = ""Foo"";
Embedding embedding = client.GenerateEmbedding(description);
ReadOnlyMemory<float> vector = embedding.Vector;
EmbeddingGenerationOptions options = new() { Dimensions = 512 };

Embedding embedding = client.GenerateEmbedding(description, options);

Stack:
System.NullReferenceException: Object reference not set to an instance of an object.
   at OpenAI.OpenAIClient.<>c__DisplayClass21_0.<CreateAddCustomHeadersPolicy>b__0(PipelineMessage message)
   at OpenAI.GenericActionPipelinePolicy.ProcessAsync(PipelineMessage message, IReadOnlyList`1 pipeline, Int32 currentIndex)
   at OpenAI.GenericActionPipelinePolicy.ProcessAsync(PipelineMessage message, IReadOnlyList`1 pipeline, Int32 currentIndex)
   at System.ClientModel.Primitives.ClientPipeline.SendAsync(PipelineMessage message)
   at OpenAI.ClientPipelineExtensions.ProcessMessageAsync(ClientPipeline pipeline, PipelineMessage message, RequestOptions options)
   at OpenAI.Embeddings.EmbeddingClient.GenerateEmbeddingsAsync(BinaryContent content, RequestOptions options)
   at OpenAI.Embeddings.EmbeddingClient.GenerateEmbeddingAsync(String input, EmbeddingGenerationOptions options, CancellationToken cancellationToken)

 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-dotnet/issues/82","Model class design is inconvenient - make them POCOs","2024-08-07T17:17:12Z","Closed issue","enhancement","I gave feedback on the serialization approach negatively impacting usability in the greater, .NET ecosystem. Here is some more:
Perhaps there is a design intention for the init only setters? Why can't I read an assistant creation definition I persisted and then modify these values? To me, it just makes the library hard to use. I don't see the upside. As it is, you might as well just have each of these properties as arguments on the create method which would certainly make the values immutable.
namespace OpenAI.Assistants
 {
 public partial class AssistantCreationOptions
 {
 internal IDictionary<string, BinaryData> _serializedAdditionalRawData;
    internal AssistantCreationOptions(string model, string name, string description, string instructions, IList<ToolDefinition> tools, ToolResources toolResources, IDictionary<string, string> metadata, float? temperature, float? nucleusSamplingFactor, AssistantResponseFormat responseFormat, IDictionary<string, BinaryData> serializedAdditionalRawData)
    {
        Model = model;
        Name = name;
        Description = description;
        Instructions = instructions;
        Tools = tools;
        ToolResources = toolResources;
        Metadata = metadata;
        Temperature = temperature;
        NucleusSamplingFactor = nucleusSamplingFactor;
        ResponseFormat = responseFormat;
        _serializedAdditionalRawData = serializedAdditionalRawData;
    }
    public string Name { get; init; }
    public string Description { get; init; }
    public string Instructions { get; init; }
    public IDictionary<string, string> Metadata { get; }
    public float? Temperature { get; init; }
}

}
Please simplify the implementation of all of the model classes and remove these arbitrary blockers.
 The text was updated successfully, but these errors were encountered: 
👍4
RogerBarreto, JadynWong, janaka, and pmacapal reacted with thumbs up emoji
All reactions
👍4 reactions"
"https://github.com/openai/openai-dotnet/issues/79","AssistantClient.CreateMessage() doesn't support using the Assistant role","2024-06-24T16:04:24Z","Closed issue","bug","This capability was added to the spec just a few months ago: openai/openai-openapi@
9fcd900
The implementation currently hard-codes the User role (which made sense prior to the above) and needs to support creating an Assistant message.
 The text was updated successfully, but these errors were encountered: 
👍1
josuefuentesdev reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/openai-dotnet/issues/78","Assistant with Multiple Vector Store Ids?","2024-06-21T14:37:38Z","Closed issue","question","Hello,
 I noticed that when creating the Assistant you could pass in a list of VectorStoreIds? At the moment however we can only pass in one VectorStoreId per Assistant. Does that mean there are plans for an Assistant to use multiple Vector Stores in the future?
Assistant assistant = await assistantClient.CreateAssistantAsync(
    model: ""gpt-4"",
    new AssistantCreationOptions()
    {
        Name = ""Assistant v2"",
        Tools =
                {
                    new FileSearchToolDefinition(),
                },
        ToolResources = new()
        {
            FileSearch = new()
            {
                VectorStoreIds = [vs1.Id] // can support multiple ids in the future?
            }
        },
    });

 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-dotnet/issues/77","CreateThreadAndRunAsync does not use RunCreationOptions.AdditionalInstructions","2024-06-20T13:19:27Z","Open issue","bug","Calling CreateThreadAndRunAsync with a RunCreationOptions parameter that sets AdditionalInstructions does not set that on the actual run. Creating the Thread and Run separately works correctly.
Package version: OpenAI 2.0.0-beta.5.
In both of the following examples, the Instructions on my assistant are ""Be serious"".
The following code results in a threadRun with Instructions set to “Be serious”:
var threadRun = await assistantClient.CreateThreadAndRunAsync(assistant, new ThreadCreationOptions
{
    InitialMessages =
    {
        new ThreadInitializationMessage(
            new List<MessageContent>
            {
                request.InitialMessage
            })
    },}, new RunCreationOptions
{
    AdditionalInstructions = ""You need to be funny.""});
The following code results in a threadRun with Instructions set to “Be serious You need to be funny.”:
var thread = await assistantClient.CreateThreadAsync(new ThreadCreationOptions
{
    InitialMessages =
    {
        new ThreadInitializationMessage(
            new List<MessageContent>
            {
                request.InitialMessage
            })
    },}, cancellationToken);

var threadRun = await assistantClient.CreateRunAsync(thread.Value.Id, assistant.Id, new RunCreationOptions
{
    AdditionalInstructions = ""You need to be funny.""}, cancellationToken);
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-dotnet/issues/74","Is it possible to use a Proxy-Server?","2024-06-20T15:52:59Z","Closed issue","No label","I wonder if there is an easy way to use a proxy server for connecting to the API. In Azure.AI.OpenAI, it is possible via HttpClientTransport in Azure.Core.Pipeline, and other libraries (such as Semantic Kernel) allow specifying a custom HttpClient instance. But I can't find a way to use a similar method with this library.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-dotnet/issues/73","add tokenizer ,","2024-06-19T09:46:59Z","Open issue","enhancement","var chatMessages= new ChatMessage[] {
 ChatMessage.CreateUserMessage(cont)
 }
int tokenCount=tokenizer (chatMessages); //
ChatCompletion completion = await _sdk.GetChatClient(""gpt-4"")
 .CompleteChatAsync(
 new ChatMessage[] {
 ChatMessage.CreateUserMessage(cont)
 }
 , new OpenAI.Chat.ChatCompletionOptions
 {
 MaxTokens = 8192 - tokenCount
 }
 );
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-dotnet/issues/72","When I upload a file with a Chinese txt format or other format in the file name, it prompts that the uploaded file does not support it","2024-06-19T21:37:21Z","Closed issue","bug","When I upload a file with a Chinese txt format or other format in the file name, it prompts that the uploaded file does not support it


 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-dotnet/issues/71","Intent of accepting additional AudioFileName parameter for overloaded TranscribeAudio and TranslateAudio APIs which already accept an AudioStream input parameter","2024-06-18T07:30:21Z","Open issue","question","Hello @trrwilson,
Is there a specific Intent of accepting additional AudioFileName parameter for overloaded TranscribeAudio and TranslateAudio APIs which already accept an AudioStream input parameter?
public virtual ClientResult TranscribeAudio(Stream audio, string audioFilename, AudioTranscriptionOptions options = null, CancellationToken cancellationToken = default(CancellationToken))
public virtual async Task<ClientResult> TranslateAudio(Stream audio, string audioFilename, AudioTranslationOptions options = null, CancellationToken cancellationToken = default(CancellationToken))
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-dotnet/issues/69","Using message file attachments with assistants","2024-06-18T01:23:16Z","Open issue","question","Are user-uploaded attachments supported yet using assistants 2.0?
Our use-case is that a user uploads a file and asks a question about the text content of the file, and the AI model produces a result based on the text content of the file and the user's question.
I've been going through the provided examples and sifting through the code, but I don't see a way of referencing/including uploaded files in an assistant thread message. I only see examples of uploading files to a newly created assistant, or using images as message content, neither of which are what we are trying to do.
...
RunCreationOptions runCreationOptions = new RunCreationOptions(){
    InstructionsOverride = ""Sample role"",
    ModelOverride = ""gpt-4o""}

runCreationOptions.ToolsOverride.Add(new FileSearchToolDefinition());

ThreadCreationOptions threadCreationOptions = new(){
    InitialMessages =
    {
        new ThreadInitializationMessage(new List<MessageContent>
        {
            ""Hello, assistant! Please tell me something about the contents of this file"",
            //MessageContent.FromImageFileId(pictureOfAppleFile.Id), // don't need this
            //MessageContent.FromImageUrl(linkToPictureOfOrange), // don't need this
            // Is there a different way to get it to reference a file that is not an image?
        })
    },};

ThreadRun threadRun = await _assistantsClient.CreateThreadAndRunAsync(_assistantId, threadCreationOptions, runCreationOptions)
...
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-dotnet/issues/68","Missing support for OrganizationID and ProjectID","2024-06-21T15:47:04Z","Closed issue","enhancement","When I generate an API key, it requires me to use an OrganizationID and ProjectID in conjunction with the API key. There appears to be no mechanism to specify these when creating a client.
https://platform.openai.com/docs/api-reference/authentication
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-dotnet/issues/67","OpenAI.Assistants.AssistantResponseFormat is not deserializable","2024-06-17T21:26:01Z","Open issue","No label","Maybe I misunderstand this property. I can set it and read it, but serializing with System.Text.Json doesn't yield a value for this property on serialize, and deserialize fails with:
System.NotSupportedException: Deserialization of types without a parameterless constructor, a singular parameterized constructor, or a parameterized constructor annotated with 'JsonConstructorAttribute' is not supported. Type 'OpenAI.Assistants.AssistantResponseFormat'.
As a simple property with three valid values, the implementation of this property is (to me) exotic and strange, why doesn't it use an enum or really any other normal .NET approach?
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-dotnet/issues/66","Best way to check for breaches of rate limit using the Assistants API?","2024-06-17T13:10:56Z","Open issue","No label","Hi,
What is the recommended approach to leveraging the Assistants API via this SDK and appropriately handling breaches of the TPM rate limits?
I'm using RAG with both GPT-3.5-turbo and GPT-4o models, and given context tokens count towards the rate limit, I'm hitting these semi-frequently. How should I handle this?
Thanks!
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-dotnet/issues/65","Streaming doesn't work properly in Blazor WASM","2024-06-25T15:54:49Z","Closed issue","bug","There is a problem with streaming using IAsyncEnumerable in Blazor WASM. When consuming the stream in a console app or any environment other than Blazor WASM, the items are processed one by one as they are fetched. However, in Blazor WASM, the stream behaves differently, and the response is only available after all items are fetched from the remote resource.
The issue affects both ChatClient.CompleteChatStreamingAsync and AssistantClient.CreateRunStreamingAsync methods (and potentially any other streaming methods).
Steps to reproduce:
Set up a Blazor WASM project.
Use the following code snippet to fetch streaming updates from a remote resource.
Observe the behavior difference compared to a console app.
Code:
var messageContent = MessageContent.FromText(prompt);

var streamingUpdates = assistantClient.CreateRunStreamingAsync(
    thread.Id,
    assistant.Id,
    new()
    {
        AdditionalMessages = { new([messageContent]) }
    });

await foreach (var streamingUpdate in streamingUpdates){
    switch (streamingUpdate.UpdateKind)
    {
        case StreamingUpdateReason.MessageCreated:
            Console.WriteLine($""Message created: {DateTimeOffset.Now:O}"");
            break;

        case StreamingUpdateReason.MessageUpdated when streamingUpdate is MessageContentUpdate messageContent:
            Console.WriteLine($""Message updated: {messageContent.Text} -- {DateTimeOffset.Now:O}"");
            break;
    }}
Expected Behavior:
Each item should be processed as it is fetched from the remote resource, similar to the behavior observed in a console app.
Actual Behavior:
In Blazor WASM, the stream processes all items only after they are completely fetched, rather than one by one.
Environment:
Blazor WASM on .NET 8
OpenAI NuGet package version: 2.0.0-beta.5
Additional Information:
The problem is detailed in this blog post. The behavior discrepancy between Blazor WASM and other environments needs to be addressed to ensure consistent streaming functionality by setting SetBrowserResponseStreamingEnabled(true) and HttpCompletionOption.ResponseHeadersRead on the HttpRequestMessage.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-dotnet/issues/64","finish_reason wrongly set to stop instead of tool_calls when model calls a tool","2024-06-16T10:59:18Z","Open issue","No label","This should be straightforward to reproduce within the context of existing examples.
There is a departure from the documentation: when calling a tool, finish_reason is set to stop instead of tool_calls.
This forces the developer to inspect the Content and ToolCalls properties in order to consume the result properly.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-dotnet/issues/63","Code Generation for Tool Definitions/Dispatching","2024-07-16T03:39:26Z","Closed issue","No label","Prior to the release of this library, I built my own (much more limited) library to access the OpenAI API, but I have a code generator package that automatically builds the JSON for tools, as well as dispatching code to deserialize incoming parameters and serialize output. Is that something that would be appreciated for this? It'd be relatively easy to port over and I'd be happy to put in a PR, but it would have to be a separate NuGet package I believe, due to the way code generators are referenced.
Below is an example usage of the code generator I have in place. I think this is a higher level of abstraction than would be appropriate for this library, but the core tool code generation could be helpful.
public interface TestMixin {
	[Tool(""Ciphers/deciphers a string"")]
	async Task<string> Cipher(
		[Desc(""string"", ""The string to encrypt or decrypt"")] string value
	) {
		Console.WriteLine($""Ciphering '{value}'"");
		return new(value.ToCharArray().Select(s => (char)(s >= 97 && s <= 122 ? s + 13 > 122 ? s - 13 : s + 13 : s >= 65 && s <= 90 ? s + 13 > 90 ? s - 13 : s + 13 : s)).ToArray());
	}}

partial class GptTest : GPT, TestMixin {
	//public override string Model => ""gpt-4-turbo"";
	public override string SystemMessage => ""You are part of an automated test suite for GPT integration. Please utilize the tools accessible to you."";

	[Tool(""reverse"", ""Reverses a given string"")]
	string Reverse(
		[Desc(""The string to reverse"")] string value
	) {
		Console.WriteLine($""Reversing '{value}'"");
		return string.Join("""", value.Reverse());
	}

	[Tool(""register_animals"", ""Registers a set of animals with the system"")]
	bool RegisterAnimals(
		[Desc(""The animals to register"")] IReadOnlyList<string> animals
	) {
		Console.WriteLine($""The list of animals: {string.Join("", "", animals)}"");
		return true;
	}

	[Tool(""register_weather"", ""Informs the user of average weather in a collection of cities"")]
	void ExpectedWeather(
		[Desc(""A set of city names and their expected high temperatures"")] IReadOnlyDictionary<string, float> temps
	) {
		Console.WriteLine($""The cities and temps: {string.Join("", "", temps.Select(x => $""{x.Key} -> {x.Value}""))}"");
	}

	[Tool(""Smile to show your happiness"")]
	void Smile() {
		Console.WriteLine(""GPT is smiling at you"");
	}

	[Tool(""Frown to show your displeasure"")]
	void Frown() {
		Console.WriteLine(""GPT is frowning at you"");
	}}
Generates:
partial class GptTest {
	protected override async Task<JToken> CallTool(string toolName, JToken args) {
		switch(toolName) {
			case ""reverse"": {
				if(args is not JObject argo) return new JValue(""Invalid parameters"");
				try {
					if(argo[""value""] is not JValue __0_v || __0_v.Value is not string __0) throw new ArgumentException(""value"");
					var ret = ((GptTest) this).Reverse(__0);
					return new JValue(ret);
				} catch(ArgumentException e) {
					return new JValue($""Invalid parameter type for '{e.Message}'"");
				}
			}
			case ""register_animals"": {
				if(args is not JObject argo) return new JValue(""Invalid parameters"");
				try {
					if(argo[""animals""] is not JArray __0_a) throw new ArgumentException(""animals"");
					var __0 = __0_a.Select(__1 => {						if(__1 is not JValue __2_v || __2_v.Value is not string __2) throw new ArgumentException(""animals"");						return __2;					}).ToList();
					var ret = ((GptTest) this).RegisterAnimals(__0);
					return new JValue(ret);
				} catch(ArgumentException e) {
					return new JValue($""Invalid parameter type for '{e.Message}'"");
				}
			}
			case ""register_weather"": {
				if(args is not JObject argo) return new JValue(""Invalid parameters"");
				try {
					if(argo[""temps""] is not JArray __0_a) throw new ArgumentException(""temps"");
					var __0 = __0_a.Select(__1 => {						if(__1 is not JObject __1_o || !__1_o.ContainsKey(""key"") || !__1_o.ContainsKey(""value"")) throw new ArgumentException(""temps"");						if(__1_o[""key""] is not JValue __2_v || __2_v.Value is not string __2) throw new ArgumentException(""temps"");						if(__1_o[""value""] is not JValue __3_v) throw new ArgumentException(""temps"");						return (Key: __2, Value: __3_v.ToObject<float>());					}).ToDictionary(__1 => __1.Key, __1 => __1.Value);
					((GptTest) this).ExpectedWeather(__0);
					return new JValue((object) null);
				} catch(ArgumentException e) {
					return new JValue($""Invalid parameter type for '{e.Message}'"");
				}
			}
			case ""Smile"": {
				try {
					((GptTest) this).Smile();
					return new JValue((object) null);
				} catch(ArgumentException e) {
					return new JValue($""Invalid parameter type for '{e.Message}'"");
				}
			}
			case ""Frown"": {
				try {
					((GptTest) this).Frown();
					return new JValue((object) null);
				} catch(ArgumentException e) {
					return new JValue($""Invalid parameter type for '{e.Message}'"");
				}
			}
			case ""Cipher"": {
				if(args is not JObject argo) return new JValue(""Invalid parameters"");
				try {
					if(argo[""string""] is not JValue __0_v || __0_v.Value is not string __0) throw new ArgumentException(""string"");
					var ret = await ((TestMixin) this).Cipher(__0);
					return new JValue(ret);
				} catch(ArgumentException e) {
					return new JValue($""Invalid parameter type for '{e.Message}'"");
				}
			}
			default:
				return new JValue($""INVALID TOOL '{toolName}'"");
		}
	}
	
	protected override JArray ToolObj => SToolObj;
	readonly static JArray SToolObj = new JArray {
		new JObject {
			[""type""] = ""function"",
			[""function""] = new JObject {
				[""name""] = ""reverse"",
				[""description""] = ""Reverses a given string"",
				[""parameters""] = new JObject {
					[""type""] = ""object"",
					[""properties""] = new JObject {
						[""value""] = new JObject {
							[""description""] = ""The string to reverse"",
							[""type""] = ""string"",
						},
					},
					[""required""] = new JArray(new JValue(""value""))
				}
			}
		},
		new JObject {
			[""type""] = ""function"",
			[""function""] = new JObject {
				[""name""] = ""register_animals"",
				[""description""] = ""Registers a set of animals with the system"",
				[""parameters""] = new JObject {
					[""type""] = ""object"",
					[""properties""] = new JObject {
						[""animals""] = new JObject {
							[""description""] = ""The animals to register"",
							[""type""] = ""array"",
							[""items""] = new JObject {
								[""type""] = ""string"",
							},
						},
					},
					[""required""] = new JArray(new JValue(""animals""))
				}
			}
		},
		new JObject {
			[""type""] = ""function"",
			[""function""] = new JObject {
				[""name""] = ""register_weather"",
				[""description""] = ""Informs the user of average weather in a collection of cities"",
				[""parameters""] = new JObject {
					[""type""] = ""object"",
					[""properties""] = new JObject {
						[""temps""] = new JObject {
							[""description""] = ""A set of city names and their expected high temperatures"",
							[""type""] = ""array"",
							[""items""] = new JObject {
								[""type""] = ""object"",
								[""properties""] = new JObject {
									[""key""] = new JObject {
										[""type""] = ""string"",
									},
									[""value""] = new JObject {
										[""type""] = ""number"",
									},
								},
							},
						},
					},
					[""required""] = new JArray(new JValue(""temps""))
				}
			}
		},
		new JObject {
			[""type""] = ""function"",
			[""function""] = new JObject {
				[""name""] = ""Smile"",
				[""description""] = ""Smile to show your happiness"",
			}
		},
		new JObject {
			[""type""] = ""function"",
			[""function""] = new JObject {
				[""name""] = ""Frown"",
				[""description""] = ""Frown to show your displeasure"",
			}
		},
		new JObject {
			[""type""] = ""function"",
			[""function""] = new JObject {
				[""name""] = ""Cipher"",
				[""description""] = ""Ciphers/deciphers a string"",
				[""parameters""] = new JObject {
					[""type""] = ""object"",
					[""properties""] = new JObject {
						[""string""] = new JObject {
							[""description""] = ""The string to encrypt or decrypt"",
							[""type""] = ""string"",
						},
					},
					[""required""] = new JArray(new JValue(""string""))
				}
			}
		},
	};
	
	protected override bool HasTools => true;}
 The text was updated successfully, but these errors were encountered: 
👍1
ChristianWeyer reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/openai-dotnet/issues/62","GetAssistantAsync does not return a useful value","2024-07-23T15:33:10Z","Closed issue","No label","AssistantClient.GetAssistantAsync only returns a ClientResult, not a ClientResult<Assistant>. Also, it should take a CancellationToken optional parameter, rather than just a RequestOptions optional parameter.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-dotnet/issues/61","GetRunStepsAsync does not yet take a CancellationToken parameter","2024-06-18T15:30:32Z","Closed issue","No label","I just got the latest NuGet package and I love all the new CancellationToken parameters. GetRunStepsAsync is one of the calls I make where it is missing.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-dotnet/issues/60","OpenAI.Assistants.ToolsOverride is readonly","2024-06-14T23:10:23Z","Closed issue","No label","The ToolsOverride property in OpenAI.Assistants appears to be defined as readonly. I'd like to use the property as I believe it is intended, which is to override the tools of my chosen assistant on a per-run basis.
Perhaps this is merely a fact of the setter/init not being implemented yet, or perhaps this is a mistake. Please add support or update the documentation to better demonstrate how this is to be done.
    //
    // Summary:
    //     A run-specific collection of tool definitions that will override the assistant-level
    //     defaults. If not provided, the assistant's defined tools will be used. Available
    //     tools include:
    //
    //     • code_interpreter - OpenAI.Assistants.CodeInterpreterToolDefinition - works
    //     with data, math, and computer code
    //     • file_search - OpenAI.Assistants.FileSearchToolDefinition - dynamically enriches
    //     an Run's context with content from vector stores
    //     • function - OpenAI.Assistants.FunctionToolDefinition - enables caller-provided
    //     custom functions for actions and enrichment
    [CodeGenMember(""Tools"")]
    public IList<ToolDefinition> ToolsOverride { get; } = new ChangeTrackingList<ToolDefinition>();

 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-dotnet/issues/57","[Tracking] On .NET Framework (net481), request URIs can contain a malformed double question mark (??) for operations with query string parameters","2024-07-16T05:46:03Z","Closed issue","No label","Note: this is benign for direct use of the OpenAI v1 endpoint, as the service ignores the extra ?; the impact of this behavior is limited to alternative destinations (like intermediate API gateways or proxies) that may not be as permissive.
This problem is addressed in the generator by the following and this issue can be resolved as soon as the change is integrated:
Azure/autorest.csharp#4818
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-dotnet/issues/55","detailed Content filter info","2024-06-14T17:05:23Z","Closed issue","No label","When completion is cancelnd in case of contentfilter Version one throes an Azure.RequestFailedException with detailed information as Json stored in Content:
_Azure.RequestFailedException: The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766
Status: 400 (model_error)
ErrorCode: content_filter


Content:
{""error"":{""message"":""The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766"",""type"":null,""param"":""prompt"",""code"":""content_filter"",""status"":400,""innererror"":

{""code"":""ResponsibleAIPolicyViolation"",""content_filter_result"":{""hate"":{""filtered"":false,""severity"":""safe""},""self_harm"":

{""filtered"":false,""severity"":""safe""},""sexual"":{""filtered"":true,""severity"":""high""},""violence"":{""filtered"":false,""severity"":""safe""}}}}}_

Version 2 throws System.ClientModel.ClientResultException without this information
_System.ClientModel.ClientResultException: HTTP 400 (content_filter)
Parameter: prompt

The response was filtered due to the prompt triggering Azure OpenAI's content management policy. 
Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766_


Is it possible to get the Content filter reason in one or an other way?
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-dotnet/issues/51","Is it possible to allow changes to the encoding_format of the embedded options?","2024-07-12T07:57:56Z","Closed issue","documentation","Currently EmbeddingClient fixes the encoding_format value to base64 for better performance.
openai-dotnet/src/Custom/Embeddings/EmbeddingGenerationOptions.cs
 Lines 77 to 82 in 45fc4d7
	// CUSTOM: Made internal. We always request the embedding as a base64-encoded string for better performance.
	/// <summary>
	/// The format to return the embeddings in. Can be either `float` or
	/// [`base64`](https://pypi.org/project/pybase64/).
	/// </summary>
	internalInternalEmbeddingGenerationOptionsEncodingFormat?EncodingFormat{get;set;}
openai-dotnet/src/Custom/Embeddings/Embedding.cs
 Lines 75 to 84 in 45fc4d7
	internalEmbedding(intindex,BinaryDataembeddingProperty,InternalEmbeddingObject@object,IDictionary<string,BinaryData>serializedAdditionalRawData)
	{
	Index=(int)index;
	EmbeddingProperty=embeddingProperty;
	Object=@object;
	_serializedAdditionalRawData=serializedAdditionalRawData;
	
	// Handle additional custom properties.
	Vector= ConvertToVectorOfFloats(embeddingProperty);
	}
It can't be changed, even if I want to use float format. I want to use this client for text-embeddings-inference, which currently does not support the encoding_format parameter.
 This results in the following error
The input is not a valid Base64 string of encoded floats.

I know that encoding_format compatibility would be a better approach in other projects, but a lot of compatible openai api's don't update as fast as they should.
Is it possible to allow users to change the encoding_format value?
 Of course, as the official SDK of OpenAI, I would respect it if it was only compatible with OpenAI.
For now I can serialize it myself using protocol methods.
 The text was updated successfully, but these errors were encountered: 
👍1
Freeesia reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/openai-dotnet/issues/49","String enum types have broken GetHashCodes","2024-07-16T05:45:51Z","Closed issue","bug","Repro:
using OpenAI.Chat;

ChatToolCallKind value1 = new(""value1"");ChatToolCallKind VALUE1 = new(""VALUE1"");

Console.WriteLine(value1.Equals(VALUE1)); // prints true as expected

Console.WriteLine(value1.GetHashCode());
Console.WriteLine(VALUE1.GetHashCode()); // should print same value as above, but doesn't

HashSet<ChatToolCallKind> set = [value1];
Console.WriteLine(set.Contains(VALUE1)); // should print true but prints false
There are over 100 types, most of them generated, with a definition of equality like this:
        [EditorBrowsable(EditorBrowsableState.Never)]
        public override bool Equals(object obj) => obj is VectorStoreBatchFileJobStatus other && Equals(other);
        public bool Equals(VectorStoreBatchFileJobStatus other) => string.Equals(_value, other._value, StringComparison.InvariantCultureIgnoreCase);

        [EditorBrowsable(EditorBrowsableState.Never)]
        public override int GetHashCode() => _value?.GetHashCode() ?? 0;
The rules for GetHashCode state:
If two objects compare as equal, the GetHashCode() method for each object must return the same value.
but here, equality is based on a case-insensitive comparison but hash code is being computed based on case-sensitive. That means two equal values can end up with different hashcodes. That in turn breaks things like dictionary/set lookups, which rely on hash codes for bucketing, and thus even if there's an equal value in the dictionary, it's likely not to be found because of differing hashcodes.
 The text was updated successfully, but these errors were encountered: 
👍2
trrwilson and ArcturusZhang reacted with thumbs up emoji
All reactions
👍2 reactions"
"https://github.com/openai/openai-dotnet/issues/46","Should OpenAIModelFactory be public?","2024-07-12T06:25:19Z","Closed issue","No label","In the initial Azure.Ai.OpenAi versions of this library, we could use AzureOpenAIModelFactory to create objects to use in our mocks, per this issue Azure/azure-sdk-for-net#36462. This was useful when writing tests that mocked out the OpenAIClient.
The new version of that seems to be marked internal instead of public. Is that intentional? If so, what should we be using to create those objects in our mocks?
 The text was updated successfully, but these errors were encountered: 
👍1
joakimriedel reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/openai-dotnet/issues/45","Streaming text to speech?","2024-06-12T17:09:01Z","Closed issue","No label","I see in the official docs at https://platform.openai.com/docs/guides/text-to-speech the following:
The Speech API provides support for real time audio streaming using chunk transfer encoding. This means that the audio is able to be played before the full file has been generated and made accessible.
There's python code for it, apparently, but it seems to be unsupported in this library so far. If it's supported, can you point me there? Otherwise, it would be really nice to have, as the latency with any vaguely long chunk of text for TTS is intolerably long for offering the ability to read back chat replies.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-dotnet/issues/41","Remove AllowUnsafeBlocks once no longer required","2024-06-11T20:36:06Z","Open issue","enhancement","#33 adds AllowUnsafeBlocks into the netstandard2.0 build in support of the polyfill for System.Net.ServerSentEvents. Once that polyfill is removed and a package reference added to System.Net.ServerSentEvents, the use of AllowUnsafeBlocks can be removed.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-dotnet/issues/35","CancellationToken support","2024-06-14T18:09:36Z","Closed issue","No label","Simple, really:
We should be able to pass CancellationTokens to any methods that make API calls.
 The text was updated successfully, but these errors were encountered: 
👍7
thomasdc, joakimriedel, nate-simpson, im-aIex, godefroi, JadynWong, and HavenDV reacted with thumbs up emoji
All reactions
👍7 reactions"
"https://github.com/openai/openai-dotnet/issues/34","LegacyCompletionClient not exposed","2024-06-10T11:13:57Z","Open issue","enhancement","The code for said client exists in this repository. The API still exists.
The gpt-3.5-turbo-instruct, a model that is still supported, requires the legacy completions endpoint to be used.
Therefore, the LegacyCompletionClient should be exposed via a GetLegacyCompletionClient method.
 The text was updated successfully, but these errors were encountered: 
👍2
joakimriedel and elychr reacted with thumbs up emoji
All reactions
👍2 reactions"
"https://github.com/openai/openai-dotnet/issues/31","parallel_tool_calls is missing from generated code","2024-06-14T18:27:19Z","Closed issue","No label","The parallel_tool_calls parameter is included in the official documentation and the specification file.
However, the code generation seems to omit this parameter altogether.
Since the parameter is set to true by default, specific scenarios are less predictable when using function tool calling (e.g., even if only one tool is present, the output can result in the same tool call being repeated with different parameters - which is not always desirable).
 The text was updated successfully, but these errors were encountered: 
👍1
HavenDV reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/openai-dotnet/issues/30","System.ClientModel.ClientResultException: 'HTTP 400 (invalid_request_error: )","2024-06-14T21:04:40Z","Closed issue","bug","I am using the following code in a .NET Standard 2.0 project within a Xamarin.Forms Solution
 string _model = ""gpt-3.5-turbo"";
 _openAI_Client = new ChatClient(model: _model, _apiKey);
 ChatCompletion chatCompletion = await _openAI_Client.CompleteChatAsync(new ChatMessage[] {
 new UserChatMessage(""Deep Reinforcement Learning for Finance"")
 });
however I am receiving the following exception on the execution of CompleteChatAsync()
System.ClientModel.ClientResultException: 'HTTP 400 (invalid_request_error: )
 Parameter: messages.[0].content
 Invalid value for 'content': expected a string, got null.'
 The text was updated successfully, but these errors were encountered: 
👍2
agneszitte and MoienTajik reacted with thumbs up emoji👀1
agneszitte reacted with eyes emoji
All reactions
👍2 reactions
👀1 reaction"
"https://github.com/openai/openai-dotnet/issues/26","Add remaining convenience (strongly typed) client surfaces: Fine Tuning, Batch","2024-06-07T18:33:21Z","Open issue","enhancement","As of beta.3, FineTuningClient and BatchClient only support protocol method use. This mechanism is available for all operations and is a fully valid mechanism to interact with the service, but the binary-in, binary-out pattern isn't nearly as idiomatic to use as convenience method support with strong types.
This issue tracks those remaining scenario clients:
 Convenience layer for BatchClient
 Convenience layer for FineTuningClient
 The text was updated successfully, but these errors were encountered: 
👍3
sscowden, r-Larch, and Swah reacted with thumbs up emoji❤️1
minzdrav reacted with heart emoji
All reactions
👍3 reactions
❤️1 reaction"
"https://github.com/openai/openai-dotnet/issues/24","FineTuningClient is not fully implemented","2024-06-07T18:37:55Z","Closed issue","No label","Hi Dev Team
 Looks like FineTuningClient is created by some tool and nobody has checked it.
 Example:
public virtual async Task<ClientResult> CreateJobAsync(BinaryContent content, RequestOptions options = null)
The content type is BinaryContent. The method should accept the base model, training file, etc: https://platform.openai.com/docs/api-reference/fine-tuning/create
 Please implement FineTuningClient. Thank you!
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-dotnet/issues/22","Installing Openai-dotnet to a .NET Standard project gives error","2024-06-08T06:45:18Z","Closed issue","bug,question","Installing Openai-dotnet package to a .NET Standard 2.0 project gives following error
NU1108: Cycle detected.
 OpenAI -> OpenAI (>= 2.0.0-beta.2).
 Package restore failed. Rolling back package changes for 'OpenAI'.
It is detecting some cyclic dependency
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-dotnet/issues/20","Trimming/NativeAOT support","2024-06-07T06:34:20Z","Open issue","enhancement","According this article: https://learn.microsoft.com/en-us/dotnet/core/deploying/trimming/prepare-libraries-for-trimming?pivots=dotnet-8-0
Now there are such warnings:
1>InternalListHelpers.cs(34,9): Warning IL2091 Trim analysis: OpenAI.InternalListHelpers.GetPageFromProtocol<TInstance,UInternalList>(ClientResult): 'T' generic argument does not satisfy 'DynamicallyAccessedMemberTypes.PublicConstructors', 'DynamicallyAccessedMemberTypes.NonPublicConstructors' in 'System.ClientModel.Primitives.ModelReaderWriter.Read<T>(BinaryData, ModelReaderWriterOptions)'. The generic parameter 'UInternalList' of 'OpenAI.InternalListHelpers.GetPageFromProtocol<TInstance,UInternalList>(ClientResult)' does not have matching annotations. The source value must declare at least the same requirements as those declared on the target location it is assigned to.
1>System.Memory.Data.dll: Warning IL2104 : Assembly 'System.Memory.Data' produced trim warnings. For more information see https://aka.ms/dotnet-illink/libraries

If I use the preview version of System.Memory.Data, the following warnings appear here
1>ModerationClient.cs(77,9): Warning IL2026 Trim analysis: OpenAI.Moderations.ModerationClient.<ClassifyTextInputAsync>d__4.MoveNext(): Using member 'System.BinaryData.FromObjectAsJson<T>(T, JsonSerializerOptions)' which has 'RequiresUnreferencedCodeAttribute' can break functionality when trimming application code. JSON serialization and deserialization might require types that cannot be statically analyzed.
1>ModerationClient.cs(110,9): Warning IL2026 Trim analysis: OpenAI.Moderations.ModerationClient.<ClassifyTextInputsAsync>d__6.MoveNext(): Using member 'System.BinaryData.FromObjectAsJson<T>(T, JsonSerializerOptions)' which has 'RequiresUnreferencedCodeAttribute' can break functionality when trimming application code. JSON serialization and deserialization might require types that cannot be statically analyzed.
1>EmbeddingClient.cs(84,9): Warning IL2026 Trim analysis: OpenAI.Embeddings.EmbeddingClient.<GenerateEmbeddingAsync>d__4.MoveNext(): Using member 'System.BinaryData.FromObjectAsJson<T>(T, JsonSerializerOptions)' which has 'RequiresUnreferencedCodeAttribute' can break functionality when trimming application code. JSON serialization and deserialization might require types that cannot be statically analyzed.
1>EmbeddingClient.cs(120,9): Warning IL2026 Trim analysis: OpenAI.Embeddings.EmbeddingClient.<GenerateEmbeddingsAsync>d__6.MoveNext(): Using member 'System.BinaryData.FromObjectAsJson<T>(T, JsonSerializerOptions)' which has 'RequiresUnreferencedCodeAttribute' can break functionality when trimming application code. JSON serialization and deserialization might require types that cannot be statically analyzed.
1>EmbeddingClient.cs(157,9): Warning IL2026 Trim analysis: OpenAI.Embeddings.EmbeddingClient.<GenerateEmbeddingsAsync>d__8.MoveNext(): Using member 'System.BinaryData.FromObjectAsJson<T>(T, JsonSerializerOptions)' which has 'RequiresUnreferencedCodeAttribute' can break functionality when trimming application code. JSON serialization and deserialization might require types that cannot be statically analyzed.
1>EmbeddingClient.cs(102,9): Warning IL2026 Trim analysis: OpenAI.Embeddings.EmbeddingClient.GenerateEmbedding(String, EmbeddingGenerationOptions): Using member 'System.BinaryData.FromObjectAsJson<T>(T, JsonSerializerOptions)' which has 'RequiresUnreferencedCodeAttribute' can break functionality when trimming application code. JSON serialization and deserialization might require types that cannot be statically analyzed.
1>EmbeddingClient.cs(139,9): Warning IL2026 Trim analysis: OpenAI.Embeddings.EmbeddingClient.GenerateEmbeddings(IEnumerable<String>, EmbeddingGenerationOptions): Using member 'System.BinaryData.FromObjectAsJson<T>(T, JsonSerializerOptions)' which has 'RequiresUnreferencedCodeAttribute' can break functionality when trimming application code. JSON serialization and deserialization might require types that cannot be statically analyzed.
1>EmbeddingClient.cs(175,9): Warning IL2026 Trim analysis: OpenAI.Embeddings.EmbeddingClient.GenerateEmbeddings(IEnumerable<IEnumerable<Int32>>, EmbeddingGenerationOptions): Using member 'System.BinaryData.FromObjectAsJson<T>(T, JsonSerializerOptions)' which has 'RequiresUnreferencedCodeAttribute' can break functionality when trimming application code. JSON serialization and deserialization might require types that cannot be statically analyzed.
1>ModerationClient.cs(93,9): Warning IL2026 Trim analysis: OpenAI.Moderations.ModerationClient.ClassifyTextInput(String): Using member 'System.BinaryData.FromObjectAsJson<T>(T, JsonSerializerOptions)' which has 'RequiresUnreferencedCodeAttribute' can break functionality when trimming application code. JSON serialization and deserialization might require types that cannot be statically analyzed.
1>ModerationClient.cs(126,9): Warning IL2026 Trim analysis: OpenAI.Moderations.ModerationClient.ClassifyTextInputs(IEnumerable<String>): Using member 'System.BinaryData.FromObjectAsJson<T>(T, JsonSerializerOptions)' which has 'RequiresUnreferencedCodeAttribute' can break functionality when trimming application code. JSON serialization and deserialization might require types that cannot be statically analyzed.
1>OpenAIModelFactory.cs(33,9): Warning IL2026 Trim analysis: OpenAI.OpenAIModelFactory.Embedding(ReadOnlyMemory<Single>, Int32): Using member 'System.BinaryData.FromObjectAsJson<T>(T, JsonSerializerOptions)' which has 'RequiresUnreferencedCodeAttribute' can break functionality when trimming application code. JSON serialization and deserialization might require types that cannot be statically analyzed.

 The text was updated successfully, but these errors were encountered: 
All reactions"
