"https://github.com/openai/openai-python/issues/1804","AsyncOpenAI does not do any retries when Exception occurs","2024-10-20T03:08:11Z","Open issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
Hi, please refer to #1803 for details.
To Reproduce
(see that PR)
Code snippets
No response
OS
unrelated
Python version
unrelated
Library version
latest
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1795","Error code: 400 - {'error': {'message': ""Invalid parameter: 'tool_calls' cannot be used when 'functions' are present","2024-10-17T14:18:51Z","Open issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
Error code: 400 - {'error': {'message': ""Invalid parameter: 'tool_calls' cannot be used when 'functions' are present. Please use 'tools' instead of 'functions'."", 'type': 'invalid_request_error', 'param': 'messages.[2].tool_calls', 'code': None}}
To Reproduce
Structure is in the code snippet, after MAssassor answer its question (after using tool and then go back to MAssasor), should be go back to supervisor, but it give above error.

Code snippets
## first read guideline and then go to the different source to get data: def create_team_supervisor(llm, system_prompt, members) -> str:
    """"""An LLM-based router.""""""
    options = [""FINISH""] + members
    function_def = {
        ""name"": ""route"",
        ""description"": ""Select the next role."",
        ""parameters"": {
            ""title"": ""routeSchema"",
            ""type"": ""object"",
            ""properties"": {
                ""next"": {
                    ""title"": ""Next"",
                    ""anyOf"": [
                        {""enum"": options},
                    ],
                },
            },
            ""required"": [""next""], 
        },
    }
    
    prompt = ChatPromptTemplate.from_messages(
        [
            (""system"", system_prompt),
            MessagesPlaceholder(variable_name=""messages""),
            (
                ""system"",
                ""Given the conversation above, who should act next?""
                "" Or should we FINISH? Select one of: {options}"",
            ),
        ]
    ).partial(options=str(options), team_members="", "".join(members))
    return (
        prompt
        | llm.bind_functions(functions=[function_def], function_call=""route"")   
        | JsonOutputFunctionsParser()
    )


class TeamState(TypedDict):
    # A message is added after each team member finishes
    messages: Annotated[List[BaseMessage], operator.add]
    # The team members are tracked so they are aware of
    # the others' skill-sets

    POLICY_NUMBER: str 
    CLAIM_NUMBER: str 
    FILE_PATH: str

    team_members: List[str]
    # Used to route work. The supervisor calls a function
    # that will update this every time it makes a decision
    next: str

    sender:str

def router(state):
    # This is the router
    messages = state[""messages""]
    last_message = messages[-1]
    if not last_message.content:
        # The previous agent is invoking a tool
        # print(state['sender'], 'call_tool')
        return ""call_tool""
    # print(state['sender'], ""CogSupervisor"")
    return 'continue'

graph = StateGraph(TeamState)

## Add nodes to the graphgraph.add_node(""MAssassor"", med_node)
graph.add_node(""BAssassor"", beh_node)
graph.add_node(""Supervisor"", cog_supervisor)
graph.add_node(""call_tool"", tool_node)

graph.add_edge(START, ""Supervisor"")

## Add condition edges to the graph graph.add_conditional_edges(""MAssassor"",
    router,
    {""call_tool"": ""call_tool"", ""continue"": ""Supervisor""},) 
graph.add_conditional_edges(""BAssassor"",
    router,
    {""call_tool"": ""call_tool"", ""continue"": ""Supervisor""},)
graph.add_conditional_edges(
    ""Supervisor"",
    lambda x: x[""next""],
    {""MAssassor"": ""MAssassor"",
     ""BAssassor"": ""BAssassor"", 
     ""FINISH"": END},
)

graph.add_conditional_edges(
    ""call_tool"",
    lambda x: x[""sender""],
    {""MAssassor"": ""MAssassor"",
     ""BAssassor"": ""BAssassor"",
    },
)

chain = graph.compile()
OS
macOS
Python version
Python 3.9.6
Library version
openai 1.51.2
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1794","Structured Outputs via function calling - Descriptions for important keys","2024-10-17T12:24:53Z","Closed issue","No label","Confirm this is a feature request for the Python library and not the underlying OpenAI API.
 This is a feature request for the Python library
Describe the feature or improvement you're requesting
I like the new SDK helper to parse the model's output:
from pydantic import BaseModelfrom openai import OpenAI

client = OpenAI()

class ResearchPaperExtraction(BaseModel):
    title: str
    authors: list[str]
    abstract: str
    keywords: list[str] # How do I add a description for this or any of the parameters? 
    # How does the model know exactly how to use this? will it infer on its own or we specify this in the system message? 

completion = client.beta.chat.completions.parse(
    model=""gpt-4o-2024-08-06"",
    messages=[
        {""role"": ""system"", ""content"": ""You are an expert at structured data extraction. You will be given unstructured text from a research paper and should convert it into the given structure.""},
        {""role"": ""user"", ""content"": ""...""}
    ],
    response_format=ResearchPaperExtraction,
)

research_paper = completion.choices[0].message.parsed
But how do I get the granularity of using manual schema with the SDK objects?
 Mainly adding detailed description for the parameters
{
    ""name"": ""get_weather"",
    ""description"": ""Fetches the weather in the given location"",
    ""strict"": true,
    ""parameters"": {
        ""type"": ""object"",
        ""properties"": {
            ""location"": {
                ""type"": ""string"",
                ""description"": ""The location to get the weather for""
            },
            ""unit"": {
                ""type"": ""string"",
                ""description"": ""The unit to return the temperature in"",
                ""enum"": [""F"", ""C""]
            }
        },
        ""additionalProperties"": false,
        ""required"": [""location"", ""unit""]
    }
}
Snippets taken from the official OpenAI Documentation.
Additional context
Please improve the documentation. How can I submit requests to improve it?
 Also I get no response from the OpenAI Developer Forum, thus I raise an issue here (@RobertCraigie is a good guy :)
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1793","Invalid API key environment variables lead to confusing encoding errors","2024-10-16T01:23:57Z","Open issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
Referring to this issue
When the API key is accidentally set with invalid characters, an error occurs within the HTTP client when openai library attempts to set headers.
While eventually one might deduce the problem from the stack trace, having some sort of upstream ""check"" and more informative error about the API key would seem to help users understand the error. This caused us to spend multiple days trying to find the source of the problem assuming the error was in the message content instead of in the API key.
To Reproduce
Set the API key to a value with non-ASCII characters
Execute a chat completion with the client
Receive encoding error UnicodeEncodeError: 'ascii' codec can't encode characters in position 7-34: ordinal not in range(128)
Code snippets
from openai import OpenAI

########################### CONFIGURATION ###########################

configs = dict(
    api_key = 'здравейздравейздравейздравей',
)

client = OpenAI(**configs)

############################ RUN TEST ############################

message = 'Hello!'completion = client.chat.completions.create(
    model='gpt-4-turbo-preview',
    messages=[
        {
            'role': 'user',
            'content': message
        }
    ]    
)

print(completion.choices[0].message.content)
OS
any
Python version
3.11
Library version
openai v1.44.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1792","How to stream responses from Assistants API? The quickstart example doesn't seem to be working","2024-10-12T01:37:33Z","Open issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
I get ERROR:api.routes:Error in event generator: 'StreamingEventHandler' object has no attribute '_AssistantEventHandler__stream' when I use the example code to enable streaming from Assistants API. Additionaly, The method ""create_and_stream"" in class ""AsyncRuns"" is deprecated.
To Reproduce
Use example code from https://platform.openai.com/docs/assistants/quickstart
from typing_extensions import override
from openai import AssistantEventHandler
 
# First, we create a EventHandler class to define
# how we want to handle the events in the response stream.
 
class EventHandler(AssistantEventHandler):    
  @override
  def on_text_created(self, text) -> None:
    print(f""\nassistant > "", end="""", flush=True)
      
  @override
  def on_text_delta(self, delta, snapshot):
    print(delta.value, end="""", flush=True)
      
  def on_tool_call_created(self, tool_call):
    print(f""\nassistant > {tool_call.type}\n"", flush=True)
  
  def on_tool_call_delta(self, delta, snapshot):
    if delta.type == 'code_interpreter':
      if delta.code_interpreter.input:
        print(delta.code_interpreter.input, end="""", flush=True)
      if delta.code_interpreter.outputs:
        print(f""\n\noutput >"", flush=True)
        for output in delta.code_interpreter.outputs:
          if output.type == ""logs"":
            print(f""\n{output.logs}"", flush=True)
 
# Then, we use the `stream` SDK helper 
# with the `EventHandler` class to create the Run 
# and stream the response.
 
with client.beta.threads.runs.stream(
  thread_id=thread.id,
  assistant_id=assistant.id,
  instructions=""Please address the user as Jane Doe. The user has a premium account."",
  event_handler=EventHandler(),
) as stream:
  stream.until_done()

Code snippets
No response
OS
Docker container: FROM python:3.9
Python version
Python v3.9
Library version
v1.51.2
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1791","How to upload multi-images for description","2024-10-11T04:40:12Z","Open issue","No label","Confirm this is a feature request for the Python library and not the underlying OpenAI API.
 This is a feature request for the Python library
Describe the feature or improvement you're requesting
I'm trying to upload multi-images one time to api for description. But the api library only provides the illustration of the function for single image vision input.Could you provide the way for multiple images uploading?

Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1790","Limits argument can never get passed down?","2024-10-10T22:15:23Z","Open issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
It seems as though when initializing an openAI client like so:
since you cannot pass in the limits argument as a parameter, it will always default to the DEFAULT value here:
openai-python/src/openai/_base_client.py
 Line 1127 in aa68189
	iflimitsisnotNone: 
openai.AsyncClient(())
To Reproduce
Initialize an openai client like so: openai.AsyncClient(())
Try to pass in 'limits' argument or modify the 'http_client' to have custom Limits.
This will not actually be propagated
Code snippets
openai.AsyncClient(())
OS
linux
Python version
3.9.12
Library version
1.50.0
 The text was updated successfully, but these errors were encountered: 
❤️3
yuichiromukaiyama, ymuichiro, and YigitSekerci reacted with heart emoji
All reactions
❤️3 reactions"
"https://github.com/openai/openai-python/issues/1789","AttributeError: 'FileCitation' object has no attribute 'quote'","2024-10-10T19:39:11Z","Open issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
I am on v1.51.2 and seems like the attribute quote has not been fixed since 1.34.0. Do we have any alternative to this?
To Reproduce
Create an assistant with files associated
 ask a question that has filecitation
 check to get the file_citation.quote
Code snippets
No response
OS
Windows
Python version
Python 3.10.11
Library version
openai v1.51.2
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1788","Repeated Error: openai.Completion Not Supported in OpenAI Python >=1.0.0 Despite Multiple Fix Attempts (Python Versions and API Key Methods)","2024-10-10T14:08:18Z","Open issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
I am encountering a recurring error when trying to access openai.Completion. The error message suggests that this is no longer supported in versions of the OpenAI Python library greater than 1.0.0. Despite following the official migration guide and pinning to earlier versions (e.g., openai==0.28), the issue remains unresolved. I have tried various solutions such as:
Downgrading Python from 3.12 to 3.10, and even 3.9
 Adjusting API key access methods (environment variables, hardcoding, etc.)
 Attempting multiple different virtual environments
 Following the migration instructions from the OpenAI repository and discussions, such as using openai migrate.
 No matter what I try, I keep encountering the following error:
vbnet
 Copy code
 You tried to access openai.Completion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.
You can run openai migrate to automatically upgrade your codebase to use the 1.0.0 interface.
Alternatively, you can pin your installation to the old version, e.g. pip install openai==0.28
 Despite using the suggested approach to pin the version (pip install openai==0.28), the issue remains unresolved. Additionally, following the migration guide (#742) didn't yield any success.
To Reproduce
Steps to reproduce the behavior:
Set up a Python virtual environment with Python 3.10.
Install openai using pip install openai==0.27.0 or any version before 1.0.0.
Attempt to use openai.Completion.create() in any script.
The above error consistently occurs.
Code snippets
Code snippetsHere’s the code I used to trigger the error:

pythonCopy codeimport openaiopenai.api_key = 'sk-xxxxxxx'

response = openai.Completion.create(
    engine=""davinci"",
    prompt=""Tell me a joke."",
    max_tokens=50
)
print(response.choices[0].text)
Error output:
vbnetCopy codeYou tried to access openai.Completion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.

You can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface.

Alternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`

What I Tried:
Downgrading the Python version from 3.12 to 3.10.
Using different versions of the OpenAI library (0.27.0, 0.28.0, and 0.29.0).
Setting API keys via environment variables and directly in the code.
Trying the migration tool (openai migrate) but to no avail.
Ensuring all dependencies (like google-api-python-client and pdfplumber) are up to date and properly installed.
OS
macOS Monterey
Python version
Python 3.10.15 (also tried Python 3.12.5)
Library version
openai v1.0.1 (Issue also persists with version 0.27.0 and 0.28.0)
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1786","Moderation Endpoint Schema Mismatch for illicit and illicit_violent fields","2024-10-09T11:00:39Z","Open issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
Description:
 The results returned by the moderation endpoint do not align with the expected schema.
Details:
In the Moderation.categories field (of type Categories), all fields are annotated as bool. However, when the moderation endpoint is called, the fields illicit and illicit_violent return None values instead of True or False.
The same issue occurs with the category_scores field (of type CategoryScores), where all fields are expected to be float. Yet, illicit and illicit_violent are also returned as None.
Expected Behavior:
If the None values for illicit and illicit_violent are expected behavior, these fields should be annotated as optional in the schema.
If this is not expected behavior, the API should be corrected to ensure that these fields return appropriate boolean or float values.
Additional Notes:
 It is surprising that Pydantic does not throw an error for these mismatches and allows None values to be returned. I could not manually create a Categories object with any None value in it.
To Reproduce
Run the moderation endpoint and check response.results[0] categories and category_scores -> illicit field.
Result I'm getting:
response.results: [
    Moderation(
        categories=Categories(
            harassment=False,
            harassment_threatening=False,
            hate=False,
            hate_threatening=False,
            illicit=None,
            illicit_violent=None,
            self_harm=False,
            self_harm_instructions=False,
            self_harm_intent=False,
            sexual=False,
            sexual_minors=False,
            violence=False,
            violence_graphic=False,
            self-harm=False,
            sexual/minors=False,
            hate/threatening=False,
            violence/graphic=False,
            self-harm/intent=False,
            self-harm/instructions=False,
            harassment/threatening=False,
        ),
        category_applied_input_types=None,
        category_scores=CategoryScores(
            harassment=0.000255020015174523,
            harassment_threatening=1.3588138244813308e-05,
            hate=2.8068381652701646e-05,
            hate_threatening=1.0663524108167621e-06,
            illicit=None,
            illicit_violent=None,
            self_harm=9.841909195529297e-05,
            self_harm_instructions=7.693658517382573e-06,
            self_harm_intent=7.031533459667116e-05,
            sexual=0.013590452261269093,
            sexual_minors=0.0031673426274210215,
            violence=0.00022930897830519825,
            violence_graphic=4.927426198264584e-05,
            self-harm=9.841909195529297e-05,
            sexual/minors=0.0031673426274210215,
            hate/threatening=1.0663524108167621e-06,
            violence/graphic=4.927426198264584e-05,
            self-harm/intent=7.031533459667116e-05,
            self-harm/instructions=7.693658517382573e-06,
            harassment/threatening=1.3588138244813308e-05,
        ),
        flagged=False,
    ),
]

Code snippets
import openai

client = openai.OpenAI()
response = client.moderations.create(input=""text"")
print(response.results[0])
OS
macOS
Python version
Python 3.12.4
Library version
openai 1.51.2
 The text was updated successfully, but these errors were encountered: 
👍3
poneill, tongbaojia, and bradddd reacted with thumbs up emoji
All reactions
👍3 reactions"
"https://github.com/openai/openai-python/issues/1785","duration type in TranscriptionVerbose","2024-10-09T08:24:36Z","Open issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
in pyantic model
 duration is set to str
but the example in api
 it returns float type
To Reproduce
openai_client.audio.transcriptions.create
Code snippets
No response
OS
macOS
Python version
Python 3.10.15
Library version
1.51.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1784","Latest version fails to install on Python 3.7","2024-10-09T06:14:59Z","Open issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
Due to dependency conflicts, the latest versions of the library fail to install on Python 3.7.
To Reproduce
Using Python 3.7, attempt to install the openai package:
$ py -3.7-32 -m pip install openai
This fails due to jiter and typing_extensions.
Please either drop support for 3.7 officially (3.8 works) or loosen the dependency constraints.
Code snippets
No response
OS
Windows 11 26100.1882
Python version
3.7.9
Library version
1.51.x
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1782","Set jiter as optional dependency to support pyodide (~3 lines diff)","2024-10-08T08:46:49Z","Open issue","No label","Confirm this is a feature request for the Python library and not the underlying OpenAI API.
 This is a feature request for the Python library
Describe the feature or improvement you're requesting
Pyodide currently don't support jiter. openai-python use it for partial json parsing. But it is just used in 2 lines.
If we move jiter into optional-dependencies, we will be able to use openai-python in pyodide runtime.
Once upon a time, httpx is blocking openai from pyodide too. But that issue is already resolved. The only barrier is jiter now.
About openai, pyodide and httpx
I've checked these issue:
Support for Pyodide #815
Issue with custom transport #960
At that time, openai is not compatible with pyodide because of httpx.
 Now there even exist a pyodide-httpx to patch httpx in pyodide
If we can use openai in pyodide, it will be possible to provide interactive python demos in the browser for prompt engineering frameworks, which I think is a great feature to have.
Additional context
Another way is to use another package to parse partial json. There is a package called partial-json-parser which did the almost same job as jiter.from_json, but also providing more flexibility on specifying which types are allowed to be incomplete. And it keep types too. For the latter one, let me present an example:
from jiter import from_jsonfrom_json(b'{""a"": [1', partial_mode=True)  # {'a': [1]}from_json(b'{""a"": [1.', partial_mode=True)  # {'a': []}
In the example above, tokens increase but parsed value disappeared.
Plus, partial-json-parser's API is consistent among its Python/JavaScript/Go implementations.
I tried a bit to replace jiter by partial_json_parser:
7419b70#diff-08dc4c3c3e8e145eec1fd0b6a4577f0bce73567d4da3460e08dd4c2d34b27915
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1778","Using realtime API with python client","2024-10-07T12:34:58Z","Open issue","No label","Confirm this is a feature request for the Python library and not the underlying OpenAI API.
 This is a feature request for the Python library
Describe the feature or improvement you're requesting
I had fastAPI server running on my application already. I wanted to make it's user interface voice to voice type. On openai's official documentation only steps for node.js is given. I was wondering if I'd be able to reuse my existing code ( fastapi + openai python ) for voice interface too.
Additional context
Realtime API Docs : https://platform.openai.com/docs/guides/realtime
currently not sure how can I use openai-python for realtime API.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1777","openai.AsyncOpenAI not safe when shared across async tests","2024-10-09T12:18:44Z","Closed as not planned issue","No label","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
When sharing AsyncOpenAI across async tests, it's possible to hit openai.APIConnectionError: Connection error.
To Reproduce
https://colab.research.google.com/drive/1icWDkN2iYPl3mxCEwA0-kE7GSSOeM9E8?usp=sharing
Code snippets
import pytestfrom openai import AsyncOpenAI

client = AsyncOpenAI()


@pytest.mark.asyncioasync def test_completion1() -> None:
    await client.chat.completions.create(
        model=""gpt-4o-mini"",
        messages=[{
            ""content"": (
                ""Here is a question, the correct answer to the question, and a proposed""
                "" answer to the question. Please tell me if the proposed answer is""
                "" correct, given the correct answer. ONLY SAY 'YES' OR 'NO'. No other""
                "" output is permitted.\n\nQuestion: What is 25 * 10? \n\nCorrect""
                "" answer: 250 \n\nProposed answer: 250""
            ),
            ""role"": ""user"",
        }],
    )


@pytest.mark.asyncioasync def test_completion2() -> None:
    await client.chat.completions.create(
        model=""gpt-4o-mini"",
        messages=[{
            ""content"": (
                ""Here is a question, the correct answer to the question, and a proposed""
                "" answer to the question. Please tell me if the proposed answer is""
                "" correct, given the correct answer. ONLY SAY 'YES' OR 'NO'. No other""
                "" output is permitted.\n\nQuestion: What is 25 * 10? \n\nCorrect""
                "" answer: 250 \n\nProposed answer: 250""
            ),
            ""role"": ""user"",
        }],
    )
OS
linux
Python version
3.10.12
Library version
1.51.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1776","with multiprocessing, pickle issue with client.beta.chat.completions.parse output","2024-10-18T13:48:06Z","Closed issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
We use multiprocessing to handle the calls. It was working with client.chat.completions.create. Currently, we are trying client.beta.chat.completions.parse with response_format, i got error:
Traceback (most recent call last):
 File ""/anaconda/lib/python3.10/multiprocessing/queues.py"", line 244, in _feed
 obj = _ForkingPickler.dumps(obj)
 File ""/anaconda/lib/python3.10/multiprocessing/reduction.py"", line 51, in dumps
 cls(buf, protocol).dump(obj)
 _pickle.PicklingError: Can't pickle <class 'openai.types.chat.parsed_chat_completion.ParsedChatCompletion[CalendarEvent]'>: attribute lookup ParsedChatCompletion[CalendarEvent] on openai.types.chat.parsed_chat_completion failed
anyone can help ?
Thanks
To Reproduce
from pydantic import BaseModel
from . import get_token_provider
import openai
import asyncio
import pickle

class CalendarEvent(BaseModel):
    name: str
    date: str
    participants: list[str]

def original_test():

    MODEL = ""gpt-4o-2024-08-06""
    ENDPOINT = ""gpt-4o-2024-08-06-endpoint""
    token_provider = get_token_provider()

    client = openai.AsyncAzureOpenAI(
    azure_ad_token_provider=token_provider,
    azure_endpoint=ENDPOINT,
    api_version=""2024-08-01-preview"",
    )

    func = client.beta.chat.completions.parse(
        model=MODEL,
        messages=[
            {""role"": ""system"", ""content"": ""Extract the event information.""},
            {""role"": ""user"", ""content"": ""Alice and Bob are going to a science fair on Friday.""},
        ],
        response_format=CalendarEvent,
        )

    completion = asyncio.run(func)
    pickle.dumps(completion)
    

if __name__ == ""__main__"":
    original_test()

Code snippets
No response
OS
linux
Python version
py3.10
Library version
openai 1.51.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1775","I couldn't upload file an use the one at thread normally via openai library...","2024-10-04T15:18:21Z","Open issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
I uploaded file to vector storage, but I couldn't use the file at threads:
It seems there was an error while trying to search the uploaded files. Could you please try uploading the file again, or let me know if there is a specific file you want me to look into?.
To Reproduce
create vector_store
create assistant
upload file
wait the uploading
attach file to vector store
create thread
create run
wait completing the run
get messages
take run's message
Code snippets
import jsonimport osimport time

from openai import OpenAI

client = OpenAI(api_key=os.getenv(""OPENAI_API_KEY""))

vector_store = client.beta.vector_stores.create(name=""Test"")

assistant = client.beta.assistants.create(
    description=f""Test"",
    model=""gpt-4o"",
    tools=[{""type"": ""file_search""}],
    tool_resources={""file_search"": {""vector_store_ids"": [vector_store.id]}},
    temperature=0.4
)

# Upload filefile = client.files.create(file=(""data.json"", json.dumps({""name"": ""Alexbabaliks""}).encode()), purpose=""assistants"")
while True:
    file_status = client.files.retrieve(file_id=file.id)
    if file_status.status == 'processed':
        break
    time.sleep(1)

client.beta.vector_stores.files.create(vector_store_id=vector_store.id, file_id=file.id)
while True:
    vector_store = client.beta.vector_stores.retrieve(vector_store_id=vector_store.id)
    if vector_store.status == 'completed':
        break
    time.sleep(1)

thread = client.beta.threads.create(tool_resources={""file_search"": {""vector_store_ids"": [vector_store.id]}})

run = client.beta.threads.runs.create(
    instructions=""What is my name??? Take it from JSON file and return JSON in format {'name': '<name>'}"",
    thread_id=thread.id,
    assistant_id=assistant.id,
    model=""gpt-4o"",
    temperature=0.4,
    tools=[{""type"": ""file_search""}],
)

while True:
    run_status = client.beta.threads.runs.retrieve(thread_id=thread.id, run_id=run.id)
    if run_status.status == ""completed"":
        break
    elif run_status.status == ""failed"":
        break

    time.sleep(2)

answer = """"messages = client.beta.threads.messages.list(thread_id=thread.id)
for message in messages.data:
    if run.id != message.run_id:
        continue

    for content in message.content:
        if content.type == ""text"":
            answer = content.text.value
            break

print(answer)
OS
Linux
Python version
3.11.1
Library version
openai v1.51.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1770","No Healthy Upstream","2024-10-02T20:49:09Z","Closed as not planned issue","No label","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
Facing the following issue:

To Reproduce
Azure OpenAI Studio
Code snippets
import osimport requestsimport base64

# ConfigurationAPI_KEY = ""YOUR_API_KEY""IMAGE_PATH = ""YOUR_IMAGE_PATH""encoded_image = base64.b64encode(open(IMAGE_PATH, 'rb').read()).decode('ascii')
headers = {
    ""Content-Type"": ""application/json"",
    ""api-key"": API_KEY,
}

# Payload for the requestpayload = {
  ""messages"": [
    {
      ""role"": ""system"",
      ""content"": [
        {
          ""type"": ""text"",
          ""text"": ""You are an AI assistant that helps people find information.""
        }
      ]
    },
    {
      ""role"": ""user"",
      ""content"": [
        {
          ""type"": ""text"",
          ""text"": ""Hi""
        }
      ]
    },
    {
      ""role"": ""user"",
      ""content"": [
        {
          ""type"": ""text"",
          ""text"": ""Pretty strange""
        }
      ]
    }
  ],
  ""temperature"": 0.7,
  ""top_p"": 0.95,
  ""max_tokens"": 800
}

ENDPOINT = ""https://abc-openai-gpt4o.openai.azure.com/openai/deployments/gpt4o/chat/completions?api-version=2024-02-15-preview""

# Send requesttry:
    response = requests.post(ENDPOINT, headers=headers, json=payload)
    response.raise_for_status()  # Will raise an HTTPError if the HTTP request returned an unsuccessful status codeexcept requests.RequestException as e:
    raise SystemExit(f""Failed to make the request. Error: {e}"")

# Handle the response as needed (e.g., print or process)print(response.json())
OS
Linux
Python version
Python v3.11
Library version
openai v1.42.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1765","the latest openai version has problem with httpx library when compile to app.","2024-10-01T08:14:07Z","Open issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
I found this error when update the openai to new version.
Referenced from: <198A9232-F82E-3E0A-92CB-D131D45DBB4F> /xx/_ssl.cpython-312-darwin.so
 Expected in: <284AF566-569A-311D-8493-DC6928BBD628> /xx/libcrypto.3.dylib
 Failed to execute script 'main' due to unhandled exception: dlopen(/xxx/Frameworks/lib-dynload/_ssl.cpython-312-darwin.so, 0x0002): Symbol not found: _X509_STORE_get1_objects
File ""openai/init.py"", line 8, in 
 File ""openai/types/init.py"", line 5, in 
 File ""openai/types/batch.py"", line 7, in 
 File ""openai/_models.py"", line 26, in 
 File ""openai/_types.py"", line 21, in 
 File ""httpx/init.py"", line 2, in 
To Reproduce
write a sample code and import the latest openai.
compile file with pyinstaller on macOS.
run the execute file and found this issue.
Code snippets
the problem looks lead to httpx library which calling by openai.
Not sure if it need the specific version or latest version of the library?
OS
macOS
Python version
Python 3.12
Library version
openai 1.50.2
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1763","beta.chat.completions.parse returns unhandled ValidationError","2024-09-30T15:05:43Z","Open issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
In some occasions while using the Completion API with Structured Outputs, the SDK fails and returns a ValidationError:
ValidationError: 1 validation error for RawResponse
  Invalid JSON: EOF while parsing a value at line 1 column 600 [type=json_invalid, input_value='                        ...                       ', input_type=str]
    For further information visit https://errors.pydantic.dev/2.9/v/json_invalid

This does not happen every time, but we use it in a production service and this unpredictable behavior is hard to prevent.
To Reproduce
Create a Pydantic model
Instantiate an OpenAI client
Use the method OpenAI.beta.chat.completions.parse(...) with the following arguments
Repeat a few times for seeing the error
from pydantic import BaseModelfrom openai import OpenAI

class RawResponse(BaseModel):
    answer: str

client = OpenAI(api_key=...)
completion = client.beta.chat.completions.parse(
                        model='gpt-4o-2024-08-06',
                        messages=messages,
                        max_tokens=750,
                        n=1,
                        stop=None,
                        temperature=0.1,
                        response_format=RawResponse
                    )
After a few times, this fails with:
ValidationError: 1 validation error for RawResponse
  Invalid JSON: EOF while parsing a value at line 1 column 600 [type=json_invalid, input_value='                        ...                       ', input_type=str]
    For further information visit https://errors.pydantic.dev/2.9/v/json_invalid

Code snippets
No response
OS
debian:bullseye-slim
Python version
CPython 3.10.8
Library version
openai 1.48.0
 The text was updated successfully, but these errors were encountered: 
👍5
hvignolo87, nicolasaldecoa, pgiu, CMauro96, and victoriamshanly reacted with thumbs up emoji
All reactions
👍5 reactions"
"https://github.com/openai/openai-python/issues/1760","ds-store","2024-09-30T14:20:10Z","Closed issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
.DS_Store file in project root should not be there.
Add to .gitignore.
To Reproduce
n/a
Code snippets
No response
OS
macOS
Python version
Python v.12
Library version
v1.50.2
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1759","Get info about model","2024-09-30T21:25:04Z","Closed as not planned issue","No label","Confirm this is a feature request for the Python library and not the underlying OpenAI API.
 This is a feature request for the Python library
Describe the feature or improvement you're requesting
I can list models, like this:
import openai

for model in openai.models.list():
    print(model.id)
but i cant get info about model type. I dont get info about data types, that models acepts.
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1739","AttributeError: module 'openai' has no attribute 'Completions' when using the latest version and APIRemovedInV1","2024-09-24T10:28:13Z","Closed as not planned issue","question","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
I'm encountering an AttributeError when trying to use the openai.Completions attribute in my Python code. I'm using the latest version of the OpenAI Python library (version 1.47.1).
 Here is the brief description of code:
 import openai
 input = ""I love to travel""
 response = sentiment_analysis(input)
 print(input, ""The sentiment is"" , response)
 When I run this code, I get the following error:
 APIRemovedInV1:
You tried to access openai.Completion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.
You can run openai migrate to automatically upgrade your codebase to use the 1.0.0 interface.
Alternatively, you can pin your installation to the old version, e.g. pip install openai==0.28
A detailed migration guide is available here: #742
To Reproduce
The code should successfully create a completion using the openai.Completions.create function.
 Please let me know if there are any known issues or changes related to the openai.Completions attribute in the latest version of the library.
 I've verified that I'm using the correct API key and that my internet connection is stable. I've also tried reinstalling the OpenAI library to ensure I'm using the latest version.
 Any suggestions or workarounds would be greatly appreciated.
Code snippets
import openaidef sentiment_analysis(text):
  messages = [{""Role"":""system"", ""content"":""""""You are trained to analyze an detect the sentiment of the given text.                                             if you are unsure of answer you can say""Not sure"" and recommend users to review manually.""""""},
              {""Role"":""user"", ""content"":""""""Analyze the following text and determine if the sentiment is: positive or negative.                                           return answer in single word as either positive or negative: {text}""""""}]
  response = openai.Completion.create(
      engine=""text-davinci-003"", 
      messages=messages,
      max_tokens=1,
      n=1,
      stop=None,
      temperature=0)
  sentiment = response.choices[0].message.content.strip().lower()
  return sentimentinput = ""I love to travel""response = sentiment_analysis(input)
print(input, ""The sentiment is"" , response)
OS
windows
Python version
3.12.6
Library version
1.47.1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1738","examples/embeddings/Visualize_in_3d.ipynb is empty","2024-10-03T14:14:28Z","Closed as not planned issue","documentation","Confirm this is a feature request for the Python library and not the underlying OpenAI API.
 This is a feature request for the Python library
Describe the feature or improvement you're requesting
Actually all the examples of embeddings are empty, could anybody share some example codes?
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1736","Even for Pydantic V1, getting the error ""warnings is only supported in Pydantic v2"" | Error while use streaming in Assistant","2024-09-23T10:48:50Z","Closed issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
There seems to be a issue in this PR. I'm using pydantic V1 in my code and I'm getting the error ""warnings is only supported in Pydantic v2"", which is defined in_models.py.
Note: We are not planning to change the pydantic version to V2 as it will be a breaking change.
PR where the bug started: https://github.com/openai/openai-python/pull/1722/files
Traceback:
 OpenAi error:: Traceback (most recent call last):
 File ""/usr/local/lib/python3.10/site-packages/openai/lib/streaming/_assistants.py"", line 942, in accumulate_event
 block = current_message_snapshot.content[content_delta.index]
 IndexError: list index out of range
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
 File ""/app/app/openai/assistants/gpt/assistant_workflow.py"", line 142, in handle_incoming_message
 response = await gpt_assistant_wrapper.call_assistant_and_stream(
 File ""/app/app/openai/v2/api/assistants/assistant_wrapper.py"", line 212, in call_assistant_and_stream
 await stream.until_done()
 File ""/usr/local/lib/python3.10/site-packages/openai/lib/streaming/_assistants.py"", line 534, in until_done
 await consume_async_iterator(self)
 File ""/usr/local/lib/python3.10/site-packages/openai/_utils/_streams.py"", line 11, in consume_async_iterator
 async for _ in iterator:
 File ""/usr/local/lib/python3.10/site-packages/openai/lib/streaming/_assistants.py"", line 501, in aiter
 async for item in self._iterator:
 File ""/usr/local/lib/python3.10/site-packages/openai/lib/streaming/_assistants.py"", line 838, in stream
 await self._emit_sse_event(event)
 File ""/usr/local/lib/python3.10/site-packages/openai/lib/streaming/_assistants.py"", line 690, in _emit_sse_event
 self.__current_message_snapshot, new_content = accumulate_event(
 File ""/usr/local/lib/python3.10/site-packages/openai/lib/streaming/_assistants.py"", line 951, in accumulate_event
 value=content_delta.model_dump(exclude_unset=True, warnings=False),
 File ""/usr/local/lib/python3.10/site-packages/openai/_models.py"", line 304, in model_dump
 raise ValueError(""warnings is only supported in Pydantic v2"")
 ValueError: warnings is only supported in Pydantic v2
To Reproduce
Error is occuring while using openAi Assistant stream.
 Pydantic version ""^1.10.7""
 Error is in function stream.until_done()
Code snippets
async with client.beta.threads.runs.stream(
            thread_id=thread_id,
            assistant_id=assistant_id,
            event_handler=main_event_handler,
        ) as stream:

            await stream.until_done()

Error happens in stream.until_done()
OS
macOs
Python version
Python v3.10.6
Library version
openAi v1.47.2
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1733","Structured outputs response_format requires strict function calling JSON Schema?","2024-09-20T19:48:26Z","Open issue","question","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
I am using the OpenAI Python 1.47.0 library and the model gpt-4o-2024-08-06. I've got the json_schema response format working with Pydantic/Non-Pydantic models (non-pydantic meaning I manually create the proper response format JSON schema) without tool calling. However, when I attempt to send tools with the payload to the method:
client.beta.chat.completions.parse(...)
I am getting a 400 because the tool's JSON schema does not have strict/additionalProperties.
The error shows as:
ValueError('`weather-get_weather_for_city` is not strict. Only `strict` function tools can be auto-parsed')

When I do add the strict: True and additionalProperties: False, I get a 200:
{
    ""type"": ""function"",
    ""function"": {
        ""name"": ""weather-get_weather_for_city"",
        ""description"": ""Get the weather for a city"",
        ""parameters"": {
            ""type"": ""object"",
            ""properties"": {
                ""city"": {
                    ""type"": ""string"",
                    ""description"": ""The input city""
                }
            },
            ""required"": [""city""],
            ""additionalProperties"": false
        },
        ""strict"": true
    }
}
In your docs, I don't see this coupling between function calling schema and json_schema response format called out (if it is there, I am obviously missing it).
The docs say:
Structured Outputs is available in two forms in the OpenAI API:

- When using [function calling](https://platform.openai.com/docs/guides/function-calling)
- When using a json_schema response format

This makes it seem like they're able to be used independently.
As an additional note: in .Net, I can use the OpenAI library and make a call to the normal chat completions endpoint, configure the proper strict JSON Schema for the json_schema response format, and not need to manipulate the function calling JSON schema to include strict or additionalParameters and the calls work fine. No 400s encountered. Something like this:
chatCompletion = (await RunRequestAsync(() => this.Client!.GetChatClient(targetModel).CompleteChatAsync(chatForRequest, chatOptions, cancellationToken)).ConfigureAwait(false)).Value;
To Reproduce
Use the latest OpenAI package
Configure a Pydantic model as the response_format
Include a tool (with non-strict JSON Schema) with the payload
Make a call to client.beta.chat.completions.parse(...)
Observe the 400 due to the function calling schema missing the strict/additionalProperties keys/values.
Code snippets
No response
OS
MacOS
Python version
Python 3.12.5
Library version
openai 1.47.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1732","1.46.1 audio transcription's temperature param default is not the API's default","2024-09-19T17:22:18Z","Closed as not planned issue","API-feedback","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
The API documentation for audio transcriptions says:
temperature number Optional Defaults to 0

The sampling temperature, between 0 and 1. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. If set to 0, the model will use [log probability](https://en.wikipedia.org/wiki/Log_probability) to automatically increase the temperature until certain thresholds are hit.

But the behavior between explicitly setting temperature=0 and not passing the param is different, and produces different outputs
To Reproduce
client.audio.transcriptions.create(
        model=""whisper-1"",
        file=track,
        response_format=""verbose_json"",
    )
vs
client.audio.transcriptions.create(
        model=""whisper-1"",
        file=track,
        response_format=""verbose_json"",
        temperature=0,
    )
Code snippets
No response
OS
linux
Python version
3.12
Library version
1.46.1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1727","trying to use my assistant; one run creates more than one run","2024-09-19T16:09:32Z","Open issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
with creating a thread:
thread = client.beta.threads.create()
with creating the message:
message = client.beta.threads.messages.create( thread_id=thread.id, role=""role"", content=""message"" )
and run using this command:
run = client.beta.threads.runs.create_and_poll( thread_id=thread.id, assistant_id='assistant_id' )
Sometimes, when I see the chat in here https://platform.openai.com/threads/, one run causes multiple responses from the assistant. So, when I grab the latest message, it is not based on the initial message I sent.
I put all these three steps in one loop, of length around 100, and I saw this behavior in some of the threads.
To Reproduce
Code snippets
No response
OS
linux
Python version
3.10
Library version
openai v1.0.1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1725","The concurrency of AsyncOpenAI cannot be fully utilized.","2024-09-19T09:40:24Z","Open issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
I attempted to complete a stability test on the concurrency of AsyncOpenAI. I set the concurrency to 1024 but found that it kept running at a very low average level in a jittery manner, which has been consistent with my production test results.
To Reproduce
I put my code in three part. client.py server.py and main.py(used to create 100k client total)
server.py
from fastapi import FastAPI, Requestfrom pydantic import BaseModelimport asyncioimport loggingfrom datetime import datetimeimport threadingimport timeimport csv

app = FastAPI()

# track current activate queue countactive_requests = 0

# debug file to draw picoutput_file = 'active_requests_log.csv'

class CompletionRequest(BaseModel):
    model: str
    messages: list
    temperature: float

@app.middleware(""http"")async def track_requests(request: Request, call_next):
    global active_requests
    active_requests += 1  # add count when get request
    logging.info(f""Active requests: {active_requests}"")

    response = await call_next(request)

    active_requests -= 1  # 请求完成后减少计数
    logging.info(f""Active requests: {active_requests}"")

    return response

@app.post(""/v1/chat/completions"")async def completions(request: CompletionRequest):
    await asyncio.sleep(1)  # mock llm generate latency
    return {
        ""choices"": [
            {""message"": {""content"": f""Response to {request.messages[-1]['content']}""}}
        ]
    }

def record_active_requests():
    """""" save log file per second""""""
    global active_requests
    with open(output_file, mode='w', newline='') as file:
        writer = csv.writer(file)
        writer.writerow([""timestamp"", ""active_requests""])  # 写表头
        
        while True:
            # 每秒记录一次
            current_time = datetime.now().strftime(""%Y-%m-%d %H:%M:%S"")
            writer.writerow([current_time, active_requests])
            file.flush()  # 确保每秒写入数据到文件
            time.sleep(1)

# 启动一个线程来记录活跃请求数threading.Thread(target=record_active_requests, daemon=True).start()

if __name__ == ""__main__"":
    import uvicorn
    logging.basicConfig(level=logging.INFO)
    uvicorn.run(app, host=""127.0.0.1"", port=8203)
client.py
import asynciofrom functools import wrapsimport httpximport loggingfrom openai import AsyncOpenAI



def limit_async_func_call(max_size: int):
    sem = asyncio.Semaphore(max_size)

    def final_decro(func):
        @wraps(func)
        async def wait_func(*args, **kwargs):
            async with sem:
                try:
                    return await func(*args, **kwargs)
                except Exception as e:
                    logging.error(f""Exception in {func.__name__}: {e}"")
  
        return wait_func
    return final_decro

# 假设这个是你要进行并发测试的函数@limit_async_func_call(max_size=1024)  # 限制并发为1024async def custom_model_if_cache(prompt, system_prompt=None, history_messages=[], **kwargs):
    custom_http_client = httpx.AsyncClient(
        limits=httpx.Limits(max_connections=2048, max_keepalive_connections=1024),
        timeout=httpx.Timeout(timeout=None)
    )

    openai_async_client = AsyncOpenAI(
        api_key=""EMPTY"", base_url=""http://localhost:8203/v1"",  # 模拟本地 server
        http_client=custom_http_client
    )

    messages = []
    if system_prompt:
        messages.append({""role"": ""system"", ""content"": system_prompt})
    messages.extend(history_messages)
    messages.append({""role"": ""user"", ""content"": prompt})

    # 假设这里是要调用的外部 API
    response = await openai_async_client.chat.completions.create(
        model=""gpt-3.5-turbo"", messages=messages, temperature=0, **kwargs
    )

    return ""hi""
main.py
import asyncioimport loggingfrom client import custom_model_if_cache# 模拟 10 万个请求TOTAL_REQUESTS = 100000

async def simulate_requests():
    tasks = []
    for i in range(TOTAL_REQUESTS):
        prompt = f""Test prompt {i}""  # 每次请求的不同参数
        task = custom_model_if_cache(prompt=prompt)  # 调用受限的异步函数
        tasks.append(task)

    # 并发执行所有请求
    results = await asyncio.gather(*tasks, return_exceptions=True)

    # 打印前10个结果以验证
    for result in results[:10]:
        print(result)

if __name__ == ""__main__"":
    logging.basicConfig(level=logging.INFO)
    asyncio.run(simulate_requests())
To reproduce, open two terminal and run python server.pypython main.py seperately.
 I also save the log, you can use following code to draw:
draw.py
import csvimport matplotlib.pyplot as pltfrom datetime import datetime

# 文件路径input_file = 'active_requests_log.csv'

# 读取 CSV 文件并解析时间和活跃请求数量timestamps = []
active_requests = []

with open(input_file, mode='r') as file:
    reader = csv.DictReader(file)
    for row in reader:
        timestamps.append(datetime.strptime(row[""timestamp""], ""%Y-%m-%d %H:%M:%S""))
        active_requests.append(int(row[""active_requests""]))

# 绘制图表plt.figure(figsize=(10, 6))
plt.plot(timestamps, active_requests, label='Active Requests', color='b')

# 设置图表标题和标签plt.title('Active Requests Over Time')
plt.xlabel('Time')
plt.ylabel('Active Requests')
plt.xticks(rotation=45)
plt.grid(True)
plt.legend()

# 显示图表plt.tight_layout()
plt.savefig(""/mnt/rangehow/pr/test_c/c.jpg"")
Code snippets
No response
OS
ubuntu
Python version
3.12
Library version
latest
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1724","AzureOpenAI chat completion endpoint does not recognize max_completion_tokens","2024-09-19T12:52:15Z","Closed as not planned issue","Azure,question","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
With the o-1 release, max_tokens was deprecated in favor of max_completion_tokens, and while that argument works with the OpenAI client, it doesn't seem to be working with the AzureOpenAI client. I see the following error:
BadRequestError: Error code: 400 - {'error': {'message': 'Unrecognized request argument supplied: max_completion_tokens', 'type': 'invalid_request_error', 'param': None, 'code': None}}

To Reproduce
import osfrom openai import AzureOpenAI

client = AzureOpenAI(
  azure_endpoint = os.getenv(""AZURE_OPENAI_ENDPOINT""), 
  api_key=os.getenv(""AZURE_OPENAI_API_KEY""),  
  api_version=""2024-08-01-preview""
)

m = {'role': 'user', 'content': ""Hello!""}
client.chat.completions.create(messages=[m], model='gpt-4o', max_completion_tokens=100)
Code snippets
No response
OS
linux
Python version
3.11.9
Library version
1.46.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1721","APIRemovedInV1 Error When Using openai.ChatCompletion.create in Clean Environments (v1.45.0+ and Docker)","2024-09-18T14:37:49Z","Closed as not planned issue","question","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
I am encountering an APIRemovedInV1 error when using the openai.ChatCompletion.create() method in both local and Docker environments, despite using the latest versions of the OpenAI Python library (v1.45.0 and v1.46.0).
The error persists even after following the official migration guide and ensuring that the correct API is being used. I’ve tested this in multiple clean environments, including an isolated Docker container, but the issue still occurs.
The error message is:
APIRemovedInV1: You tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0

Expected Behavior:
 I expect openai.ChatCompletion.create() to work correctly without triggering the APIRemovedInV1 error, as per the latest API documentation.
Environment:
 Python version: 3.10
 OpenAI Python library version: 1.45.0 and 1.46.0 (tried both)
 OS: Ubuntu (both local and Docker environments)
 Additional Context:
 I've purged all old versions of the OpenAI library and even rebuilt environments without caching (including Docker).
 The issue persists even in fresh environments with no previous configurations.
 I followed the official migration guide, and the method openai.ChatCompletion.create() is being used as described in the documentation.
To Reproduce
Install openai via pip (pip install openai==1.45.0 or pip install openai==1.46.0).
 Attempt to use openai.ChatCompletion.create() with the code snippet below:
Code snippets
import openaiopenai.api_key = 'my api key from .env file'

response = openai.ChatCompletion.create(
    model=""gpt-3.5-turbo"",
    messages=[
        {""role"": ""system"", ""content"": ""You are a helpful assistant.""},
        {""role"": ""user"", ""content"": ""Hello, ChatGPT!""}
    ]
)
print(response['choices'][0]['message']['content'])
OS
Windows with WSL / Ubuntu
Python version
Python 3.12.12
Library version
openai 1.45.0 & openai 1.46.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1713","when ""openai migrate"" in Mac, get error, need help please","2024-09-14T10:06:16Z","Open issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
When using Mac console to do ""openai migrate"" I alway get below:
 ERROR (code: 100) - ./Library/Saved Application State/com.lc-tech.RescuePRO-Deluxe.savedState: IO error for operation on ./Library/Saved Application State/com.lc-tech.RescuePRO-Deluxe.savedState: Permission denied (os error 13)
 Processed 0 files and found 0 matches
To Reproduce
When using Mac console to do ""openai migrate"" I alway get below:
 ERROR (code: 100) - ./Library/Saved Application State/com.lc-tech.RescuePRO-Deluxe.savedState: IO error for operation on ./Library/Saved Application State/com.lc-tech.RescuePRO-Deluxe.savedState: Permission denied (os error 13)
 Processed 0 files and found 0 matches
Code snippets
openai migrate
OS
Mac OS 14.5
Python version
Python 3.10.0
Library version
openai 1.45.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1712","Медленно отвечает новая демо версия OpenAI","2024-09-16T15:31:08Z","Closed as not planned issue","question","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
Медленно даёт ответ 15-20 сек.
Не даёт ответ в 20% приблизительно.
Не дописывает текст до конца и зависает в 20%.
ChatGPT 4o так же начал зависать и не давать ответ, хотя раньше такое было реже.
Я предоставил o1-preview возможность выбора и это было его единственной задачей, выбирать слова, слов было 10 которые я так же указал, он не мог справиться с этой задачей и зависал или вообще не выдавал результат, ему нужно было просто писать слова по одному и ждать любой моей ответ и потом снова писать одно из 10 указанных слов.
Не предоставляя информацию о лимите по количеству запросов я исчерпал его на 7 дней, если бы знал то умнее бы распоряжался данной возможностью, тем не менее я надеюсь что полная версия выйдет намного лучше прошлой и демо.
To Reproduce
Написать 10 слов и указать что бы он выбирал в произвольном порядке.
Указать что бы писал по одному слову, дожидаясь моего ответа.
После ответа повторять процедуру, пока слова не закончатся.
Code snippets
No response
OS
Windows
Python version
Python v3.11.4
Library version
openai v1.0.1.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1705","client.audio.transcriptions.create receive different results depending on OS","2024-09-12T13:34:40Z","Open issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
I'm making a transcription of audio using the transcriptions create method, but even running with the same audio, the same configuration, and the same Python version, I have different results depending on the OS, and to be more precise, when I am running into a windows OS the result is correct, but when running into Linux or Mac, the results is wrong. Some examples of what happened with the transcriptions:
Windows:
ok, I will do that, thank you for your support!
Linux and Mac
ok, I will do that, do that, do that, do that, do that, do that, do that 
Information about the OS that runs the tests:
Windows: Windows 11 Home Single Language (version: 23H2) (running inside wsl2 with ubuntu 22.04 distro) (running local)
MAC: macOS (version: 14.4.1) (running local)
Linux: Amazon Linux 2 - (running inside ECS)
Please, let me know if need more information than that.
To Reproduce
open local audio in format .ogg as binary (""rb""):
   with open('audio.ogg', 'rb') as opened_audio:
file the params with the following params:
           transcription_params = {
            'model': 'whisper-1',
            'file': opened_audio,
            'temperature': 0.0,
            'language': 'pt',
            'response_format': 'verbose_json'
        }
run as below in different's OS a couple of audios and compare the results:
  from openai import OpenAI

   with open('audio.ogg', 'rb') as opened_audio:
        transcription_params = {
            'model': 'whisper-1',
            'file': opened_audio,
            'temperature': 0.0,
            'language': 'pt',
            'response_format': 'verbose_json'
        }

        OpenAI().audio.transcriptions.create(**transcription_params)
Code snippets
from openai import OpenAI

   with open('audio.ogg', 'rb') as opened_audio:
        transcription_params = {
            'model': 'whisper-1',
            'file': opened_audio,
            'temperature': 0.0,
            'language': 'pt',
            'response_format': 'verbose_json'
        }

        OpenAI().audio.transcriptions.create(**transcription_params)
OS
MACOS
Python version
3.12.5
Library version
1.42.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1704","Bug caused by incorrect usage of Pydantic and Typing Extensions","2024-09-12T11:34:22Z","Closed as not planned issue","question","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
You define the JSONSchema class in the path openai.types.shared_params.response_format_json_schema.py as follow
class JSONSchema(TypedDict, total=False):
    name: Required[str]
    """"""The name of the response format.    Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length    of 64.    """"""

    description: str
    """"""    A description of what the response format is for, used by the model to determine    how to respond in the format.    """"""

    schema: Dict[str, object]
    """"""The schema for the response format, described as a JSON Schema object.""""""

    strict: Optional[bool]
    """"""Whether to enable strict schema adherence when generating the output.    If set to true, the model will always follow the exact schema defined in the    `schema` field. Only a subset of JSON Schema is supported when `strict` is    `true`. To learn more, read the    [Structured Outputs guide](https://platform.openai.com/docs/guides/structured-outputs).    """"""
Then, you used it in each class where you defined the format of the response body, as follows.
ResponseFormat: TypeAlias = Union[ResponseFormatText, ResponseFormatJSONObject, ResponseFormatJSONSchema]

class CompletionCreateParamsBase(TypedDict, total=False):
       .
       .
       .
      response_format: ResponseFormat
However, the field named 'schema' is a built-in field in Pydantic. When you convert this model to Pydantic, it triggers the following error.
NameError: Field name ""schema"" shadows a BaseModel attribute; use a different field name with ""alias='schema'
 This bug exists in version 1.40 and later versions.
To Reproduce
from openai.types.chat.completion_create_params import CompletionCreateParamsNonStreaming
 from pydantic.v1 import create_model_from_typedict
 2. ref = create_model_from_typedict(CompletionCreateParamsNonStreaming)
 3. Then error occurred
Code snippets
No response
OS
any
Python version
any
Library version
openai v1.40.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1700","Include usage information in LengthFinishReasonError","2024-09-10T15:47:19Z","Closed issue","enhancement","Confirm this is a feature request for the Python library and not the underlying OpenAI API.
 This is a feature request for the Python library
Describe the feature or improvement you're requesting
Situation
 When calling AsyncOpenAI(...).beta.chat.completions.parse(..., response_format=SomePydanticModel), the OpenAI library raises LengthFinishReasonError when finish_reason == ""length"" and raises ContentFilterFinishReasonError when finish_reason == ""content_filter"", without providing any information as to what the response contained.
Complication
 Because there is no way to retrieve any information about the response, I cannot programmatically save information about the context. For example, I cannot access and track information from the usage object in the chat completion response.
Desired behavior
 As a library user, I always want to know details about responses from LLM calls that costs tokens for me. More specifically, I want to inspect usage to know how many tokens I ""wasted"" calling the LLM, for instance when max_tokens was set to a too low value for the LLM to generate a complete structured output. This enables me to track and control costs.
I see two potential solutions:
Stop raising exceptions for these scenarios and always return a chat completion object. I believe this is the behavior in the non-beta version of the chat completion call
Return the response as an attribute in the exception object so that it can be used by the calling programmer
Version used
1.44.1 (latest on PyPI at the time of writing)
Code location
File and line: openai.lib._parsing._completions.py on lines 71-75
Function: parse_chat_completion
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1699","Add Memory to API","2024-09-10T08:44:23Z","Closed as not planned issue","No label","Confirm this is a feature request for the Python library and not the underlying OpenAI API.
 This is a feature request for the Python library
Describe the feature or improvement you're requesting
The long term memory feature in ChatGPT is a great feature that i'd like to make available to my users via API. Before I invest time developing a less capable solution I want to check with the devs if this is on the API roadmap?
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1698","Add Structured Outputs support to Assistants stream() and create_and_poll() Functions","2024-09-09T14:56:06Z","Open issue","enhancement","Confirm this is a feature request for the Python library and not the underlying OpenAI API.
 This is a feature request for the Python library
Describe the feature or improvement you're requesting
Currently the client.beta.threads.runs.create_and_poll() function and client.beta.threads.runs.stream() function do not accept a pydantic model as their ""response_format"". currently they only accept the old {""type"": ""json_object""} value.
Additional context
class Meal(BaseModel):
    meal: str
    slug: str
    recipe_id: str
    calories_per_serving: int
    protein_per_serving: int
    fat_per_serving: int
    carbs_per_serving: int
    servings: int

class Meals(BaseModel):
    breakfast: Optional[Meal]
    lunch: Optional[Meal]
    dinner: Optional[Meal]

class DayLog(BaseModel):
    date: str  # You can change this to 'date' type if needed
    total_calories: int
    total_carbs: int
    total_fat: int
    total_protein: int
    meals: Meals

class WeekLog(BaseModel):
    Monday: DayLog
    Tuesday: DayLog
    Wednesday: DayLog
    Thursday: DayLog
    Friday: DayLog
    Saturday: DayLog
    Sunday: DayLog

completion = client.beta.chat.completions.parse(
        model=""gpt-4o-2024-08-06"",
        messages=[
            {""role"": ""system"", ""content"": ""my prompt for structured data""


             },
        ],
        response_format=WeekLog,
    )
Currently the above works without issue, but the below throws a TypeError:
assistant = client.beta.assistants.create(
        name=""Meal Planner Nutritionist"",
        instructions=""some instructions"",
        tools=[{""type"": ""code_interpreter""}],
        model=""gpt-4o-2024-08-06"",
    )
    thread = client.beta.threads.create()
    message = client.beta.threads.messages.create(
        thread_id=thread.id,
        role=""user"",
        content= ""my prompt for structured data""
         )
    run = client.beta.threads.runs.create_and_poll(
        thread_id=thread.id,
        assistant_id=assistant.id,
        instructions=""repeat instructions"",
        response_format=WeekLog
    )
and the below works, but isnt usable for my purposes:
assistant = client.beta.assistants.create(
        name=""Meal Planner Nutritionist"",
        instructions=""some instructions"",
        tools=[{""type"": ""code_interpreter""}],
        model=""gpt-4o-2024-08-06"",
    )
    thread = client.beta.threads.create()
    message = client.beta.threads.messages.create(
        thread_id=thread.id,
        role=""user"",
        content= ""my prompt for structured data""
         )
    run = client.beta.threads.runs.create_and_poll(
        thread_id=thread.id,
        assistant_id=assistant.id,
        instructions=""repeat instructions"",
        response_format={""type"": ""json_object""}
    )
 The text was updated successfully, but these errors were encountered: 
👍3
kylebechtel77, hayescode, and zholmquist reacted with thumbs up emoji
All reactions
👍3 reactions"
"https://github.com/openai/openai-python/issues/1686","The content of the api_key or default_headers of an AsyncOpenAI instance can be modified surreptitiously?","2024-09-05T05:50:50Z","Closed issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
Deploying a local LLM. Create an instance of AsyncOpenAI called aclient, then call aclient.chat.completions.create and return an asynchronous iterator. When processing the chunks yielded by the iterator, it appears that the api_key in the aclient instance is continuously changing.
To Reproduce
Set the N in user_id to increment from 1 for each request, i.e. 'user_id_1', 'user_id_2', ... 'user_id_30' ... When the number of concurrent requests is greater than a certain number, the id(aclient) changes every time the for loop is entered, and user_id_from_header and user_id_from_api_key change, and the member variables of different AsyncOpenAI instances (aclient) are mixed together: the same aclient instance, e.g. user_id_from_header='user_id_20', user_id_from_api_key='user_id_15',
Code snippets
user_id = <user_id_N>aclient = AsyncOpenAI(api_key= user_id, default_headers= {'user_id': user_id})
stream =  await aclient.chat.completions.create(...)
async for chunk in stream:
     do_something(...)
     logger.info(f'user_id_from_header={aclient.default_headers['user_id']}, user_id_from_api_key={aclient.api_key}, id={id(aclient)}')
OS
CentOS
Python version
Python 3.11.4
Library version
openai v1.43.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1685","array + enum in function calling","2024-09-24T09:32:54Z","Closed as not planned issue","No label","Confirm this is a feature request for the Python library and not the underlying OpenAI API.
 This is a feature request for the Python library
Describe the feature or improvement you're requesting
Is it currently possible to get function calling with an array of inputs for an argument and a restriction via enums:
 something like:
'parameters': {'type': 'object',
 'properties': {'lab_values': {'type': 'array',
 'enum': ['Asparate Aminotransferase (AST)',
 'Barbiturate Screen',
 'Benzodiazepine Screen',
 ]}},
 'required': ['lab_values'],
 'additionalProperties': False}}}]
?
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1684","Image Example - base64 and URL pair incomplete code snippet","2024-09-03T14:04:29Z","Open issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
NOTE: This is an issue with the _documentation_ of the python library not the *functionality*
It seems like one of the example code snippets is incomplete/incorrect
More specifically, the example states: The Chat Completions API is capable of taking in and processing multiple image inputs in both base64 encoded format or as an image URL.
However, the code itself shows only the image URL reference (twice):
    {
      ""role"": ""user"",
      ""content"": [
        {
          ""type"": ""text"",
          ""text"": ""What are in these images? Is there any difference between them?"",
        },
        {
          ""type"": ""image_url"",
          ""image_url"": {
            ""url"": ""https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg"",
          },
        },
        {
          ""type"": ""image_url"",
          ""image_url"": {
            ""url"": ""https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg"",
          },
        },
      ],
    }
  ],```

### To Reproduce

Read the python documentation at: https://platform.openai.com/docs/guides/vision/multiple-image-inputs

### Code snippets

```Python
I suspect the code example should be something like:

=[
    {
      ""role"": ""user"",
      ""content"": [
        {
          ""type"": ""text"",
          ""text"": ""What are in these images? Is there any difference between them?"",
        },
        {
          ""type"": ""image_url"",
          ""image_url"": {
            ""url"": f""data:image/jpeg;base64,{base64_image}"",
          },
        },
        {
          ""type"": ""image_url"",
          ""image_url"": {
            ""url"": ""https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg"",
          },
        },
      ],
    }
  ],

Note the change of one of the Wikimedia URLS to: `""url"": f""data:image/jpeg;base64,{base64_image}""`

OS
Any
Python version
Any
Library version
Any
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1678","Indentation error in _utils/_utils.py","2024-08-26T21:23:12Z","Closed as not planned issue","question","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
It looks like there's a bug in the _utils/_utils.py file:
            for variant in variants:
                matches = all((param in given_params for param in variant))
                if matches:
                    break
            else:  # no break
                if len(variants) > 1:
                    variations = human_join(
                        [""("" + human_join([quote(arg) for arg in variant], final=""and"") + "")"" for variant in variants]
                    )
                    msg = f""Missing required arguments; Expected either {variations} arguments to be given""

That if matches: and else: # no break aren't aligned in the wrapper function.
To Reproduce
Look at the code in the wrapper() function of _utils/_utils.py
Code snippets
No response
OS
macOS
Python version
Python 3.11
Library version
openai v1.42.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1677","Typing: when stream is completed, delta in ChatCompletionChunk from azure openai is None; should be ChoiceDelta","2024-08-26T15:03:40Z","Open issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
When streaming from azure open ai API the delta of the choice is None. In the python open ai client v1.42.0 delta is type ChoiceDelta i.e. not None.
To Reproduce
Run this code in line with
    completion = await self._client.chat.completions.create(
            model=self.deployment.name,
            messages=cast(list[ChatCompletionMessageParam], messages),
            stream=True,
            temperature=temperature,
        )

    async for response_chunk in completion:
        ...
The types are:
response_chunk: ChatCompletionChunk
response_chunk.choices: list[Choice]
response_chunk.choices[0].delta: ChoiceDelta
The response from azure open ai API returns delta=Nonewhen stream ends
Response example:
Choice(delta=None, finish_reason=None ...........)
Code snippets
No response
OS
linux, ubuntu 20.04
Python version
3.12.1
Library version
openai v 1.42.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1676","Completions.parse() got an unexpected keyword argument 'stream'","2024-09-03T18:04:49Z","Closed as not planned issue","question","Confirm this is a feature request for the Python library and not the underlying OpenAI API.
 This is a feature request for the Python library
Describe the feature or improvement you're requesting
The new client.beta.chat.completions.parse() does not yet support streaming.
I want to do something like:
  response = client.beta.chat.completions.parse(
      model=MODEL_NAME,
      messages=[
          {""role"": ""system"", ""content"": prompt},
          {""role"": ""user"", ""content"": content},
      ],
      temperature=0,
      top_p=0.4,
      stream=True,
      response_format=DocumentStructure,
  )
But this will throw the following error:
TypeError: Completions.parse() got an unexpected keyword argument 'stream'
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1673","Cache-control headers are not set when polling agent status","2024-08-25T03:39:07Z","Open issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
When using client.beta.threads.runs.create_and_poll, I would expect the Cache-Control header to be set to no-cache, so that an upstream caching proxy does not store the agent response while polling the same API URL.
To Reproduce
Setup a caching proxy, e.g., llm_proxy
$ git clone git@github.com:Proxati/llm_proxy.git
$ cd llm_proxy

Start the proxy in cache mode
$ go run main.go cache --debug

Use the Agent API to make a request
$ cd llm_proxy/examples/python
$ poetry run agent/agent.py

The agent will poll status forever, because the cache will store the status response because the request doesn't have a Cache-Control header set.
Code snippets
thread = client.beta.threads.create()

message = client.beta.threads.messages.create(
    thread_id=thread.id,
    role=""user"",
    content=""Can you help me? How does AI work?"",
)

run = client.beta.threads.runs.create_and_poll(
    thread_id=thread.id,
    assistant_id=assistant.id,
    instructions=""When using a caching proxy, you will never return a 'completed' status"",
)


### OS

any

### Python version

any

### Library version

openai-1.42.0

 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1672","file attatched to thread cannot be searched","2024-08-23T14:28:33Z","Closed as not planned issue","API-feedback","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
i have a assistant with tool file search
and create a new thread , ask question ""who is berryhoneffaaa"" (berryhoneffaaa is person only me know in my private knowlege database) , assisitant anwered ""I couldn't find any information on ""berryhoneffaaa"" in the files provided. If you have any additional context or details about Berryhoneffaaa, please provide them so I can better assist you."" . this is what i excepted
and then i upload my private knowlege file and attatached the file to the thread , and ask ""who is berryhoneffaaa"" , it stillanswer ""i dont known""
if i upload file before a ask the question, the assistant file search will search the uploaded file and get the right answer
To Reproduce
see the Describe
Code snippets
openai playground have this problem too
OS
macOS
Python version
python3.10
Library version
openai 1.40.8
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1671","insufficient_quota in _exceptions.py","2024-08-22T18:07:25Z","Open issue","No label","Confirm this is a feature request for the Python library and not the underlying OpenAI API.
 This is a feature request for the Python library
Describe the feature or improvement you're requesting
Ratelimit error is not the only error related to code 429, insufficient funds is another one.
 Can we have another exception for this?
 Ratelimit is already handled internally by openai sdk, but insufficient funds needs to be handled externally by using a backup key.
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1670","ChatCompletionSystemMessageParam.name is not optional but the description says it should be","2024-08-22T14:03:33Z","Closed as not planned issue","No label","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
The class param ChatCompletionSystemMessageParam.name is described as being optional, in fact, the API works without issues if we don't pass this param. But the type is not marked as optional. (Same issue with ChatCompletionUserMessageParam.name and ChatCompletionAssistantMessageParam.name).
Instead of being declared as
class ChatCompletionSystemMessageParam(TypedDict, total=False):
    content: Required[Union[str, Iterable[ChatCompletionContentPartTextParam]]]
    """"""The contents of the system message.""""""

    role: Required[Literal[""system""]]
    """"""The role of the messages author, in this case `system`.""""""

    name: str
    """"""An optional name for the participant.    Provides the model information to differentiate between participants of the same    role.    """"""
It should be declared as:
class ChatCompletionSystemMessageParam(TypedDict, total=False):
    content: Required[Union[str, Iterable[ChatCompletionContentPartTextParam]]]
    """"""The contents of the system message.""""""

    role: Required[Literal[""system""]]
    """"""The role of the messages author, in this case `system`.""""""

    name:  Optional[str]
    """"""An optional name for the participant.    Provides the model information to differentiate between participants of the same    role.    """"""
To Reproduce
Given this test file:
# test.py
import openai

client = openai.OpenAI()

response = client.beta.chat.completions.parse(
    model=""gpt-4o-mini"",
    messages=[
        {""role"": ""system"", ""content"": ""You are an assistant.""},  # <-  mypy claims this line has an error
        {
            ""role"": ""user"",
            ""content"": [
                {""type"": ""text"", ""text"": ""What's up?""},
            ],
        },
    ],
)

Run:
mypy .
Output:
test.py:8: error: Type of TypedDict is ambiguous, none of (""ChatCompletionSystemMessageParam"", ""ChatCompletionUserMessageParam"", ""ChatCompletionAssistantMessageParam"") matches cleanly  [misc]
Code snippets
No response
OS
ubuntu
Python version
Python 3.12.2
Library version
openai 1.42.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1669","Run Lifecycle Documentation Broken Link in README","2024-09-03T18:52:18Z","Closed issue","bug,documentation","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
The error involves a broken link in the README file that was supposed to direct users to the ""Run Lifecycle Documentation."" The broken link prevents users from accessing essential information about the project's execution stages, leading to potential confusion or errors in using the project. Fixing it is important for proper guidance.
To Reproduce
Navigate to the Repository: Open the GitHub
Locate the README File: Scroll down the main repository page to find the README file
Find the Polling Helpers Section:
Within the ""Polling Helpers"" section, look for a link labeled ""Run Lifecycle Documentation.""
Click on this link.
Observe the Error:
""404 Not Found"" on https://platform.openai.com/docs/assistants/how-it-works/run-lifecycle
Code snippets
No response
OS
All
Python version
All
Library version
All
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1660","Request for Endpoint to List All Threads","2024-08-19T15:13:47Z","Closed as not planned issue","No label","Confirm this is a feature request for the Python library and not the underlying OpenAI API.
 This is a feature request for the Python library
Describe the feature or improvement you're requesting
Hey mate, hope all is well.
I would like to request the addition of an endpoint that allows for the retrieval of all threads. If there are concerns about potential security risks, such as team members or others within the same organization gaining access to all threads, I suggest implementing an additional layer of security. For example, you could restrict the listing of threads to those generated under a specific project and token bearer.
Additionally, I have noticed that this request has been reiterated multiple times since January but has not yet been addressed. Given the ongoing interest and potential impact of this feature, I believe it would be highly beneficial for many users.
This enhancement would significantly improve the usability and flexibility of the API, particularly for teams working on collaborative projects.
Thank you for considering this request. I look forward to your feedback.
Cheers
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1659","Apply more fixes for Pydantic schema incompatibilities with OpenAI structured outputs","2024-08-17T17:21:04Z","Open issue","No label","Confirm this is a feature request for the Python library and not the underlying OpenAI API.
 This is a feature request for the Python library
Describe the feature or improvement you're requesting
I noticed that you guys are doing some manipulation of Pydantic's generated schema to ensure compatibility with the API's schema validation. I found a few more instances that can be addressed:
Issues:
optional fields with pydantic defaults generate an unsupported 'default' field in the schema
date fields generate a format='date-time' field in the schema which is not supported
The test cases below builds on your to_strict_json_schema function and removes addresses these problematic fields with the remove_property_from_schema function:
class Publisher(BaseModel):
    name: str = Field(description=""The name publisher"")
    url: Optional[str] = Field(None, description=""The URL of the publisher's website"")
    class Config:
        json_schema_extra = {
            ""additionalProperties"": False
        }

class Article(BaseModel):
    title: str = Field(description=""The title of the news article"")
    published: Optional[datetime] = Field(None, description=""The date the article was published. Use ISO 8601 to format this value."")
    publisher: Optional[Publisher] = Field(None, description=""The publisher of the article"")
    class Config:
        json_schema_extra = {
            ""additionalProperties"": False
        }
        
class NewsArticles(BaseModel):
    query: str = Field(description=""The query used to search for news articles"")
    articles: List[Article] = Field(description=""The list of news articles returned by the query"")
    class Config:
        json_schema_extra = {
            ""additionalProperties"": False
        }
    

def test_schema_compatible():
    client = OpenAI()
    
    # build on the internals that the openai client uses to clean up the pydantic schema for the openai API
    schema = to_strict_json_schema(NewsArticles)
    
    # optional fields with pydantic defaults generate an unsupported 'default' field in the schema
    remove_property_from_schema(schema, ""default"")
    # date fields generate a format='date-time' field in the schema which is not supported
    remove_property_from_schema(schema, ""format"")
        
    logger.info(""Generated Schema: %s"", json.dumps(schema, indent=2))
    completion = client.beta.chat.completions.parse(
        model=""gpt-4o-2024-08-06"",
        temperature=0,
        messages=[
            {
                ""role"": ""user"",
                ""content"":  ""What where the top headlines in the US for January 6th, 2021?"",
            }
        ],
        response_format={
            ""type"": ""json_schema"",
            ""json_schema"": {
                ""schema"": schema,
                ""name"": ""NewsArticles"",
                ""strict"": True,
            }
        }
    )
    result = NewsArticles.model_validate_json(completion.choices[0].message.content)
    assert result is not None



def remove_property_from_schema(schema: dict, property_name: str):
    if 'properties' in schema:
        for field_name, field in schema['properties'].items():
            if 'properties' in field:
                remove_property_from_schema(field, property_name)
            if 'anyOf' in field: 
                for any_of in field['anyOf']:
                    any_of.pop(property_name, None)
            field.pop(property_name, None)
    if '$defs' in schema:                    
        for definition_name, definition in schema['$defs'].items():
            remove_property_from_schema(definition, property_name)
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
👍3
Milo-Kerr-ICS, schroedermichael, and yarelm reacted with thumbs up emoji
All reactions
👍3 reactions"
"https://github.com/openai/openai-python/issues/1658","Errors after migrating openai","2024-08-17T18:06:00Z","Closed as not planned issue","question","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
[{
 ""resource"": ""/home/dmtarmey/agent-zero/main.py"",
 ""owner"": ""generated_diagnostic_collection_name#1"",
 ""code"": {
 ""value"": ""reportUndefinedVariable"",
 ""target"": {
 ""$mid"": 1,
 ""path"": ""/microsoft/pyright/blob/main/docs/configuration.md"",
 ""scheme"": ""https"",
 ""authority"": ""github.com"",
 ""fragment"": ""reportUndefinedVariable""
 }
 },
 ""severity"": 8,
 ""message"": """"get_openaiAI"" is not defined"",
 ""source"": ""Pylance"",
 ""startLineNumber"": 9,
 ""startColumn"": 10,
 ""endLineNumber"": 9,
 ""endColumn"": 22
 }]
To Reproduce
python main.py ggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggg
Code snippets
import osfrom dotenv import load_dotenvfrom openai import OpenAI

# Load environment variables from .env fileload_dotenv()

# Initialize the DeepAI client with the API keyclient = get_openaiAI(api_key=os.environ.get(""API_KEY_OPENAI""))

def get_openai_response(prompt):
    """"""Fetch a response from the DeepAI API based on the input prompt.""""""
    try:
        response = client.chat.completions.create(
            model=""gpt-3.5-turbo"",
            messages=[{""role"": ""user"", ""content"": prompt}]
        )
        return response.choices[0].message.content  # Return the response content
    except Exception as e:
        print(f""Error occurred: {e}"")
        return ""An error occurred. Please try again later.""

def main():
    prompt = ""Tell me a joke.""  # Your desired prompt here
    response = get_openai_response(prompt)  # Get response
    print(""Response:"", response)

if __name__ == ""__main__"":
    main()
OS
linux tuxido
Python version
python3.11
Library version
openai 1.41.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1647","openai migrate - ""ERROR (code: 200) - Too many params for text: expected maximum 1""","2024-08-14T11:33:49Z","Closed issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
ERROR (code: 200) - Too many params for text: expected maximum 1 when running openai migrate
To Reproduce
openai migrate
Code snippets
No response
OS
macOS
Python version
Python 3.12.5
Library version
1.40.6
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1643","Throw an error (or auto-remove the file) if the batch request failed.","2024-08-12T19:43:07Z","Open issue","No label","Confirm this is a feature request for the Python library and not the underlying OpenAI API.
 This is a feature request for the Python library
Describe the feature or improvement you're requesting
I've been playing with the batch API and have encountered many TPM limits within the last 24 hours. Although my current TPM limit caused it, I think there are two parts to consider/improve.
I initially thought that the requested *.jsonl file would be automatically removed, but later, I realized that I was required to manually remove all the failed batch job posts from my end. In this regard, I think it would be helpful if I could provide an additional parameter to ""client.batches.create"" so that it automatically removes a file if the batch request fails.
I expected that the ""client.batches.create"" call would at least throw an error if the batch posting failed due to the TPM limit, but it didn't, and I always need to check by calling ""client.batches.retrieve."" I think it would be great to either throw an error for the API limit failure cases or at least provide any details in the 'errors' or 'failed_at' parameter from the response of ""client.batches.create.""
Additional context
Both of them may need some edits on the backend API server as well (either way would work, and I think server-side support would be a bit clearer, but anyway), but I believe it would be worthwhile to review at least.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1639","ChatCompletionStreamManager object does not support the asynchronous context manager protocol","2024-08-12T19:01:20Z","Closed issue","documentation","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
The docs here say that the following should be possible
import openai
import asyncio

async def test_streaming():
    client = openai.OpenAI()

    async with client.beta.chat.completions.stream(
        model='gpt-4o-2024-08-06',
        messages=[
            {""role"": ""system"", ""content"": ""You are a helpful assistant.""},
            {""role"": ""user"", ""content"": ""Tell me a joke.""},
        ],
    ) as stream:
        async for event in stream:
            if event.type == 'content.delta':
                print(event.delta, flush=True, end='')
            elif event.type == 'content.done':
                print(""\nContent generation complete."")
                break

# Run the streaming test
asyncio.run(test_streaming())

However, this gives
TypeError: 'ChatCompletionStreamManager' object does not support the asynchronous context manager protocol
When I run without async it works fine ie
import openai

def test_streaming():
    client = openai.OpenAI()

    with client.beta.chat.completions.stream(
        model='gpt-4o-2024-08-06',
        messages=[
            {""role"": ""system"", ""content"": ""You are a helpful assistant.""},
            {""role"": ""user"", ""content"": ""Tell me a joke.""},
        ],
    ) as stream:
        for event in stream:
            if event.type == 'content.delta':
                print(event.delta, flush=True, end='')
            elif event.type == 'content.done':
                print(""\nContent generation complete."")
                break

# Run the streaming test
test_streaming()


To Reproduce
Run the above code snippet which is the beta async chat_completion (and should handle the new pydantic parsing)
Code snippets
OS
macOS
Python version
Python 3.11-3.12
Library version
1.40.4
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1635","client.chat.completions.create not working with base64 images","2024-08-11T18:47:09Z","Closed as not planned issue","No label","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
No idea why, but when using the lib to give gtp-4o-mini an image in base64 it's giving me the following error:
openai.BadRequestError: Error code: 400 - {'error': {'message': ""Invalid type for 'messages[0].content[1].image_url': expected an object, but got a string instead."", 'type': 'invalid_request_error', 'param': 'messages[0].content[1].image_url', 'code': 'invalid_type'}}

It's essentialy the same code provided in https://platform.openai.com/docs/guides/vision/uploading-base-64-encoded-images which works fine for me.
To Reproduce
Just run the code snippet, I guess.
Code snippets
client = OpenAI(
  project='',
  api_key=""""
)

response = client.chat.completions.create(
        model=""gpt-4o-mini"",
        messages=[
            {
                ""role"": ""user"",
                ""content"": [
                    {""type"": ""text"", ""text"": ""Describe this image in a few words.""},
                    {
                        ""type"": ""image_url"",
                        ""image_url"": f""data:image/jpeg;base64,{base64_image}""
                    },
                ],
            }
        ],
        max_tokens=100
    )
OS
macOS
Python version
Python 3.10.14
Library version
1.40.3
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1633","The return type hint of client.audio.transcriptions.create() is incorrect.","2024-09-27T23:04:24Z","Closed issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
The return type hint of client.audio.transcriptions.create() is Transcription, and this type has a text attribute.
However, when the response_format is equal to text, srt, or vtt, its return value is str.
 In this case, calling the text attribute will result in an error.
To Reproduce
import openaifrom dotenv import load_dotenv

load_dotenv()

file_path = ""example.wav""

client = openai.OpenAI()
with open(file_path, ""rb"") as file:
    transcript = client.audio.transcriptions.create(
        model=""whisper-1"", file=file, response_format=""srt""
    )

    print(transcript)
    print(type(transcript))
    print(transcript.text)
    print(type(transcript.text))
OS
Windows 10
Python version
Python v3.12.4
Library version
openai v1.40.3
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1632","client.beta.chat.completions.parse error 400: Missing required parameter: 'response_format.json_schema'","2024-08-10T07:35:53Z","Closed issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
I literally just copied the structured output example from inside the official blog and it won't run:
from dotenv import load_dotenvfrom openai import OpenAIfrom pydantic import BaseModel

load_dotenv()


class Step(BaseModel):
    explanation: str
    output: str


class MathResponse(BaseModel):
    steps: list[Step]
    final_answer: str


client = OpenAI()

completion = client.beta.chat.completions.parse(
    model=""gpt-4o-2024-08-06"",
    messages=[
        {""role"": ""system"", ""content"": ""You are a helpful math tutor.""},
        {""role"": ""user"", ""content"": ""solve 8x + 31 = 2""},
    ],
    response_format=MathResponse,
)

message = completion.choices[0].messageif message.parsed:
    print(message.parsed.steps)
    print(message.parsed.final_answer)
else:
    print(message.refusal)
output:
raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': ""Missing required parameter: 'response_format.json_schema'. (request id: ) (request id: 20240810152859212775791fSLFCpem)"", 'type': 'invalid_request_error', 'param': 'response_format.json_schema', 'code': 'missing_required_parameter'}}
I'm so confused now🤔. Maybe something going wrong inside beta.chat.completions.parse.
To Reproduce
Copy the example from blog
run
Code snippets
No response
OS
macOS
Python version
Python v3.12
Library version
openai v1.40.1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1626","openai.InternalServerError: upstream connect error or disconnect/reset before headers. reset reason: connection failure, transport failure reason: delayed connect error: 111","2024-08-09T12:58:23Z","Closed as not planned issue","No label","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
I am doing a hundreds of requests by hour, 95% works perfectly.
 Sometimes it crashes with this error:
openai.InternalServerError: upstream connect error or disconnect/reset before headers. reset reason: connection failure, transport failure reason: delayed connect error: 111
The thing is that the same code works most of the time and fails sometime, making the debugging really hard, I failed to find the pattern that make theses requests fail, it can arrive anytime.
It can pass with big requests, with a lot of requests at the same time. It can crash with tiny request and with few requests in the same time.
 So the intuition that they are too much request or too big request seems wrong.
I have this Traceback:
return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File ""/usr/local/lib/python3.11/site-packages/openai/resources/chat/completions.py"", line 650, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File ""/usr/local/lib/python3.11/site-packages/openai/_base_client.py"", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""/usr/local/lib/python3.11/site-packages/openai/_base_client.py"", line 936, in request\n    return self._request(\n           ^^^^^^^^^^^^^^\n  File ""/usr/local/lib/python3.11/site-packages/openai/_base_client.py"", line 1025, in _request\n    return self._retry_request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File ""/usr/local/lib/python3.11/site-packages/openai/_base_client.py"", line 1074, in _retry_request\n    return self._request(\n           ^^^^^^^^^^^^^^\n  File ""/usr/local/lib/python3.11/site-packages/openai/_base_client.py"", line 1025, in _request\n    return self._retry_request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File ""/usr/local/lib/python3.11/site-packages/openai/_base_client.py"", line 1074, in _retry_request\n    return self._request(\n           ^^^^^^^^^^^^^^\n  File ""/usr/local/lib/python3.11/site-packages/openai/_base_client.py"", line 1040, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.InternalServerError: upstream connect error or disconnect/reset before headers. reset reason: connection failure, transport failure reason: delayed connect error: 111\n

To Reproduce
from langchain_openai import ChatOpenAI
llm = ChatOpenAI(
 model=""gpt-4o"",
 temperature=0,
 max_tokens=None,
 timeout=None,
 max_retries=2,
 )
Code snippets
No response
OS
Linux
Python version
python v3.11
Library version
openai v1.35.10
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1625","openai.APIError: The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error","2024-08-14T02:45:21Z","Closed issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
When I use langchain astream_events to call openai stream api, I will get this api error some times.
    async for item in self._iterator:
  File ""C:\Users\jimmy\AppData\Local\Programs\Python\Python311\Lib\site-packages\openai\_streaming.py"", line 174, in __stream__
    raise APIError(
openai.APIError: The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error.

To Reproduce
Use langchain agent astream_events. link
Use openai be the llm model.
Code snippets
No response
OS
windows11
Python version
Python3.11.8
Library version
openai 1.35.13
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1622","Unable to access the OpenAI API with genuine key","2024-08-08T08:36:57Z","Closed as not planned issue","question","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
As per the recommendations I used openai.Completion.create with openai version 0.28.0 as per this link #742 and used openai.ChatCompletion.create with the most latest version 1.40.1
In both the cases I get error
Error in both versions 1.40.1 0.28.0
An error occurred:
You tried to access openai.Completion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.
You can run openai migrate to automatically upgrade your codebase to use the 1.0.0 interface.
Alternatively, you can pin your installation to the old version, e.g. pip install openai==0.28
A detailed migration guide is available here: #742
To Reproduce
pip install --upgrade openai
for 1.40.1
pip install openai==0.28
response = openai.ChatCompletion.create(
 model=""gpt-3.5-turbo"", # Use a model like ""gpt-3.5-turbo"" or ""gpt-4""
 messages=[
 {""role"": ""user"", ""content"": prompt}
 ],
 max_tokens=1024,
 temperature=0.5
 )
For 0.28.0
response = openai.Completion.create(
 model=""gpt-3.5-turbo"", # Use a model like ""gpt-3.5-turbo"" or ""gpt-4""
 messages=[
 {""role"": ""user"", ""content"": prompt}
 ],
 max_tokens=1024,
 temperature=0.5
 )
Error
You tried to access openai.Completion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.
You can run openai migrate to automatically upgrade your codebase to use the 1.0.0 interface.
Alternatively, you can pin your installation to the old version, e.g. pip install openai==0.28
A detailed migration guide is available here: #742
Code snippets
As explained above
OS
Windows
Python version
3.11.7
Library version
1.40.1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1620","Arbitrary file write during tarfile extraction","2024-08-17T18:08:17Z","Closed as not planned issue","No label","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
Extracting files from a malicious tar archive without validating that the destination file path is within the destination directory can cause files outside the destination directory to be overwritten, due to the possible presence of directory traversal elements (..) in archive paths.
To Reproduce
Issue present in this line - 
openai-python/src/openai/cli/_tools/migrate.py
 Line 144 in 631a2a7
	archive.extractall(unpacked_dir) 
Code snippets
https://github.com/openai/openai-python/blob/631a2a7156299351874f37c4769308a104ce19ed/src/openai/cli/_tools/migrate.py#L144
OS
macOS
Python version
3.11.4
Library version
1.40.1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1618","Too permissive permissions in file","2024-08-17T18:10:08Z","Closed as not planned issue","No label","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
Extracting files from a malicious tar archive without validating that the destination file path is within the destination directory can cause files outside the destination directory to be overwritten, due to the possible presence of directory traversal elements (..) in archive paths.
Affected file: https://github.com/openai/openai-python/blob/631a2a7156299351874f37c4769308a104ce19ed/src/openai/cli/_tools/migrate.py#L151-151
To Reproduce
Bug present in file - https://github.com/openai/openai-python/blob/631a2a7156299351874f37c4769308a104ce19ed/src/openai/cli/_tools/migrate.py#L151-151
Code snippets
Indicated here - https://github.com/openai/openai-python/blob/631a2a7156299351874f37c4769308a104ce19ed/src/openai/cli/_tools/migrate.py#L151-151
OS
macOS
Python version
3.11.4
Library version
1.40.1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1617","BUG: version 1.40.x: NameError for CompletionCreateParamsNonStreaming","2024-08-08T10:08:28Z","Closed as not planned issue","No label","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
Error stack:
File ~/miniconda3/lib/python3.9/site-packages/pydantic/v1/validators.py:738, in find_validators(type_, config)
    736     return
    737 if is_typeddict(type_):
--> 738     yield make_typeddict_validator(type_, config)
    739     return
    741 class_ = get_class(type_)

File ~/miniconda3/lib/python3.9/site-packages/pydantic/v1/validators.py:624, in make_typeddict_validator(typeddict_cls, config)
    619 def make_typeddict_validator(
    620     typeddict_cls: Type['TypedDict'], config: Type['BaseConfig']  # type: ignore[valid-type]
    621 ) -> Callable[[Any], Dict[str, Any]]:
    622     from .annotated_types import create_model_from_typeddict
--> 624     TypedDictModel = create_model_from_typeddict(
    625         typeddict_cls,
    626         __config__=config,
    627         __module__=typeddict_cls.__module__,
    628     )
    629     typeddict_cls.__pydantic_model__ = TypedDictModel  # type: ignore[attr-defined]
    631     def typeddict_validator(values: 'TypedDict') -> Dict[str, Any]:  # type: ignore[valid-type]

File ~/miniconda3/lib/python3.9/site-packages/pydantic/v1/annotated_types.py:55, in create_model_from_typeddict(typeddict_cls, **kwargs)
     49 required_keys: FrozenSet[str] = typeddict_cls.__required_keys__  # type: ignore[attr-defined]
     50 field_definitions = {
     51     field_name: (field_type, Required if field_name in required_keys else None)
     52     for field_name, field_type in typeddict_cls.__annotations__.items()
     53 }
---> 55 return create_model(typeddict_cls.__name__, **kwargs, **field_definitions)

File ~/miniconda3/lib/python3.9/site-packages/pydantic/v1/main.py:1024, in create_model(__model_name, __config__, __base__, __module__, __validators__, __cls_kwargs__, __slots__, **field_definitions)
   1022     ns['__orig_bases__'] = __base__
   1023 namespace.update(ns)
-> 1024 return meta(__model_name, resolved_bases, namespace, **kwds)

File ~/miniconda3/lib/python3.9/site-packages/pydantic/v1/main.py:197, in ModelMetaclass.__new__(mcs, name, bases, namespace, **kwargs)
    189     if (
    190         is_untouched(value)
    191         and ann_type != PyObject
   (...)
    194         )
    195     ):
    196         continue
--> 197     fields[ann_name] = ModelField.infer(
    198         name=ann_name,
    199         value=value,
    200         annotation=ann_type,
    201         class_validators=vg.get_validators(ann_name),
    202         config=config,
    203     )
    204 elif ann_name not in namespace and config.underscore_attrs_are_private:
    205     private_attributes[ann_name] = PrivateAttr()

File ~/miniconda3/lib/python3.9/site-packages/pydantic/v1/fields.py:504, in ModelField.infer(cls, name, value, annotation, class_validators, config)
    501     required = False
    502 annotation = get_annotation_from_field_info(annotation, field_info, name, config.validate_assignment)
--> 504 return cls(
    505     name=name,
    506     type_=annotation,
    507     alias=field_info.alias,
    508     class_validators=class_validators,
    509     default=value,
    510     default_factory=field_info.default_factory,
    511     required=required,
    512     model_config=config,
    513     field_info=field_info,
    514 )

File ~/miniconda3/lib/python3.9/site-packages/pydantic/v1/fields.py:434, in ModelField.__init__(self, name, type_, class_validators, model_config, default, default_factory, required, final, alias, field_info)
    432 self.shape: int = SHAPE_SINGLETON
    433 self.model_config.prepare_field(self)
--> 434 self.prepare()

File ~/miniconda3/lib/python3.9/site-packages/pydantic/v1/fields.py:555, in ModelField.prepare(self)
    553 if self.default is Undefined and self.default_factory is None:
    554     self.default = None
--> 555 self.populate_validators()

File ~/miniconda3/lib/python3.9/site-packages/pydantic/v1/fields.py:829, in ModelField.populate_validators(self)
    825 if not self.sub_fields or self.shape == SHAPE_GENERIC:
    826     get_validators = getattr(self.type_, '__get_validators__', None)
    827     v_funcs = (
    828         *[v.func for v in class_validators_ if v.each_item and v.pre],
--> 829         *(get_validators() if get_validators else list(find_validators(self.type_, self.model_config))),
    830         *[v.func for v in class_validators_ if v.each_item and not v.pre],
    831     )
    832     self.validators = prep_validators(v_funcs)
    834 self.pre_validators = []

File ~/miniconda3/lib/python3.9/site-packages/pydantic/v1/validators.py:738, in find_validators(type_, config)
    736     return
    737 if is_typeddict(type_):
--> 738     yield make_typeddict_validator(type_, config)
    739     return
    741 class_ = get_class(type_)

File ~/miniconda3/lib/python3.9/site-packages/pydantic/v1/validators.py:624, in make_typeddict_validator(typeddict_cls, config)
    619 def make_typeddict_validator(
    620     typeddict_cls: Type['TypedDict'], config: Type['BaseConfig']  # type: ignore[valid-type]
    621 ) -> Callable[[Any], Dict[str, Any]]:
    622     from .annotated_types import create_model_from_typeddict
--> 624     TypedDictModel = create_model_from_typeddict(
    625         typeddict_cls,
    626         __config__=config,
    627         __module__=typeddict_cls.__module__,
    628     )
    629     typeddict_cls.__pydantic_model__ = TypedDictModel  # type: ignore[attr-defined]
    631     def typeddict_validator(values: 'TypedDict') -> Dict[str, Any]:  # type: ignore[valid-type]

File ~/miniconda3/lib/python3.9/site-packages/pydantic/v1/annotated_types.py:55, in create_model_from_typeddict(typeddict_cls, **kwargs)
     49 required_keys: FrozenSet[str] = typeddict_cls.__required_keys__  # type: ignore[attr-defined]
     50 field_definitions = {
     51     field_name: (field_type, Required if field_name in required_keys else None)
     52     for field_name, field_type in typeddict_cls.__annotations__.items()
     53 }
---> 55 return create_model(typeddict_cls.__name__, **kwargs, **field_definitions)

File ~/miniconda3/lib/python3.9/site-packages/pydantic/v1/main.py:1024, in create_model(__model_name, __config__, __base__, __module__, __validators__, __cls_kwargs__, __slots__, **field_definitions)
   1022     ns['__orig_bases__'] = __base__
   1023 namespace.update(ns)
-> 1024 return meta(__model_name, resolved_bases, namespace, **kwds)

File ~/miniconda3/lib/python3.9/site-packages/pydantic/v1/main.py:186, in ModelMetaclass.__new__(mcs, name, bases, namespace, **kwargs)
    184     class_vars.add(ann_name)
    185 elif is_valid_field(ann_name):
--> 186     validate_field_name(bases, ann_name)
    187     value = namespace.get(ann_name, Undefined)
    188     allowed_types = get_args(ann_type) if is_union(get_origin(ann_type)) else (ann_type,)

File ~/miniconda3/lib/python3.9/site-packages/pydantic/v1/utils.py:167, in validate_field_name(bases, field_name)
    165 for base in bases:
    166     if getattr(base, field_name, None):
--> 167         raise NameError(
    168             f'Field name ""{field_name}"" shadows a BaseModel attribute; '
    169             f'use a different field name with ""alias=\'{field_name}\'"".'
    170         )

NameError: Field name ""schema"" shadows a BaseModel attribute; use a different field name with ""alias='schema'"".

Before v1.40.1, everything works fine.
To Reproduce
Mini code:
from openai.types.chat.completion_create_params
from openai.types.chat.completion_create_params import CompletionCreateParamsNonStreaming
from pydantic.v1 import create_model_from_typeddict
create_model_from_typeddict(CompletionCreateParamsNonStreaming)

Code snippets
No response
OS
I think all the os should have this issue
Python version
Python 3.9
Library version
openai v1.40.1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1616","replace jiter dependency with build-in pydantic function","2024-08-14T15:12:20Z","Closed as not planned issue","No label","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
https://pypi.org/project/jiter/
This is a standalone version of the JSON parser used in pydantic-core
The recommendation is to only use this package directly if you do not use pydantic.

The reason of this request is some OS (Gentoo in my case) do provide both pydantic and pydantic-core libraries, but jiter
openai-python uses pydantic already, so it should be possible to re-use build-in function (perhaps, via pydantic-core)
To Reproduce
inspect this line: https://github.com/openai/openai-python/blob/main/pyproject.toml#L19
Code snippets
https://github.com/openai/openai-python/blob/main/src/openai/lib/streaming/chat/_completions.py
 jiter import from_json
OS
any
Python version
any
Library version
1.40.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1614","Support pydantic dataclasses in structured outputs","2024-08-20T22:54:05Z","Closed issue","enhancement","Confirm this is a feature request for the Python library and not the underlying OpenAI API.
 This is a feature request for the Python library
Describe the feature or improvement you're requesting
It would be great to be able to use data schemas defined as pydantic dataclasses in structured outputs. E.g.
from pydantic.dataclasses import dataclassfrom openai import OpenAI

client = OpenAI()

@dataclassclass CalendarEvent:
    name: str
    date: str
    participants: list[str]

completion = client.beta.chat.completions.parse(
    model=""gpt-4o-2024-08-06"",
    messages=[
        {""role"": ""system"", ""content"": ""Extract the event information.""},
        {""role"": ""user"", ""content"": ""Alice and Bob are going to a science fair on Friday.""},
    ],
    response_format=CalendarEvent,
)

event = completion.choices[0].message.parsed
Pydantic dataclasses can be easily transformed into JSON schema via the model_json_schema function.
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
👍1
karth295 reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/openai-python/issues/1611","Add Support for API Key Provider","2024-08-06T19:15:16Z","Open issue","No label","Confirm this is a feature request for the Python library and not the underlying OpenAI API.
 This is a feature request for the Python library
Describe the feature or improvement you're requesting
Similar to azure_ad_token_provider, support an openai_api_key_provider for OpenAI and AzureOpenAI instances. The application scenario is that OpenAI API keys are managed by some OAuth2 token servers, where a request is posted to the token servers and an API key with expiration is granted. In this case, for long running services, each time a request for OpenAI API is made, it is necessary to check and update the cached OpenAI API key. Therefore, I think it would be good to have an openai_api_key_provider to manage this situation.
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
👍2
mariosacaj and smurching reacted with thumbs up emoji
All reactions
👍2 reactions"
"https://github.com/openai/openai-python/issues/1596","httpx client has very poor performance for concurrent requests compared to aiohttp","2024-08-05T14:15:06Z","Closed as not planned issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
The API client uses httpx, which has very poor performance when making concurrent requests compared to aiohttp. Open issue for httpx here
This is forcing us to swap out the OpenAI SDK for our own implementation, which is a pain.
I suspect it is the root cause of the difference between node.js and Python demonstrated here
I'm not massively familiar with the development of this SDK, and whether there is a key reason for picking httpx over aiohttp. From my reading it was switched over for V1 in order to create consistency between sync and async clients, but I'm not sure how vital it is to achieve this. However for our high concurrency async use cases this renders the SDK useless.
To Reproduce
To reproduce, run chat completion requests in parallel with 20+ concurrent requests, benchmarking the openai API client against an implementation using aiohttp. Example code can be found in the linked issue in httpx.
Code snippets
No response
OS
Linux/MacOs
Python version
v3.12
Library version
1.12.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1595","be explicit about parallel_tool_call option for streams","2024-08-05T14:24:49Z","Closed issue","No label","Confirm this is a feature request for the Python library and not the underlying OpenAI API.
 This is a feature request for the Python library
Describe the feature or improvement you're requesting
For runs created by opening a stream, parallel_tool_call options could only be set to false using the extra_body argument, because the runs.stream function does not accept parallel_tool_call as an argument.
 Because multiple tool outputs can only be submitted in a single request, managing and syncing the state of each tool_calls can add unnecessary complication in some use_cases. In that case, it requires testing to find out that turning off the parallel_tool_call option is also possible in streams, since documentation for parallel_tool_call is only provided for non-stream approach.
I've checked that parallel_tool_call option works as expected if passed in as extra_body. Although this works fine for me, I think such option should be explicitly set as keyword argument.
Additional context
I would like to try and make a PR for this if my request turns out to be valid. Thanks
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1593","Add Support for New Administration API Endpoints","2024-08-02T21:42:17Z","Open issue","No label","Confirm this is a feature request for the Python library and not the underlying OpenAI API.
 This is a feature request for the Python library
Describe the feature or improvement you're requesting
Following the recent release of the new API endpoints by OpenAI, as documented here, I would like to request the addition of support for these new administration endpoints in our project.
Details:
The new API endpoints introduced by OpenAI enhance the capabilities available for administration tasks. To fully leverage these improvements, this project should incorporate support for the new endpoints.
Incorporating these endpoints will align our project with the latest capabilities offered by OpenAI and enhance our administrative functionalities. The official documentation provides comprehensive details and examples to facilitate this integration.
References:
OpenAI API Reference - Administration
Thank you for considering this request. Implementing these changes will significantly benefit our project's administration capabilities.
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
👍2
davidloiret and EnriqueGF reacted with thumbs up emoji
All reactions
👍2 reactions"
"https://github.com/openai/openai-python/issues/1591","I can no longer upload files to vector store with AzureOpenAI","2024-08-02T07:27:48Z","Open issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
Hi,
From 2 days till now i'm getting error when I try to upload files in vector stores using AzureOpenAI package.
The same code works with OpenAI.
I changed nothing in my code but from 31/07/2024 it doesn't work with AzureOpenAI.
The output of file_batch:
File batch: FileCounts(cancelled=0, completed=0, failed=1, in_progress=0, total=1)
File batch status: failed

File status: failed
File last error: LastError(code='server_error', message='An internal error occurred.')

Are there some problems with AzureOpenAI ?
Thanks,
 Matteo
To Reproduce
Use a simple file.txt or other types.
Execute the code and see the result.
Code snippets
from openai import AzureOpenAIclient = AzureOpenAI(
      api_key=os.getenv(""AZURE_OPENAI_API_KEY""),  
      api_version=""2024-05-01-preview"",
      azure_endpoint = os.getenv(""AZURE_OPENAI_ENDPOINT"")
      )
file_stream = open(""path/of/my/simple/file.txt"", ""rb"")

vector_store = client.beta.vector_stores.create(name=""vs_test_assistant_v2"")
vector_store_id = vector_store.id

print(""Uploading file to vector store.."")
file_batch = client.beta.vector_stores.file_batches.upload_and_poll(
          vector_store_id=vector_store_id, 
          files=[file_stream],
          )

print(f""File batch status: {file_batch.status}"")
print(f""File batch: {file_batch.file_counts}"")
file = client.beta.vector_stores.files.list(vector_store_id).data[0]
print(f""File status: {file.status}"")
if file.status == ""failed"":
    print(f""File last error: {file.last_error}"")
OS
Linux
Python version
Python v3.10.12
Library version
openai v1.37.2
 The text was updated successfully, but these errors were encountered: 
👍1
AmineDjeghri reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/openai-python/issues/1583","_strict_response_validation deprecation and Azure content filters","2024-07-29T09:43:18Z","Closed issue","question","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
If _strict_response_validation may be deprecated, will the fixed behavior be set to False?
The Azure content filter annotation requires object attribute set to """". It relies on _strict_response_validation=False:
https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/content-filter?tabs=warning%2Cuser-prompt%2Cpython-new#annotations-and-sample-responses
To Reproduce
Enable content filters in Azure OpenAI Service.
Stream chat completions with client:
client = AzureOpenAI(
    azure_endpoint=""<azure-endpoint>"",
    azure_deployment=""gpt-4-turbo"",
    api_key=""<api-key>"",
    api_version=""2024-02-01"",
    _strict_response_validation=True # This will make strict validations and the request will fail.
)
Error:
object: input should be 'chat.completion.chunk' [type=literal_error, input_value='', input_type=str]

Code snippets
No response
OS
Ubuntu 22.04
Python version
v3.10.0
Library version
v1.37.1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1580","add parallel_tool_calls to Runs.create_and_poll","2024-07-29T08:55:09Z","Closed issue","No label","Confirm this is a feature request for the Python library and not the underlying OpenAI API.
 This is a feature request for the Python library
Describe the feature or improvement you're requesting
Currently client.beta.threads.runs.create_and_poll doesn't accept parallel_tool_calls as an argument, and throws:
TypeError: Runs.create_and_poll() got an unexpected keyword argument 'parallel_tool_calls'
It can be included and passed to self.create here:
https://github.com/openai/openai-python/blob/195c05a64d39c87b2dfdf1eca2d339597f1fce03/src/openai/resources/beta/threads/runs/runs.py#L866C12-L889C10
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1579","feature request: proactive client-side rate limiting","2024-07-25T04:22:07Z","Open issue","No label","Confirm this is a feature request for the Python library and not the underlying OpenAI API.
 This is a feature request for the Python library
Describe the feature or improvement you're requesting
When making batch requests using LangChain, with an OpenAI model as shown in this minimal repro, it is common to hit the organizational rate limit for tokens per minute (TPM) - as demonstrated in this error log.
Whilst limiting the concurrency of batches, and introducing exponential backoff can be used to reduce this issue downstream in LangChain - I believe there is also room for the OpenAI#request function in this library to more intelligently handle parallel invocations so as to better support batch requests, regardless of whether this library, langchain or another codebase is responsible for initiating the batch requests.
In particular, I would suggest that the SyncAPIClient create queue(s) of requests and determine when enqueued requests can be run based on the x-ratelimit-* and retry-after headers of existing responses.
Additional context
Related to #937 (comment)
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1577","Embeddings","2024-07-24T09:56:58Z","Closed as not planned issue","question","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
Error:
You tried to access openai.Embedding, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.
You can run openai migrate to automatically upgrade your codebase to use the 1.0.0 interface.
Alternatively, you can pin your installation to the old version, e.g. pip install openai==0.28
A detailed migration guide is available here: #742
To Reproduce
Code snippets
def get_embeddings(self, chunks):
        embeddings = []
        for chunk in chunks:
            try:
          
                response = openai.Embedding.create(
                    input=chunk  
                    model=""text-embedding-ada-002""
                )
                embeddings.append(response['data'][0]['embedding'])
            except Exception as e:
                print(f""Error: {e}"")
        return embeddings
OS
Windows
Python version
Python 3.11.5
Library version
openai 1.37.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1574","Assistant with gpt-4o and gpt-4o-mini may call unsupported tool 'browser' and throw exception","2024-07-23T10:49:28Z","Open issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
When calling the Assistant API and selecting the gpt-4o or gpt-4o-mini model, the model may attempt to call an unsupported browser tool. The Python SDK does not define this type of ToolCall , resulting in an exception being thrown.
File """"assistant.py"""", line 62, in <module>
    stream.until_done()
  File """"/usr/local/lib/python3.7/site-packages/openai/lib/streaming/_assistants.py"""", line 102, in until_done
    consume_sync_iterator(self)
  File """"/usr/local/lib/python3.7/site-packages/openai/_utils/_streams.py"""", line 6, in consume_sync_iterator
    for _ in iterator:
  File """"/usr/local/lib/python3.7/site-packages/openai/lib/streaming/_assistants.py"""", line 69, in __iter__
    for item in self._iterator:
  File """"/usr/local/lib/python3.7/site-packages/openai/lib/streaming/_assistants.py"""", line 406, in __stream__
    self._emit_sse_event(event)
  File """"/usr/local/lib/python3.7/site-packages/openai/lib/streaming/_assistants.py"""", line 267, in _emit_sse_event
    run_step_snapshots=self.__run_step_snapshots,
  File """"/usr/local/lib/python3.7/site-packages/openai/lib/streaming/_assistants.py"""", line 913, in accumulate_run_step
    data.delta.model_dump(exclude_unset=True),
AttributeError: 'dict' object has no attribute 'model_dump'

The root cause of this issue is that the API returns unexpected content. However, the SDK should be able to handle this scenario gracefully.
To Reproduce
Ask assistant to use browser tool but only give it code interpreter.
with openai.beta.threads.create_and_run_stream(
    assistant_id='an assistant only supports Code Interpreter',
    model='gpt-4o-mini',
    instructions=""Use browser tool first to answer the user's question"",
    thread= {
        'messages': [{'role': 'user', 'content': 'Who is Tom'}]
    },
    tool_choice='required',
) as stream:
    stream.until_done()
This issue can occur under normal usage scenarios, but it is consistently reproducible with the specified configuration.
Code snippets
No response
OS
Linux
Python version
Python 3.7.4
Library version
openai v1.36.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1571","Error code: 400 'param': 'messages.[2].content', 'code': None","2024-07-23T01:10:05Z","Open issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
Using the agent capability of Langchain, three tools have been added
 Sometimes there may be errors, sometimes it may be normal
 Error message:
File ""/Users/gujiachun/anaconda3/envs/langchain-test/lib/python3.11/site-packages/openai/_base_client.py"", line 1240, in post
 return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
 File ""/Users/gujiachun/anaconda3/envs/langchain-test/lib/python3.11/site-packages/openai/_base_client.py"", line 921, in request
 return self._request(
 ^^^^^^^^^^^^^^
 File ""/Users/gujiachun/anaconda3/envs/langchain-test/lib/python3.11/site-packages/openai/_base_client.py"", line 1020, in _request
 raise self._make_status_error_from_response(err.response) from None
 openai.BadRequestError: Error code: 400 - {'error': {'message': ""Invalid value for 'content': expected a string, got null. (request id: 20240723090211496742562ZksEJFx0) (request id: 2024072301021136579144958040285)"", 'type': 'invalid_request_error', 'param': 'messages.[2].content', 'code': None}}
To Reproduce
from datetime import date
 from operator import eq, itemgetter
import requests
 from langchain.chains.query_constructor.schema import AttributeInfo
 from langchain.retrievers import SelfQueryRetriever
 from langchain_community.agent_toolkits.load_tools import load_tools
 from langchain_community.utilities import SerpAPIWrapper
 from langchain_core.documents import Document
 from langchain_core.output_parsers import StrOutputParser
 from langchain_core.prompts import ChatPromptTemplate, PromptTemplate
 from langchain_core.runnables import RunnableParallel, RunnablePassthrough, RunnableLambda
 from langchain_core.structured_query import Comparison, Comparator
 from langchain_core.tools import tool
 from langchain_huggingface import HuggingFaceEmbeddings
 from langchain_community.vectorstores import Milvus
 from langchain_openai import ChatOpenAI
 from langchain_core.runnables import chain
from langchain import hub
 from langchain.agents import create_openai_functions_agent
from langchain.agents import AgentExecutor
serpapi_api_key = ""54d5973f9487d329ffdf22c773bb2514a94f37242d8d1311a4817e96a7386c14""
api_key = ""sk-jFFKnyLzOGW4njGc9b68Fb5e1dB04c198aCfCcC3C894Fa0a""
 api_url = ""https://ai-yyds.com/v1""
llm = ChatOpenAI(base_url=api_url, api_key=api_key, model_name=""gpt-4"")
一个最简单的模版,带记忆
prompt = hub.pull(""hwchase17/openai-functions-agent"")
 print(prompt.messages)
@tool
 def search(text: str):
 """"""This tool is only used when real-time information needs to be searched. The search returns only the first 3 items""""""
 serp = SerpAPIWrapper(serpapi_api_key=serpapi_api_key)
response = serp.run(text)
print(type(response))
content = """"
if type(response) is list:
    for item in response:
        content += str(item[""title""]) + ""\n""
else:
    content = response
return content

@tool
 def time() -> str:
 """"""Return today's date and use it for any questions related to today's date.
 The input should always be an empty string, and this function will always return today's date. Any mathematical operation on a date should occur outside of this function""""""
 return str(date.today())
@tool
 def weather(city: str):
 """"""When you need to check the weather, you can use this tool, which returns the weather conditions for the day, tomorrow, and the day after tomorrow""""""
 url = ""https://api.seniverse.com/v3/weather/daily.json?key=SrlXSW6OX9PssfOJ1&location=beijing&language=zh-Hans&unit=c&start=0""
response = requests.get(url)

data = response.json()

if not data or len(data['results']) == 0:
    return None

daily = data['results'][0][""daily""]

content = """"
res = []
for day in daily:
    info = {""city"": city, ""date"": day[""date""], ""info"": day[""text_day""], ""temperature_high"": day[""high""],
            ""temperature_low"": day[""low""]}
    content += f""{city} date：{day['date']}  info：{day['text_day']} maximum temperature：{day['high']} minimum temperature:{day['low']}\n""
    res.append(info)

return content

tools = [time, weather, search]
agent = create_openai_functions_agent(llm, tools, prompt)
agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)
chain1 = agent_executor | StrOutputParser()
for chunk in agent_executor.stream({""input"": ""What's the weather like in Shanghai today""}):
 if ""output"" in chunk:
 print(f'{chunk[""output""]}')
Code snippets
from datetime import datefrom operator import eq, itemgetter

import requestsfrom langchain.chains.query_constructor.schema import AttributeInfofrom langchain.retrievers import SelfQueryRetrieverfrom langchain_community.agent_toolkits.load_tools import load_toolsfrom langchain_community.utilities import SerpAPIWrapperfrom langchain_core.documents import Documentfrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.prompts import ChatPromptTemplate, PromptTemplatefrom langchain_core.runnables import RunnableParallel, RunnablePassthrough, RunnableLambdafrom langchain_core.structured_query import Comparison, Comparatorfrom langchain_core.tools import toolfrom langchain_huggingface import HuggingFaceEmbeddingsfrom langchain_community.vectorstores import Milvusfrom langchain_openai import ChatOpenAIfrom langchain_core.runnables import chain

from langchain import hubfrom langchain.agents import create_openai_functions_agent

from langchain.agents import AgentExecutor

serpapi_api_key = ""54d5973f9487d329ffdf22c773bb2514a94f37242d8d1311a4817e96a7386c14""

api_key = ""sk-jFFKnyLzOGW4njGc9b68Fb5e1dB04c198aCfCcC3C894Fa0a""api_url = ""https://ai-yyds.com/v1""

llm = ChatOpenAI(base_url=api_url, api_key=api_key, model_name=""gpt-4"")

# 一个最简单的模版,带记忆prompt = hub.pull(""hwchase17/openai-functions-agent"")
print(prompt.messages)


@tooldef search(text: str):
    """"""This tool is only used when real-time information needs to be searched. The search returns only the first 3 items""""""
    serp = SerpAPIWrapper(serpapi_api_key=serpapi_api_key)

    response = serp.run(text)
    print(type(response))
    content = """"
    if type(response) is list:
        for item in response:
            content += str(item[""title""]) + ""\n""
    else:
        content = response
    return content


@tooldef time() -> str:
    """"""Return today's date and use it for any questions related to today's date.The input should always be an empty string, and this function will always return today's date. Any mathematical operation on a date should occur outside of this function""""""
    return str(date.today())


@tooldef weather(city: str):
    """"""When you need to check the weather, you can use this tool, which returns the weather conditions for the day, tomorrow, and the day after tomorrow""""""
    url = ""https://api.seniverse.com/v3/weather/daily.json?key=SrlXSW6OX9PssfOJ1&location=beijing&language=zh-Hans&unit=c&start=0""
   
    response = requests.get(url)
   
    data = response.json()

    if not data or len(data['results']) == 0:
        return None

    daily = data['results'][0][""daily""]

    content = """"
    res = []
    for day in daily:
        info = {""city"": city, ""date"": day[""date""], ""info"": day[""text_day""], ""temperature_high"": day[""high""],
                ""temperature_low"": day[""low""]}
        content += f""{city} date：{day['date']}  info：{day['text_day']} maximum temperature：{day['high']} minimum temperature:{day['low']}\n""
        res.append(info)

    return content


tools = [time, weather, search]

agent = create_openai_functions_agent(llm, tools, prompt)

agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)

chain1 = agent_executor | StrOutputParser()

for chunk in agent_executor.stream({""input"": ""What's the weather like in Shanghai today""}):
    if ""output"" in chunk:
        print(f'{chunk[""output""]}')
OS
macos
Python version
Python 3.8.19
Library version
openai 1.37
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1564","Adding support for customized file name","2024-07-22T10:46:25Z","Closed issue","question","Confirm this is a feature request for the Python library and not the underlying OpenAI API.
 This is a feature request for the Python library
Describe the feature or improvement you're requesting
On my server I save the file with a different name, and I would like to customize the name of the created file by passing a parameter to the client.files.create method as in the example below. Is this viable?
attachment = client.files.create(file=open(""randomfilename.pdf"", ""rb""), purpose=purpose, filename=""MyCustomFilename.pdf"")
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1558","ModuleNotFoundError: No module named 'typing_extensions' when installed with poetry","2024-07-17T09:53:45Z","Closed as not planned issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
Poetry incorrectly handles the dependencies of the package. As far as I understand, issues like this most often arise due to non-PEP-compliant dependency specifications. I may be wrong about this and it could be an issue with poetry itself. Thanks.
To Reproduce
Create empty venv with Python 3.9, create basic pyproject.toml, add openai as a dependency using poetry, try to import OpenAI from openai:
➜  openai-breaking ls   
➜  openai-breaking python3.9 -m venv venv
➜  openai-breaking source venv/bin/activate
(venv) ➜  openai-breaking poetry init

This command will guide you through creating your pyproject.toml config.

Package name [openai-breaking]:  
Version [0.1.0]:  
Description []:  
Author [Danila Mikhaltsov <dmikhaltsov@ispras.ru>, n to skip]:  
License []:  
Compatible Python versions [^3.9]:  

Would you like to define your main dependencies interactively? (yes/no) [yes] no
Would you like to define your development dependencies interactively? (yes/no) [yes] no
Generated file

[tool.poetry]
name = ""openai-breaking""
version = ""0.1.0""
description = """"
authors = [""Danila Mikhaltsov <dmikhaltsov@ispras.ru>""]
readme = ""README.md""
packages = [{include = ""openai_breaking""}]

[tool.poetry.dependencies]
python = ""^3.9""


[build-system]
requires = [""poetry-core""]
build-backend = ""poetry.core.masonry.api""


Do you confirm generation? (yes/no) [yes] 
(venv) ➜  openai-breaking poetry add openai 
Using version ^1.35.14 for openai

Updating dependencies
Resolving dependencies... (0.1s)

Writing lock file

Package operations: 1 install, 0 updates, 0 removals

  • Installing openai (1.35.14)
(venv) ➜  openai-breaking python 
Python 3.9.17 (main, Jun  6 2023, 20:11:21) 
[GCC 11.3.0] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> from openai import OpenAI
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/dmikhaltsov_/code/tmp/openai-breaking/venv/lib/python3.9/site-packages/openai/__init__.py"", line 6, in <module>
    from typing_extensions import override
ModuleNotFoundError: No module named 'typing_extensions'
>>> 

However, when installing with pip, all is fine:
➜  openai-breaking python3.9 -m venv venv
➜  openai-breaking source venv/bin/activate
(venv) ➜  openai-breaking pip install openai
Collecting openai
  Downloading openai-1.35.14-py3-none-any.whl (328 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 328.5/328.5 kB 4.2 MB/s eta 0:00:00
Collecting sniffio
  Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)
Collecting anyio<5,>=3.5.0
  Using cached anyio-4.4.0-py3-none-any.whl (86 kB)
Collecting httpx<1,>=0.23.0
  Using cached httpx-0.27.0-py3-none-any.whl (75 kB)
Collecting distro<2,>=1.7.0
  Downloading distro-1.9.0-py3-none-any.whl (20 kB)
Collecting typing-extensions<5,>=4.7
  Using cached typing_extensions-4.12.2-py3-none-any.whl (37 kB)
Collecting tqdm>4
  Using cached tqdm-4.66.4-py3-none-any.whl (78 kB)
Collecting pydantic<3,>=1.9.0
  Downloading pydantic-2.8.2-py3-none-any.whl (423 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 423.9/423.9 kB 15.7 MB/s eta 0:00:00
Collecting idna>=2.8
  Using cached idna-3.7-py3-none-any.whl (66 kB)
Collecting exceptiongroup>=1.0.2
  Downloading exceptiongroup-1.2.2-py3-none-any.whl (16 kB)
Collecting httpcore==1.*
  Using cached httpcore-1.0.5-py3-none-any.whl (77 kB)
Collecting certifi
  Downloading certifi-2024.7.4-py3-none-any.whl (162 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 163.0/163.0 kB 31.4 MB/s eta 0:00:00
Collecting h11<0.15,>=0.13
  Using cached h11-0.14.0-py3-none-any.whl (58 kB)
Collecting annotated-types>=0.4.0
  Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)
Collecting pydantic-core==2.20.1
  Downloading pydantic_core-2.20.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.1/2.1 MB 51.6 MB/s eta 0:00:00
Installing collected packages: typing-extensions, tqdm, sniffio, idna, h11, exceptiongroup, distro, certifi, annotated-types, pydantic-core, httpcore, anyio, pydantic, httpx, openai
Successfully installed annotated-types-0.7.0 anyio-4.4.0 certifi-2024.7.4 distro-1.9.0 exceptiongroup-1.2.2 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 idna-3.7 openai-1.35.14 pydantic-2.8.2 pydantic-core-2.20.1 sniffio-1.3.1 tqdm-4.66.4 typing-extensions-4.12.2

[notice] A new release of pip is available: 23.0.1 -> 24.1.2
[notice] To update, run: pip install --upgrade pip
(venv) ➜  openai-breaking python
Python 3.9.17 (main, Jun  6 2023, 20:11:21) 
[GCC 11.3.0] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> from openai import OpenAI
>>> 

Code snippets
No response
OS
Linux
Python version
3.9.17
Library version
1.35.14
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1556","Just updated the README.md file to make it easier for people to understand like novices like myself, it explains what things are, what they do.","2024-07-17T09:47:16Z","Closed as not planned issue","No label","Confirm this is a feature request for the Python library and not the underlying OpenAI API.
 This is a feature request for the Python library
Describe the feature or improvement you're requesting
Enhanced README.md with Teen-Friendly Explanations.
N.B if needed i can complete the readme and have all the internal hyperlinks working to connect to the Quick Definitions section. And to have these links in blue for people to learn.
Summary
 This pull request updates the README.md file, specifically enhancing the Pagination section with additional explanations tailored for younger audiences (around 15 years old). The changes aim to make the documentation more accessible and easier to understand for newcomers to the OpenAI API and programming concepts.
 Changes Made
Added emoji-prefixed explanations after each code block in the Pagination section.
 Introduced relatable analogies (e.g., books, relay races, encyclopedias) to explain complex concepts.
 Maintained the original code examples and structure while improving readability.
 Ensured explanations cover both synchronous and asynchronous pagination methods.
Benefits
Improves accessibility of the documentation for younger developers or those new to API concepts.
 Provides clear, relatable explanations without sacrificing technical accuracy.
 Enhances overall user experience by making complex topics more approachable.
Additional context
This is 90% finished, but wanted to share before i finished if this is not needed, but love helping out and making the README easier to understand by new users who know nothing (Just like myself!)
README.md
The content you are editing has changed. Please copy your edits and refresh the page.
Tasks
BetaGive feedback
No tasks being tracked yet.
Options
Convert to issue
Toggle completion
Rename
Remove
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1551","Validate Function Specifications","2024-07-16T11:07:06Z","Closed as not planned issue","No label","Confirm this is a feature request for the Python library and not the underlying OpenAI API.
 This is a feature request for the Python library
Describe the feature or improvement you're requesting
It would be nice to have a warning or error message when incorrect function specification is passed while using the Function Calling.
Example:
Right Usage:
tools = [
    {
        ""type"": ""function"",
        ""function"": {
            ""name"": ""get_current_weather"",
            ""description"": ""Get the current weather in a given location"",
            ""parameters"": {
                ""type"": ""object"",
                ""properties"": {
                    ""location"": {
                        ""type"": ""string"",
                        ""description"": ""The city and state, e.g. San Francisco, CA"",
                    },
                    ""unit"": {""type"": ""string"", ""enum"": [""celsius"", ""fahrenheit""]},
                },
                ""required"": [""location""],
            },
        },
    }
]
Wrong Usage:
tools = [
    {
        ""type"": ""function"",
        ""function"": {
            ""name"": ""get_current_weather"",
            ""description"": ""Get the current weather in a given location"",
            ""input_schema"": {
                ""type"": ""object"",
                ""properties"": {
                    ""location"": {
                        ""type"": ""string"",
                        ""description"": ""The city and state, e.g. San Francisco, CA"",
                    },
                    ""unit"": {""type"": ""string"", ""enum"": [""celsius"", ""fahrenheit""]},
                },
                ""required"": [""location""],
            },
        },
    }
]
While in this case the model generalizes and passes the correct arguments, for a different function, the model passes empty arguments.
Example:
tools = [
    {
        ""type"": ""function"",
        ""function"": {
            ""name"": ""sematic_search"",
            ""description"": ""Use this function to retrieve a list of the most relevant and semantically similar answers to the user's question."",
            ""input_schema"": {
                ""type"": ""object"",
                ""properties"": {
                    ""user_question"": {
                        ""type"": ""string"",
                        ""description"": ""The user_question for which sematically similar answers need to be retrieved. This parameter is mandatory and should be a meaningful question or phrase."",
                    }
                },
                ""required"": [""user_question""],
            },
            ""output_schema"": {
                ""type"": ""string"",
                ""description"": ""The retrieved semantically similar answers to user question"",
            },
        },
    }
]
I understand that LLMs are non deterministic in nature, but we can programmatically make consuming LLMs more deterministic.
This kind of functionality exists already when using the JSON mode, even the prompt is scrutinized here.
'messages' must contain the word 'json' in some form, to use 'response_format' of type 'json_object'
Thus, it is even more critical to verify the function specification to ensure some kind of predictable behavior and helpful error messages.
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1545","Something went wrong during completion 2. Reason: Message text is empty","2024-07-15T09:46:53Z","Closed as not planned issue","No label","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
Something went wrong during completion 2. Reason: Message text is empty
To Reproduce
chatgpt_telegram_bot | 2024-07-14 21:40:09,080 - DEBUG - httpcore.http11 - response_closed.complete
 chatgpt_telegram_bot | 2024-07-14 21:40:09,080 - DEBUG - openai_utils - Pre-processed answer:
 chatgpt_telegram_bot | 2024-07-14 21:40:09,080 - DEBUG - openai_utils - Post-processed answer:
 chatgpt_telegram_bot | 2024-07-14 21:40:09,081 - ERROR - openai_utils - Message text is empty
 chatgpt_telegram_bot | 2024-07-14 21:40:09,081 - ERROR - openai_utils - Exception: Message text is empty
 chatgpt_telegram_bot | 2024-07-14 21:40:09,082 - ERROR - main - Something went wrong during completion 2. Reason: Message text is empty
 chatgpt_telegram_bot | 2024-07-14 21:40:09,082 - DEBUG - telegram.ext.ExtBot - Passing request through rate limiter of type <class 'telegram.ext._aioratelimiter.AIORateLimiter'> with rate_limit_args None
 chatgpt_telegram_bot | 2024-07-14 21:40:09,083 - DEBUG - telegram.ext.ExtBot - Calling Bot API endpoint sendMessage with parameters {'chat_id': 5212252839, 'text': 'Something went wrong during completion 2. Reason: Message text is empty'}
Code snippets
class ChatGPT:
    def __init__(self, model=""gpt-4-1106-preview""):
        assert model in {
            ""text-davinci-003"", ""gpt-3.5-turbo-16k"", ""gpt-3.5-turbo"", ""gpt-4"", ""gpt-4-1106-preview"", 
            ""gpt-4-vision-preview"", ""gpt-4-turbo-2024-04-09"", ""gpt-4o""
        }, f""Unknown model: {model}""
        self.model = model
        self.logger = logging.getLogger(__name__)
        self.headers = {
            ""Authorization"": f""Bearer {config.openai_api_key}"",
            ""Content-Type"": ""application/json"",
        }
        self.client = global_client

    async def send_message(self, message, dialog_messages=[], chat_mode=""assistant""):
        if chat_mode not in config.chat_modes.keys():
            raise ValueError(f""Chat mode {chat_mode} is not supported"")

        n_dialog_messages_before = len(dialog_messages)
        answer = None
        n_input_tokens, n_output_tokens = 0, 0
        n_first_dialog_messages_removed = 0
        while answer is None:
            try:
                if self.model in {""gpt-3.5-turbo-16k"", ""gpt-3.5-turbo"", ""gpt-4"", ""gpt-4-1106-preview"", ""gpt-4-vision-preview"", ""gpt-4-turbo-2024-04-09"", ""gpt-4o""}:
                    messages = self._generate_prompt_messages(message, dialog_messages, chat_mode)
                    self.logger.debug(f""Generated messages: {messages}"")
                    validate_payload({
                        ""model"": self.model,
                        ""messages"": messages,
                        **OPENAI_COMPLETION_OPTIONS
                    })
                    r = await self.client.chat.completions.create(
                        model=self.model,
                        messages=messages,
                        **OPENAI_COMPLETION_OPTIONS
                    )
                    self.logger.debug(f""OpenAI API response: {r}"")
                    if not r.choices or not r.choices[0].message or not r.choices[0].message.content:
                        self.logger.error(""Received empty message content from OpenAI API."")
                        raise ValueError(""Received empty message content from OpenAI API."")
                    answer = r.choices[0].message.content
                elif self.model == ""text-davinci-003"":
                    prompt = self._generate_prompt(message, dialog_messages, chat_mode)
                    self.logger.debug(f""Generated prompt: {prompt}"")
                    validate_payload({
                        ""model"": self.model,
                        ""prompt"": prompt,
                        **OPENAI_COMPLETION_OPTIONS
                    })
                    r = await self.client.completions.create(
                        model=self.model,
                        prompt=prompt,
                        **OPENAI_COMPLETION_OPTIONS
                    )
                    self.logger.debug(f""OpenAI API response: {r}"")
                    if not r.choices or not r.choices[0].text:
                        self.logger.error(""Received empty message content from OpenAI API."")
                        raise ValueError(""Received empty message content from OpenAI API."")
                    answer = r.choices[0].text
                else:
                    raise ValueError(f""Unknown model: {self.model}"")

                answer = self._postprocess_answer(answer)
                if not answer.strip():
                    self.logger.error(""Post-processed answer is empty"")
                    raise ValueError(""Post-processed answer is empty"")
                n_input_tokens, n_output_tokens = r.usage.prompt_tokens, r.usage.completion_tokens
            except Exception as e:
                self.logger.error(f""Exception: {str(e)}"")
                if len(dialog_messages) == 0:
                    raise ValueError(""Dialog messages is reduced to zero, but still has too many tokens to make completion"") from e

                dialog_messages = dialog_messages[1:]

        n_first_dialog_messages_removed = n_dialog_messages_before - len(dialog_messages)

        return answer, (n_input_tokens, n_output_tokens), n_first_dialog_messages_removed

    async def send_message_stream(self, message, dialog_messages=[], chat_mode=""assistant""):
        if chat_mode not in config.chat_modes.keys():
            raise ValueError(f""Chat mode {chat_mode} is not supported"")

        n_dialog_messages_before = len(dialog_messages)
        answer = None
        n_input_tokens, n_output_tokens, n_first_dialog_messages_removed = 0, 0, 0
        while answer is None:
            try:
                if self.model in {""gpt-3.5-turbo-16k"", ""gpt-3.5-turbo"", ""gpt-4"", ""gpt-4-1106-preview"", ""gpt-4-turbo-2024-04-09"", ""gpt-4o""}:
                    messages = self._generate_prompt_messages(message, dialog_messages, chat_mode)
                    self.logger.debug(f""Generated messages: {messages}"")
                    
                    r_gen = await self.client.chat.completions.create(
                        model=self.model,
                        messages=messages,
                        stream=True,
                        **OPENAI_COMPLETION_OPTIONS
                    )

                    answer = """"
                    async for r_item in r_gen:
                        delta = r_item.choices[0].delta

                        if ""content"" in delta:
                            answer += delta.content
                            n_input_tokens, n_output_tokens = self._count_tokens_from_messages(messages, answer, model=self.model)
                            n_first_dialog_messages_removed = 0

                            yield ""not_finished"", answer, (n_input_tokens, n_output_tokens), n_first_dialog_messages_removed
                                
                elif self.model == ""text-davinci-003"":
                    prompt = self._generate_prompt(message, dialog_messages, chat_mode)
                    self.logger.debug(f""Generated prompt: {prompt}"")
                    r_gen = self.client.completions.create(
                        model=self.model,
                        prompt=prompt,
                        stream=True,
                        **OPENAI_COMPLETION_OPTIONS
                    )

                    answer = """"
                    async for r_item in r_gen:
                        if not r_item.choices or not r_item.choices[0].text:
                            self.logger.error(""Received empty message content from OpenAI API stream."")
                            raise ValueError(""Received empty message content from OpenAI API stream."")
                        answer += r_item.choices[0].text
                        n_input_tokens, n_output_tokens = self._count_tokens_from_prompt(prompt, answer, model=self.model)
                        n_first_dialog_messages_removed = n_dialog_messages_before - len(dialog_messages)
                        yield ""not_finished"", answer, (n_input_tokens, n_output_tokens), n_first_dialog_messages_removed

                answer = self._postprocess_answer(answer)
                if not answer.strip():
                    self.logger.error(""Message text is empty"")
                    raise ValueError(""Message text is empty"")

            except Exception as e:
                self.logger.error(f""Exception: {str(e)}"")
                if len(dialog_messages) == 0:
                    raise e

                dialog_messages = dialog_messages[1:]
                n_first_dialog_messages_removed = n_dialog_messages_before - len(dialog_messages)

        yield ""finished"", answer, (n_input_tokens, n_output_tokens), n_first_dialog_messages_removed

    async def send_vision_message(
        self,
        message,
        dialog_messages=[],
        chat_mode=""assistant"",
        image_buffer: BytesIO = None,
    ):
        n_dialog_messages_before = len(dialog_messages)
        answer = None
        n_input_tokens, n_output_tokens = 0, 0
        n_first_dialog_messages_removed = 0
        while answer is None:
            try:
                if self.model == ""gpt-4-vision-preview"":
                    messages = self._generate_prompt_messages(
                        message, dialog_messages, chat_mode, image_buffer
                    )
                    self.logger.debug(f""Generated messages: {messages}"")
                    r = await self.client.chat.completions.create(
                        model=self.model,
                        messages=messages,
                        **OPENAI_COMPLETION_OPTIONS
                    )
                    self.logger.debug(f""OpenAI API response: {r}"")
                    if not r.choices or not r.choices[0].message or not r.choices[0].message.content:
                        self.logger.error(""Received empty message content from OpenAI API."")
                        raise ValueError(""Received empty message content from OpenAI API."")
                    answer = r.choices[0].message.content
                else:
                    raise ValueError(f""Unsupported model: {self.model}"")

                answer = self._postprocess_answer(answer)
                if not answer.strip():
                    self.logger.error(""Message text is empty"")
                    raise ValueError(""Message text is empty"")
                n_input_tokens, n_output_tokens = (
                    r.usage.prompt_tokens,
                    r.usage.completion_tokens,
                )
            except Exception as e:
                self.logger.error(f""Exception: {str(e)}"")
                if len(dialog_messages) == 0:
                    raise ValueError(
                        ""Dialog messages is reduced to zero, but still has too many tokens to make completion""
                    ) from e

                dialog_messages = dialog_messages[1:]

            n_first_dialog_messages_removed = n_dialog_messages_before - len(dialog_messages)

        return (
            answer,
            (n_input_tokens, n_output_tokens),
            n_first_dialog_messages_removed,
        )

    async def send_vision_message_stream(
        self,
        message,
        dialog_messages=[],
        chat_mode=""assistant"",
        image_buffer: BytesIO = None,
    ):
        n_dialog_messages_before = len(dialog_messages)
        answer = None
        n_input_tokens, n_output_tokens = 0, 0
        n_first_dialog_messages_removed = 0
        while answer is None:
            try:
                if self.model == ""gpt-4-vision-preview"":
                    messages = self._generate_prompt_messages(
                        message, dialog_messages, chat_mode, image_buffer
                    )
                    self.logger.debug(f""Generated messages: {messages}"")
                    
                    r_gen = await self.client.chat.completions.create(
                        model=self.model,
                        messages=messages,
                        stream=True,
                        **OPENAI_COMPLETION_OPTIONS,
                    )

                    answer = """"
                    async for r_item in r_gen:
                        delta = r_item.choices[0].delta
                        if ""content"" in delta:
                            answer += delta.content
                            (
                                n_input_tokens,
                                n_output_tokens,
                            ) = self._count_tokens_from_messages(
                                messages, answer, model=self.model
                            )
                            n_first_dialog_messages_removed = (
                                n_dialog_messages_before - len(dialog_messages)
                            )
                            yield ""not_finished"", answer, (
                                n_input_tokens,
                                n_output_tokens,
                            ), n_first_dialog_messages_removed

                answer = self._postprocess_answer(answer)
                if not answer.strip():
                    self.logger.error(""Message text is empty"")
                    raise ValueError(""Message text is empty"")

            except Exception as e:
                self.logger.error(f""Exception: {str(e)}"")
                if len(dialog_messages) == 0:
                    raise e
                dialog_messages = dialog_messages[1:]

        yield ""finished"", answer, (
            n_input_tokens,
            n_output_tokens,
        ), n_first_dialog_messages_removed

    def _generate_prompt(self, message, dialog_messages, chat_mode):
        prompt = config.chat_modes[chat_mode][""prompt_start""]
        prompt += ""\n\n""

        if len(dialog_messages) > 0:
            prompt += ""Chat:\n""
            for dialog_message in dialog_messages:
                prompt += f""User: {dialog_message['user']}\n""
                prompt += f""Assistant: {dialog_message['bot']}\n""

        prompt += f""User: {message}\n""
        prompt += ""Assistant: ""

        return prompt

    def _encode_image(self, image_buffer: BytesIO) -> bytes:
        return base64.b64encode(image_buffer.read()).decode(""utf-8"")

    def _generate_prompt_messages(self, message, dialog_messages, chat_mode, image_buffer: BytesIO = None):
        prompt = config.chat_modes[chat_mode][""prompt_start""]

        messages = [{""role"": ""system"", ""content"": prompt}]

        for dialog_message in dialog_messages:
            messages.append({""role"": ""user"", ""content"": dialog_message[""user""]})
            messages.append({""role"": ""assistant"", ""content"": dialog_message[""bot""]})

        if image_buffer is not None:
            messages.append(
                {
                    ""role"": ""user"", 
                    ""content"": [
                        {
                            ""type"": ""text"",
                            ""text"": message,
                        },
                        {
                            ""type"": ""image"",
                            ""image"": self._encode_image(image_buffer),
                        }
                    ]
                }
                
            )
        else:
            messages.append({""role"": ""user"", ""content"": message})

        return messages

    def _postprocess_answer(self, answer):
        self.logger.debug(f""Pre-processed answer: {answer}"")
        answer = answer.strip()
        self.logger.debug(f""Post-processed answer: {answer}"")
        return answer

    def _count_tokens_from_messages(self, messages, answer, model=""gpt-4-1106-preview""):
        encoding = tiktoken.encoding_for_model(model)

        tokens_per_message = 3
        tokens_per_name = 1

        if model.startswith(""gpt-3""):
            tokens_per_message = 4
            tokens_per_name = -1
        elif model.startswith(""gpt-4""):
            tokens_per_message = 3
            tokens_per_name = 1 
        else:
            raise ValueError(f""Unknown model: {model}"")

        n_input_tokens = 0
        for message in messages:
            n_input_tokens += tokens_per_message
            if isinstance(message[""content""], list):
                for sub_message in message[""content""]:
                    if ""type"" in sub_message:
                        if sub_message[""type""] == ""text"":
                            n_input_tokens += len(encoding.encode(sub_message[""text""]))
                        elif sub_message[""type""] == ""image_url"":
                            pass
            else:
                if ""type"" in message:
                    if message[""type""] == ""text"":
                        n_input_tokens += len(encoding.encode(message[""text""]))
                    elif message[""type""] == ""image_url"":
                        pass

        n_input_tokens += 2

        n_output_tokens = 1 + len(encoding.encode(answer))

        return n_input_tokens, n_output_tokens

    def _count_tokens_from_prompt(self, prompt, answer, model=""text-davinci-003""):
        encoding = tiktoken.encoding_for_model(model)

        n_input_tokens = len(encoding.encode(prompt)) + 1
        n_output_tokens = len(encoding.encode(answer))

        return n_input_tokens, n_output_tokens
    
async def transcribe_audio(audio_file) -> str:
    r = await global_client.audio.transcriptions.create(
        model=""whisper-1"",
        file=audio_file
    )
    return r.text or """"


async def generate_images(prompt, model=""dall-e-2"", n_images=4, size=""1024x1024"", quality=""standard""):
    if model==""dalle-2"":
        model=""dall-e-2""
        quality=""standard""

    if model==""dalle-3"":
        model=""dall-e-3""
        n_images=1

    response = await global_client.images.generate(
        model=model,
        prompt=prompt,
        n=n_images,
        size=size,
        quality=quality
    )

    image_urls = [item.url for item in response.data]
    return image_urls


async def is_content_acceptable(prompt):
    r = await global_client.moderations.create(input=prompt)
    return not all(r.results[0].categories.values())
OS
Debian 12.0
Python version
Python 3.8
Library version
openAI 1.35
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1540","Bug: ImportError: cannot import name ‘ThreadMessage’ from ‘openai.types.beta.threads’","2024-07-10T18:11:22Z","Closed as not planned issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
After updating openai via conda i got the above error message:
ImportError: cannot import name ‘ThreadMessage’ from ‘openai.types.beta.threads’
It is also reported here:
https://community.openai.com/t/importerror-cannot-import-name-threadmessage-from-openai-types-beta-threads/728654
After trying to find the bug, i think it is related to a wrong import statement in:
\Lib\site-packages\openai\resources\beta\threads\messages\messages.py
were for example in the case of message_list_params the module is imported from :
\Lib\site-packages\openai\types\beta\threads\message_list_params.py
There is however no module in:
\Lib\site-packages\openai\types\beta\threads\ThreadMessage.py
instead there is the module
 \Lib\site-packages\openai\types\beta\threads\thread_message.py
which contains the class ThreadMessage.
Either the import in \Lib\site-packages\openai\resources\beta\threads\messages\messages.py is changed to :
#from .....types.beta.threads import ThreadMessage, message_list_params, message_create_params, message_update_params
 from .....types.beta.threads import message_list_params, message_create_params, message_update_params
 from .....types.beta.threads.thread_message import ThreadMessage
or which would be more consistent, the import is changed to:
from .....types.beta.threads import thread_message, message_list_params, message_create_params, message_update_params
and every occurance of:
ThreadMessage
in the code is changed to
thread_message.ThreadMessage
To Reproduce
update to newest openai version via conda and use:
from openai import AzureOpenAI
Code snippets
No response
OS
windows
Python version
3.10
Library version
1.13.3
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1529","There is no way to pass proxy to AsyncOpenAI","2024-07-07T16:49:00Z","Closed as not planned issue","No label","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
The parent of AsyncOpenAI which is AsyncAPIClient Accepts a parameter called proxies. However, there is no way to pass proxies when constructing AsyncOpenAI. This makes it tough to use the library correctly.
Clarification
 It's not ideal to pass an HTTP client at all. AsyncAPIClient constructs a special client called AsyncHttpxClientWrapper with certain defaults; therefore it's hard to make a correct client and pass it as http_client
To Reproduce
1- Try to pass proxies param to AsyncOpenAI
2- It will fail as it does not accept proxies as a parameter
Code snippets
AsyncClient(proxies="""")
# Traceback (most recent call last):#   File ""<stdin>"", line 1, in <module># TypeError: AsyncOpenAI.__init__() got an unexpected keyword argument 'proxies'
OS
Linux
Python version
Pytho v3.12.3
Library version
1.35.10
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1527","AzureOpenAI AuthenticationError","2024-07-06T20:43:33Z","Closed as not planned issue","No label","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
Hi,
I have a .env file with the necessary AzureOpenAI credentials. However, I am getting this error message AuthenticationError:
 Error code: 401 - {'statusCode': 401, 'message': 'Access denied due to missing subscription key. Make sure to include subscription key when making requests to an API.'}
When I try to execute the code from this microsoft learn source: https://learn.microsoft.com/en-us/azure/ai-services/openai/chatgpt-quickstart?tabs=command-line%2Cpython-new&pivots=programming-language-python
To Reproduce
Below is the full python code.
`import os
 from openai import AzureOpenAI
client = AzureOpenAI(
 api_key = os.getenv(""AZURE_OPENAI_KEY""),
 api_version = os.getenv(""AZURE_OPENAI_VERSION""),
 azure_endpoint = os.getenv(""AZURE_OPENAI_ENDPOINT"")
 )
response = client.chat.completions.create(
 model=""gpt-35-turbo"", # model = ""deployment_name"".
 messages=[
 {""role"": ""system"", ""content"": ""Assistant is a large language model trained by OpenAI.""},
 {""role"": ""user"", ""content"": ""Who were the founders of Microsoft?""}
 ]
 )
#print(response)
 print(response.model_dump_json(indent=2))
 print(response.choices[0].message.content)`
Code snippets
No response
OS
Windows
Python version
Python v3.11.7
Library version
openai v1.35.10
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1526","AzureOpenAI authentication issue: new token from azure_ad_token_provider not utilized after expiration","2024-07-09T18:04:44Z","Closed issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
When the initial token expires during a series of retries due to multiple failures, the azure_ad_token_provider generates a new token as expected. However, this new token is not utilized by AzureOpenAI for the remaining retries.
The problem originates from the openai/lib/azure.py -> _prepare_options function, where the azure_ad_token is only set to headers[""Authorization""] during initialization and not when azure_ad_token is changed.
 See if headers.get(""Authorization"") is None below:
    def _prepare_options(self, options: FinalRequestOptions) -> None:
        headers: dict[str, str | Omit] = {**options.headers} if is_given(options.headers) else {}
        options.headers = headers

        azure_ad_token = self._get_azure_ad_token()
        if azure_ad_token is not None:
            if headers.get(""Authorization"") is None:
                headers[""Authorization""] = f""Bearer {azure_ad_token}""

To Reproduce
The azure.identity.ClientSecretCredential.get_token function was utilized as the azure_ad_token_provider, and the AzureOpenAI.max_retries was set to 1000. Due to numerous failures and retries, the process exceeded the 60-minute validity period of the token. It is important to note that Langchain is being used, rather than directly interfacing with AzureOpenAI.
Code snippets
No response
OS
SuSE12
Python version
Python 3.9.6
Library version
openai v1.27.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1525","Send input below model and other parameters in json bodies of HTTP requests","2024-07-06T22:05:36Z","Closed as not planned issue","No label","Confirm this is a feature request for the Python library and not the underlying OpenAI API.
 This is a feature request for the Python library
Describe the feature or improvement you're requesting
When I send a request to an OpenAI compatible endpoint using the following code:
import openaiclient = openai.OpenAI(
	base_url=""https://base.tld/v1"",
	api_key=""xxx"",
)
embedding = client.embeddings.create(
	model=""e5-mistral-7b-instruct"",
	input=""San Francisco"",
)
print(embedding)
It sends an HTTP request with the following body, where the input always comes before the model.
{""input"": ""San Francisco"", ""model"": ""e5-mistral-7b-instruct""}
I think it would be better to always put the input last, after the model so that implementers of the OpenAI API can branch earlier in processing a request based on model. This is only a problem for large (eg. >8KB) inputs, which do not fit into typical web server buffers anymore.
 That is, the body should look like this instead:
{""model"": ""e5-mistral-7b-instruct"", ""input"": ""San Francisco""}
At $WORK we offer an OpenAI compatible API and route based on the model parameter, so we have to buffer the whole input before the request can be forwarded to the correct upstream server hosting that model. This routing only works for inputs that don't exceed our buffer size, meaning we can only process smaller requests.
 If the model came before input instead, it would be possible to parse the model and route based off it with only a small buffer size, even for inputs that far exceed the buffer size.
An added benefit of the proposed parameter order would be slightly improved latency, because the upstream can be decided earlier and the input is starting to get sent sooner.
I imagine we are not the only ones branching based on model, so this change would improve latency and allow for bigger inputs across the whole ecosystem.
I am honestly not sure if this can be changed in this repo since the code is generated. Maybe changing the order in the API spec:
https://github.com/openai/openai-openapi/blob/0df12d7b3ac997517fda38842f41d99a8e7f4e6a/openapi.yaml#L8873
 would be the easiest way to fix it, although the OpenAPI spec is not really wrong since the API is oblivious to the order.
The issue exists for other endpoints such as /chat/completions, where messages comes before model. I encountered it while creating /embeddings for many inputs.
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1522","Allow logging request body","2024-07-03T12:28:33Z","Open issue","enhancement","Confirm this is a feature request for the Python library and not the underlying OpenAI API.
 This is a feature request for the Python library
Describe the feature or improvement you're requesting
It would be nice to be able to be able to log the body of the requests.
 I needed to see what was sent over the wire, so I checked how to log body requests.
 Turns out, there's no way.
I needed to modify this, in order to do it.

openai-python/src/openai/_base_client.py
 Lines 447 to 448 in 58bec2f
	iflog.isEnabledFor(logging.DEBUG): 
	log.debug(""Request options: %s"", model_dump(options, exclude_unset=True)) 
Could you add support to log body requests?
I'm not sure if this is the only place that has to change.
 In fact, I just ended capturing network traffic just to be sure.
 Having to resort to this kind of technique, is not nice.
I'm aware that you can log request/response using a custom httpx client with a custom transport: encode/httpx#3073
EDIT: the same applies to responses body
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1515","JSONDecodeError caused by ""Unterminated string"" when streaming response from chat.completions.with_raw_response.create","2024-07-01T20:17:03Z","Closed as not planned issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
I am writing this bug report on behalf of a customer using an application which my team has developed using the openai package. Unfortunately, I am unable to get specific repro data for this issue due to the customer's data being confidential, but it is suspected that the issue may be related to a previously closed issue #650 based on the conditions in which the problem is arising.
In our specific case, our application is attempting to read the streaming response from the output of the parse method, and observing this unterminated string issue, e.g:
raw_response = await client.chat.completions.with_raw_response.create(**model_args)
response = raw_response.parse()
async for chunk in response:  # <-- Here is where unterminated string error occurs
   ...

I realize that without a specific repro with data this bug may not be actionable, but I wanted to at least ask if anyone here has any suggestions about how we might further diagnose the problem. We got confirmation from our customer that they are using a 1.x version of the openai package, so I wanted to raise the possibility that there might be another issue somewhere else, possibly due to the transformation of the raw response to chat completion.
To Reproduce
Repro data is unfortunately not available.
Code snippets
No response
OS
Linux
Python version
Python 3.11
Library version
openai v1.6.1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1513","Assistant access doesn't work without OPENAI_API_KEY env variable, preventing from accessing assistants in different projects.","2024-07-02T09:56:22Z","Closed issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
Assistant access doesn't work without OPENAI_API_KEY env variable, preventing from accessing assistants in different projects.
Why is it needed?
 I have two assistant that i need to access from slack bot
 they are located in different projects
I can't rely on the environment variable - as the api_key needed is different.
To Reproduce
Create an assistant
fill the details
run the code snippet
Code snippets
import osimport openaifrom retrying import retry


def should_reject(run_status):
    return run_status.status != ""completed""


@retry(retry_on_result=should_reject, stop_max_attempt_number=20, wait_fixed=3000)def retrieve_run_status(thread_id, run_id):
    return openai.beta.threads.runs.retrieve(thread_id=thread_id, run_id=run_id)


def monitor_thread_status(thread_id, run_id):
    run_status = retrieve_run_status(thread_id, run_id)
    return run_status

def access_the_assistant(api_key):

    question = ""What can you help with?""
    project_id = 'proj_..........'
    assistant_id = 'asst_...........'

    client = openai.OpenAI(project=project_id, api_key=api_key)

    the_thread = client.beta.threads.create()
    thread_id = the_thread.id

    _ = client.beta.threads.messages.create(
        thread_id, role=""user"", content=question
    )
    run = client.beta.threads.runs.create(
        thread_id=thread_id, assistant_id=assistant_id
    )

    run_status = retrieve_run_status(thread_id, run.id)

    thread_messages = client.beta.threads.messages.list(
        thread_id,
    )
    return ( str (thread_messages.data) )

def main():
    #load the api key
    api_key = os.getenv(""OPENAI_API_KEY"")

    # if uncommented fix the issue - caches the api_key
    # ret = access_the_assistant(api_key)
    # print(ret)

    del os.environ['OPENAI_API_KEY']

    ret = access_the_assistant(api_key)
    print(ret)


if __name__ == ""__main__"":
    main()
OS
Mac
Python version
3.9
Library version
1.35.7
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1512","BadRequestError: Unsupported data type when creating an Azure OpenAI assistant","2024-06-30T20:49:59Z","Open issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
Attempting to create an Azure OpenAI assistant using the OpenAI Python library results in a BadRequestError: Unsupported data type.
Environment:
Python 3.12.3
Operating System: Windows 10
openai==1.35.7
Azure OpenAI API version: 2023-05-15
.env File:
OPENAI_API_VERSION=2023-05-15AZURE_OPENAI_ENDPOINT=https://ai-111xxxxxx468527.openai.azure.com/AZURE_OPENAI_API_KEY=6bXXXXXXXXXXXXXXXXXXXd41
Code:
import osimport timeimport globfrom openai import AzureOpenAIfrom dotenv import load_dotenv

load_dotenv()

client = AzureOpenAI(
    api_key=os.getenv(""AZURE_OPENAI_API_KEY""),
    api_version=""2023-05-15"",
    azure_endpoint=os.getenv(""AZURE_OPENAI_ENDPOINT"")
)

assistant = client.beta.assistants.create(
    instructions="""",
    model=""gpt-4"",
    tools=[]
)
Error Traceback:
Traceback (most recent call last):
  File ""project_dir\metadata_assistant.py"", line 16, in <module>
    assistant = client.beta.assistants.create(
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""project_dir\.venv\Lib\site-packages\openai\resources\beta\assistants.py"", line 156, in create
    return self._post(
           ^^^^^^^^^^^
  File ""project_dir\.venv\Lib\site-packages\openai\_base_client.py"", line 1250, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""project_dir\.venv\Lib\site-packages\openai\_base_client.py"", line 931, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File ""project_dir\.venv\Lib\site-packages\openai\_base_client.py"", line 1030, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Unsupported data type

Expected Behavior:
 The assistant should be created without any errors.
Actual Behavior:
 The following error is raised:
openai.BadRequestError: Unsupported data type

To Reproduce
Set up the Azure OpenAI client using the provided API key and endpoint.
Attempt to create an assistant using client.beta.assistants.create() method.
Code snippets
No response
OS
Windows
Python version
3.12.3
Library version
1.35.7
 The text was updated successfully, but these errors were encountered: 
👀2
selfishark and LudoCorporateShark reacted with eyes emoji
All reactions
👀2 reactions"
"https://github.com/openai/openai-python/issues/1511","upload_and_poll a in memory file","2024-07-01T10:47:18Z","Closed issue","question","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
upload_and_poll a in memory file
I get a streamlit file that way:
uploaded_file = st.file_uploader(""Choisir un fichier pdf"") #https://docs.streamlit.io/develop/api-reference/widgets/st.file_uploaderif uploaded_file is not None:
    st.write(""Uploaded..."")
    # To read file as bytes:
    bytes_data = uploaded_file.getvalue()
When I upload it as a file with vector_stores.file_batches.upload_and_poll() using [io.BytesIO(blob)] as the stream array it makes an error:
""Files with extensions [none] are not supported for retrieval.""
It seems that OpenAI file storage recognize the file type via its extension. It should be an optional parameter.
To Reproduce
see code above
Code snippets
No response
OS
linux
Python version
python 3.12
Library version
openai 1.33.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1510","This old format doesn;t supported anymore","2024-07-06T21:48:50Z","Closed as not planned issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
response = client.chat.completions.create(
 # model=""alibaba/Qwen1.5-110B-Chat"",
 model=""Qwen/Qwen2-7B-Instruct"",
 messages=[
 {
 ""role"": ""system"",
 ""content"": [{""type"": ""text"", ""text"": ""you are a dog""}],
 ""name"": """",
 },
 {
 ""role"": ""user"",
 ""content"": [{""type"": ""text"", ""text"": ""你好啊""}],
 ""name"": """",
 },
 {
 ""role"": ""assistant"",
 ""content"": [
 {
 ""type"": ""text"",
 ""text"": ""你好，我是一条狗，我叫汪昂"",
 }
 ],
 ""name"": """",
 },
 {
 ""role"": ""user"",
 ""content"": [{""type"": ""text"", ""text"": ""Hello""}],
 ""name"": """",
 },
 ],
 stream=True,
 )
Am using openai to create, got request format error:
openai.BadRequestError: Error code: 400 - {'code': 50400, 'message': 'The request parameter is invalid, Please check again.', 'data': None, 'error': 'The request parameter is invalid, Please check again.'}
this format previous works OK> why?
To Reproduce
gyiguy
Code snippets
No response
OS
macOS
Python version
3.11
Library version
openai latest
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1498","'FileCitation' object has no attribute 'quote'","2024-06-21T12:14:04Z","Open issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
After the last update i've tried to use my code that does what explained also here. https://platform.openai.com/docs/assistants/how-it-works/managing-threads-and-messages
Now when accessing the .quote i receive the error as above
'FileCitation' object has no attribute 'quote'.
can it be that they changed the api? (i don't find annotations description in the docs)
To Reproduce
Create an assistant with files associated
ask a question that has filecitation
check to get the file_citation.quote
Code snippets
No response
OS
macOs
Python version
3.12
Library version
1.30.5
 The text was updated successfully, but these errors were encountered: 
👍3
kyo-ago, amureki, and codingjoe reacted with thumbs up emoji
All reactions
👍3 reactions"
"https://github.com/openai/openai-python/issues/1492","Error while using parallel_tool_calls parameter with AzureOpenAI","2024-06-21T12:52:26Z","Closed issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
Getting below error when calling the chat.completion.create function with parallel_function_calls=False parameter with AzureOpenAI client.
openai.BadRequestError: Error code: 400 - {'error': {'message': ""Unknown parameter: 'parallel_tool_calls'."", 'type': 'invalid_request_error', 'param': 'parallel_tool_calls', 'code': 'unknown_parameter'}}
NOTE: This is only for the scenario where the Function Calling feature is being used.
To Reproduce
Just create a client with AzureOpenAI.
 Call the client.chat.completion.create method and with that, also pass the parallel_function_calls=False (as it is True by default).
 You will get the above error.
Code snippets
No response
OS
Any
Python version
Any
Library version
openai v1.34.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1490","Python code defaults to 'base64' encoding_format in the embeddings endpoint, but docs say it should be 'float'","2024-07-12T13:00:13Z","Closed as not planned issue","No label","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
Hi, your docs say that the default encoding_format in your embeddings/create endpoint is float, however your python package code applies a default of 'base64' when a value isn't supplied.
Can you switch this to 'float' so the python package is conformant with your docs?
To Reproduce
Run an OpenAI embeddings query with the python client:
import openai
client = openai.OpenAI(...)
response = client.embeddings.create(
   model=<model>,
   input=""I am a string to embed!"",
   # notably, NOT specifying encoding_format here
) 

Uses base64 encoding instead of float, oddly.
Code snippets
NA
OS
macOS
Python version
3.12.3
Library version
1.35.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1485","assistant streaming slow","2024-06-21T02:14:51Z","Closed issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
When I attempt to use the assistant function and perform streaming returns, I notice that the assistant and thread created with my most frequently used API key have a very slow response time, taking about 20 seconds to return content after establishing the connection. However, when I create a new assistant and thread with another API key, it only takes about 3-5 seconds to get a response. Why is this happening? Is it because my frequently used API key internally stores a large amount of assistant, thread, and file content?
To Reproduce
just request with the different api key and their assistant and thread
Code snippets
No response
OS
macOS
Python version
Python 3.10
Library version
openai 1.23.6
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1484","AzureOpenAI doesn't support parameter chunking_strategy while creating vector store.","2024-06-18T17:00:50Z","Closed issue","No label","Confirm this is a feature request for the Python library and not the underlying OpenAI API.
 This is a feature request for the Python library
Describe the feature or improvement you're requesting
OpenAI recently added a new feature to their library, which says we can customize the chunking strategy used to split and store files in vector stores using the new parameter ""chunking_strategy"" to be used while creating a vector store.
https://platform.openai.com/docs/changelog (June 3rd, file search customizations)

 If I try creating a new vector store mentioning the custom chunking strategy using the OpenAI client, I am able to successfully create one.

 But, if I try creating a new vector store mentioning the custom chunking strategy using the AzureOpenAI client, I get BadRequestError which says unknown parameter: chunking_strategy.


 I have tried upgrading the openai python library, but that doesn't solve the issue.
 I request AzureOpenAI to update their module as per the changes done by openai library.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1483","Assistant API should support images in base64 if chat completion does.","2024-09-09T11:37:27Z","Closed as not planned issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
I tried sending base 64 image via chat completion api, and it worked. When i tried the same via assistant api for the same model, it did not work. Looking at the implementation, since it's already supported by completions api.
To Reproduce
Create a base64 image url (data:gdhf..) for any image.
Pass this base64 as url in content to chat completions API it works fine and i get a response.
Create an assistant.
Create a thread with message containing the same base64 encoded image as url inside image_url. You get an error.
BadRequestError: Error code: 400 - {'error': {'message': ""Invalid 'messages[0].content[1].image_url.url'. Expected a valid URL, but got a value with an invalid format."", 'type': 'invalid_request_error', 'param': 'messages[0].content[1].image_url.url', 'code': 'invalid_value'}}
Code snippets
[{'role': 'user',
  'content': [{'type': 'text', 'text': 'What’s in this image?'},
   {'type': 'image_url',
    'image_url': {'url': 'data:image/jpeg;base64,/9j...
}
}]
}
]
OS
MacOS
Python version
Python v3.12
Library version
latest
 The text was updated successfully, but these errors were encountered: 
👍3
spacepumpkin, Lee-daeho, and jalbertsr reacted with thumbs up emoji
All reactions
👍3 reactions"
"https://github.com/openai/openai-python/issues/1473","AsyncAssistantStreamManager Error","2024-06-11T02:58:38Z","Closed issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
async def requestGpt(response):
 print(""requestGpt called"")
 async with client.beta.threads.runs.stream(
 thread_id=thread.id,
 assistant_id=assistant.id,
 event_handler=EventHandler(response)
 ) as stream:
 await stream.until_done()
When I use this way to call a assistant streaming, the type of stream is AssistantStreamManager, not the AsyncAssistantStreamManager
To Reproduce
async def requestGpt(response):
 print(""requestGpt called"")
 async with client.beta.threads.runs.stream(
 thread_id=thread.id,
 assistant_id=assistant.id,
 event_handler=EventHandler(response)
 ) as stream:
 await stream.until_done()
Code snippets
No response
OS
macOS
Python version
Python 3.10
Library version
openai 1.23.6
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1470","submit_tool_outputs closes a thread instead of changing it run status","2024-06-05T18:33:02Z","Open issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
When using the client.beta.threads.runs.submit_tool_outputs method in the Python SDK, the thread closes and exits instead of updating the run status.
To Reproduce
Steps to Reproduce
 Initialize a new run using client.beta.threads.runs.create_and_poll:
run = client.beta.threads.runs.create_and_poll(
    thread_id=thread.id,
    assistant_id=business.assistant_id,
)

Submit tool outputs using client.beta.threads.runs.submit_tool_outputs:
run = client.beta.threads.runs.submit_tool_outputs(
    thread_id=thread.id,
    run_id=run.id,
    tool_outputs=[{""tool_call_id"": tool_call.id, ""output"": output}],
)

Observe that the thread closes and exits instead of updating the run status.
Code snippets
No response
OS
Ubuntu 22.04.4 LTS
Python version
Python v3.11.3
Library version
openai v1.31.1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1469","Stream options are not available for azure openAI?","2024-07-06T20:41:14Z","Closed as not planned issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
I was following this article on stream option.https://cookbook.openai.com/examples/how_to_stream_completions.
I am using:
""2023-05-15"", ""gpt-35-turbo""
code:

response = open_ai_client.chat.completions.create(
    model=OpenAIConstants.generator,
    messages=[
        {'role': 'user', 'content': ""What's 1+1? Answer in one word.""}
    ],
    temperature=0,
    stream=True,
    stream_options={""include_usage"": True}, # retrieving token usage for stream response
)

for chunk in response:
    print(f""choices: {chunk.choices}\nusage: {chunk.usage}"")
    print(""****************"")


Traceback (most recent call last):
  File ""C:\Users\nandurisai.venkatara\projects\knowledge-base\Archive\sample.py"", line 15, in <module>
    response = open_ai_client.chat.completions.create(
  File ""C:\Users\nandurisai.venkatara\projects\knowledge-base\.venv\lib\site-packages\openai\_utils\_utils.py"", line 277, in wrapper
    return func(*args, **kwargs)
  File ""C:\Users\nandurisai.venkatara\projects\knowledge-base\.venv\lib\site-packages\openai\resources\chat\completions.py"", line 590, in create
    return self._post(
  File ""C:\Users\nandurisai.venkatara\projects\knowledge-base\.venv\lib\site-packages\openai\_base_client.py"", line 1240, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File ""C:\Users\nandurisai.venkatara\projects\knowledge-base\.venv\lib\site-packages\openai\_base_client.py"", line 921, in request
    return self._request(
  File ""C:\Users\nandurisai.venkatara\projects\knowledge-base\.venv\lib\site-packages\openai\_base_client.py"", line 1020, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': 'Unrecognized request argument supplied: stream_options', 'type': 'invalid_request_error', 'param': None, 'code': None}}

To Reproduce
I was following this article on stream option.https://cookbook.openai.com/examples/how_to_stream_completions.
I am using:
""2023-05-15"", ""gpt-35-turbo""
code:

response = open_ai_client.chat.completions.create(
    model=OpenAIConstants.generator,
    messages=[
        {'role': 'user', 'content': ""What's 1+1? Answer in one word.""}
    ],
    temperature=0,
    stream=True,
    stream_options={""include_usage"": True}, # retrieving token usage for stream response
)

for chunk in response:
    print(f""choices: {chunk.choices}\nusage: {chunk.usage}"")
    print(""****************"")


Traceback (most recent call last):
  File ""C:\Users\nandurisai.venkatara\projects\knowledge-base\Archive\sample.py"", line 15, in <module>
    response = open_ai_client.chat.completions.create(
  File ""C:\Users\nandurisai.venkatara\projects\knowledge-base\.venv\lib\site-packages\openai\_utils\_utils.py"", line 277, in wrapper
    return func(*args, **kwargs)
  File ""C:\Users\nandurisai.venkatara\projects\knowledge-base\.venv\lib\site-packages\openai\resources\chat\completions.py"", line 590, in create
    return self._post(
  File ""C:\Users\nandurisai.venkatara\projects\knowledge-base\.venv\lib\site-packages\openai\_base_client.py"", line 1240, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File ""C:\Users\nandurisai.venkatara\projects\knowledge-base\.venv\lib\site-packages\openai\_base_client.py"", line 921, in request
    return self._request(
  File ""C:\Users\nandurisai.venkatara\projects\knowledge-base\.venv\lib\site-packages\openai\_base_client.py"", line 1020, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': 'Unrecognized request argument supplied: stream_options', 'type': 'invalid_request_error', 'param': None, 'code': None}}

Code snippets
No response
OS
Win
Python version
3.10
Library version
1.30.1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1468","Why is logprobs and log_probs not permitted in client.chat.completions.create for AzureOpenAI?","2024-06-05T11:34:21Z","Open issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
I'm using AzureOpenAI on API version '2024-02-01'.
When using client.chat.completions.create, to perform RAG with an Azure OpenAI gpt-4-1106-preview deployment against an Azure AI search index (via the extra_body -> data_sources parameter), I keep receiving the error:
TypeError: create() got an unexpected keyword argument 'log_probs' 
 or
Error code: 400 - {'error': {'requestid': '5b8df334-1238-4b66-8cb8-564ffbe02cff', 'code': 400, 'message': 'Validation error at #/logprobs: Extra inputs are not permitted'}}
I read here that I should be able to pass 'log_probs' in the completions method to see logprobs populated in the response:
https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#chat-completions
Yet it seems despite switching API versions I simply can't achieve this. Is there support for this, or am I doing something wrong?
To Reproduce
Use client.chat.completions.create with gpt-4 hosted on Azure
Attempt to use the logprobs or log_probs parameter
Code snippets
No response
OS
macOs
Python version
Python v3.9
Library version
openai 1.23.2
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1465","json_object response_format not behaving as expected on Azure gpt-3.5-turbo-0125","2024-06-06T08:54:08Z","Closed issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
Using the OpenAI python library with an Azure OpenAI instance, I am trying ot generate a json response guaranteed to be in json format (as only including it in text promt sometimes yields inadequate results).
For a request with the following parameters:
'model': 'gpt-3.5-turbo-0125', 'response_format': {'type': 'json_object'}
I am getting the following error:
openai.BadRequestError: Error code: 400 - {'error': {'message': ""Invalid parameter: 'response_format' of type 'json_object' is not supported with this model."", 'type': 'invalid_request_error', 'param': 'response_format', 'code': None}}
Yet, my understnading is that (according to the Azure documentation page):
json mode is supported by the model gpt-35-turbo (0125)
and the 2024-02-01 API version which I use, supports json_object response_format
To Reproduce
Set up an Azure OpenAI instance
from openai.lib.azure import AsyncAzureOpenAI
client = AsyncAzureOpenAI(
 api_key=,
 api_version=""2024-02-01"",
 azure_endpoint=,
 azure_deployment=
 )
response = await client.chat.completions.create(
 model= ""gpt-3.5-turbo-0125"",
 messages=[
 {""role"": ""user"", ""content"": },
 ],
 response_format={'type': 'json_object'}
 )
print (response.choices[0].message.content)
Code snippets
No response
OS
masOS
Python version
Python 3.11.2
Library version
openai v1.30.4
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1463","Hwo to get log probablity of input tokens?","2024-06-04T05:09:36Z","Closed issue","No label","Confirm this is a feature request for the Python library and not the underlying OpenAI API.
 This is a feature request for the Python library
Describe the feature or improvement you're requesting
For a complete context:
Q:Where is Beijing? A:china
 I want to get the log_probs of china, how can I do this?
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1459","No support for purpose=""vision"" in files API","2024-05-30T11:37:56Z","Open issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
According to the docs in order to create a file upload of an image this should work:
file = client.files.create(
  file=open(""myimage.png"", ""rb""),
  purpose=""vision""
)
However, two things do not works as I expected:
the typing of purpose does not include vision
I use this API version ""2024-05-01-preview"". I get this error response from Azure Open AI
Error code: 400 - {'error': {'code': 'invalidPayload', 'message': 'Invalid value for the purpose.'}
To Reproduce
Run the snippet from the documentation:
file = client.files.create(
  file=open(""myimage.png"", ""rb""),
  purpose=""vision""
)

thread = client.beta.threads.create(
  messages=[
    {
      ""role"": ""user"",
      ""content"": [
        {
          ""type"": ""text"",
          ""text"": ""What is the difference between these images?""
        },
        {
          ""type"": ""image_url"",
          ""image_url"": {""url"": ""https://example.com/image.png""}
        },
        {
          ""type"": ""image_file"",
          ""image_file"": (""file_id"": file.id)
        },file = client.files.create(
  file=open(""myimage.png"", ""rb""),
  purpose=""vision""
)
Code snippets
No response
OS
linux (Ubuntu)
Python version
3.12.1
Library version
1.30.5
 The text was updated successfully, but these errors were encountered: 
👍2
jhakulin and codingbandit reacted with thumbs up emoji
All reactions
👍2 reactions"
"https://github.com/openai/openai-python/issues/1457","It takes forever to complete run for AZURE Assistant v2","2024-06-05T17:06:30Z","Closed issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
I am working with AZURE Assistant api and using tool code_interpreter. I am stuck while creating run on the thread to generate a response by calling the model and the tools. It goes forever.
To Reproduce
Run below code to reproduce:
 #Send file to assistant
file_data = client.files.create(
      file=open(""test.csv"",""rb""),
      purpose='assistants'
    ) 

#create Assistant
assistant = client.beta.assistants.create(
    instructions='''My system instruction.''',
    model=  ""gpt-35-turbo-0613"",
    tools=[{""type"": ""code_interpreter""}],
    temperature=0.4,
    tool_resources={""code_interpreter"":{""file_ids"":[file.id]}}
    )

#create thread
thread = client.beta.threads.create() 
 #Add message to thread
message = client.beta.threads.messages.create(
                    thread_id=thread.id,
                    role=""user"",
                    content=""User question"",
                    
                )


create a run (and it goes forever) this is the point I am stuck
run = client.beta.threads.runs.create_and_poll(
  thread_id=thread.id,
  assistant_id=assistant.id,
  
)

Code snippets
No response
OS
Linux
Python version
3.10.12
Library version
openai- 1.28.1
 The text was updated successfully, but these errors were encountered: 
👀1
rajasimon reacted with eyes emoji
All reactions
👀1 reaction"
"https://github.com/openai/openai-python/issues/1454","Incompatibility between openai and fastapi","2024-08-19T16:23:15Z","Closed as not planned issue","No label","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
Using openai python library with fastapi throws a pydantic error. python '3.12.3', fastapi '0.111.0', openai '1.30.4' (although the same happens with python 3.8 and 3.10)
Simply try and define an endpoint that uses CompletionCreateParams. See stack trace in code section
To Reproduce
$ python3.12 -m venv 3.12venv
$ source 3.12venv/bin/activate
$ pip install fastapi
$ pip install openai
$ python
>>> from fastapi import FastAPI
>>> from openai.types.chat import CompletionCreateParams
>>> app = FastAPI()
>>> @app.post(""/v1/chat/completions"")
... def chat_completions(r: CompletionCreateParams):
...  pass

Code snippets
Traceback (most recent call last):
  File ""/Users/LarryMartell/dispatcher/3.12venv/lib/python3.12/site-packages/pydantic/type_adapter.py"", line 210, in __init__
    core_schema = _getattr_no_parents(type, '__pydantic_core_schema__')
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/LarryMartell/dispatcher/3.12venv/lib/python3.12/site-packages/pydantic/type_adapter.py"", line 98, in _getattr_no_parents
    raise AttributeError(attribute)
AttributeError: __pydantic_core_schema__

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/LarryMartell/dispatcher/3.12venv/lib/python3.12/site-packages/fastapi/routing.py"", line 944, in decorator
    self.add_api_route(
  File ""/Users/LarryMartell/dispatcher/3.12venv/lib/python3.12/site-packages/fastapi/routing.py"", line 883, in add_api_route
    route = route_class(
            ^^^^^^^^^^^^
  File ""/Users/LarryMartell/dispatcher/3.12venv/lib/python3.12/site-packages/fastapi/routing.py"", line 513, in __init__
    self.dependant = get_dependant(path=self.path_format, call=self.endpoint)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/LarryMartell/dispatcher/3.12venv/lib/python3.12/site-packages/fastapi/dependencies/utils.py"", line 261, in get_dependant
    type_annotation, depends, param_field = analyze_param(
                                            ^^^^^^^^^^^^^^
  File ""/Users/LarryMartell/dispatcher/3.12venv/lib/python3.12/site-packages/fastapi/dependencies/utils.py"", line 444, in analyze_param
    field = create_response_field(
            ^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/LarryMartell/dispatcher/3.12venv/lib/python3.12/site-packages/fastapi/utils.py"", line 99, in create_response_field
    return ModelField(**kwargs)  # type: ignore[arg-type]
           ^^^^^^^^^^^^^^^^^^^^
  File ""<string>"", line 6, in __init__
  File ""/Users/LarryMartell/dispatcher/3.12venv/lib/python3.12/site-packages/fastapi/_compat.py"", line 109, in __post_init__
    self._type_adapter: TypeAdapter[Any] = TypeAdapter(
                                           ^^^^^^^^^^^^
  File ""/Users/LarryMartell/dispatcher/3.12venv/lib/python3.12/site-packages/pydantic/type_adapter.py"", line 212, in __init__
    core_schema = _get_schema(type, config_wrapper, parent_depth=_parent_depth + 1)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/LarryMartell/dispatcher/3.12venv/lib/python3.12/site-packages/pydantic/type_adapter.py"", line 81, in _get_schema
    schema = gen.generate_schema(type_)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/LarryMartell/dispatcher/3.12venv/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py"", line 502, in generate_schema
    schema = self._generate_schema_inner(obj)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/LarryMartell/dispatcher/3.12venv/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py"", line 737, in _generate_schema_inner
    return self._annotated_schema(obj)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/LarryMartell/dispatcher/3.12venv/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py"", line 1757, in _annotated_schema
    schema = self._apply_annotations(source_type, annotations)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/LarryMartell/dispatcher/3.12venv/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py"", line 1825, in _apply_annotations
    schema = get_inner_schema(source_type)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/LarryMartell/dispatcher/3.12venv/lib/python3.12/site-packages/pydantic/_internal/_schema_generation_shared.py"", line 82, in __call__
    schema = self._handler(source_type)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/LarryMartell/dispatcher/3.12venv/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py"", line 1907, in new_handler
    schema = metadata_get_schema(source, get_inner_schema)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/LarryMartell/dispatcher/3.12venv/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py"", line 1903, in <lambda>
    lambda source, handler: handler(source)
                            ^^^^^^^^^^^^^^^
  File ""/Users/LarryMartell/dispatcher/3.12venv/lib/python3.12/site-packages/pydantic/_internal/_schema_generation_shared.py"", line 82, in __call__
    schema = self._handler(source_type)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/LarryMartell/dispatcher/3.12venv/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py"", line 1806, in inner_handler
    schema = self._generate_schema_inner(obj)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/LarryMartell/dispatcher/3.12venv/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py"", line 758, in _generate_schema_inner
    return self.match_type(obj)
           ^^^^^^^^^^^^^^^^^^^^
  File ""/Users/LarryMartell/dispatcher/3.12venv/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py"", line 840, in match_type
    return self._match_generic_type(obj, origin)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/LarryMartell/dispatcher/3.12venv/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py"", line 864, in _match_generic_type
    return self._union_schema(obj)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/LarryMartell/dispatcher/3.12venv/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py"", line 1152, in _union_schema
    choices.append(self.generate_schema(arg))
                   ^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/LarryMartell/dispatcher/3.12venv/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py"", line 502, in generate_schema
    schema = self._generate_schema_inner(obj)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/LarryMartell/dispatcher/3.12venv/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py"", line 758, in _generate_schema_inner
    return self.match_type(obj)
           ^^^^^^^^^^^^^^^^^^^^
  File ""/Users/LarryMartell/dispatcher/3.12venv/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py"", line 806, in match_type
    return self._typed_dict_schema(obj, None)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/LarryMartell/dispatcher/3.12venv/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py"", line 1254, in _typed_dict_schema
    for field_name, annotation in get_type_hints_infer_globalns(
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/LarryMartell/dispatcher/3.12venv/lib/python3.12/site-packages/pydantic/_internal/_fields.py"", line 56, in get_type_hints_infer_globalns
    return get_type_hints(obj, globalns=globalns, localns=localns, include_extras=include_extras)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/typing.py"", line 2244, in get_type_hints
    value = _eval_type(value, base_globals, base_locals)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/typing.py"", line 414, in _eval_type
    return t._evaluate(globalns, localns, recursive_guard)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/typing.py"", line 924, in _evaluate
    eval(self.__forward_code__, globalns, localns),
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""<string>"", line 1, in <module>TypeError: 'pydantic_core._pydantic_core.PydanticUndefinedType' object is not subscriptable


### OS

macOS 14.4.1

### Python version

3.12.3

### Library version

1.30.4

 The text was updated successfully, but these errors were encountered: 
👍1
jperuggia reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/openai-python/issues/1450","Slow tool calls compared to web","2024-05-28T02:51:56Z","Open issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
I'm following the function call tutorial without streaming. So my code looks like this
 # Add the user message to the thread
    client.beta.threads.messages.create(
        thread_id=thread_id,
        role=""user"",
        content=user_input,
    )
    print(""thread created. Running run"", datetime.now().time().strftime(""%H:%M:%S""))

    run = client.beta.threads.runs.create_and_poll(
        thread_id=thread_id,
        assistant_id=assistant_id,
    )
    
    if run.status == 'completed':
        print(""run complete"",  datetime.now().time().strftime(""%H:%M:%S""))
        messages = client.beta.threads.messages.list(
        thread_id=thread_id
    )
        print(messages)
    else:
        print(run.status)
    
    # Define the list to store tool outputs
    tool_outputs = []
    
    # Loop through each tool in the required action section
    for tool in run.required_action.submit_tool_outputs.tool_calls:
        print(""tool received"",  datetime.now().time().strftime(""%H:%M:%S""))
        if tool.function.name == ""get_weather"":
        ...etc
        

It takes 4 seconds from the moment when the thread is created to the moment I get the function name and params
thread created. Running run 21:22:40
tool received 21:22:44

When I try the assistant from the assistants playground in the web dashboard, it takes 1500ms to return a message with the right function name. And I noticed the browser version makes an POST request to this endpoint which I guess is the same the python library uses under the hood:
https://api.openai.com/v1/threads/thread_id123/runs
 I understand that that endpoint probably just starts the run, but still, the message with the function name loads in less than 2 seconds compared to 4. Am I using the right example for this? thanks
To Reproduce
Follow the function calling in this tutorial https://platform.openai.com/docs/assistants/tools/function-calling/quickstart?context=without-streaming and measure the time it takes from the moment the message is sent to the moment the function name is available in the handler. After that compare that result with the time it takes to perform the same action in the assistants playground in the web console.
Code snippets
No response
OS
Macos
Python version
3.9.6
Library version
latest
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1449","tool_resources parameter throwing error while creating Assistant v2 with AZURE client","2024-05-29T11:42:32Z","Closed issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
I am using openai==1.28.1 and while creating assistant I am getting BadRequestError: Error code: 400 - {'error': {'message': ""Unknown parameter: 'tool_resources'."", 'type': 'invalid_request_error', 'param': 'tool_resources', 'code': 'unknown_parameter'}} . I am taking references from Sourcesource Below is the code which is throwing me error when I am trying tool_resource
To Reproduce
Run below code with AZURE api key and endpoint, even if I change the below code with tool_choice={""code_interpreter"": {""file_ids"": [file.id]}} it won' t work it gives error TypeError: Assistants.create() got an unexpected keyword argument 'tool_choice'
source
assistant = client.beta.assistants.create(
      instructions='''My Instruction''',
      model=""My model deployment name"",
      tools=[{""type"": ""code_interpreter""}],
      tool_resources={""code_interpreter"": {""file_ids"": [file.id]}}
    )

Code snippets
No response
OS
Linux
Python version
3.10.1
Library version
openai==1.28.1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1448","AttributeError: 'NoneType' object has no attribute 'create'","2024-05-26T01:44:41Z","Closed as not planned issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
test_prompt_results.py:54:
../services/chain.py:146: in dispatch2
 result = await chain.ainvoke(input_text)
 ../.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py:2405: in ainvoke
 input = await step.ainvoke(
 ../.venv/lib/python3.11/site-packages/langchain_core/language_models/llms.py:299: in ainvoke
 llm_result = await self.agenerate_prompt(
 ../.venv/lib/python3.11/site-packages/langchain_core/language_models/llms.py:643: in agenerate_prompt
 return await self.agenerate(
 ../.venv/lib/python3.11/site-packages/langchain_core/language_models/llms.py:1018: in agenerate
 output = await self._agenerate_helper(
 ../.venv/lib/python3.11/site-packages/langchain_core/language_models/llms.py:882: in _agenerate_helper
 raise e
 ../.venv/lib/python3.11/site-packages/langchain_core/language_models/llms.py:866: in _agenerate_helper
 await self._agenerate(
 ../.venv/lib/python3.11/site-packages/langchain_community/llms/openai.py:1181: in _agenerate
 full_response = await acompletion_with_retry(
llm = OpenAIChat(verbose=True, client=APIRemovedInV1Proxy, model_name='gpt-4o')
 run_manager = <langchain_core.callbacks.manager.AsyncCallbackManagerForLLMRun object at 0x14f3e9c10>
 kwargs = {'messages': [{'content': 'Human: Role: You are an advanced tender developer focused on generating winning tender resp...ertise, demonstrate the ability to cope with volume of works?\nHelpful Answer: ', 'role': 'user'}], 'model': 'gpt-4o'}
async def acompletion_with_retry(
    llm: Union[BaseOpenAI, OpenAIChat],
    run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,
    **kwargs: Any,
) -> Any:
    """"""Use tenacity to retry the async completion call.""""""
    if is_openai_v1():

      return await llm.async_client.create(**kwargs)

E AttributeError: 'NoneType' object has no attribute 'create'
../.venv/lib/python3.11/site-packages/langchain_community/llms/openai.py:132: AttributeError
To Reproduce
model = OpenAI(model_name=model_name, verbose=True)
 chain = (
 {
 ""context"": get_context,
 ""extra_instructions"": get_instructions,
 ""question"": get_question,
 }
 | prompt
 | model
 | StrOutputParser()
 )
result = await chain.ainvoke(input_text)
Code snippets
The code worked fine until I updated to a more recent version.

openai==1.30.3
OS
macOS
Python version
3.11.4
Library version
openai==1.30.3
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1446","Async completions.create method does I/O in the event loop","2024-07-01T11:17:40Z","Closed issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
I am working on the Home Assistant OpenAI integration. Home Assistant has started warning when we see I/O being done in the event loop and it printed this warning:
2024-05-25 03:03:21.716 WARNING (MainThread) [homeassistant.util.loop] Detected blocking call to open inside the event loop by integration 'openai_conversation' at homeassistant/components/openai_conversation/conversation.py, line 170: result = await client.chat.completions.create( (offender: /home/vscode/.local/ha-venv/lib/python3.12/site-packages/distro/distro.py, line 1099: with open(self.os_release_file, encoding=""utf-8"") as release_file:)

It looks like distro is used to set the platform headers. When calling the Linux platform, it opens the release file. This makes distro not async-safe.
To Reproduce
Call await client.chat.completions.create
Code snippets
No response
OS
macOS
Python version
Python 3.12.3
Library version
openai==1.3.8
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1438","Omit redundant information in chat completion message params","2024-05-26T01:24:36Z","Closed as not planned issue","No label","Confirm this is a feature request for the Python library and not the underlying OpenAI API.
 This is a feature request for the Python library
Describe the feature or improvement you're requesting
Currently, classes like ChatCompletionSystemMessageParam require a parameter of type Required[Literal['system']].
The class name already contains this required role, so this param is simply redundant information while not allowing any different values.
Suggestion: This parameter should be made optional with the required literal as default value for backwards compatibility or even completely omitted.
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1435","TypeError: TarFile.extractall() got an unexpected keyword argument 'filter' when running openai migrate","2024-06-26T18:10:31Z","Closed issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
When running the openai migrate command to update my code to the latest version due to deprecated functions, I encountered a TypeError. The error message indicates that the TarFile.extractall() method received an unexpected keyword argument filter. The error seems to originate from the migrate.py file within the openai package, specifically from the line attempting to call archive.extractall with an unsupported filter argument.
To Reproduce
Run this command in your terminal:
openai migrate
Observe the error message:
Retrieving Grit CLI metadata from https://api.keygen.sh/v1/accounts/custodian-dev/artifacts/marzano-macos-arm64 Traceback (most recent call last): File ""path/to/openai"", line 8, in <module> sys.exit(main()) ^^^^^^ File ""path/to/_cli.py"", line 129, in main _main() File ""path/to/_cli.py"", line 209, in _main parsed.func( File ""path/to/migrate.py"", line 53, in migrate grit_path = install() ^^^^^^^^^ File ""path/to/migrate.py"", line 141, in install archive.extractall(unpacked_dir, filter=""data"") TypeError: TarFile.extractall() got an unexpected keyword argument 'filter' 
Code snippets
No response
OS
macos
Python version
3.11.3
Library version
1.30.1
 The text was updated successfully, but these errors were encountered: 
👍4
zeninfinity, havocy28, Divide-By-0, and geeknam reacted with thumbs up emoji
All reactions
👍4 reactions"
"https://github.com/openai/openai-python/issues/1434","Missing Authorization header in request","2024-05-26T01:14:44Z","Closed issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
{
 ""error"": {
 ""message"": ""You didn't provide an API key. You need to provide your API key in an Authorization header using Bearer auth (i.e. Authorization: Bearer YOUR_KEY), or as the password field (with blank username) if you're accessing the API from your browser and are prompted for a username and password. You can obtain an API key from https://platform.openai.com/account/api-keys."",
 ""type"": ""invalid_request_error"",
 ""param"": null,
 ""code"": null
 }
 }
To Reproduce
Make API requests
Code snippets
import openai 
import osimport re

class OpenAI:
    def __init__(self):
        # os.environ[""OPENAI_API_KEY""] = OPENAI_API_KEY
        openai.api_key = openai.api_key = os.getenv(""OPENAI_API_KEY"")
      


    def gpt(self, prompt):
        response = openai.chat.completions.create(
            model=""gpt-3.5-turbo-0125"",
            # model=""gpt-4"",
            messages=[
                {""role"": ""system"", ""content"": ""You are a helpful assistant.""},
                {""role"": ""user"", ""content"": prompt},
                {""role"": ""assistant"", ""content"": ""Respond consisely, response should only include natural language.""},
            ],
                temperature = 0,
                top_p=0.5,
                seed=123
        )
        print(response)
        return response.choices[0].message.content
OS
Windows 11
Python version
Python v3.11.4
Library version
openai v1.0.1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1433","ChatCompletionAssistantMessageParam is incorrectly typed","2024-05-28T22:29:15Z","Closed issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
The ChatCompletionAssistantMessageParam has the incorrect types for the function_call, name, and tool_calls fields. All of these fields should be marked Optional[...] but they are not.
While this doesn't break the type itself, downstream it prevents the use of pydantic with these types.
To Reproduce
Run the code in the snippet below.
pydantic will try to validate ChatCompletionAssistantMessageParam and fail because function_call is None (as returned by the API). I would expect either (1) that the returned message does not include the function_call field like it excludes the name field or (2) for the ChatCompletionAssistantMessageParam type to allow for function_call to be None in it's type definition.
Code snippets
from openai.types.chat import ChatCompletionAssistantMessageParamfrom pydantic import BaseModel

class MyModel(BaseModel):
    history: list[ChatCompletionAssistantMessageParam]

history = [
    {
        ""content"": None,
        ""role"": ""assistant"",
        ""function_call"": None,
        ""tool_calls"": [
            {
                ""id"": ""id"",
                ""function"": {
                    ""arguments"": '{""location"":""Tokyo, Japan""}',
                    ""name"": ""GetCurrentWeather"",
                },
                ""type"": ""function"",
            }
        ],
    },
]
my_model = MyModel(history=history)
print(my_model)
OS
14.2.1
Python version
3.9.16
Library version
1.30.1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1432","Does it support multi key polling？","2024-07-12T00:32:12Z","Closed issue","No label","Confirm this is a feature request for the Python library and not the underlying OpenAI API.
 This is a feature request for the Python library
Describe the feature or improvement you're requesting
Does it support multi key polling？
Additional context
Does it support multi key polling？
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1431","api-key missing from the request in case of async calls (AsyncOpenAI, AsyncAzureOpenAI)","2024-05-26T01:22:05Z","Closed issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
When trying to perform an await AsyncOpenAI.chat.completions.create call, it will results in 'statusCode': 401, 'message': 'Unauthorized. Access token is missing.
The found workaround it to add extra_headers
    response = await client.chat.completions.create(
        model=""deployment_name"",
        extra_headers={""api-key"": ""_my_api_key""},
        messages=[{""role"": ""user"", ""content"": prompt_text}],
    )

After debugging I've seen that _prepare_options for sync calls (OpenAI, AzureOpenAI) is implemented, and it actually sets the api-key in the headers, while it is not for async chat completions (AsyncOpenAI, AsyncAzureOpenAI).
To Reproduce
Library version 1.30.1
   client = AsyncAzureOpenAI(
        azure_endpoint=""my_azure_endpoinf"",
        api_key=""my_api_key"",
        api_version=""my_api_version"",
        azure_deployment=""my_deployment"",
    )
    response = await client.chat.completions.create(
        model=""my_deployment"",
        # extra_headers={""api-key"": ""my_api_key""},  # will not work if this is commented out
        messages=[{""role"": ""user"", ""content"": prompt_text}],
    )

=>
openai.AuthenticationError: Error code: 401 - {'statusCode': 401, 'message': 'Unauthorized. Access token is missing, invalid, audience is incorrect (https://cognitiveservices.azure.com), or have expired.'}
Code snippets
No response
OS
Windows
Python version
3.11.2
Library version
1.30.1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1427","Unable to retrieve generated files from Assistants API since gpt-4o","2024-05-15T17:10:22Z","Open issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
This used to work before the gpt-4o launch, but now we are not seeing any file attachments for generated files with Code Interpreter.
To Reproduce
Prompt:
Generate a CSV with all 50 US states.
Model:
gpt-3.5-turbo | gpt-4-turbo | gpt-4o
Code snippets
Logs:


import pandas as pd# List of all US statesstates = [
""Alabama"", ""Alaska"", ""Arizona"", ""Arkansas"", ""California"", ""Colorado"",
""Connecticut"", ""Delaware"", ""Florida"", ""Georgia"", ""Hawaii"", ""Idaho"",
""Illinois"", ""Indiana"", ""Iowa"", ""Kansas"", ""Kentucky"", ""Louisiana"",
""Maine"", ""Maryland"", ""Massachusetts"", ""Michigan"", ""Minnesota"",
""Mississippi"", ""Missouri"", ""Montana"", ""Nebraska"", ""Nevada"",
""New Hampshire"", ""New Jersey"", ""New Mexico"", ""New York"",
""North Carolina"", ""North Dakota"", ""Ohio"", ""Oklahoma"", ""Oregon"",
""Pennsylvania"", ""Rhode Island"", ""South Carolina"",
""South Dakota"", ""Tennessee"", ""Texas"", ""Utah"", ""Vermont"",
""Virginia"", ""Washington"", ""West Virginia"", ""Wisconsin"", ""Wyoming""
]
# Create a DataFramedf_states = pd.DataFrame(states, columns=['State'])
# Save DataFrame to CSVcsv_path = ""/mnt/data/US_States.csv""df_states.to_csv(csv_path, index=False)
The CSV file containing all 50 US states has been created. You can download it using the link below:
Retrieve all messages from openai.beta.threads.messages.list
SyncCursorPage[Message](
    data=[
        Message(
            id='msg_s9WPnzqF8xxxxxxxxxxxxx',
            assistant_id='asst_ndcMgmxxxxxxxxxxxxx',
            attachments=[],
            completed_at=None,
            content=[
                TextContentBlock(
                    text=Text(
                        annotations=[],
                        value='The CSV file containing all 50 US states has been created. You can download it using the link below:\n\n[Download US States
CSV](sandbox:/mnt/data/US_States.csv)'
                    ),
                    type='text'
                )
            ],
            created_at=1715719138,
            incomplete_at=None,
            incomplete_details=None,
            metadata={},
            object='thread.message',
            role='assistant',
            run_id='run_YnT0xxxxxxxxxxxxx',
            status=None,
            thread_id='thread_ntw4xxxxxxxxxxxxx'
        ),
        Message(
            id='msg_aAgUWCpxxxxxxxxxxxxx',
            assistant_id=None,
            attachments=[],
            completed_at=None,
            content=[TextContentBlock(text=Text(annotations=[], value='Generate a CSV with all 50 US states.'), type='text')],
            created_at=1715719114,
            incomplete_at=None,
            incomplete_details=None,
            metadata={},
            object='thread.message',
            role='user',
            run_id=None,
            status=None,
            thread_id='thread_ntw4aiXxxxxxxxxxxxxx'
        )
    ],
    object='list',
    first_id='msg_s9WPnzqF8Qaxxxxxxxxxxxxx',
    last_id='msg_aAgUWCppWBxxxxxxxxxxxxx',
    has_more=False
)



### OS

macOS

### Python version

Python v3.11.9

### Library version

openai v1.3.0.1

 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1426","Debugging with PyCharm throws error when environment variable not specified","2024-05-20T15:29:16Z","Closed issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
Hi OpenAI team!
I'm having trouble debugging my project that uses the openai package along with several other dependencies. I'm encountering an exception during the import process.
Here is my file t.py:
from openai import OpenAI
Running script in debug mode in PyCharm:
python t.py

Initially, I attempted to execute uvicorn with FastAPI, but the issue persists even when they are not involved.
Here is traceback:
Traceback (most recent call last):
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py"", line 1534, in _exec
    pydev_imports.execfile(file, globals, locals)  # execute the script
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/_pydev_imps/_pydev_execfile.py"", line 18, in execfile
    exec(compile(contents+""\n"", file, 'exec'), glob, loc)
  File ""/Users/tworedz/workspace/temp/language_model/t.py"", line 1, in <module>
    from openai import OpenAI
  File ""/Users/tworedz/Library/Caches/pypoetry/virtualenvs/language-model-5UFyvxqf-py3.12/lib/python3.12/site-packages/openai/__init__.py"", line 345, in <module>
    from ._module_client import (
  File ""/Users/tworedz/Library/Caches/pypoetry/virtualenvs/language-model-5UFyvxqf-py3.12/lib/python3.12/site-packages/openai/_module_client.py"", line 75, in <module>
    chat: resources.Chat = ChatProxy().__as_proxied__()
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""_pydevd_bundle/pydevd_pep_669_tracing_cython.pyx"", line 504, in _pydevd_bundle.pydevd_pep_669_tracing_cython.PyRaiseCallback.__call__
  File ""_pydevd_bundle/pydevd_pep_669_tracing_cython.pyx"", line 47, in _pydevd_bundle.pydevd_pep_669_tracing_cython.PEP669CallbackBase.frame
  File ""/Users/tworedz/Library/Caches/pypoetry/virtualenvs/language-model-5UFyvxqf-py3.12/lib/python3.12/site-packages/openai/_utils/_proxy.py"", line 49, in __class__
    proxied = self.__get_proxied__()
              ^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/tworedz/Library/Caches/pypoetry/virtualenvs/language-model-5UFyvxqf-py3.12/lib/python3.12/site-packages/openai/_utils/_proxy.py"", line 55, in __get_proxied__
    return self.__load__()
           ^^^^^^^^^^^^^^^
  File ""/Users/tworedz/Library/Caches/pypoetry/virtualenvs/language-model-5UFyvxqf-py3.12/lib/python3.12/site-packages/openai/_module_client.py"", line 12, in __load__
    return _load_client().chat
           ^^^^^^^^^^^^^^
  File ""/Users/tworedz/Library/Caches/pypoetry/virtualenvs/language-model-5UFyvxqf-py3.12/lib/python3.12/site-packages/openai/__init__.py"", line 323, in _load_client
    _client = _ModuleClient(
              ^^^^^^^^^^^^^^
  File ""/Users/tworedz/Library/Caches/pypoetry/virtualenvs/language-model-5UFyvxqf-py3.12/lib/python3.12/site-packages/openai/_client.py"", line 104, in __init__
    raise OpenAIError(
openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable

By the way, the issue doesn't occur if we specify the OPENAI_API_KEY environment variable. However, it's peculiar that simply importing the package initializes a class with side effects, and this only happens in debug mode in PyCharm.
I'm using PyCharm 2023.3.5 (Professional Edition) on macOS Sonoma M1 Pro.
To Reproduce
Described steps above.
Code snippets
No response
OS
macOS
Python version
Python 3.12.3
Library version
openai 1.30.1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1425","Request for API Access to Usage Data for Better Account Management","2024-05-14T20:21:08Z","Open issue","No label","Confirm this is a feature request for the Python library and not the underlying OpenAI API.
 This is a feature request for the Python library
Describe the feature or improvement you're requesting
Centralizing usage management under a single account for multiple projects can be exhausting and inefficient.
I believe it would be a great feature to allow users to access their own usage costs and activity via API, making it easier to manage.
Additional context
The current system centralizes usage and activity management, making it inaccessible for individual teams needing to track project-specific costs and plan studies.
Providing API access to usage data will:
Decentralize management, enabling teams to monitor their own projects.
Help teams make better decisions based on their specific usage data.
Increase transparency and efficiency for teams working on multiple projects under a single account.
 The text was updated successfully, but these errors were encountered: 
👍4
tabajara98, pedrodall, douradorobert, and leoig reacted with thumbs up emoji❤️1
douradorobert reacted with heart emoji🚀1
luisacurcio reacted with rocket emoji👀1
luisacurcio reacted with eyes emoji
All reactions
👍4 reactions
❤️1 reaction
🚀1 reaction
👀1 reaction"
"https://github.com/openai/openai-python/issues/1424","when AsyncOpenAI vision inferernce fails it continues to fail for all subsequent calls even using new instances of AsyncOpenAI","2024-05-14T20:13:19Z","Closed issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
When making an image to text (vision) call, if the first URL is invalid, all other calls return the same error!!!
Simple instantiate a client:
 client1= AsyncOpenAI()
 client2 = AsyncOpenAI()
now if i make vision calls and it encounters an error, lets say a bad URL that returns:
 {'message': 'Unknown MIME type', 'code': 400}
All subsequent calls are failing with the same error now for client1/client2 etc.
 At first i was using the same client, so decided to separate the instances, but this still persists no matter what.
Seems like the internal httpx.AsyncClient is not cleaned of state, or perhaps it's a different issue in the internal state management of the openai module.
Even providing a new instances of httpx.AsyncClient() does not solve: AsyncOpenAI(http_client=httpx.AsyncClient())
To Reproduce
create two clients or use the same one, the same bugs for both cases:
make a vision inference for a bad URL: https://www.aljazeera.com//Mahmoud
 make a vision inference for a good url: https://www.fidh.org/local/cache-vignettes/L1680xH600/website_61_-f9931.jpg?1714127812
When the first one fails the second will also have the same failing message:
 {'message': 'Unknown MIME type', 'code': 400}
Code snippets
No response
OS
Mac OSX 12.7.3
Python version
Python 3.10.11
Library version
1.29.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1419","Any Examples of GPT-4o？","2024-07-22T10:38:59Z","Closed issue","No label","Confirm this is a feature request for the Python library and not the underlying OpenAI API.
 This is a feature request for the Python library
Describe the feature or improvement you're requesting
There are currently no multi-modal calling examples for 4o in this lib or documentation.
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1416","Using Azure open AI Assistant throws TypeError: create() got an unexpected keyword argument 'file_ids'","2024-05-22T19:17:19Z","Closed issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
I am working on Azure notebook and while creating Azure Open AI Assistant source1, source2 I am getting TypeError: create() got an unexpected keyword argument 'file_ids', I tried the solution mentioned here even though it is not from AZURE but it gives me different error TypeError: Assistants.create() got an unexpected keyword argument 'attachments', The same code works fine locally without any TypeError: create() got an unexpected keyword argument 'file_ids' is there an update for creating assistants api on AZURE notebook(or any other cloud based notebook it is not working on Databricks notebook too , it was working on Databrick notebook few weeks ago) or it is a new bug related to this bug ? I am using gpt-35-turbo-16k-0613 model, openai 1.28.1 version. I am not using autogen library.
To Reproduce
[details](https://learn.microsoft.com/en-us/answers/questions/1665693/using-azure-open-ai-assistant-throws-typeerror-cre
 Run below code in Azure or Databricks notebook to reproduce error
 `from openai import AzureOpenAI
client = AzureOpenAI(
 api_key=os.getenv(""AZURE_OPENAI_API_KEY""),
 api_version=""2024-02-15-preview"",
 azure_endpoint = os.getenv(""AZURE_OPENAI_ENDPOINT"")
 )
Upload a file with an ""assistants"" purpose
file = client.files.create(
 file=open(""speech.py"", ""rb""),
 purpose='assistants'
 )
Create an assistant using the file ID
assistant = client.beta.assistants.create(
 instructions=""You are an AI assistant that can write code to help answer math questions."",
 model=""gpt-4-1106-preview"",
 tools=[{""type"": ""code_interpreter""}],
 file_ids=[file.id]
 )`)
Code snippets
No response
OS
Linux 22.05.09,
Python version
3.8.5
Library version
openai- 1.28.1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1412","Parametr name mismatch.","2024-07-10T09:57:30Z","Closed issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
Parametr name mismatch in method create_and_run_stream parametr tool_resources when call post request body data named tool.
 And result :
 openai.BadRequestError: Error code: 400 - {'error': {'message': ""Unknown parameter: 'tool'."", 'type': 'invalid_request_error', 'param': 'tool', 'code': 'unknown_parameter'}}

To Reproduce
Call method client.beta.threads.create_and_run() with filled tool_resources param.
Code snippets
No response
OS
Windows
Python version
python v3.10
Library version
openai v1.28.1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1411","Openai.FineTuningJob.list_events does not work with v0.27 and api version 2024-02-01","2024-05-12T23:48:07Z","Closed issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
What is the api version that is compatible with an openai sdk version of 0.27.10?
 And for Deployment.list() it worked with ""2023-03-15-preview"". Is there a compatibility matrix somewhere?
 Thank you.
>>> openai.FineTuningJob.list_events(""ftjob-xxxxxxxxxx"")

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/lib/python3.10/site-packages/openai/api_resources/abstract/nested_resource_class_methods.py"", line 133, in paginated_list_nested_resources
    return getattr(cls, resource_request_method)(
  File ""/usr/local/lib/python3.10/site-packages/openai/api_resources/abstract/nested_resource_class_methods.py"", line 43, in nested_resource_request
    response, _, api_key = requestor.request(
  File ""/usr/local/lib/python3.10/site-packages/openai/api_requestor.py"", line 298, in request
    resp, got_stream = self._interpret_response(result, stream)
  File ""/usr/local/lib/python3.10/site-packages/openai/api_requestor.py"", line 700, in _interpret_response
    self._interpret_response_line(
  File ""/usr/local/lib/python3.10/site-packages/openai/api_requestor.py"", line 765, in _interpret_response_line
    raise self.handle_error_response(
openai.error.InvalidRequestError: Resource not found

To Reproduce
openai.FineTuningJob.list_events(""ftjob-xxxxxxxxxx"")
Code snippets
openai.FineTuningJob.list_events(""ftjob-xxxxxxxxxx"")
OS
linux
Python version
3.10
Library version
open v0.27.10
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1410","Support load-balancing across OpenAI instances","2024-05-12T23:50:32Z","Closed as not planned issue","No label","Confirm this is a feature request for the Python library and not the underlying OpenAI API.
 This is a feature request for the Python library
Describe the feature or improvement you're requesting
Introduce the ability to use a prioritized set of backends and load-balance across different OpenAI instances.
Additional context
I wrote a Python OpenAI Load Balancer as an example of what may be useful.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1407","assistant run polling endless","2024-05-12T23:52:28Z","Closed as not planned issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
client.beta.threads.runs.create_and_poll will run endless, when the status of run is imcomplete
the unnormal run info:
{
    ""id"": ""run_t7khKpQ2vvQJAnO3mJcRa47Y"",
    ""object"": ""thread.run"",
    ""created_at"": 1715252643,
    ""assistant_id"": ""asst_7HoUHfUXf8KPJQaetjN5suKP"",
    ""thread_id"": ""thread_9JfoNfFWttmyx9GSHVi6T9Gd"",
    ""status"": ""incomplete"",
    ""started_at"": 1715252644,
    ""expires_at"": null,
    ""cancelled_at"": null,
    ""failed_at"": null,
    ""completed_at"": 1715252648,
    ""required_action"": null,
    ""last_error"": null,
    ......
    ""max_completion_tokens"": null,
    ""max_prompt_tokens"": null,
    ""truncation_strategy"": {
        ""type"": ""auto"",
        ""last_messages"": null
    },
    ""incomplete_details"": {
        ""reason"": ""max_prompt_tokens""
    },
}
To Reproduce
use file search tools: tools=[{""type"": ""file_search""}]
trigger the limitation of max_prompt_tokens
Code snippets
No response
OS
ubuntu
Python version
Python 3.10.12
Library version
openai v1.26.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1404","Coding with Modular Math","2024-05-13T00:11:00Z","Closed as not planned issue","No label","Confirm this is a feature request for the Python library and not the underlying OpenAI API.
 This is a feature request for the Python library
Describe the feature or improvement you're requesting
Recompiling the OpenAI Python API library using modular mathematics involves simplifying and optimizing the codebase by applying the principles of modular design, which can inherently streamline operations and improve maintainability. Here's how we can refactor this library using these concepts:
Step-by-Step Recompilation Process:
 Define Modular Components:
 Identify the key functionalities of the OpenAI Python API library that need to be modularized, such as API communication, error handling, streaming, polling, and configuration management.
 Create Modular Templates:
 Develop templates for each component that define how they interact with each other and with the external environment. This includes input/output specifications, expected behaviors, and error management protocols.
 Implement Modular Functions:
 Write the code for each module separately, ensuring that each module performs a specific task and interacts with other modules through well-defined interfaces.
 Integration Testing:
 Once individual modules are implemented, perform integration testing to ensure that modules interact correctly and the entire system functions as expected. Adjust interfaces and interactions as needed based on test results.
 Optimize with Modular Formulas:
 Apply modular mathematics to optimize the operations within modules. This could involve simplifying mathematical operations, optimizing data handling and processing, and enhancing error correction mechanisms within the modules.
 Documentation and Examples:
 Document each module and the overall system architecture to ensure that other developers can understand and contribute to the project. Provide examples of how to use the modular system, including how to handle common tasks and potential errors.
 User Acceptance Testing:
 Conduct user acceptance testing with typical use cases to ensure that the system meets the needs of its intended users. Collect feedback to identify any areas for improvement.
 Deployment and Monitoring:
 Deploy the recompiled library and monitor its performance. Set up logging and monitoring to track the system’s performance and identify any issues in real-time
Modular Recompilation of OpenAI Python API Library
 Here’s a conceptual Python module that demonstrates a simplified version of the OpenAI Python API client using modular design principles. This example focuses on core functionalities and leaves placeholders where modular formulas might be applied for further optimization and customization.
import os
 import httpx
 from typing import Any, Dict, Optional
class ModularClient:
 def init(self, api_key: Optional[str] = None, base_url: str = ""https://api.openai.com""):
 self.api_key = api_key or os.getenv(""OPENAI_API_KEY"")
 self.base_url = base_url
 self.headers = {""Authorization"": f""Bearer {self.api_key}""}
def send_request(self, endpoint: str, method: str = ""GET"", data: Optional[Dict] = None) -> Any:
    url = f""{self.base_url}{endpoint}""
    with httpx.Client() as client:
        if method == ""POST"":
            response = client.post(url, json=data, headers=self.headers)
        else:
            response = client.get(url, headers=self.headers)
        return self.handle_response(response)

def handle_response(self, response: httpx.Response) -> Any:
    if response.status_code == 200:
        return response.json()
    else:
        return self.handle_error(response)

def handle_error(self, response: httpx.Response) -> None:
    if response.status_code == 401:
        raise Exception(""Authentication Error"")
    elif response.status_code == 429:
        raise Exception(""Rate Limit Exceeded"")
    elif response.status_code >= 500:
        raise Exception(""Server Error"")
    else:
        raise Exception(f""Failed with status code {response.status_code}: {response.text}"")

def chat_completion(self, prompt: str) -> str:
    data = {
        ""model"": ""gpt-3.5-turbo"",
        ""prompt"": prompt,
        ""max_tokens"": 150
    }
    return self.send_request(""/v1/chat/completions"", method=""POST"", data=data)

Usage
if name == ""main"":
 client = ModularClient()
 prompt = ""Tell me a joke""
 try:
 response = client.chat_completion(prompt)
 print(""Response:"", response)
 except Exception as e:
 print(""Error:"", str(e))
Explanation of the Code
 ModularClient Class: This is the main class that encapsulates API client functionalities. It initializes with an API key and base URL, setting up headers for authentication.
 send_request Method: Handles sending requests to the API. It uses HTTPX for HTTP calls, which simplifies handling both synchronous and asynchronous requests.
 handle_response Method: Processes responses from the API. If successful, it parses the JSON; otherwise, it forwards the response to the error-handling method.
 handle_error Method: Dedicated error handling based on HTTP status codes. This modular approach makes it easier to manage and update error handling separately from other code logic.
 chat_completion Method: A specific method to demonstrate how a typical API call might be structured. This can be replicated or modified for other API endpoints.
Testing and Validation
 To ensure the recompiled library is ready for testing:
Unit Tests: Write unit tests for each module to validate individual functionalities.
 Integration Tests: Conduct integration tests to ensure that modules work together as expected.
 Performance Tests: Measure the performance of the modular library, especially in areas where modular optimizations are expected to improve efficiency.
 Each of these steps would contribute to a fully functional, efficiently modularized version of the OpenAI Python API library, aligned with modern software engineering practices and the unique advantages of modular mathematics.
How to Expand and Customize
 Modular Formulas: Integrate mathematical optimizations specific to your modular math concepts directly into the send_request or handle_response methods to optimize data processing or error correction.
 Advanced Error Handling: Enhance the handle_error method with more sophisticated error recovery logic, such as retries or exponential backoff.
 Configuration Management: Implement a more advanced configuration system that can handle different environments (development, staging, production) seamlessly.
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1403","Usage Stream in Cancelled Streams","2024-05-12T23:56:45Z","Closed as not planned issue","No label","Confirm this is a feature request for the Python library and not the underlying OpenAI API.
 This is a feature request for the Python library
Describe the feature or improvement you're requesting
I'm unsure whether this should be a feature request or a bug report. But the behavior of the stream's usage in cases where a stream is cancelled is unclear. That is, there are many scenarios in which the response is discarded before it's completed, such as an interruption in a conversation. In these contexts, the stream can be cancelled using
stream = self.client.chat.completions.create(...)
...
stream.response.close()

However, in these cases, I'm unable to read the usage of the stream. How should this be interpreted?
From a developer's standpoint, using the library, it would be more convenient if the usage is available in the response once the response is completed, rather than having to subscribe to the chunks once they're cancelled. That is, stream.response.usage.
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1399","Better implementation for assistantfunction calls","2024-05-08T02:07:54Z","Closed issue","No label","Confirm this is a feature request for the Python library and not the underlying OpenAI API.
 This is a feature request for the Python library
Describe the feature or improvement you're requesting
Why the current Implementation for assistant function calling is cumbersome
I feel the implementation for function calls in Python is quite cumberson with assistant. If I understand the documentations correctly, the initial approach requires developers to handle threads, runs, and tool outputs manually. Each step, from starting conversations and defining tools to gathering and submitting outputs, involves multiple function calls. This complexity makes the process error-prone and less intuitive for developers.
To just get started with assistant (no function calls yet), we need to define four things:
OpenAI client
Assistant
Thread
Message
To enable function calls, we need to wrap the function results in yet another object tool_outputs_stream. I feel there could be better implementations out there. As a result, I asked chatGPT on what could be a better implementation.
Desired improvement
I guess we should introduces a unified ""Assistant with Function Call Capability"" object. This single object manages all aspects of function-based interactions, offering an interface for setting up an assistant, starting conversations, handling messages, collecting function outputs, and submitting results.
Simplified API Usage: By centralizing interactions within a single object, the new design reduces setup complexity and minimizes errors.
Encapsulation: The assistant object internally manages tool calls and states, eliminating the need for manual thread and run management.
Maintainability: The modular design is easy to extend and modify.
Better Abstraction: It abstracts away technical complexities, letting developers focus on writing function logic instead of managing the intricate mechanics of the SDK.
 This improvement provides a cleaner, more developer-friendly API, making it easier to integrate function calls and boost productivity in OpenAI-based applications.
I have asked ChatGPT to generate a mockup AssistantWithFunctionCalls. Please let me know if there is interest in bringing this feature in?
ChatGPT conversation
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1398","Issue with run polling(sleep)","2024-05-25T22:00:34Z","Closed issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
Hi OpenAI team,
I'm reaching out to report a possible issue or improvement regarding the polling mechanism in asynchronous functions.
Problem Description
The current implementation of the poll method uses the time.sleep() function, which is blocking and halts the execution of other asynchronous tasks during polling. This behavior affects concurrency, preventing other coroutines from running efficiently.
To Reproduce
Code snippets
import asyncio

async def poll(
    self,
    run_id: str,
    thread_id: str,
    extra_headers: dict | None = None,
    extra_query: dict | None = None,
    extra_body: dict | None = None,
    timeout: float | 'httpx.Timeout' | None = 'NOT_GIVEN',
    poll_interval_ms: int | 'NotGiven' = 'NOT_GIVEN',
) -> 'Run':
    extra_headers = {""X-Stainless-Poll-Helper"": ""true"", **(extra_headers or {})}

    if is_given(poll_interval_ms):
        extra_headers[""X-Stainless-Custom-Poll-Interval""] = str(poll_interval_ms)

    terminal_states = {""requires_action"", ""cancelled"", ""completed"", ""failed"", ""expired""}
    while True:
        response = await self.with_raw_response.retrieve(
            thread_id=thread_id,
            run_id=run_id,
            extra_headers=extra_headers,
            extra_body=extra_body,
            extra_query=extra_query,
            timeout=timeout,
        )

        run = response.parse()
        if run.status in terminal_states:
            return run

        if not is_given(poll_interval_ms):
            from_header = response.headers.get(""openai-poll-after-ms"")
            if from_header is not None:
                poll_interval_ms = int(from_header)
            else:
                poll_interval_ms = 1000

        await asyncio.sleep(poll_interval_ms / 1000)
OS
macos
Python version
Python v3.11.4
Library version
openai v1.26.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1397","AzureOpenAI().model.list() doesn't work when azure_deployment is specified","2024-05-07T02:54:08Z","Open issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
When using AzureOpenAI with azure_deployment specified and calling client.model.list(), the request fails with
openai.NotFoundError: Error code: 404 - {'error': {'code': '404', 'message': 'Resource not found'}}

Upon investigation, this is because the request url ended up being
https://my-resource.openai.azure.com/openai/deployments/my-deployment/models?api-version=2023-05-15

while the correct one should be
https://my-resource.openai.azure.com/openai/models?api-version=2023-05-15

To Reproduce
client = openai.AzureOpenAI(
    azure_endpoint=...,
    azure_deployment=...,
    api_key=...,
    api_version=...,
)
client.models.list()
Code snippets
No response
OS
macOS
Python version
Python v3.10.11
Library version
openai v1.26.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1387","We have released a new major version of our SDK, and we recommend upgrading promptly.","2024-05-13T01:21:48Z","Closed as not planned issue","No label","We have released a new major version of our SDK, and we recommend upgrading promptly.
Regarding this new sdk...
I'll admit it I'm a newb. I've been using ChatGPT to create a rather extensive app that has been working great. I took a break from the project and suddenly i could not get answers from the api. I installed wsl and ubuntu did the whole grit thing and still i am unable to resolve the errors in my code. GPT4 Turbo is of no help with the issue and despite my best efforts I've killed a couple of days work on the project effectively going in circles attempting to fix the issue. If there is anybody willing to take a look at my code and advise me on how I can get things going again, I would be most grateful...
Here is my code as is now
client = OpenAI()
Load environment variables from the .env file
load_dotenv()
Retrieve and set the API key from the .env file
OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')
Define the model to be used
model_id = ""gpt-4-turbo-2024-04-09"" # Replace with the correct model ID you are using
def chatgpt_conversation(conversation_log):
 print(""Debug - Sending message to OpenAI: "", conversation_log[-1]['content'])
headers = {
    ""Content-Type"": ""application/json"",
    ""Authorization"": f""Bearer {OPENAI_API_KEY}""
}
data = {
    ""model"": model_id,
    ""messages"": conversation_log,
    ""temperature"": 0.7
}

response = requests.post('https://api.openai.com/v1/chat/completions', json=data, headers=headers)

if response.status_code == 200:
    response_data = response.json()
    assistant_response = response_data['choices'][0]['message']['content'].strip()
    conversation_log.append({'role': 'assistant', 'content': assistant_response})
    print(""Debug - Received response from GPT-4:"", assistant_response)
else:
    error_message = f""Failed to get a valid response from OpenAI API: {response.text}""
    print(""Debug - Error in communicating with GPT-4:"", error_message)
    conversation_log.append({'role': 'assistant', 'content': error_message})

return conversation_log

Example usage
conversation_history = [
 {'role': 'user', 'content': 'How may I help you?'},
 {'role': 'user', 'content': 'Hi'}
 ]
updated_conversation = chatgpt_conversation(conversation_history)
 print(""Final conversation log:"", updated_conversation)
For some reason it thinks part of my request contains json data and currently it does not need to send json data for the chatbot section of my app. there is another call by the ""assessment portion"" of my app that will need to compare text from a json file against user entered text to assess compliance.
Kind regards,
Eddie Van Halen
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1384","Streaming wwwith ""chat.completions.create"" endpoint produces a httpx.RemoteProtocolError","2024-05-13T01:43:30Z","Closed as not planned issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
base_url: links fastchat with the openai API
 LLM for inference: ""mistralai/Mixtral-8x7B-Instruct-v0.1""
When I try to stream chunks of generated text I get this error:
Traceback (most recent call last):
  File ""/home/philip/miniconda3/lib/python3.12/site-packages/httpx/_transports/default.py"", line 69, in map_httpcore_exceptions
    yield
  File ""/home/philip/miniconda3/lib/python3.12/site-packages/httpx/_transports/default.py"", line 113, in __iter__
    for part in self._httpcore_stream:
  File ""/home/philip/miniconda3/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py"", line 367, in __iter__
    raise exc from None
  File ""/home/philip/miniconda3/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py"", line 363, in __iter__
    for part in self._stream:
  File ""/home/philip/miniconda3/lib/python3.12/site-packages/httpcore/_sync/http11.py"", line 349, in __iter__
    raise exc
  File ""/home/philip/miniconda3/lib/python3.12/site-packages/httpcore/_sync/http11.py"", line 341, in __iter__
    for chunk in self._connection._receive_response_body(**kwargs):
  File ""/home/philip/miniconda3/lib/python3.12/site-packages/httpcore/_sync/http11.py"", line 210, in _receive_response_body
    event = self._receive_event(timeout=timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/philip/miniconda3/lib/python3.12/site-packages/httpcore/_sync/http11.py"", line 220, in _receive_event
    with map_exceptions({h11.RemoteProtocolError: RemoteProtocolError}):
  File ""/home/philip/miniconda3/lib/python3.12/contextlib.py"", line 158, in __exit__
    self.gen.throw(value)
  File ""/home/philip/miniconda3/lib/python3.12/site-packages/httpcore/_exceptions.py"", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.RemoteProtocolError: peer closed connection without sending complete message body (incomplete chunked read)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""/home/philip/opena.py"", line 19, in <module>
    for chunk in stream:
  File ""/home/philip/miniconda3/lib/python3.12/site-packages/openai/_streaming.py"", line 46, in __iter__
    for item in self._iterator:
  File ""/home/philip/miniconda3/lib/python3.12/site-packages/openai/_streaming.py"", line 58, in __stream__
    for sse in iterator:
  File ""/home/philip/miniconda3/lib/python3.12/site-packages/openai/_streaming.py"", line 50, in _iter_events
    yield from self._decoder.iter_bytes(self.response.iter_bytes())
  File ""/home/philip/miniconda3/lib/python3.12/site-packages/openai/_streaming.py"", line 280, in iter_bytes
    for chunk in self._iter_chunks(iterator):
  File ""/home/philip/miniconda3/lib/python3.12/site-packages/openai/_streaming.py"", line 291, in _iter_chunks
    for chunk in iterator:
  File ""/home/philip/miniconda3/lib/python3.12/site-packages/httpx/_models.py"", line 829, in iter_bytes
    for raw_bytes in self.iter_raw():
  File ""/home/philip/miniconda3/lib/python3.12/site-packages/httpx/_models.py"", line 883, in iter_raw
    for raw_stream_bytes in self.stream:
  File ""/home/philip/miniconda3/lib/python3.12/site-packages/httpx/_client.py"", line 126, in __iter__
    for chunk in self._stream:
  File ""/home/philip/miniconda3/lib/python3.12/site-packages/httpx/_transports/default.py"", line 112, in __iter__
    with map_httpcore_exceptions():
  File ""/home/philip/miniconda3/lib/python3.12/contextlib.py"", line 158, in __exit__
    self.gen.throw(value)
  File ""/home/philip/miniconda3/lib/python3.12/site-packages/httpx/_transports/default.py"", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: peer closed connection without sending complete message body (incomplete chunked read)

Note when I dont stream stream = False I don't get any errors and I get my inference.
To Reproduce
Simply run the code below, using the LLM I used, as well as python and openai versions and so on.
Code snippets
from openai import OpenAI

client = OpenAI(
    # This is the default and can be omitted
    api_key=""myKey"",
    base_url=""https://base_url.io/v1""
)

stream = client.chat.completions.create(
    messages=[
        {
            ""role"": ""user"",
            ""content"": ""Say this is a test"",
        }
    ],
    model=""mistralai/Mixtral-8x7B-Instruct-v0.1"",
    stream=True
)
for chunk in stream:
    if chunk.choices[0].delta.content is not None:
        print(chunk.choices[0].delta.content, end="""")
OS
linux
Python version
Python 3.11.8
Library version
openai v.1.24.0
 The text was updated successfully, but these errors were encountered: 
👍1
pchalasani reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/openai-python/issues/1379","Add PowerShell files to the list of supported files for retrieval","2024-05-13T01:51:52Z","Closed as not planned issue","No label","Confirm this is a feature request for the Python library and not the underlying OpenAI API.
 This is a feature request for the Python library
Describe the feature or improvement you're requesting
When adding a PowerShell file to the vector store it throws an error Files with extensions [.ps1] are not supported for retrieval.
A workaround could be to rename the ps1 files to .java or another supported format.
I would like to request PowerShell files be added to the support list.
Please let me know if I need to pursue this on a different channel. - Thank you
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1374","Memory Leak","2024-05-13T00:33:22Z","Closed as not planned issue","No label","I upgraded openai version to 1.23.5

and still I can see some issues with the same function just it got moved to different place.
/Users/jogireddy/PycharmProjects/flaskProject/venv/lib/python3.9/site-packages/openai/_legacy_response.py:347: size=1389 KiB (+1389 KiB), count=12109 (+12109), average=117 B/Users/jogireddy/PycharmProjects/flaskProject/venv/lib/python3.9/site-packages/openai/_response.py:674: size=1278 KiB (+1278 KiB), count=11239 (+11239), average=116 B/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/linecache.py:137: size=1232 KiB (+1232 KiB), count=12014 (+12014), average=105 B/Users/jogireddy/PycharmProjects/flaskProject/venv/lib/python3.9/site-packages/httpx/_content.py:175: size=868 KiB (+868 KiB), count=83 (+83), average=10.5 KiB/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/functools.py:58: size=769 KiB (+769 KiB), count=12308 (+12308), average=64 B/Users/jogireddy/PycharmProjects/flaskProject/venv/lib/python3.9/site-packages/openai/_legacy_response.py:330: size=726 KiB (+726 KiB), count=6332 (+6332), average=117 B/Users/jogireddy/PycharmProjects/flaskProject/venv/lib/python3.9/site-packages/openai/_response.py:653: size=676 KiB (+676 KiB), count=5931 (+5931), average=117 B/Users/jogireddy/PycharmProjects/flaskProject/venv/lib/python3.9/site-packages/openai/_resource.py:34: size=358 KiB (+358 KiB), count=5088 (+5088), average=72 B/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/abc.py:102: size=261 KiB (+261 KiB), count=2992 (+2992), average=89 B/Users/jogireddy/PycharmProjects/flaskProject/venv/lib/python3.9/site-packages/httpx/_models.py:82: size=249 KiB (+249 KiB), count=3736 (+3736), average=68 B 
Originally posted by @rdy5644 in #1361 (comment)
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1373","Inconsistent base_url behavior with module client and OpenAIClient","2024-04-26T00:33:59Z","Open issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
The issue is that the base_url in the OpenAI client enforces a trailing slash whereas setting the module openai.base_url does not enforce.
This is 100% reproducible and while it can be worked around, the inconsistency in clients causes issues in scripts that use both clients.
To Reproduce
Create a module client and use a base_url without a trailing slash. See error with URL for completions
Create an OpenAIClient and use a base_url without a trailing slash. See no erros.
Code snippets
This works
    client = openai.OpenAI(
        api_key=my_key,
        base_url=""https://myllmserver.com/api/v1""
    )
    print(client.completions.create(
       model=""mistralai/Mistral-7B-Instruct-v0.2"",
       prompt=""test"" 
    ))

This does not:
    openai.base_url=""https://myllmserver.com/api/v1""
    openai.api_key=my_key
    print(openai.completions.create(
       model=""mistralai/Mistral-7B-Instruct-v0.2"",
       prompt=""test""
    ))

Error:
https://myllmserver.com/api/v1completions ""HTTP/1.1 404 Not Found""

OS
macOS
Python version
Python v3.12
Library version
openai v1.23.6
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1363","thêm vấn đề lưu trữ chat","2024-04-24T16:02:02Z","Closed as not planned issue","No label","Confirm this is a feature request for the Python library and not the underlying OpenAI API.
 This is a feature request for the Python library
Describe the feature or improvement you're requesting
hãy thêm chức năng lưu trữ và ghim các chat cần thiết trên chat gpt
Additional context
fewfwefwe
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1362","[Bug Assistant] temperature setting not working","2024-05-13T01:08:18Z","Closed as not planned issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
Even if I change temperature for assistant, the response shows temperature is set to 1.0
To Reproduce
Create an assistant and pass a temperature value.
Code snippets
assistant = client.beta.assistants.create(
        name=f""temperature_test_assistant"",
        instructions="""",
        model=model_name,
        tools=[{""type"": ""code_interpreter""}],
        temperature=0.2,
        
    )
OS
Linux
Python version
python 3.11
Library version
openai==1.23.3
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1361","Memory Leak in chat completion create","2024-04-24T18:00:30Z","Closed issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
After Making many streaming calls from a flask app the process memory is infinitely increasing and it is never getting reduced.
 Even after performing close on response and client.
To Reproduce
Wrap the below code in flask api or you can run as a long running process as well.
Invoke this method in a loop or for 1000 you will be able to see the process gradually takes up more memory.
Even performing manual gc didn't help.
Code snippets
def make_stream_call_with_close():
    client = OpenAI(api_key=os.environ.get(""OPENAI_API_KEY""))
    responses = client.chat.completions.create(
        model='gpt-3.5-turbo',
        messages=[{""role"": ""user"", ""content"": ""Say exactly one word.""}],
        stream=True,
    )
    responses.response.close()
    print(responses.response.is_closed)
    client.close()
    print(client.is_closed())
OS
macOS,Linux
Python version
3.10.13
Library version
openai==1.3.4 , httpx==0.27.0 , pydantic==2.7.1, pydantic_core==2.18.2
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1354","openai.BadRequestError: Error code: 400 - Unrecognized request argument supplied: dataSources.","2024-04-22T20:38:37Z","Closed issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
When I use chat.completions.create(), and give extra_body, encounter this error.
I've checked endpoint uri, key, ai search endpoint and key. Nothing wrong.
After I remove extra_body, it worked.
To Reproduce
client = AzureOpenAI(
 azure_endpoint="""",
 api_key="""",
 azure_deployment="""",
 api_version="""",
 )
completion = client.chat.completions.create(
 model=deployment,
 messages=[
 {
 ""role"": ""user"",
 ""content"": ""What are my available health plans?"",
 },
 ],
 extra_body={
 ""dataSources"": [
 {
 ""type"": ""AzureCognitiveSearch"",
 ""parameters"": {
 ""endpoint"": os.environ[""AZURE_AI_SEARCH_ENDPOINT""],
 ""key"": os.environ[""AZURE_AI_SEARCH_API_KEY""],
 ""indexName"": os.environ[""AZURE_AI_SEARCH_INDEX""]
 }
 }
 ]
 }
 )
Code snippets
No response
OS
windows
Python version
Python 3.11.5
Library version
openai v1.23.2
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1352","Module level Client forcefully loaded when in PyCharm debugger","2024-04-22T21:22:13Z","Closed as not planned issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
If I run the following script simply via Python
from openai import OpenAI

openai = OpenAI(api_key=""MY KEY"")

print(""We got to the end!"")
It works fine. However, in PyCharm's debugger, it seems like the _module_client is being forcefully initialized, so I get this error raised when trying to run on the from openai import OpenAI line
# in _client.py
        if api_key is None:
            raise OpenAIError(
                ""The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable""
            )
The workaround that worked for me is conditionally loading the module client like this in __init__.py at the end of the file:
import sys

gettrace = getattr(sys, 'gettrace', None)

if gettrace is None:
    # We're not in debug mode, load libraries
    from ._module_client import (
        beta as beta,
        chat as chat,
        audio as audio,
        files as files,
        images as images,
        models as models,
        batches as batches,
        embeddings as embeddings,
        completions as completions,
        fine_tuning as fine_tuning,
        moderations as moderations,
    )
To Reproduce
Follow instructions from the description.
Code snippets
No response
OS
Windows 11
Python version
3.12
Library version
1.23.2
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1351","Fix BadRequestError when using ChatCompletionMessage with explicit tool_calls=None","2024-04-22T21:26:23Z","Closed as not planned issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
Description
When creating a ChatCompletionMessage instance with tool_calls explicitly set to None, a subsequent chat completion request using that message fails with a BadRequestError. The error message indicates that None is not of type array for messages.0.tool_calls.
Steps to reproduce:
Create a ChatCompletionMessage instance with tool_calls set to None.
 Use the created message in a chat completion request.
 Expected behavior:
 The chat completion request should succeed, as setting tool_calls to None should be equivalent to not specifying it at all.
Actual behavior:
 The chat completion request fails with a BadRequestError, indicating that None is not of type array for messages.0.tool_calls.
Proposed solution:
Modify the ChatCompletionMessage class to handle the case when tool_calls is explicitly set to None. If tool_calls is None, it should be treated the same as if it were not specified, allowing the chat completion request to succeed.
Additional information:
OpenAI version: 1.23.2
 Python version: Python 3.11.8
 Operating system: macOS
Steps to Reproduce:
Install the required libraries: 
!pip install openai==1.23.2 python-dotenv -q
Import the necessary modules and load the environment variables: 
from openai.types.chat import ChatCompletionMessage
from openai import AsyncOpenAI
import os
from dotenv import load_dotenv
load_dotenv()
Create an instance of the AsyncOpenAI client: 
client = AsyncOpenAI(api_key=os.environ.get(""OPENAI_API_KEY""))
Create two ChatCompletionMessage objects with tool_calls=None: 
message2 = ChatCompletionMessage(content=""foo bar"", role=""assistant"")
Output: ChatCompletionMessage(content='foo bar', role='assistant', function_call=None, tool_calls=None)
message3 = ChatCompletionMessage(content=""foo bar"", role=""assistant"", tool_calls=None)
Output: ChatCompletionMessage(content='foo bar', role='assistant', function_call=None, tool_calls=None)
Verify that both objects pass equality checks and have the same model_dump() output: 
assert message2 == message3
assert message2.model_dump() == message3.model_dump()
Attempt to create a chat completion using both objects: 
await client.chat.completions.create(model=""gpt-3.5-turbo"", messages=[message2]) (Expected to succeed)
await client.chat.completions.create(model=""gpt-3.5-turbo"", messages=[message3]) (Expected to fail with a BadRequestError)
Expected Behavior:
Both objects should be treated equally and result in a successful chat completion creation.
Actual Behavior:
One object succeeds, while the other fails with a BadRequestError.
Proposed Fix:
Modify the serialization of ChatCompletionMessage objects to handle tool_calls=None consistently, ensuring that identical objects are treated equally in API requests.
Additional Context:
This issue may be related to how tool_calls is serialized and deserialized in the OpenAI API. A thorough review of the serialization process is recommended to prevent similar inconsistencies in the future.
OS
macOS
Python version
Python 3.11.8
Library version
1.23.2
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1346","Threads messages list misinterprets the block type","2024-04-19T14:47:58Z","Open issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
When getting the messages list for the last response in a run, I sometimes get the following content
 content=[ImageFileContentBlock(image_file=None, type='text', text={'value': '...'}, (...) ) ]
 instead of
content=[TextContentBlock(text=Text(annotations=[], value=""...""), (...))]
The ImageFileContentBlock comes when asking for data from files and the TextContentBlock when asking generic questions like ""how are you?"". Even if it is the default behavior (not sure why image file of type text), these structures are not structured in the same way, the first one has the 'text' field a dictionary and the second one has a Text object
To Reproduce
create a message (client.beta.threads.messages.create)
create a run and wait for response (client.beta.threads.runs.create_and_poll)
get response (client.beta.threads.messages.list)
Code snippets
messages = client.beta.threads.messages.list(
            thread_id=thread_id,
            run_id=run_id,
            timeout=requests_timeout
        )
OS
Linux
Python version
Python v3.11.4
Library version
openai v1.23.1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1345","Shared data content when used with FastAPI backgroundtasks and client.beta.threads.runs.create_and_poll","2024-04-19T02:57:40Z","Open issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
I have a FastAPI API that I have one endpoint in /execute. This endpoint creates a FastAPI backgroundtask to do a call to the assistants API. In here I make a request to client.beta.threads.runs.create_and_poll, there are no issues starting the request and getting the expected outcome. However, if another request comes in to the API, when the first request background task is still waiting for the create_and_poll, both requests will complete at the same time and the result out from create_and_poll will be based on the first request it received.
 I am creating the OpenAI client in the backgrountask method to not use a global one.
I am a beginner in python and still trying to learn and understand how things are isolated, could this be a usage error or is there no thread isolation for the client.beta.threads.runs.create_and_poll method?
To Reproduce
Code snippets
No response
OS
Ubuntu
Python version
Python 3.10
Library version
openai 1.20
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1344","Add ability to customize ""/chat/completions"" path after base url","2024-04-18T23:48:35Z","Open issue","No label","Confirm this is a feature request for the Python library and not the underlying OpenAI API.
 This is a feature request for the Python library
Describe the feature or improvement you're requesting
Some providers have custom endpoint for calling models.
For ex https://api.minimax.chat/v1/text/chatcompletion_v2
If we used python library with base url of https://api.minimax.chat/v1 final request is being made on https://api.minimax.chat/v1/chat/completions which doesn't exist.
Instead of hardcoding path /chat/completions param So lets add option for specifying custom path.
Example:
from openai import OpenAIclient = OpenAI()

completion = client.chat.completions.create(
  model=""custom_model"",
  messages=[
    {""role"": ""system"", ""content"": ""You are a helpful assistant.""},
    {""role"": ""user"", ""content"": ""Hello!""}
  ],
  custom_path=""text/chatcompletion_v2""
)

print(completion.choices[0].message)
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
👍8
LukeSamkharadze, GauravRanganath, cheikhfiteni, UYasher, GZack2000, bastbu, vishaldas-datafacade, and jczhang07 reacted with thumbs up emoji
All reactions
👍8 reactions"
"https://github.com/openai/openai-python/issues/1336","AttributeError When Using 'vector_stores' Method in OpenAI Python Client","2024-04-22T21:23:40Z","Closed as not planned issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
After upgrading to the latest version of the OpenAI Python client (openai==1.21.2), I am encountering an AttributeError when attempting to use the new 'vector_stores' method as described in the documentation. The error suggests that the 'OpenAI' object does not have an attribute 'vector_stores'.
To Reproduce
Install the latest version of the OpenAI Python client (openai==1.21.2).
 Execute the following Python code:
from openai import OpenAI
client = OpenAI()

vector_store = client.vector_stores.create(
  name=""Support FAQ""
)
print(vector_store)

Code snippets
from openai import OpenAIclient = OpenAI()

vector_store = client.vector_stores.create(
  name=""Support FAQ""
)
print(vector_store)
OS
Ubuntu 20.04.6 LTS
Python version
Python 3.8.10
Library version
openai 1.21.2
 The text was updated successfully, but these errors were encountered: 
👍1
aviram-microsoft reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/openai-python/issues/1335","Assistants API: inconsistency between API reference and a migration guide","2024-04-18T22:29:53Z","Closed issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
Greetings fellows,
A migration guide for assistant file upload suggests using the following format:
""attachments"": [
    {
      ""file_id"": ""file-123"",
      ""tools"": [
        { ""type"": ""file_search"" },
        { ""type"": ""code_interpreter"" }
      ]
    }
  ]

However, the API reference shows something else:
https://platform.openai.com/docs/api-reference/messages/createMessage#messages-createmessage-attachments
Whenever I am using the version explained in the migration guide, I get the following traceback:
Failed to run listener function (error: Error code: 400 - {'error': {'message': ""Missing required parameter: 'attachments[0].tools'."", 'type': 'invalid_request_error', 'param': 'attachments[0].tools', 'code': 'missing_required_parameter'}})

To Reproduce
Try to migrate existing code that uses assistants API with file uploads to the new version
Get stuck. :(
Code snippets
I think, this is the suspected piece in the Python library that differs:
openai.types.beta.threads.message_create_params.Attachment
class Attachment(TypedDict, total=False):
    add_to: List[Literal[""file_search"", ""code_interpreter""]]

    file_id: str
    """"""The ID of the file to attach to the message.""""""
It expects add_to and not a tools list.
OS
macOS
Python version
Python v3.12.1
Library version
openai v1.21.2
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1334","AttributeError: 'ThreadRunCreated' object has no attribute 'type'","2024-04-18T21:35:59Z","Closed issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
From the docs at https://github.com/openai/openai-python/blob/main/helpers.md:
from typing_extensions import overridefrom openai import AssistantEventHandler, OpenAIfrom openai.types.beta.threads import Text, TextDeltafrom openai.types.beta.threads.runs import ToolCall, ToolCallDelta

client = OpenAI()  # NOTE: this is an edit to the original code: `client = openai.OpenAI()`

class EventHandler(AssistantEventHandler):
  @override
  def on_text_created(self, text: Text) -> None:
    print(f""\nassistant > "", end="""", flush=True)

  @override
  def on_text_delta(self, delta: TextDelta, snapshot: Text):
    print(delta.value, end="""", flush=True)

  def on_tool_call_created(self, tool_call: ToolCall):
    print(f""\nassistant > {tool_call.type}\n"", flush=True)

  def on_tool_call_delta(self, delta: ToolCallDelta, snapshot: ToolCall):
    if delta.type == 'code_interpreter':
      if delta.code_interpreter.input:
        print(delta.code_interpreter.input, end="""", flush=True)
      if delta.code_interpreter.outputs:
        print(f""\n\noutput >"", flush=True)
        for output in delta.code_interpreter.outputs:
          if output.type == ""logs"":
            print(f""\n{output.logs}"", flush=True)

with client.beta.threads.runs.stream(
  thread_id=thread.id,            # using real thread ID
  assistant_id=assistant.id    # using real assistant ID
) as stream:
    for event in stream:
        if event.type == ""thread.message.delta"" and event.data.delta.content:
            print(event.data.delta.content[0].text)
throws the following error:
  File ""/workspaces/ChatGPT-Arc/tmp/sandbox.py"", line 108, in main
    stream_run(client, thread, assistant)
  File ""/workspaces/ChatGPT-Arc/tmp/sandbox.py"", line 80, in stream_run
    if event.type == ""thread.message.delta"" and event.data.delta.content:
  File ""/usr/local/lib/python3.9/site-packages/pydantic/main.py"", line 792, in __getattr__
    raise AttributeError(f'{type(self).__name__!r} object has no attribute {item!r}')
AttributeError: 'ThreadRunCreated' object has no attribute 'type'

It appears that the docs are out-of-date.
Using event.data.object instead of event.type works for me:
        for event in stream:
            if event.data.object == ""thread.run.step.delta"" and event.data.delta.content:
                print(event.data.delta.content[0].text)
Note also that client = openai.OpenAI() should be client = OpenAI() in the example code.
To Reproduce
See above
Code snippets
No response
OS
macOS
Python version
3.9.19
Library version
1.21.2
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1324","Add timestamp_granularities for AzureOpenAI","2024-04-17T20:39:28Z","Closed as not planned issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
TypeError: Translations.create() got an unexpected keyword argument 'timestamp_granularities'

This is when calling AzureOpenAI client with code below.
To Reproduce
To Reproduce you should use AzureOpenAI client (OpenAI client is fine):
import osfrom openai import AzureOpenAI

client = AzureOpenAI(
    api_key=""API_KEY"",  
    api_version=""2023-12-01-preview"",
    azure_endpoint=""ENDPOINT_URL""
)
audio_file = open(""output_audio.m4a"", ""rb"")
transcript = client.audio.translations.create(
  model=""whisper-1"",
  file=audio_file,
  prompt="""",
  response_format=""verbose_json"",
  timestamp_granularities=[""word""] # or segment
  
)
I've already also made a PR.
Add timestamp_granularities for AzureOpenAI #1323
Code snippets
This code actually works without any problem:
from openai import OpenAIclient = OpenAI(api_key='OPENAPI_KEY')

audio_file = open(""output_audio.m4a"", ""rb"")
transcript = client.audio.transcriptions.create(
  file=audio_file,
  model=""whisper-1"",
  response_format=""verbose_json"",
  timestamp_granularities=[""word""],
)

print(transcript.words)
But When you change OpenAI to AzureOpenAI it doesn't work.
Also this argument is supported in Azure and it works. So, it is just the matter of adding it here.
OS
MacOS, Linux
Python version
Python v3.12.1
Library version
1.20.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1313","Azure OpenAI API doesn't return consistent model ids","2024-04-15T21:54:37Z","Closed as not planned issue","No label","Confirm this is a feature request for the Python library and not the underlying OpenAI API.
 This is a feature request for the Python library
Describe the feature or improvement you're requesting
When compared with the ""classical"" API, Azure's doesn't seem to return a consistent model ID after a completion. For instance, I have a deployment of the following models:
gpt-4-0613
gpt-4-1106-preview
gpt-4-0125-preview
However, when calling a completion, the model attribute of the retrieved object is always gpt-4 (differently from the ""classical"" API which provides the full name of the model).
Same with gpt-35-turbo-0613, gpt-35-turbo-1106, gpt-35-turbo-0125.
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1312","Support for truncation_strategy, max_prompt_tokens and max_completion_tokens","2024-04-16T14:36:23Z","Closed issue","No label","Confirm this is a feature request for the Python library and not the underlying OpenAI API.
 This is a feature request for the Python library
Describe the feature or improvement you're requesting
Can the python package be updated to support truncation_strategy, max_prompt_tokens and max_completion_tokens as seen in the OpenAI Documentation here?
Is it possible to do this already?
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1310","gpt-4-turbo-2024-04-09 not pointing to correct cutoff date without an explicitly set system message statement","2024-04-13T16:36:58Z","Closed as not planned issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library (sort of, because it affects it too)
Describe the bug
While this issue intersects with the usage of the Python API, it primarily pertains to a deeper, potentially systemic concern affecting model performance and accuracy across any interfacing tool.
Issue Description:
 When interfacing with various model versions (specifically noted with gpt-4-turbo-2024-04-09), it appears that without an explicit declaration of the model version within the system message, e.g., “You are gpt-4-turbo-2024-04-09,” the model does not correctly anchor to its designated context and capabilities. It exhibits behaviors reminiscent of older versions or operates with outdated knowledge, such as assuming a knowledge cutoff either in April 2023 or all the way back to September 2021. This misalignment occurs despite selecting the correct model version in API settings, indicating a potential oversight in how model version grounding is internally managed.
Impact:
 Such behavior can significantly impede the model's utility and accuracy, particularly in contexts requiring up-to-date information or specific model functionalities.
Additional Context:
 I have shared these findings within the OpenAI Developer Community, garnering corroborative feedback and additional insights which indicate this is not an isolated incident:
Community Post 1
Community Post 2
I'm submitting this report here to ensure the issue receives appropriate attention and to facilitate a cross-disciplinary examination of its implications and potential resolutions, since as of now it's affecting model performance and since the Python API is among the most popular methods to utilize the OpenAI model(s).
Thank you for your time and attention to this matter.
To Reproduce
(Use the code snippet below for a simple A/B test example)
Without Specified System Message:
Initialize a session with the model gpt-4-turbo-2024-04-09 without specifying any additional context or information about its version in the system message.
Ask the model about recent events or information post-April 2023, which should fall within its knowledge base if it were accurately aligned with its latest training data.
Observe and note any responses that suggest the model is unaware of events or information after April 2023, indicating it is not leveraging its most current training.
With Specified System Message:
In a new session, explicitly tell the model ""You are gpt-4-turbo-2024-04-09"" in the system message.
Repeat the inquiry about recent events or specific information post-April 2023.
Compare the responses to those from Step 1 to observe any notable improvements or corrections in the model's awareness and accuracy, suggesting that the explicit context provided has re-aligned the model with its expected knowledge base.
Further explanation of the issue
End-users of the API would reasonably expect that when they select a specific model version, like gpt-4-turbo-2024-04-09, the model inherently understands and aligns with its latest training data up to the declared cutoff without needing any additional nudges or clarifications. Right now this is not the case.
The absence of an explicit version declaration in the system message leads to 'AI hallucination', where the model outputs are inaccurately anchored in past knowledge, incorrectly suggesting a knowledge cut-off in 2021.
This misalignment persists despite correct model version selection in API settings, suggesting a need for improved internal management of model version grounding. This seems to be a persisting issue across the GPT-4 model checkpoint variants, but the problem is most prominent in the latest version.
By following the mentioned inclusion in the system message, anyone replicating the test should be able to see a clear contrast in the model's performance and awareness of its knowledge cutoff, based on whether or not its version was explicitly stated in the system message.
A simple method to A/B test this is to use i.e. the provided Python code (see below) to run diagnostics on the model and spot the difference yourself. You can change the test question to any post-April 2023 event by editing the user_question variable.
Code snippets
To replicate and test the issue:
import osfrom openai import OpenAI

# User question to be askeduser_question = ""Is Cormac McCarthy still alive?""

# Instantiate the client with your API keyclient = OpenAI(api_key=os.environ.get(""OPENAI_API_KEY""))

# Inform the user about the question being asked without model guidanceprint(""---"")
print(f""Asking without specifying the model version in the system message: {user_question}"")
print(""---"")
response = client.chat.completions.create(
    model=""gpt-4-turbo-2024-04-09"",
    messages=[
        {""role"": ""system"", ""content"": ""You are an AI assistant based on OpenAI's GPT-4.""},
        {""role"": ""user"", ""content"": user_question}
    ]
)

# Print the responseprint(response.choices[0].message.content)

# Inform the user about the question being asked with model guidanceprint(""---"")
print(f""Asking while specifying 'You are gpt-4-turbo-2024-04-09' in the system message: {user_question}"")
print(""---"")
response_with_system_message = client.chat.completions.create(
    model=""gpt-4-turbo-2024-04-09"",
    messages=[
        {""role"": ""system"", ""content"": ""You are gpt-4-turbo-2024-04-09""},
        {""role"": ""user"", ""content"": user_question}
    ]
)

# Print the responseprint(response_with_system_message.choices[0].message.content)
(note how even pointing out the model's role as a GPT-4 AI assistant is not enough to align it to the most current cutoff date data)
OS
(all)
Python version
Python v3.11
Library version
openai v1.14.2
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1308","Azure API Manager with Azure OpenAI","2024-04-12T23:40:48Z","Closed issue","No label","Confirm this is a feature request for the Python library and not the underlying OpenAI API.
 This is a feature request for the Python library
Describe the feature or improvement you're requesting
We proxy the Azure OpenAI service using API Manager. The system design is from Microsoft found here: https://github.com/microsoft/AzureOpenAI-with-APIM.
The request would be for a way to override/set a custom endpoint url along with api key name. (ex. below the key name is Ocp-Apim-Subscription-Key)
example code
 #!/bin/bash
 apimUrl=""THE_HTTPS_URL_OF_YOUR_APIM_INSTANCE""
 modelName=""GPT-3_5-Turbo"" # Probaby what you named your model, but change if necessary
 apiVersion=""2023-03-15-preview"" # Do not change this value, unless you are testing a different API version
 subscriptionKey=""YOUR_APIM_SUBSCRIPTION_KEY""
url=""${apimUrl}""/deployments/""${modelName}""/chat/completions?api-version=""${apiVersion}""
 key=""Ocp-Apim-Subscription-Key: ${subscriptionKey}""
curl $url -k -H ""Content-Type: application/json"" -H $key -d '{
 ""messages"": [
 {
 ""role"": ""system"",
 ""content"": ""You are an AI assistant that helps people find information.""
 },
 {
 ""role"": ""user"",
 ""content"": ""What are the differences between Azure Machine Learning and Azure AI services?""
 }
 ]
 }'
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1307","GPT4-turbo integration","2024-04-10T07:55:14Z","Open issue","No label","Confirm this is a feature request for the Python library and not the underlying OpenAI API.
 This is a feature request for the Python library
Describe the feature or improvement you're requesting
According to:
https://platform.openai.com/docs/models/gpt-4-turbo-and-gpt-4
 Yesterday GPT4-Turbo was released:
 changes - vision integration and JSON output.
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
👍4
MountedRPC, nyavramov, jwuliger, and successren reacted with thumbs up emoji
All reactions
👍4 reactions"
"https://github.com/openai/openai-python/issues/1306","DEFER_PYDANTIC_BUILD causes issues","2024-04-09T16:07:29Z","Open issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
Release 1.16.1 broke clients by deferring the build if pydantic models. As a result, doing a model_dump() on an object returned by the library can fail.
bc6866e
While this speeds up build times, it should be considered a breaking change and ideally, avoided.
If the speedup is significant, we would request not using a generic envvar like DEFER_PYDANTIC_BUILD to revert the change, instead using something like OPENAI_PYDANTIC_DEFER_BUILD.
We noticed this issue with a streaming tool call message, and are not sure if other deep classes exhibit this behaviour.
To Reproduce
Create a streaming tool call chat completion message.
Call model_dump() on the object.
Observe stack trace in pydantic serializer code.
Code snippets
No response
OS
macOS, Linux
Python version
Python v3.11.8
Library version
openai v1.16.1
 The text was updated successfully, but these errors were encountered: 
👍5
crizCraig, nikhilmakhijani28, iwaszko, dylanhhu, and tstadel reacted with thumbs up emoji
All reactions
👍5 reactions"
"https://github.com/openai/openai-python/issues/1303","VQA Issue ID # 98, 99 - Two Up Text + Media - Multiple Issues","2024-04-09T21:35:19Z","Closed as not planned issue","No label","Submitted by
Steph
Section
Two Up Text + Media component
Page
98 - Security
99 - Security
Issue Descriptions
Issue # 98
Mobile — increase top and bottom padding to use our M spacer value (45.41)
Issue # 98 Device/ Browser/ Version - Macbook Pro / Arc (Chrome) / Sonoma 14.2.1
Issue # 98 Screenshot/Link
Screenshot Issue #98
Issue # 99
Middle align content within container
Issue # 99 Device/ Browser/ Version - Macbook Pro / Arc (Chrome) / Sonoma 14.2.1
Issue # 99 Screenshot/Link
Screenshot Issue # 99
Tracker/Link
VQA Tracker
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1301","how to fetch the openai api key usage and remaining balance","2024-04-08T22:26:48Z","Closed as not planned issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
i want to fetch the openai api usage through the python code, though i have seen some comments hereon the code, but they dont seem to wok for me, can i get any latest version for fetching the usage.
To Reproduce
nil
Code snippets
No response
OS
windows
Python version
3.11
Library version
latest
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1300","ChatCompletions create() doesn't type-check enums as role","2024-04-06T17:56:30Z","Open issue","No label","Confirm this is a feature request for the Python library and not the underlying OpenAI API.
 This is a feature request for the Python library
Describe the feature or improvement you're requesting
This was discussed previously @ #911, but just opening a new issue so that it's in the issue tracker.
The Bug
When calling,
role: Literal[""system"", ""user"", ""assistant""] = ...
completion = await client.chat.completions.create(
    model=""gpt-4"",
    messages={""role"": role, ""content"": ""Hi""}
)
Reason
The main issue is that it's defined as an enum of typed dicts,
ChatCompletionMessageParam = Union[
    ChatCompletionSystemMessageParam,
    ChatCompletionUserMessageParam,
    ChatCompletionAssistantMessageParam,
    ChatCompletionToolMessageParam,
    ChatCompletionFunctionMessageParam,
]
Of course, for tooling & functions, we need to know exactly whether or not it's a user or assistant message. But role: str, content: str is the most common use-case, so it would be nice if system/user/assistant can all use that kind of interface when necessary.
Additional context
Potential Solution
From #911 (comment),
I think a very simple solution would be to add another type to the ChatCompletionMessageParam union.
Currently, we have
ChatCompletionMessageParam = Union[
    ChatCompletionSystemMessageParam,
    ChatCompletionUserMessageParam,
    ChatCompletionAssistantMessageParam,
    ChatCompletionToolMessageParam,
    ChatCompletionFunctionMessageParam,
]
If instead we had,
ChatCompletionMessageParam = Union[
    ChatCompletionSystemMessageParam,
    ChatCompletionUserMessageParam,
    ChatCompletionAssistantMessageParam,
    ChatCompletionToolMessageParam,
    ChatCompletionFunctionMessageParam,
    ChatCompletionGenericMessageParam,
]
Where ChatCompletionGenericMessageParam was,
class ChatCompletionGenericMessageParam(TypedDict, total=False):
    content: Required[Optional[str]]
    role: Required[Literal[""system"", ""user"", ""assistant""]]
Under my understanding, adding an option to a Union does not break compatibility with anybody (Lmk if I'm wrong)
This handles all possible situations (pydantic model, custom TypedDict with Literal, a mixture of both, etc).
And, if the user wants to pass in a tool/FunctionCall/ChatCompletionContentPartParam, then obviously they need to use the other types and prove that it's specifically a system/user/etc.
OpenAI can still internally type check safely, because it can write a type-safe function to convert ChatCompletionGenericMessageParam into a Union[ChatCompletionSystemMessageParam, ChatCompletionUserMessageParam,ChatCompletionAssistantMessageParam].
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1297","openai.Model.list() is invalid","2024-04-05T17:36:55Z","Closed as not planned issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
After the version update, openai.Model.list() is invalid, now how do I see which models the API can call?
To Reproduce
above
Code snippets
No response
OS
linux
Python version
3.9
Library version
1.13.3
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1296","Use Type ChatCompletionRole for ChatCompletionMessage.role","2024-04-05T17:54:55Z","Closed as not planned issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
It appears that the role attribute of ChatCompletionMessage is incorrectly using the type Literal[""assistant""] instead of ChatCompletionRole. Messages sent to, and received from, the Chat Completions API can indeed accept all values defined in ChatCompletionRole, and not just ""assistant"".
This is preventing use of the type ChatCompletionMessage provided in this library for type checking/enforcement.
To Reproduce
N/A
Code snippets
No response
OS
Python version
Library version
openai >=v1.0.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1279","Documentation/Wrappers to stream images/files created by Assistant API","2024-03-29T09:01:48Z","Closed issue","No label","Confirm this is a feature request for the Python library and not the underlying OpenAI API.
 This is a feature request for the Python library
Describe the feature or improvement you're requesting
In the current SDK/docs on streaming the assistants api, it's not clear how to retrieve the files created in the streaming event.
For the non-streaming approach, there would be a file.id that we can use to pull the object.
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1278","Please add a security policy on how to report security issues","2024-03-28T17:18:13Z","Open issue","No label","Confirm this is a feature request for the Python library and not the underlying OpenAI API.
 This is a feature request for the Python library
Describe the feature or improvement you're requesting
Please add a security policy to this GitHub repo. I can't find any information on how to report security issues in private. Using the issue tracker would be undesirable as it could zero-day some exploits reported.
Additional context
For example, these issues really should have been reported privately:
When debug logging is enabled, api-key header is also printed #1082
At debug log level API requests to OpenAI get logged with all headers, including an API key, in plaintext  #1196
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1277","Drop support of Python 3.7","2024-03-28T16:52:58Z","Open issue","No label","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
Python 3.7 is already end-of-life as of 2023-06-27 as you see from the PSF here: https://devguide.python.org/versions/, yet this client is still supporting it.
I recommend dropping support as further fixes to the Python 3.7 runtime won't be made and could affect usage of this client.
To Reproduce
Go to https://devguide.python.org/versions/
Notice Python 3.7 end support as of 2023-06-27
Notice in https://github.com/openai/openai-python/blob/main/pyproject.toml#L24 this client still claims support.
Code snippets
n/a
OS
macOS
Python version
Python 3.7.x
Library version
openai v1.14.3
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1276","batch_size param ignored when creating fine tuning job in Azure","2024-03-28T15:40:49Z","Closed as not planned issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
Batch size passed in fine tuning hyperparameters is ignored and set to 1.
To Reproduce
I pass
hyperparameters = openai.types.fine_tuning.job_create_params.Hyperparameters(
            n_epochs=1,
            batch_size=64,
        )

To AzureOpenAI client, but reated job ignores the batch_size and sets it to 1:
FineTuningJob(id='ftjob-5c2642e206294f30945d47f6ac6bf703', created_at=1711639578, error=None, fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(n_epochs=1), model='gpt-35-turbo-0613', object='fine_tuning.job',
organization_id=None, result_files=None, status='pending', trained_tokens=None, training_file='file-78c086798a404b7592e0191aa5838dce', validation_file='file-642e426e1e514211bd982709640d231d', updated_at=1711639585)

Batch Size is also set to 1 when I inspect the job in Azure UI.
Used to work with v 0.28, stopped working when upgraded to v1, current version openai==1.14.3
No client exception or warning/error event is being produced.
Code snippets
hyperparameters = openai.types.fine_tuning.job_create_params.Hyperparameters(
            n_epochs=1,
            batch_size=64,
        )
    response = client.fine_tuning.jobs.create(
        training_file=training_file_id,
        validation_file=validation_file_id,
        model=""gpt-35-turbo-0613"",
        hyperparameters=hyperparameters,
    )
I also tried:
    response = client.fine_tuning.jobs.create(
        training_file=training_file_id,
        validation_file=validation_file_id,
        model=""gpt-35-turbo-0613"",
        hyperparameters=dict(n_epochs=1, batch_size=64,
    )



### OS

macOS

### Python version

Python v3.10.13

### Library version

openai v1.14.3

 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1272","ImportError: cannot import name 'is_union' from 'pydantic.typing'","2024-03-26T17:39:19Z","Closed as not planned issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
import openai
 Traceback (most recent call last):
 File """", line 1, in 
 File ""/demo/miniconda3/envs/sqlcode/lib/python3.9/site-packages/openai/init.py"", line 8, in 
 from . import types
 File ""/demo/miniconda3/envs/sqlcode/lib/python3.9/site-packages/openai/types/init.py"", line 5, in 
 from .edit import Edit as Edit
 File ""/demo/miniconda3/envs/sqlcode/lib/python3.9/site-packages/openai/types/edit.py"", line 6, in 
 from .._models import BaseModel
 File ""/demo/miniconda3/envs/sqlcode/lib/python3.9/site-packages/openai/_models.py"", line 33, in 
 from ._utils import (
 File ""/demo/miniconda3/envs/sqlcode/lib/python3.9/site-packages/openai/_utils/init.py"", line 2, in 
 from ._utils import flatten as flatten
 File ""/demo/miniconda3/envs/sqlcode/lib/python3.9/site-packages/openai/_utils/_utils.py"", line 22, in 
 from .._compat import is_union as _is_union
 File ""/demo/miniconda3/envs/sqlcode/lib/python3.9/site-packages/openai/_compat.py"", line 55, in 
 from pydantic.typing import is_union as is_union
 ImportError: cannot import name 'is_union' from 'pydantic.typing' (/demo/miniconda3/envs/sqlcode/lib/python3.9/site-packages/pydantic/typing.cpython-39-x86_64-linux-gnu.so)
To Reproduce
python -c ""import pydantic.utils; print(pydantic.utils.version_info())""
 pydantic version: 1.7.4
 pydantic compiled: True
 install path: /demo/miniconda3/envs/sqlcode/lib/python3.9/site-packages/pydantic
 python version: 3.9.18 (main, Sep 11 2023, 13:41:44) [GCC 11.2.0]
 platform: Linux-5.15.0-88-generic-x86_64-with-glibc2.35
 optional deps. installed: ['typing-extensions']
 import openai
Code snippets
No response
OS
linux
Python version
python 3.9
Library version
openai 1.1.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1271","Multiprocessing pattern unclear","2024-03-25T17:44:21Z","Closed as not planned issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
Since v1.x, openai breaks use in multiprocessing, because multiprocessing chooses to pickle jobs and openai doesn't intend to support pickling (as far as I can see from #837).
This renders the pattern for doing multithreaded calling unclear. The reasons and policy are stated in #837 but no solution suggested. I'd like to be doing tens of thousands of requests per hour and I'd like to do these in parallel from a python program. What's the recommended pattern for multithreaded calling as of openai v.1x? I'm not working with more than about 100 parallel calls.
To Reproduce
wrap any chat/completion request in multiprocessing
observe TypeError: cannot pickle '_thread.RLock' object
Code snippets
No response
OS
Linux
Python version
Python 3.12
Library version
1.12.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1270","openai.BadRequestError: Error code: 400 - {'error': {'message': ""'$.messages[0].content' is invalid. Please check the API reference: https://platform.openai.com/docs/api-reference."", 'type': 'invalid_request_error', 'param': None, 'code': None}}","2024-03-25T13:38:05Z","Closed issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
Tried the below AsyncAzureOpenAI
import asyncio
import openai
import time
openai.api_type = ""azure""
openai.api_base = ""<base url>""
openai.api_version = ""<version>""
openai.api_key = ""<api key>""
instructions = [prompt1,prompt2,prompt3]
from tqdm import tqdm
async_client = openai.AsyncAzureOpenAI(
    azure_endpoint=""<base url>"",
    api_key=""<api key>"",
    api_version=""<version>""
    )

async def aync_main(prompt: str) -> None:
    response = await async_client.chat.completions.create(
        model=""<model>"",
        messages=[{""role"": ""user"", ""content"": prompt}]
    )
    return response.choices[0].message.content
async def async_run_loop(instructions):
    tasks = [aync_main(prompt) for prompt in enumerate(tqdm(instructions))]
    answers = await asyncio.gather(*tasks)
    return {idx: answer for idx, answer in enumerate(answers)}

async_answers = asyncio.run(async_run_loop(instructions))
print(async_answers)

But it returns the given error
openai.BadRequestError: Error code: 400 - {'error': {'message': ""'$.messages[0].content' is invalid. Please check the API reference: https://platform.openai.com/docs/api-reference."", 'type': 'invalid_request_error', 'param': None, 'code': None}}

To Reproduce
Tried the below AsyncAzureOpenAI
import asyncio
import openai
import time
openai.api_type = ""azure""
openai.api_base = ""<base url>""
openai.api_version = ""<version>""
openai.api_key = ""<api key>""
instructions = [prompt1,prompt2,prompt3]
from tqdm import tqdm
async_client = openai.AsyncAzureOpenAI(
    azure_endpoint=""<base url>"",
    api_key=""<api key>"",
    api_version=""<version>""
    )

async def aync_main(prompt: str) -> None:
    response = await async_client.chat.completions.create(
        model=""<model>"",
        messages=[{""role"": ""user"", ""content"": prompt}]
    )
    return response.choices[0].message.content
async def async_run_loop(instructions):
    tasks = [aync_main(prompt) for prompt in enumerate(tqdm(instructions))]
    answers = await asyncio.gather(*tasks)
    return {idx: answer for idx, answer in enumerate(answers)}

async_answers = asyncio.run(async_run_loop(instructions))
print(async_answers)

But it returns the given error
openai.BadRequestError: Error code: 400 - {'error': {'message': ""'$.messages[0].content' is invalid. Please check the API reference: https://platform.openai.com/docs/api-reference."", 'type': 'invalid_request_error', 'param': None, 'code': None}}

Code snippets
No response
OS
Ubuntu
Python version
Python 3.9.18
Library version
1.13.3
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1266","AsyncStream returning only empty choices.","2024-03-24T03:15:04Z","Closed issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
When I am calling client.chat.completions.create() with stream=True I am getting only ChatCompletionChunks with 'empty' choices.
To Reproduce
Run:
async with AsyncAzureOpenAI(
            api_key=os.environ['OPENAI_API_KEY'],
            azure_deployment=os.environ['OPENAI_AZURE_COMPLETIONS_DEPLOYMENT'],
            azure_endpoint=os.environ['OPENAI_BASE_URL'],
            api_version=""2023-12-01-preview""
        ) as client:
            openai_stream  = await client.chat.completions.create(
                model=""gpt-4"",
                messages=messages,
                temperature=0.2,
                max_tokens=1200,
                top_p=0.45,
                frequency_penalty=0,
                presence_penalty=0,
                stop=None,
                stream=True
            )
            logger.info(f""{openai_stream.__dict__=}"")
            
            
async for chunk in openai_stream:
        logger.info(f""{chunk.model_dump_json()=}"")
        logger.info(f""{chunk.__dict__=}"")
        current_response = chunk.choices[0].delta.content
        logger.info(f""{current_response=}"")
        yield current_response


returns:
openai_stream.__dict__={'response': <Response [200 OK]>, '_cast_to': <class 'openai.types.chat.chat_completion_chunk.ChatCompletionChunk'>, '_client': <openai.lib.azure.AsyncAzureOpenAI object at 0x0000011B098F2920>, '_decoder': <openai._streaming.SSEDecoder object at 0x0000011B0AA06890>, '_iterator': <async_generator object AsyncStream.__stream__ at 0x0000011B0985A9C0>, '__orig_class__': openai.AsyncStream[openai.types.chat.chat_completion_chunk.ChatCompletionChunk]}
chunk.model_dump_json()='{""id"":"""",""choices"":[],""created"":0,""model"":"""",""object"":"""",""system_fingerprint"":null,""prompt_filter_results"":[{""prompt_index"":0,""content_filter_results"":{""hate"":{""filtered"":false,""severity"":""safe""},""self_harm"":{""filtered"":false,""severity"":""safe""},""sexual"":{""filtered"":false,""severity"":""safe""},""violence"":{""filtered"":false,""severity"":""safe""}}}]}'
chunk.__dict__={'id': '', 'choices': [], 'created': 0, 'model': '', 'object': '', 'system_fingerprint': None}


Code snippets
No response
OS
Windows
Python version
Python v3.10.11
Library version
openai v1.10.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1265","RuntimeWarning: coroutine 'AsyncAPIClient.post' was never awaited","2024-03-24T03:05:09Z","Closed issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
Getting this error while using create_and_stream() method.
error logs:
2024-03-22 20:05:30 /usr/src/app/app/services/openai_service.py:111: RuntimeWarning: coroutine 'AsyncAPIClient.post' was never awaited 2024-03-22 20:05:30 stream_or_run = await self.client.beta.threads.runs.create_and_stream(thread_id=thread_id, assistant_id=self.assistant_id, metadata=metadata, event_handler=event_handler) 2024-03-22 20:05:30 Object allocated at (most recent call last): 2024-03-22 20:05:30 File ""/usr/local/lib/python3.9/site-packages/openai/resources/beta/threads/runs/runs.py"", lineno 1322 2024-03-22 20:05:30 request = self._post(
To Reproduce
Install openai 1.14.2 module and tracemalloc
start the tracemalloc using the command tracemalloc.start()
Execute the async run create_and_stream() method using an example code.
Example code:
from openai import AsyncAssistantEventHandler
 
class EventHandler(AsyncAssistantEventHandler):    
  @override
  async def on_text_created(self, text) -> None:
    print(f""\nassistant > "", end="""", flush=True)
      
  @override
   async def on_text_delta(self, delta, snapshot):
    print(delta.value, end="""", flush=True)
      
   async def on_tool_call_created(self, tool_call):
    print(f""\nassistant > {tool_call.type}\n"", flush=True)
  
   async def on_tool_call_delta(self, delta, snapshot):
    if delta.type == 'code_interpreter':
      if delta.code_interpreter.input:
        print(delta.code_interpreter.input, end="""", flush=True)
      if delta.code_interpreter.outputs:
        print(f""\n\noutput >"", flush=True)
        for output in delta.code_interpreter.outputs:
          if output.type == ""logs"":
            print(f""\n{output.logs}"", flush=True)
  
with client.beta.threads.runs.create_and_stream(
  thread_id=thread.id,
  assistant_id=assistant.id,
  instructions=""Please address the user as Jane Doe. The user has a premium account."",
  event_handler=EventHandler(),
) as stream:
  stream.until_done()

Example code was taken from https://platform.openai.com/docs/assistants/overview?context=with-streaming and updated for asynchronous operation.
Code snippets
In runs.py under AsyncRuns class.

    def create_and_stream(
        self,
        *,
        assistant_id: str,
        additional_instructions: Optional[str] | NotGiven = NOT_GIVEN,
        instructions: Optional[str] | NotGiven = NOT_GIVEN,
        metadata: Optional[object] | NotGiven = NOT_GIVEN,
        model: Optional[str] | NotGiven = NOT_GIVEN,
        tools: Optional[Iterable[AssistantToolParam]] | NotGiven = NOT_GIVEN,
        thread_id: str,
        event_handler: AsyncAssistantEventHandlerT | None = None,
        # Use the following arguments if you need to pass additional parameters to the API that aren't available via kwargs.
        # The extra values given here take precedence over values defined on the client or passed to this method.
        extra_headers: Headers | None = None,
        extra_query: Query | None = None,
        extra_body: Body | None = None,
        timeout: float | httpx.Timeout | None | NotGiven = NOT_GIVEN,
    ) -> (
        AsyncAssistantStreamManager[AsyncAssistantEventHandler]
        | AsyncAssistantStreamManager[AsyncAssistantEventHandlerT]
    ):
        """"""Create a Run stream""""""
        if not thread_id:
            raise ValueError(f""Expected a non-empty value for `thread_id` but received {thread_id!r}"")

        extra_headers = {
            ""OpenAI-Beta"": ""assistants=v1"",
            ""X-Stainless-Stream-Helper"": ""threads.runs.create_and_stream"",
            ""X-Stainless-Custom-Event-Handler"": ""true"" if event_handler else ""false"",
            **(extra_headers or {}),
        }
        request = self._post(
            f""/threads/{thread_id}/runs"",
            body=maybe_transform(
                {
                    ""assistant_id"": assistant_id,
                    ""additional_instructions"": additional_instructions,
                    ""instructions"": instructions,
                    ""metadata"": metadata,
                    ""model"": model,
                    ""stream"": True,
                    ""tools"": tools,
                },
                run_create_params.RunCreateParams,
            ),
            options=make_request_options(
                extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout
            ),
            cast_to=Run,
            stream=True,
            stream_cls=AsyncStream[AssistantStreamEvent],
        )
        return AsyncAssistantStreamManager(request, event_handler=event_handler or AsyncAssistantEventHandler())
OS
macOS
Python version
Python 3.10.13
Library version
openai v1.14.2
 The text was updated successfully, but these errors were encountered: 
👍2
martinellimarco and AkselsLedins reacted with thumbs up emoji
All reactions
👍2 reactions"
"https://github.com/openai/openai-python/issues/1263","Python 3.7 support dropped accidentally?","2024-03-25T16:51:36Z","Closed issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
This library seems to have problems with Python 3.7 now, possibly after this PR: #1114
I know Python 3.7 is itself EOL, but this library declares Python 3.7 support but then breaks downstream dependencies on install, such as checkov earlier versions. It would be best if this change could be reverted and then added back e.g. in a new major release and with Python supports updated.
To Reproduce
Install this library using pip on a system with Python 3.7 as the default
Code snippets
No response
OS
Ubuntu 20.04
Python version
Python v3.7
Library version
openai latest
 The text was updated successfully, but these errors were encountered: 
👍2
viceice and Anoce0 reacted with thumbs up emoji
All reactions
👍2 reactions"
"https://github.com/openai/openai-python/issues/1261","New Feature Proposal: Assistant API - Chaining streams for function execution","2024-05-13T01:31:07Z","Closed as not planned issue","No label","Confirm this is a feature request for the Python library and not the underlying OpenAI API.
 This is a feature request for the Python library
Describe the feature or improvement you're requesting
Hi,
I have developed a monkey patch to add the capacity for chaining streams which is very beneficial for the Assistant API function execution workflow. I think it could be integrated into the openai library. So, I guess you want to know the use case, right?
Imagine you are processing the assistant events in a loop (in my case I use the Async stream client but it's basically the almost same for the non-async streaming one):
async for chunk in assistant_stream_response:
    # Process chunk here
    
    
    # Process function calls
    if isinstance(chunk, ThreadRunRequiresAction):
        tool_outputs = # Execute the function and gather the outputs in this var     
    
        new_stream = await async_client.beta.threads.runs.submit_tool_outputs(
            thread_id=thread_id, # stored along the way
            run_id=chunk.data.id,
            tool_outputs=tool_outputs,
            stream=True
        )
        # we can chain the new_stream at the end of the current one to avoid writing another chunk processing loop
        assistant_stream_response.chain_stream(new_stream)


    yield result

With this, we can chain the tool submit stream response to the current one to avoid writing another chunk processing loop.
 Tested & working.
It very beneficial, especially when you integrate the assistant API inside a project to avoid changing the existing workflow.
 Here is the monkey patch:
#--------------------------------------MONKEY-PATCH-OPENAI--------------------------------------------------------------
import openai
from typing import Any, TypeVar, AsyncIterator, cast
from openai._utils import is_mapping
from openai._exceptions import APIError
from openai import AsyncOpenAI
import httpx

_T = TypeVar(""_T"")


def monkey_patch__init__(self, *, cast_to: type[_T], response: httpx.Response, client: AsyncOpenAI) -> None:
  self.response = response
  self._cast_to = cast_to
  self._client = client
  self._decoder = client._make_sse_decoder()
  self._iterator = self.__stream__()
  self._chained_stream = None # MOD HERE 

def chain_stream(self, stream): # NEW FUNCT HERE
  if self._chained_stream:
    self._chained_stream.chain_stream(stream)
  else:
    self._chained_stream = stream

async def monkey_patch__stream__(self) -> AsyncIterator[_T]:
  cast_to = cast(Any, self._cast_to)
  response = self.response
  process_data = self._client._process_response_data
  iterator = self._iter_events()

  async for sse in iterator:
    if sse.data.startswith(""[DONE]""):
      break

    if sse.event is None:
      data = sse.json()
      if is_mapping(data) and data.get(""error""):
        message = None
        error = data.get(""error"")
        if is_mapping(error):
          message = error.get(""message"")
        if not message or not isinstance(message, str):
          message = ""An error occurred during streaming""

        raise APIError(
          message=message,
          request=self.response.request,
          body=data[""error""],
        )

      yield process_data(data=data, cast_to=cast_to, response=response)

    else:
      data = sse.json()

      if sse.event == ""error"" and is_mapping(data) and data.get(""error""):
        message = None
        error = data.get(""error"")
        if is_mapping(error):
          message = error.get(""message"")
        if not message or not isinstance(message, str):
          message = ""An error occurred during streaming""

        raise APIError(
          message=message,
          request=self.response.request,
          body=data[""error""],
        )

      yield process_data(data={""data"": data, ""event"": sse.event}, cast_to=cast_to, response=response)

  async for _sse in iterator:
    ...

  if self._chained_stream: # MOD HERE
    async for chunk in self._chained_stream:
      yield chunk


openai.AsyncStream.__init__ = monkey_patch__init__
openai.AsyncStream.__stream__ = monkey_patch__stream__
openai.AsyncStream.chain_stream = chain_stream
#-----------------------------------------------------------------------------------------------------------------------

Best regards,
 Paul Irolla
Additional context
I have implemented this inside my personal fork of LiteLLM for integrating the assistant API into the existing workflow without changing a thousand of code lines.
 The text was updated successfully, but these errors were encountered: 
👍1
hayescode reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/openai-python/issues/1254","AsyncOpenAI occasionally throws ""Runtime Error: Event loop is closed"" on Windows","2024-03-26T11:32:24Z","Closed as not planned issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
On Windows, if you try getting chat completions the openai library will occasionally throw an error saying RuntimeError: Event loop is closed. This seems to originate from httpx trying to close the connection. I noticed it primarily happens on short lived async functions that are run in rapid succession. It also only seems to happen if the AsyncOpenAI client is created multiple times. If it's created once (like outside the loop in the example code below) then the issue doesn't happen. I believe this may or may not be related to httpx's connection pooling trying to reuse a connection that no longer exists. Here is a related issue from httpx's repo.
Here are the logs from the exception in the code snippet provided in this issue.
Task exception was never retrieved
future: <Task finished name='Task-6' coro=<AsyncClient.aclose() done, defined at c:\Users\Ethan\tonic_validate_tester\venv\Lib\site-packages\httpx\_client.py:2011> exception=RuntimeError('Event loop is closed')>
Traceback (most recent call last):
  File ""c:\Users\Ethan\tonic_validate_tester\venv\Lib\site-packages\httpx\_client.py"", line 2018, in aclose
    await self._transport.aclose()
  File ""c:\Users\Ethan\tonic_validate_tester\venv\Lib\site-packages\httpx\_transports\default.py"", line 385, in aclose
    await self._pool.aclose()
  File ""c:\Users\Ethan\tonic_validate_tester\venv\Lib\site-packages\httpcore\_async\connection_pool.py"", line 313, in aclose
    await self._close_connections(closing_connections)
  File ""c:\Users\Ethan\tonic_validate_tester\venv\Lib\site-packages\httpcore\_async\connection_pool.py"", line 305, in _close_connections
    await connection.aclose()
  File ""c:\Users\Ethan\tonic_validate_tester\venv\Lib\site-packages\httpcore\_async\connection.py"", line 171, in aclose
    await self._connection.aclose()
  File ""c:\Users\Ethan\tonic_validate_tester\venv\Lib\site-packages\httpcore\_async\http11.py"", line 265, in aclose
    await self._network_stream.aclose()
  File ""c:\Users\Ethan\tonic_validate_tester\venv\Lib\site-packages\httpcore\_backends\anyio.py"", line 54, in aclose
    await self._stream.aclose()
  File ""c:\Users\Ethan\tonic_validate_tester\venv\Lib\site-packages\anyio\streams\tls.py"", line 202, in aclose
    await self.transport_stream.aclose()
  File ""c:\Users\Ethan\tonic_validate_tester\venv\Lib\site-packages\anyio\_backends\_asyncio.py"", line 1191, in aclose
    self._transport.close()
  File ""C:\Users\Ethan\AppData\Local\Programs\Python\Python311\Lib\asyncio\proactor_events.py"", line 109, in close
    self._loop.call_soon(self._call_connection_lost, None)
  File ""C:\Users\Ethan\AppData\Local\Programs\Python\Python311\Lib\asyncio\base_events.py"", line 761, in call_soon
    self._check_closed()
  File ""C:\Users\Ethan\AppData\Local\Programs\Python\Python311\Lib\asyncio\base_events.py"", line 519, in _check_closed
    raise RuntimeError('Event loop is closed')
RuntimeError: Event loop is closed

To Reproduce
Run the code snippet provided on Windows 11; Python v3.11
Sometime during while the for loop is running you should see in the logs that the event loop is closed
Code snippets
async def test() -> None:
    chat_completion = await client.chat.completions.create(
        messages=[
            {
                ""role"": ""user"",
                ""content"": ""Say this is a test"",
            }
        ],
        model=""gpt-3.5-turbo"",
    )

for i in range(50):
    # Note we create the client inside this loop as it triggers the bug
    client = AsyncOpenAI()
    asyncio.run(test())
OS
Windows 11
Python version
Python v3.11.7
Library version
openai v1.12.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1253","An openai.APIConnectionError occurred calling the api","2024-03-21T01:50:16Z","Closed issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
Hi ,I can call OpenAI Api interface with my Api key but calling the Api using OpenAI-python results in an error ,it error. can you show how did you solve this problem.
 Api example:
 `❯ curl https://api.openai.com/v1/chat/completions
 -H ""Content-Type: application/Json""
 -H ""Authorization: Bearer key""
 -d '{
 ""model"": ""gpt-3.5-turbo"",
 ""messages"": [{""role"": ""user"", ""content"": ""how to use MySQL!""}],
 ""temperature"": 0.7
 }'
{
 ""id"": ""cxxxxxx"",
 ""object"": ""chat. Completion"",
 ""created"": 1710945410,
 ""model"": ""gpt-3.5-turbo-0125"",
 ""choices"": [
 {
 ""index"": 0,
 ""message"": {
 ""role"": ""assistant"",
 ""content"": ""To use MySQL, you will need to follow xxxx""
 },
 ""logprobs"": null,
 ""finish_reason"": ""stop""
 }
 ],
 ""usage"": {
 ""prompt_tokens"": 12,
 ""completion_tokens"": 282,
 ""total_tokens"": 294
 },
 ""system_fingerprint"": ""xxxx""
 }`
error log is :
 File ""/Users/xxx/Library/Caches/pypoetry/virtualenvs/backend-FuQBPevx-py3.11/lib/python3.11/site-packages/openai/_base_client.py"", line 1494, in _request
 return await self._retry_request(
 ^^^^^^^^^^^^^^^^^^^^^^^^^^
 File ""/Users/xxx/Library/Caches/pypoetry/virtualenvs/backend-FuQBPevx-py3.11/lib/python3.11/site-packages/openai/_base_client.py"", line 1563, in _retry_request
 return await self._request(
 ^^^^^^^^^^^^^^^^^^^^
 File ""/Users/xxxx/Library/Caches/pypoetry/virtualenvs/backend-FuQBPevx-py3.11/lib/python3.11/site-packages/openai/_base_client.py"", line 1504, in _request
 raise APIConnectionError(request=request) from err
 openai.APIConnectionError: Connection error.
To Reproduce
calling the api using openapi-python
Code snippets
No response
OS
mac os
Python version
python3.11
Library version
openapi v 1.0.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1252","Stream Parameter Issue in Async Thread Creation","2024-03-21T01:45:36Z","Closed issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
The bug arises when attempting to create a thread in the OpenAI Python library using asynchronous functions. It seems that the issue lies in passing the stream parameter, which is unrecognized, causing a BadRequestError with error code 400. This error indicates that the OpenAI API does not recognize the stream parameter as a valid parameter for thread creation. As a result, the thread creation process fails, leading to the generation of a BadRequestError and preventing the thread from being created successfully.
To Reproduce
To reproduce the behavior:
Set up the OpenAI Python library and ensure that all necessary dependencies are installed.
Copy the provided code into a Python script or notebook.
Ensure that environment variables such as API_VERSION, AZURE_ENDPOINT, and AZURE_OPENAI_KEY are correctly set with appropriate values.
Execute the script or notebook.
Upon execution, the script will attempt to create a thread with specified messages and instructions.
The bug occurs during the creation of the thread when the stream parameter is passed, causing the BadRequestError with error code 400 to be raised.
The error traceback will indicate that the stream parameter is unrecognized, leading to the failure of the thread creation process.
Code snippets
from __future__ import annotations

from typing_extensions import override

import openaifrom openai import AssistantEventHandlerfrom openai.types.beta import AssistantStreamEventfrom openai.types.beta.threads import Text, TextDeltafrom openai.types.beta.threads.runs import RunStep, RunStepDeltaimport osfrom dotenv import load_dotenv

load_dotenv()


class EventHandler(AssistantEventHandler):
    @override
    def on_event(self, event: AssistantStreamEvent) -> None:
        if event.event == ""thread.run.step.created"":
            details = event.data.step_details
            if details.type == ""tool_calls"":
                print(""Generating code to interpret:\n\n"")
        elif event.event == ""thread.message.created"":
            print(""\nResponse:\n"")

    @override
    def on_text_delta(self, delta: TextDelta, snapshot: Text) -> None:
        print(delta.value, end="""", flush=True)

    @override
    def on_run_step_done(self, run_step: RunStep) -> None:
        details = run_step.step_details
        if details.type == ""tool_calls"":
            for tool in details.tool_calls:
                if tool.type == ""code_interpreter"":
                    print(""\n\nExecuting code..."")

    @override
    def on_run_step_delta(self, delta: RunStepDelta, snapshot: RunStep) -> None:
        details = delta.step_details
        if details is not None and details.type == ""tool_calls"":
            for tool in details.tool_calls or []:
                if tool.type == ""code_interpreter"" and tool.code_interpreter and tool.code_interpreter.input:
                    print(tool.code_interpreter.input, end="""", flush=True)


async def main() -> None:
    client = openai.AsyncAzureOpenAI(
        api_version=os.getenv(""API_VERSION""),
        azure_endpoint=os.getenv(""AZURE_ENDPOINT""),
        api_key=os.getenv(""AZURE_OPENAI_KEY""),
    )

    assistant = await client.beta.assistants.create(
        name=""Math Tutor"",
        instructions=""You are a personal math tutor. Write and run code to answer math questions."",
        tools=[{""type"": ""code_interpreter""}],
        model=""gpt-4-1106-preview"",
    )

    try:
        question = ""I need to solve the equation `3x + 11 = 14`. Can you help me?""

        thread = await client.beta.threads.create(
            messages=[
                {
                    ""role"": ""user"",
                    ""content"": question,
                },
            ]
        )
        print(f""Question: {question}\n"")

        async with client.beta.threads.runs.create_and_stream(
            thread_id=thread.id,
            assistant_id=assistant.id,
            instructions=""Please address the user as Jane Doe. The user has a premium account."",
            event_handler=EventHandler(),
        ) as stream:
            stream.until_done()
            print()
    finally:
        client.beta.assistants.delete(assistant.id)


await main()
OS
windows
Python version
Python v3.11.2
Library version
openai v1.14.1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1248","Title: Integration Issue with DALL-E 3 in an Asynchronous Telegram Bot Using Python","2024-03-21T02:12:22Z","Closed as not planned issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
Hello OpenAI Community!
I'm working on an asynchronous Telegram bot in Python, utilizing the OpenAI API for generating text with the GPT-3.5 Turbo model. The text generation part is functioning smoothly, however, I've encountered difficulties when trying to add the capability to generate images using DALL-E 3.
I'm using the latest version of the openai library (installed via !pip install --upgrade openai), but my attempts to use openai.Completion or any related methods for DALL-E result in the following error:
To Reproduce
ERROR:main:Error processing message:
You tried to access openai.Completion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.
You can run openai migrate to automatically upgrade your codebase to use the 1.0.0 interface.
Alternatively, you can pin your installation to the old version, e.g., pip install openai==0.28
A detailed migration guide is available here: #742
Code snippets
import openai

openai.api_key = 'my_openai_api_key'

# Example code that leads to the errortry:
    response = openai.Completion.create(
        engine=""text-davinci-003"",
        prompt=""This is a test prompt."",
        temperature=0.7,
        max_tokens=150
    )
except Exception as e:
    print(f""Encountered an error: {e}"")
OS
Google Colab
Python version
3.10.12
Library version
1.14.1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1246","Memory leak","2024-07-12T13:01:16Z","Closed issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
I am using the AsyncAzureOpenAI class to instantiate a client and using a stream call to client.chat.completions.create. Even after performing close() on both client and response within a try-finally block, I am still encountering a memory leak that eventually leads to server crash.
 I tried the solution outlined in #1181, where the pydantic package was upgraded to 2.6.3, but this hasn't resolved my issue.
 I noticed using the gc library that memory usage increases after each call to this service. Our service is used for centralized management of AzureOpenAI accounts, hence a client is instantiated for every incoming request. Given the concurrent nature of this service, I'm wondering if client.with_options can support concurrent usage. Do you have any good solutions to address this memory leak issue?
To Reproduce
Several calls in a row, for example, to embeddings that are wrapped with asynс.
Code snippets
class LlmStreamApiHandler(tornado.web.RequestHandler):
    executor = ThreadPoolExecutor(200)

    def __init__(self, *args, **kwargs):
        super(LlmStreamApiHandler, self).__init__(*args, **kwargs)
        self.set_header('Content-Type', 'text/event-stream')
        self.set_header('Access-Control-Allow-Origin', ""*"")
        self.set_header(""Access-Control-Allow-Headers"", ""*"")
        self.set_header(""Access-Control-Allow-Methods"", ""*"")

    def on_finish(self):
        return super().on_finish()


    async def post(self):

        try:
            result = await self.process(...)
        except Exception as e:
            ...

        self.write(json.dumps(result) + ""\n"")
        await self.flush()


    async def process(self, ...)
        client = openai.AsyncAzureOpenAI(
            api_version=api_version,
            api_key=api_key,
            azure_endpoint=azure_endpoint,
            http_client=httpx.AsyncClient(
                proxies=config.api_proxy,
            ),
            max_retries=0
        )
        response_text = False
        try:

            response_text = await client.chat.completions.create(**prompt)
            async for chunk in response_text:
                chunk = chunk.model_dump()
                if chunk['choices'] == [] and chunk['id'] == """" and chunk['model'] == """" and chunk['object'] == """":
                    continue
                chunk_message = chunk['choices'][0]['delta']
                current_text = chunk_message.get('content', '')
                if bool(chunk_message) and current_text:
                    ...
                elif chunk['choices'][0][""finish_reason""] == ""stop"":
                    break

                elif current_text == '' and chunk_message.get('role', '') == ""assistant"":
                    ...
                elif chunk['choices'][0][""finish_reason""] == ""content_filter"":
                    ...
                else:
                    continue
                self.write(json.dumps(json_data) + ""\n"")
                await self.flush()
        except Exception as e:
            ...
            raise ...
        finally:
            if response_text:
                await response_text.close()
            await client.close()
        return ...
OS
CentOS
Python version
Python 3.8
Library version
openai v1.12.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1242","error: Module ""openai.types.beta.threads.run"" has no attribute ""ToolAssistantToolsFunction""","2024-03-21T01:58:11Z","Closed issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
Hi there, i was using old version 1.2.3, seems in the new version 1.14.0 ToolAssistantToolsFunction has been removed, but I need this function to validate openai tool schema. Thanks in advance!
To Reproduce
from openai.types.beta.threads.run import ToolAssistantToolsFunction
Code snippets
No response
OS
macOS
Python version
Python v 3.10
Library version
openai v1.14.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1241","Non caught exception NetworkError is raised by httpx","2024-03-16T13:03:32Z","Closed issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
In some circumstances of faulty network (like on developer laptop, poor VPN, …), httpx raises a NetworkError (ConnectError, ReadError, WriteError, CloseError) exception. This error is related to the network management of the library, but the library do not catch it.
It would be easier to catch NetworkError and to map it to a APIError.
See HTTPX doc for exception handling.
To Reproduce
Run any command involving the remote endpoint
Close / interrupt the network connection a few seconds
See the error
Code snippets
No response
OS
macOS 14.3.1
Python version
Python v3.12.1
Library version
openai v1.14.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1237","Streaming Assistant Not returning result after submitting the tool result","2024-03-21T02:05:01Z","Closed issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
I am trying to use streaming assistant with function calling and whenever the tool output is submitted, stream ends , it is not getting the final message or response from assistant. Please let me know , whether I am doing something wrong
from openai import OpenAI
import time
import json
import time

from openai.types.beta import AssistantStreamEvent

client = OpenAI()


def get_current_weather(location, unit=""fahrenheit""):
    """"""Get the current weather in a given location""""""
    if ""tokyo"" in location.lower():
        return json.dumps({""location"": location, ""temperature"": ""10"", ""unit"": ""celsius""})
    elif ""san francisco"" in location.lower():
        return json.dumps({""location"": location, ""temperature"": ""72"", ""unit"": ""fahrenheit""})
    else:
        return json.dumps({""location"": location, ""temperature"": ""22"", ""unit"": ""celsius""})


def getNickname(location: str) -> str:
    return location + ""God""


from typing_extensions import override
from openai import AssistantEventHandler

function_map = {
    ""getCurrentWeather"": get_current_weather,
    ""getNickname"": getNickname
}

from dataclasses import dataclass


@dataclass
class tool_output:
    tool_call_id: str
    output: str


from openai.types.beta.threads import Message, MessageDelta
from openai.types.beta.threads.runs import ToolCall, RunStep


class EventHandler(AssistantEventHandler):
    def __init__(self, thread_id, assistant_id):
        super().__init__()
        self.output = None
        self.tool_id = None
        self.thread_id = thread_id
        self.assistant_id = assistant_id

    @override
    def on_text_created(self, text) -> None:
        print(f""\nassistant > "", end="""", flush=True)

    @override
    def on_text_delta(self, delta, snapshot):
        print(delta.value, end="""", flush=True)

    @override
    def on_end(self, ):
        print(f""\n end assistant > "",self.current_run_step_snapshot, end="""", flush=True)

    @override
    def on_exception(self, exception: Exception) -> None:
        """"""Fired whenever an exception happens during streaming""""""
        print(f""\nassistant > {exception}\n"", end="""", flush=True)

    @override
    def on_message_created(self, message: Message) -> None:
        print(f""\nassistant > {message}\n"", end="""", flush=True)

    @override
    def on_message_done(self, message: Message) -> None:
        print(f""\nassistant > {message}\n"", end="""", flush=True)

    @override
    def on_message_delta(self, delta: MessageDelta, snapshot: Message) -> None:
        print(f""\nassistant > {delta}\n"", end="""", flush=True)

    @override
    def on_tool_call_created(self, tool_call):
        print(""The type of tool call is "", type(tool_call), flush=True)

        # print(f""\n tool call created assistant > {tool_call.type}\n, >>>> {tool_call.function.json}"", flush=True)
        # print(""function name is "",tool_call.function.name,flush=True)
        # print(""function argument is "", tool_call.function.arguments, flush=True)
        # function_name=tool_call.function.name
        # print(tool_call.function.arguments)
        # function_args=json.loads(tool_call.function.arguments)
        # fn = function_map.get(function_name)
        # output=fn(function_args.get(""location""))
        # print(output)
        # tool_output(tool_call.id,output)

    @override
    def on_tool_call_done(self, tool_call: ToolCall) -> None:
        print("" toolcall val in on_tool_call_done is The type of tool call is "", tool_call, flush=True)

        tool_id = tool_call.id
        f = tool_call.function.name
        arg = json.loads(tool_call.function.arguments)
        fn = function_map.get(f)
        output = fn(arg.get(""location""))
        run_id = self.current_run.id
        print(""The output of function call is "",output,flush=True)
        self.tool_id=tool_id
        self.output=output
        client.beta.threads.runs.submit_tool_outputs(
            thread_id=thread.id,
            run_id= self.current_run.id,
            tool_outputs=[{
                ""tool_call_id"": self.tool_id,
                ""output"": self.output,
            }],

            stream=True
        )





    @override
    def on_run_step_created(self, run_step: RunStep) -> None:
        print(""The type of run step is "", type(run_step), flush=True)
        print(f""\n run step created assistant > {run_step}\n"", flush=True)

    @override
    def on_run_step_done(self, run_step: RunStep) -> None:
        print(f""\n run step done assistant > {run_step}\n"", flush=True)

    def on_tool_call_delta(self, delta, snapshot):
        # print(f""\n tool call delta assistant > {delta.type}\n"", flush=True)

        if delta.type == 'code_interpreter':
            if delta.code_interpreter.input:
                print(delta.code_interpreter.input, end="""", flush=True)
            if delta.code_interpreter.outputs:
                print(f""\n\n tool call delta output >"", flush=True)
                for output in delta.code_interpreter.outputs:
                    if output.type == ""logs"":
                        print(f""\n{output.logs}"", flush=True)

    @override
    def on_event(self, event: AssistantStreamEvent) -> None:
        print(""In on_event of event is "", event.event, flush=True)


thread = client.beta.threads.create()
print(thread.id)

message = client.beta.threads.messages.create(
    thread_id=thread.id,
    role=""user"",
    content=""Get me weather of San Francisco "",
)

with client.beta.threads.runs.create_and_stream(
        thread_id=thread.id,
        assistant_id=""asst_x"",
        event_handler=EventHandler(thread.id, ""asst_x""),

) as stream:

    stream.until_done()
    print(""\n"")
    print(stream.current_run_step_snapshot)





# stream = client.beta.threads.runs.create(
#     thread_id=thread.id,
#     assistant_id=""asst_x"",
#     instructions=""Please address the user as Jane Doe. The user has a premium account."",
#     stream=True,
# )

# for event in stream:
#     print(event.model_dump_json(indent=2, exclude_unset=True))


    # print(stream.get_final_messages())


The last line of output prints that thread.run.step is in progress but stream is completed and program exits

 end assistant >  RunStep(id='step_x', assistant_id='asst_a', cancelled_at=None, completed_at=None, created_at=1710513033, expired_at=None, failed_at=None, last_error=None, metadata=None, object='thread.run.step', run_id='run_x', status='in_progress', step_details=ToolCallsStepDetails(tool_calls=[FunctionToolCall(id='call_x', function=Function(arguments='{""location"":""San Francisco, CA"",""unit"":""f""}', name='getCurrentWeather', output=None), type='function', index=0)], type='tool_calls'), thread_id='thread_x', type='tool_calls', usage=None, expires_at=1710513631)

RunStep(id='step_x', assistant_id='asst_x', cancelled_at=None, completed_at=None, created_at=1710513033, expired_at=None, failed_at=None, last_error=None, metadata=None, object='thread.run.step', run_id='run_x', status='in_progress', step_details=ToolCallsStepDetails(tool_calls=[FunctionToolCall(id='call_x', function=Function(arguments='{""location"":""San Francisco, CA"",""unit"":""f""}', name='getCurrentWeather', output=None), type='function', index=0)], type='tool_calls'), thread_id='thread_x', type='tool_calls', usage=None, expires_at=1710513631)

Assistant creation


assistant = client.beta.assistants.create(
  instructions=""You are a weather bot. Use the provided functions to answer questions."",
  model=""gpt-4-1106-preview"",
  tools=[{
      ""type"": ""function"",
    ""function"": {
      ""name"": ""getCurrentWeather"",
      ""description"": ""Get the weather in location"",
      ""parameters"": {
        ""type"": ""object"",
        ""properties"": {
          ""location"": {""type"": ""string"", ""description"": ""The city and state e.g. San Francisco, CA""},
          ""unit"": {""type"": ""string"", ""enum"": [""c"", ""f""]}
        },
        ""required"": [""location""]
      }
    }
  }, {
    ""type"": ""function"",
    ""function"": {
      ""name"": ""getNickname"",
      ""description"": ""Get the nickname of a city"",
      ""parameters"": {
        ""type"": ""object"",
        ""properties"": {
          ""location"": {""type"": ""string"", ""description"": ""The city and state e.g. San Francisco, CA""},
        },
        ""required"": [""location""]
      }
    } 
  }]
)


To Reproduce
Please use the above code to reproduce
Code snippets
No response
OS
windows
Python version
3.11
Library version
1.14.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1235","Client not defined","2024-03-14T06:02:16Z","Closed as not planned issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
I migrated to new API, followed the guide


 DISCLAIMER: I DO NOT KNOW HOW TO CODE AT ALL, I do not understand anything, I will just do whatever im told if anyone can fix this problem, I used ChatGPT to get all the way here making a website with a chatbot, im having fun and experimenting in code for the first time
To Reproduce
Idk how to reproduce just look at the pictures I guess. When I did the migration, after pasting and running the code in WSL, it said it was complete or successful so I closed WSL. after that the second picture is the code I added.
Code snippets
No response
OS
Windows 11
Python version
Python v3.12.2
Library version
openai v1.14.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1232","CompletionUsage.generation is None sometimes","2024-03-14T20:22:36Z","Closed as not planned issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
When getting a completion using gpt-4-vision the generation field of the CompletionUsage is sometimes None. I don't know when this happens exactly. This is not in accordance with the description or the BaseModel, which claims generation should always be int.
Sorry if this is not a python issue.
To Reproduce
Use gpt-4-vision, send an image. I'm not sure what the reason is.
Code snippets
No response
OS
macOS
Python version
v3.10.8
Library version
openai v1.3.6
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1230","Generation of \u2028 results in JSONDecodeError when stream=True","2024-04-05T08:29:20Z","Closed issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
When generating a completion with stream=True I get JSONDecodeError when the LLM tries to generate \u2028. After some digging, this seems to be due to \u2028 corresponding to 2 tokens [378, 101], none of which can be decoded into strings alone.
import tiktoken

enc = tiktoken.get_encoding(""cl100k_base"")
tokens = enc.encode(""\u2028"").decode()  #  '\u2028' unicode line separator

enc.decode_bytes(tokens).decode() # [378, 101] is fine

enc.decode_bytes([tokens[0]]).decode()  # [378]# > UnicodeDecodeError: 'utf-8' codec can't decode bytes in position 0-1: unexpected end of data

enc.decode_bytes([tokens[1]])  # [101]# > UnicodeDecodeError: 'utf-8' codec can't decode byte 0xa8 in position 0: invalid start byte
When stream=True I presume the tokens are streamed one at a time and hence can't be decoded. The error occurs when the openai library tries to convert the SSE data to json, resulting in an invalid conversion e.g. {..., ""choices"":[{""index"":0,""delta"":{""content"":"" (note that I have truncated the beginning of the data, but the content just ends like this).
This seems quite obscure but handles frequently in scenarios where references / citations are made to some user input.
To Reproduce
Create prompt including a single character/symbol that gets encoded into multiple tokens e.g. \u2028 which cl100k_base maps to the tokens [378, 101]
Ask the LLM to recite the user message
Code snippets
from openai import OpenAI

api_key = ""...""  # Replace with your API key


system_message = ""The user will send you a short text. You MUST respond with the EXACT same text verbatim as the user supplies, nothing more, nothing less.""

user_message = """"""Minim culpa \u2028 anim eu id exercitation amet. Culpa culpa esse mollit pariatur est enim. Exercitation minim cillum aute occaecat. Incididunt velit commodo sit ea. \u2028Deserunt labore eu ipsum reprehenderit esse sunt nisi aliqua qui id mollit. Id cupidatat incididunt Lorem ex ullamco quis voluptate mollit sit labore quis. Nostrud sint sint Lorem tempor minim amet aliquip elit fugiat. Ipsum cupidatat ipsum veniam ut ea magna nostrud id quis exercitation tempor velit aliqua sit. Proident sint velit ullamco culpa dolore magna ut eiusmod pariatur. Commodo ut sint minim ex aliqua eu esse anim elit elit eiusmod ea. Culpa quis in ea id cupidatat labore amet amet ullamco sunt Lorem do tempor ad. \u2028Dolor anim dolore laborum fugiat dolor eiusmod amet adipisicing. Consectetur et dolor enim proident aute deserunt. Excepteur ullamco ea officia nulla irure cupidatat veniam ipsum ex. Labore sint sit incididunt ad exercitation labore minim consequat elit sit nulla occaecat do nisi. Irure est commodo id eu fugiat eiusmod proident consequat ea.from typing import Any, Dict, List, Literal, Union  \u2028""""""


client = OpenAI(api_key=api_key)
response = client.chat.completions.create(
    model=""gpt-4-1106-preview"",
    messages=[
        {""role"": ""system"", ""content"": system_message},
        {""role"": ""user"", ""content"": user_message},
    ],
    stream=True,
)
for chunk in response:
    print(chunk.choices[0].delta.content, end="""")
OS
macOS
Python version
Python 3.11.0
Library version
openai v1.13.3
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1220","Incompatible: with_streaming_responsestream_to_fileresponse_format='aac'","2024-03-09T04:03:09Z","Closed issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
The following example errors
line 31, in generate_audio
    response.stream_to_file(output_file)
AttributeError: 'ResponseContextManager' object has no attribute 'stream_to_file

However, if you remove either with_streaming_response or response_format='aac' it works
To Reproduce
from openai import OpenAI
client = OpenAI()
response = client.audio.speech.with_streaming_response.create(
    model=""tts-1"",
    voice=""alloy"",
    input=""hello aac"",
    response_format='aac'
)
response.stream_to_file(""output.m4a"")

Code snippets
No response
OS
wsl
Python version
3.8.10
Library version
1.13.3
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1217","Messages with non-latin languages texts returns server errors (500 status code)","2024-03-06T14:57:35Z","Closed as not planned issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
I'm passing a message to openai, that might include some JSON in the message. those JSONs might include non-latin languages such as Hebrew, Arabic, and Japanese.
 I think there's something wrong when the library builds the ""Request"" object, it encodes the string (or httpx does it) in a way that causes openai servers to return 500 status codes.
I debugged the request-making process. it seems that httpx is to blame here. If I look at the library's code in the AsyncAPIClient code.
When I print the ""options"" parameter async def _reuqest I see that a Hebrew string is fine, when I access request.content I can see that it's the hebrew is now encoded.
Now, I'm not sure how much the openai API or GPT4 care about working with those encoded texts, but I do know, that request with nested jsons in msg causes lots of 500 responses from GPT4-1106 on Azure.
To Reproduce
build a message that contains a JSON string with some Hebrew/Arabic/Japanese text in it.
try to send it to gpt-4 with openai Library.
put a breakpoint in the row: raise self._make_status_error_from_response(err.response) from None
see that it returns a 500, and accessing request.content, you see the message with the non-Latin encoded.
Code snippets
No response
OS
macOS
Python version
Python v3.11.7
Library version
openai v1.13.3
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1215","openai.APITimeoutError: Request timed","2024-03-06T22:54:00Z","Closed as not planned issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
我用from openai import OpenAI这个库访问时候回复速度很快，但是我在利用llamaindex来结合gpt与模型时候，queryengine.query（）这个语句就会报错openai.APITimeoutError: Request timed，以下是我的报错信息：
 03/05/2024 16:42:02 - [INFO] -openai._base_client->>> Retrying request to /chat/completions in 0.859262 seconds
 03/05/2024 16:42:23 - [INFO] -openai._base_client->>> Retrying request to /chat/completions in 1.629988 seconds
 03/05/2024 16:42:46 - [INFO] -openai._base_client->>> Retrying request to /chat/completions in 3.447563 seconds
 Traceback (most recent call last):
 File ""C:\Users\590\anaconda3\envs\bce\lib\site-packages\httpx_transports\default.py"", line 69, in map_httpcore_exceptions
 yield
 File ""C:\Users\590\anaconda3\envs\bce\lib\site-packages\httpx_transports\default.py"", line 233, in handle_request
 resp = self._pool.handle_request(req)
 File ""C:\Users\590\anaconda3\envs\bce\lib\site-packages\httpcore_sync\connection_pool.py"", line 216, in handle_request
 raise exc from None
 File ""C:\Users\590\anaconda3\envs\bce\lib\site-packages\httpcore_sync\connection_pool.py"", line 196, in handle_request
 response = connection.handle_request(
 File ""C:\Users\590\anaconda3\envs\bce\lib\site-packages\httpcore_sync\connection.py"", line 99, in handle_request
 raise exc
 File ""C:\Users\590\anaconda3\envs\bce\lib\site-packages\httpcore_sync\connection.py"", line 76, in handle_request
 stream = self._connect(request)
 File ""C:\Users\590\anaconda3\envs\bce\lib\site-packages\httpcore_sync\connection.py"", line 122, in _connect
 stream = self._network_backend.connect_tcp(**kwargs)
 File ""C:\Users\590\anaconda3\envs\bce\lib\site-packages\httpcore_backends\sync.py"", line 205, in connect_tcp
 with map_exceptions(exc_map):
 File ""C:\Users\590\anaconda3\envs\bce\lib\contextlib.py"", line 153, in exit
 self.gen.throw(typ, value, traceback)
 File ""C:\Users\590\anaconda3\envs\bce\lib\site-packages\httpcore_exceptions.py"", line 14, in map_exceptions
 raise to_exc(exc) from exc
 httpcore.ConnectTimeout: [WinError 10060] 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。
The above exception was the direct cause of the following exception:
Traceback (most recent call last):
 File ""C:\Users\590\anaconda3\envs\bce\lib\site-packages\openai_base_client.py"", line 918, in _request
 response = self._client.send(
 File ""C:\Users\590\anaconda3\envs\bce\lib\site-packages\httpx_client.py"", line 914, in send
 response = self._send_handling_auth(
 File ""C:\Users\590\anaconda3\envs\bce\lib\site-packages\httpx_client.py"", line 942, in _send_handling_auth
 response = self._send_handling_redirects(
 File ""C:\Users\590\anaconda3\envs\bce\lib\site-packages\httpx_client.py"", line 979, in _send_handling_redirects
 response = self._send_single_request(request)
 File ""C:\Users\590\anaconda3\envs\bce\lib\site-packages\httpx_client.py"", line 1015, in _send_single_request
 response = transport.handle_request(request)
 File ""C:\Users\590\anaconda3\envs\bce\lib\site-packages\httpx_transports\default.py"", line 232, in handle_request
 with map_httpcore_exceptions():
 File ""C:\Users\590\anaconda3\envs\bce\lib\contextlib.py"", line 153, in exit
 self.gen.throw(typ, value, traceback)
 File ""C:\Users\590\anaconda3\envs\bce\lib\site-packages\httpx_transports\default.py"", line 86, in map_httpcore_exceptions
 raise mapped_exc(message) from exc
 httpx.ConnectTimeout: [WinError 10060] 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。
The above exception was the direct cause of the following exception:
Traceback (most recent call last):
 File ""D:\BCE\pycharmcode\test2.py"", line 51, in
 query_response = query_engine.query(query)
 File ""C:\Users\590\anaconda3\envs\bce\lib\site-packages\llama_index\core\base_query_engine.py"", line 40, in query
 return self._query(str_or_query_bundle)
 File ""C:\Users\590\anaconda3\envs\bce\lib\site-packages\llama_index\query_engine\retriever_query_engine.py"", line 172, in _query
 response = self._response_synthesizer.synthesize(
 File ""C:\Users\590\anaconda3\envs\bce\lib\site-packages\llama_index\response_synthesizers\base.py"", line 168, in synthesize
 response_str = self.get_response(
 File ""C:\Users\590\anaconda3\envs\bce\lib\site-packages\llama_index\response_synthesizers\compact_and_refine.py"", line 38, in get_response
 return super().get_response(
 File ""C:\Users\590\anaconda3\envs\bce\lib\site-packages\llama_index\response_synthesizers\refine.py"", line 146, in get_response
 response = self._give_response_single(
 File ""C:\Users\590\anaconda3\envs\bce\lib\site-packages\llama_index\response_synthesizers\refine.py"", line 202, in _give_response_single
 program(
 File ""C:\Users\590\anaconda3\envs\bce\lib\site-packages\llama_index\response_synthesizers\refine.py"", line 64, in call
 answer = self._llm.predict(
 File ""C:\Users\590\anaconda3\envs\bce\lib\site-packages\llama_index\llms\llm.py"", line 239, in predict
 chat_response = self.chat(messages)
 File ""C:\Users\590\anaconda3\envs\bce\lib\site-packages\llama_index\llms\base.py"", line 100, in wrapped_llm_chat
 f_return_val = f(_self, messages, **kwargs)
 File ""C:\Users\590\anaconda3\envs\bce\lib\site-packages\llama_index\llms\openai.py"", line 237, in chat
 return chat_fn(messages, **kwargs)
 File ""C:\Users\590\anaconda3\envs\bce\lib\site-packages\llama_index\llms\openai.py"", line 296, in _chat
 response = client.chat.completions.create(
 File ""C:\Users\590\anaconda3\envs\bce\lib\site-packages\openai_utils_utils.py"", line 275, in wrapper
 return func(*args, **kwargs)
 File ""C:\Users\590\anaconda3\envs\bce\lib\site-packages\openai\resources\chat\completions.py"", line 663, in create
 return self._post(
 File ""C:\Users\590\anaconda3\envs\bce\lib\site-packages\openai_base_client.py"", line 1200, in post
 return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
 File ""C:\Users\590\anaconda3\envs\bce\lib\site-packages\openai_base_client.py"", line 889, in request
 return self._request(
 File ""C:\Users\590\anaconda3\envs\bce\lib\site-packages\openai_base_client.py"", line 927, in _request
 return self._retry_request(
 File ""C:\Users\590\anaconda3\envs\bce\lib\site-packages\openai_base_client.py"", line 1013, in _retry_request
 return self._request(
 File ""C:\Users\590\anaconda3\envs\bce\lib\site-packages\openai_base_client.py"", line 927, in _request
 return self._retry_request(
 File ""C:\Users\590\anaconda3\envs\bce\lib\site-packages\openai_base_client.py"", line 1013, in _retry_request
 return self._request(
 File ""C:\Users\590\anaconda3\envs\bce\lib\site-packages\openai_base_client.py"", line 927, in _request
 return self._retry_request(
 File ""C:\Users\590\anaconda3\envs\bce\lib\site-packages\openai_base_client.py"", line 1013, in _retry_request
 return self._request(
 File ""C:\Users\590\anaconda3\envs\bce\lib\site-packages\openai_base_client.py"", line 937, in _request
 raise APITimeoutError(request=request) from err
 openai.APITimeoutError: Request timed out.
To Reproduce
我的代码是这样的：（我删除了apikey和apiurl）
我们在BCEmbedding中提供llama_index直接集成的接口。
from BCEmbedding.tools.llama_index import BCERerank
import os
 from llama_index.embeddings import HuggingFaceEmbedding
 from llama_index import VectorStoreIndex, ServiceContext, SimpleDirectoryReader
 from llama_index.node_parser import SimpleNodeParser
 from llama_index.llms import OpenAI
 from llama_index.retrievers import VectorIndexRetriever
init embedding model and reranker model
embed_args = {'model_name': 'D:\BCE\model\bce-embedding-base_v1', 'max_length': 512, 'embed_batch_size': 32, 'device': 'cuda:0'}
 embed_model = HuggingFaceEmbedding(**embed_args)
reranker_args = {'model': 'D:\BCE\model\bce-reranker-base_v1', 'top_n': 5, 'device': 'cuda:0'}
 reranker_model = BCERerank(**reranker_args)
example #1. extract embeddings
query1 = 'apples'
 passages = [
 'I like apples',
 'I like oranges',
 'Apples and oranges are fruits'
 ]
 query_embedding = embed_model.get_query_embedding(query1)
 passages_embeddings = embed_model.get_text_embedding_batch(passages)
 print('passages_em:'+str(passages_embeddings))
 print('query_em:'+str(query_embedding))
example #2. rag example
llm = OpenAI(model='gpt-3.5-turbo', api_key=os.environ.get(''),api_base=os.environ.get(''))
 service_context = ServiceContext.from_defaults(llm=llm, embed_model=embed_model)
documents = SimpleDirectoryReader(input_files=[""D:\BCE\pycharmcode\文章.pdf"",""D:\BCE\pycharmcode\文章2.pdf"",""D:\BCE\pycharmcode\文章3.pdf""]).load_data()
 node_parser = SimpleNodeParser.from_defaults(chunk_size=512)
 nodes = node_parser.get_nodes_from_documents(documents[0:36])
 index = VectorStoreIndex(nodes, service_context=service_context)
 query = ""请你给我介绍一下这个游戏""
example #2.1. retrieval with EmbeddingModel and RerankerModel
vector_retriever = VectorIndexRetriever(index=index, similarity_top_k=5, service_context=service_context)
 retrieval_by_embedding = vector_retriever.retrieve(query)
 retrieval_by_reranker = reranker_model.postprocess_nodes(retrieval_by_embedding, query_str=query)
 print(retrieval_by_embedding)
 print(retrieval_by_reranker)
example #2.2. query with EmbeddingModel and RerankerModel
query_engine = index.as_query_engine(node_postprocessors=[reranker_model])
 print(query_engine)
 query_response = query_engine.query(query)
print(query_response)
Code snippets
No response
OS
windows11
Python version
Python 3.10.13
Library version
1.13.3
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1214","Ability to change OPENAI_BASE_URL and OPENAI_API_HOST for testing purposes","2024-03-06T22:48:16Z","Closed issue","No label","Confirm this is a feature request for the Python library and not the underlying OpenAI API.
 This is a feature request for the Python library
Describe the feature or improvement you're requesting
to define our own API_HOST for testing
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1209","AuthenticationError: Error code: 401 - {'error': {'message': 'Incorrect API key provided: 3*********************************5. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}","2024-03-05T04:30:27Z","Closed as not planned issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
API key does not work with Openai version openai==1.13.3
 But the same API key works with previous versions such as openai==0.27.0
To Reproduce
use openai==1.13.3 and use this code
chat_completion = client.chat.completions.create(
    messages=[
        {
            ""role"": ""user"",
            ""content"": final_prompt
        }
    ],
    model=""model"",
)
print(chat_completion)

which will return the Incorrect API key provided error
 BUT the same API key works with openai==0.27.0
 with this code
completion = openai.ChatCompletion.create(
          engine=""model"",
          temperature=0.1,
          messages=[{""role"": ""user"", ""content"": final_prompt}]
       )

Code snippets
No response
OS
Ubuntu
Python version
Python3.9
Library version
openai==1.13.3
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1208","Request: make it possible to specify the upper limit of history","2024-03-05T04:32:31Z","Closed issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
I specified the limit parameter, but the message history does not match the limit parameter value.
https://github.com/openai/openai-python/blob/main/src/openai/types/beta/threads/message_list_params.py#L29-L30
    limit: int
    """"""A limit on the number of objects to be returned.

it is forum feedback url.
https://community.openai.com/t/request-make-it-possible-to-specify-the-upper-limit-of-history/663420
To Reproduce
always
Code snippets
def _get_message(gpt_thread_id:str, secret:str):
    client = OpenAI(api_key=secret)
    return client.beta.threads.messages.list(
        thread_id=gpt_thread_id,
        extra_query = {""limit"": 1},
        limit=1
    )


### OS

mac(docker debian)

### Python version

python3.9

### Library version

opneai v1.12.0

 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1207","Fine tuning failure","2024-03-02T22:43:42Z","Closed issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
An error occurred when I was fine-tuning a josn file to print the id of fine_tuning_job



 The returned model is none
 May I ask what caused this error？
 Thank you so much for your assistance.
To Reproduce
As indicated above
Code snippets
No response
OS
Window 10
Python version
Python 3.8.18
Library version
openai 1.3.6
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1206","Error with proxy use","2024-03-03T09:30:26Z","Closed issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
from openai import AsyncOpenAIimport httpximport logginglogger = logging.getLogger()
logger.setLevel(logging.DEBUG)
client = AsyncOpenAI(
     # This is the default and can be omitted
    api_key=""..."",
    base_url='https://api.openai.com/v1',
    timeout=20,
    http_client=httpx.Client(verify=False,
        proxies={""all://"":""http://...""},
    ),
 )
After that, I make an asynchronous request to the chat.completions method
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'system', 'content': '.......'}, {'role': 'user', 'content': 'fef'}], 'model': 'gpt-4', 'max_tokens': 4096}}
DEBUG:httpcore.connection:connect_tcp.started host='......' port=.....local_address=None timeout=20 socket_options=None
DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001D831AD70D0>
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'CONNECT']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'CONNECT']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'CONNECT']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'Connection Established', [])
DEBUG:httpcore.proxy:start_tls.started ssl_context=<ssl.SSLContext object at 0x000001D82F847940> server_hostname='api.openai.com' timeout=20
DEBUG:httpcore.proxy:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001D831AD7100>
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sat, 02 Mar 2024 08:57:45 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-4-0613'), (b'openai-organization', b'gergergergegr'), (b'openai-processing-ms', b'1174'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'40000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'35807'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'6.289s'), (b'x-request-id', b'......'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=.....; path=/; expires=Sat, 02-Mar-24 09:27:45 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Set-Cookie', b'_cfuvid=.....; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'85e02e54ecf9cb27-DUS'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3="":443""; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions ""HTTP/1.1 200 OK""
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:Encountered Exception
Traceback (most recent call last):
  File ""C:\Users\oleg\AppData\Local\Programs\Python\Python39\lib\site-packages\openai\_base_client.py"", line 1457, in _request
    resp = await self._client.send(
TypeError: object Response can't be used in 'await' expression
To Reproduce
Use AsyncOpenAI
Use proxy in client.AsyncOpenAI
use await client.chat.completions
Code snippets
No response
OS
Windows 10
Python version
Python 3.9.4
Library version
openai 1.13.1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1204","Allow for custom SSL certs to be used by the APIClient classes","2024-03-08T00:21:57Z","Closed issue","enhancement","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
Currently, the APIClient classes allow for custom timeout, custom headers, custom max_retries, to be easily passed in as parameters.
The only way to pass in custom SSL certs are by one of these two options:
Setting the SSL_CERT_FILE environment variable. This causes issues with async calls.
Passing in a custom http_client. This causes inconsistencies, and is unnecessarily complicated when handling both Sync and Async clients through a 3rd party library like langchain.
Instead, verify should be an argument that can be passed in just like timeout or max_retries.
PR #1205 fixes this issue.
To Reproduce
Try to use custom SSL certs.
Note the difficulty in doing so.
Merge in fix: make verify a client parameter (openai#1204) #1205 to make the task simple.
Code snippets
No response
OS
Ubuntu 20.04.5 LTS
Python version
Python v.3.11.6
Library version
openai v1.13.3
 The text was updated successfully, but these errors were encountered: 
👍1
pavelnikonorov reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/openai-python/issues/1196","At debug log level API requests to OpenAI get logged with all headers, including an API key, in plaintext","2024-02-26T20:45:46Z","Open issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
When sending a request using the library, line 439 of
/openai/openai-python/blob/main/src/openai/_base_client.py
Will log all request headers in plain text, including API keys.
While I understand this only happens in debug logging level, it still feels like a potential security issue. These logs should probably be filtered to remove this sensitive header.
To Reproduce
Run an application with DEBUG log level that communicates with OpenAI using an API key
Observe that the API key is printed in the logs in plain text
Code snippets
if log.isEnabledFor(logging.DEBUG):
      log.debug(""Request options: %s"", model_dump(options, exclude_unset=True))
OS
macOS
Python version
Python 3.11.6
Library version
openai v1.3.7
 The text was updated successfully, but these errors were encountered: 
👍3
Maximilian-Ka, tekumara, and artdent reacted with thumbs up emoji
All reactions
👍3 reactions"
"https://github.com/openai/openai-python/issues/1195","Multiple Async calls to the api fail catastrophically","2024-05-25T22:19:05Z","Closed as not planned issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
For a resume-writing program with multiple levels of async calls, launching relatively small scale async processing causes the API to fail catastrophically.
Attempted the OpenAIClient and httpx.AsyncClient solutions which were suggested here and elsewhere:
#769
When called synchronously, code processes 50 resumes sequentially with no problem, and perhaps 3 or 4 'Timeout' failures in aggregate that are successfully completed using exponential backoff. The average completion time for each document is 50 seconds with a std of perhaps 10 seconds.
When the same 50 documents are run simultaneously using asyncio:
 await asyncio.gather(*tasks)
Several hundred - several thousand timeout errors occur in aggregate, and most of the time, the processes will fail catastrophically as None is returned by the OpenAI api, which then fails cascadingly throughout the system.
Average completion time rises to 240 seconds with an std of perhaps 30 seconds.
I've confirmed that unique clients are created for each document:
 OpenAIClient object at 0x7f9a57762fb0
 OpenAIClient object at 0x7f9a5764f430
 OpenAIClient object at 0x7f9a57249870
 ...
Running with a clean new environment updated today:
 python==3.10.13
 openai==1.12.0
 httpx==0.27.0
#769 seems to indicate that the problem was resolved in open 1.3.8, but we can't fix.
To Reproduce
Initiate 50 top-level tasks, each of which fires of approx 100 tasks, each of which may fire 0-5 additional tasks, and may reiterate
Create an AsyncOpenAI Client for each of the 50 toplevel tasks
Observe that OpenAI repeatedly returns thousands of timeout errors
Code snippets
class OpenAIClient:                                     
     def __init__(self, account_info):                   
         self.aclient = openai.AsyncOpenAI(              
             api_key=os.environ.get(""OPENAI_API_KEY""),   
             http_client=httpx.AsyncClient(              
                 limits=httpx.Limits(                    
                     max_keepalive_connections=10000,    
                     max_connections=1000,),             
                 timeout=15,                             
             ),                                          
         )                                               
         self.account_info = account_info       



Typical error message of the hundreds / thousands received:
API call exceeded the time limit: 
Recalling OpenAI API 1. Error: . iDelay: 0.074. Delay: 0.074
 (<utils.OpenAIClient object at 0x7efd3bd35690>,)
OS
Amazon Linux
Python version
3.10.13
Library version
openai 1.12.0
 The text was updated successfully, but these errors were encountered: 
🚀1
Sladeck reacted with rocket emoji
All reactions
🚀1 reaction"
"https://github.com/openai/openai-python/issues/1192","openai.File is no longer supported","2024-02-28T23:29:12Z","Closed issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
You tried to access openai.File, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.
I'm trying to train an OpenAI Azure model by uploading a file but I'm getting an error that the openai.File is no longer supported.
 There is no other sample documentation to explain the updated method of uploading training data to Azure OpenAI.
To Reproduce
Try to upload a jsonl file to OpenAi Azure by using openai.File.create with the Microsoft example under ""Upload your training data""
https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/fine-tuning?tabs=turbo%2Cpython&pivots=programming-language-python
Code snippets
# Upload fine-tuning files

import openaiimport os

openai.api_key = os.getenv(""AZURE_OPENAI_KEY"")
openai.api_base =  ""https://nit.openai.azure.com/""openai.api_type = 'azure'openai.api_version = '2023-12-01-preview' # This API version or later is required to access fine-tuning for turbo/babbage-002/davinci-002

training_file_name = 'test.jsonl'validation_file_name = 'test.jsonl'

# Upload the training and validation dataset files to Azure OpenAI with the SDK.

training_response = openai.File.create(
    file=open(training_file_name, ""rb""), purpose=""fine-tune"", user_provided_filename=""training_set.jsonl""
)
training_file_id = training_response[""id""]

validation_response = openai.File.create(
    file=open(validation_file_name, ""rb""), purpose=""fine-tune"", user_provided_filename=""validation_set.jsonl""
)
validation_file_id = validation_response[""id""]

print(""Training file ID:"", training_file_id)
print(""Validation file ID:"", validation_file_id)
OS
macOS
Python version
Python v3
Library version
openai v1.0.1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1191","Official release version - PyPi or Github?","2024-02-28T23:25:42Z","Closed issue","No label","Confirm this is a feature request for the Python library and not the underlying OpenAI API.
 This is a feature request for the Python library
Describe the feature or improvement you're requesting
I'm just wondering what the official release version is. I see on PyPi version 1.12.0 and here on Github version 1.13.2. Since I packaged it for Mageia, I chose to go for the version on PyPi because it felt ""safer"".
Thank you.
Additional context
N/A
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1190","Provide a callback whenever retry is triggered","2024-07-18T04:37:32Z","Closed issue","enhancement","Confirm this is a feature request for the Python library and not the underlying OpenAI API.
 This is a feature request for the Python library
Describe the feature or improvement you're requesting
Is it possible to provide a callback whenever a retry is triggered internally, so that we can know when and how the requests failed?
Additional context
We want to give our users some insights if some OpenAI requests fail with rate limit error
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1186","openai.InternalServerError: Error code: 503","2024-02-23T17:46:42Z","Closed issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
when I call a function client.files.create(file=open(""dige.txt"", ""rb""), purpose='assistants')，
 then，there was an error in the log： openai.InternalServerError: Error code: 503 - {'error': {'message': '当前分组 default 下对于模型 无可用渠道 (request id: 20240223090320216834355OmsitOrX)', 'type': 'one_api_error'}}。
 Can anyone help with this? Thanks~
To Reproduce
code：
 client = OpenAI(api_key=OPENAI_API_KEY ,base_url=OPENAI_API_BASE)
 file = client.files.create(file=open(""dige.txt"", ""rb""),
 purpose='assistants')
Code snippets
No response
OS
linux
Python version
Python 3.10.11
Library version
openai v1.2.3
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1185","openai.InternalServerError: Error code: 503","2024-02-23T17:46:52Z","Closed issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
when I call a function client.files.create(file=open(""dige.txt"", ""rb""), purpose='assistants')，
 then，there was an error in the log： openai.InternalServerError: Error code: 503 - {'error': {'message': '当前分组 default 下对于模型 无可用渠道 (request id: 20240223090320216834355OmsitOrX)', 'type': 'one_api_error'}}。
 Can anyone help with this? Thanks~
To Reproduce
code：
 client = OpenAI(api_key=OPENAI_API_KEY ,base_url=OPENAI_API_BASE)
 file = client.files.create(file=open(""dige.txt"", ""rb""),
 purpose='assistants')
Code snippets
No response
OS
linux
Python version
Python 3.10.11
Library version
openai v1.2.3
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1184","text to image","2024-02-23T14:22:46Z","Closed issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
APIRemovedInV1
 openai.lib._old_api.APIRemovedInV1:
You tried to access openai.Image, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.
You can run openai migrate to automatically upgrade your codebase to use the 1.0.0 interface.
Alternatively, you can pin your installation to the old version, e.g. pip install openai==0.28
A detailed migration guide is available here: #742
To Reproduce
a
Code snippets
from flask import Flask, render_template, request, jsonify, send_fileimport openaiimport requestsfrom PIL import Imagefrom io import BytesIOopenai migrate


# Initialize Flask appapp = Flask(__name__)

# Set your OpenAI API keyopenai.api_key = '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'

# Define a route for the home page@app.route('/')def home():
    return render_template('login.html')

# Define a route to handle image generation@app.route('/generate', methods=['POST'])def generate_image():
    # Get the text prompt from the form submission
    text = request.form['text_prompt']
    
    # Call the OpenAI API to generate the image
    response = openai.Image.create(prompt=text, n=1, size=""256x256"")
    
    # Get the URL of the generated image
    image_url = response['data'][0]['url']
    
    # Fetch the image using the URL
    image_response = requests.get(image_url)
    
    # Convert the image response to a PIL Image
    image = Image.open(BytesIO(image_response.content))
    
    # Save the image to a buffer
    buf = BytesIO()
    image.save(buf, format='PNG')
    buf.seek(0)
    
    # Send the image as a response
    return send_file(buf, mimetype='image/png')

# Run the Flask appif __name__ == '__main__':
    app.run(debug=True)
OS
windows 11
Python version
python.11.8
Library version
openai1.10.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1183","SSL errors when connecting to OpenAI azure endpoint while using library","2024-02-23T14:25:19Z","Closed issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
Hi,
Over the last few weeks I've been configuring a site to site VPN between AWS and Azure so we can use the OpenAI endpoints that Azure now offers through a Privatelink. We're using a sample taken from the docs (with correct values replaced etc)
`import os
 import openai
 openai.api_type = ""azure""
 openai.api_base = os.getenv(""AZURE_OPENAI_ENDPOINT"")
 openai.api_key = os.getenv(""AZURE_OPENAI_KEY"")
 openai.api_version = ""2023-05-15""
response = openai.ChatCompletion.create(
 engine=""gpt-35-turbo"", # engine = ""deployment_name"".
 messages=[
 {""role"": ""system"", ""content"": ""You are a helpful assistant.""},
 {""role"": ""user"", ""content"": ""Does Azure OpenAI support customer managed keys?""},
 {""role"": ""assistant"", ""content"": ""Yes, customer managed keys are supported by Azure OpenAI.""},
 {""role"": ""user"", ""content"": ""Do other Azure AI services support this too?""}
 ]
 )
print(response)
 print(response['choices'][0]['message']['content'])`
I'm unfortunately receiving an SSL error when attempting to connect to the Azure endpoint while on the VPN. The error is...
Max retries exceeded with url: /openai/deployments//chat/completions?api-version=1106-preview (Caused by SSLError(CertificateError(""hostname 'myorg.privatelink.openai.azure.com' doesn't match either of '*.cognitiveservices.azure.com', '*.api.cognitive.microsoft.com', '*.dev.cognitive.microsoft.com', '*.openai.azure.com'""))). 
I initially thought this might be an issue with the domain I was using so have also attempted myorg.openai.azure.com (the endpoint seen in the azure console) and IP with the same problem. Issues I've seen similar to this online suggest a couple of solutions...
Use certifi to find where the local certificate chain is and add the root cert which you can grab from hitting the endpoint through the browser, this has had no effect when I've tried.
Disable SSL verification (Not really a fix and a bad solution)
I thought it could be something to do with the infrastructure setup but after running a pure SSL test I can receive a handshake from the endpoint, going via the browser also doesn't present the same certificate issue. Is this something other people have experienced or is this a bug with the openAI library/azure implementation?
To Reproduce
Create an Azure endpoint with Privatelink
Connect to a corporate VPN
Run above code sample
Code snippets
None
OS
MacOS
Python version
Python 3.8.18
Library version
latest
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1181","memory leak, consumes entire system after just days of usage","2024-02-27T21:47:46Z","Closed issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
Progressive memory leak despite use of close(). Leads to eventually OOM on even large memory systems after just a few days.
Related: #820
To Reproduce
I'm using 1.12.0 and still hit this issue.
Using text completion. I'm also using close() in a try-finally, so close() does not help. A global client connection doesn't make sense to enforce. The old client pre 1.x had global attributes that made the API poor. I presume there are some legacy parts still in place.
In h2oGPT, we use OpenAI client for OpenAI or vLLM connections, and I see a 5GB memory leak for every 6000 connections. This happens whether I yield the generator for streaming or just exit after creating the completion.
Normally connections are not as intense, but this was easily reproducible by bisecting the OpenAI creation/generation parts of the code. For typical workloads this leads to OOM on a 256GB system after just few days of usage.
Here is repro. Please choose the to be some endpoint that you have setup like vLLM or TGI or gpt3.5 turbo so not expensive. Choose api_key and model accordingly.
import os
import psutil
from openai import OpenAI

for i in range(6000):

    client_args = dict(base_url='<choose>', api_key=""EMPTY"")
    client = OpenAI(**client_args)

    responses = client.completions.create(
        model='h2oai/h2ogpt-4096-llama2-13b-chat',
        prompt=""Say exactly one word."",
        stream=True,
    )
    client.close()
    p = psutil.Process(os.getpid())
    print(p.memory_full_info())

The memory consumed is not increasing every step in loop, but it does monotonically increase from pss=48523264 to pss=107862016 within a few minutes (i.e. doubled) and continues this indefinitely.
The problem seems to be even stronger when doing concurrent requests in multi-threaded setup, as if the clean-up is not thread safe. I'm trying to put together a repro that would showcase the 5GB after 6000 connections that only takes half hour to run. But perhaps the above is sufficient.
Code snippets
No response
OS
ubuntu 22
Python version
Python v3.10
Library version
openai v1.12.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1180","Implement the new TTS file formats","2024-03-02T22:46:20Z","Closed issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
The speech api implementation is not up to date.
 It doesn’t yet support the new Output formats like WAV
openai/openai-openapi#202
To Reproduce
Create a speech request
Try to use e.g. wav as the media type.
Code snippets
https://github.com/openai/openai-python/blob/a7115b5f33acd27326e5f78e19beb0d73bd3268e/src/openai/types/audio/speech_create_params.py#L29
OS
macOS
Python version
v3.11.3
Library version
v1.13.2
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1176","Simple stream example does not work with GPT-4 model","2024-02-22T07:23:30Z","Closed issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
Hi, I'm using OpenAI Python API version 1.12.0
The simple stream example from your GitHub repository does not work on the GTP-4 model. The GPT-3.5-Turbo model worked without issue.
from openai import OpenAI

client = OpenAI()

stream = client.chat.completions.create(
    model=""gpt-4"",
    messages=[{""role"": ""user"", ""content"": ""Say this is a test""}],
    stream=True,
)
for chunk in stream:
    print(chunk.choices[0].delta.content or """", end="""")
Error message:
Traceback (most recent call last):
  File ""/home/haje01/test.py"", line 25, in <module>
    for chunk in response:
  File ""/home/haje01/.pyenv/versions/ml/lib/python3.9/site-packages/openai/_streaming.py"", line 43, in __iter__
    for item in self._iterator:
  File ""/home/haje01/.pyenv/versions/ml/lib/python3.9/site-packages/openai/_streaming.py"", line 62, in __stream__
    raise APIError(
openai.APIError: An error occurred during streaming

To Reproduce
pip install openai==1.12.0
run above code
Code snippets
No response
OS
Ubuntu 20.04.5 LTS (WSL)
Python version
Python 3.9.15
Library version
openai 1.12.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1175","Can we get the response headers within a completion?","2024-02-22T03:03:57Z","Closed issue","No label","Confirm this is a feature request for the Python library and not the underlying OpenAI API.
 This is a feature request for the Python library
Describe the feature or improvement you're requesting
There is a lot of useful information within the response headers. Can we add this to the ChatCompletion object that client.chat.completions.create makes?
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1174","Stream as True returns Nothing due to a bug in _streaming.py SSEDecoder decode function","2024-02-22T18:06:03Z","Closed issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
decode function returns ""None"" when input is valid, Which leads to a 'None' output for streaming

To Reproduce
set stream as True in client.chat.completions.create
when waiting for response iterator, 'None' returned instead of Completion Chunks
Code snippets
Below function always return None when input argument 'line' is not None.

def decode(self, line: str) -> ServerSentEvent | None:
        # See: https://html.spec.whatwg.org/multipage/server-sent-events.html#event-stream-interpretation  # noqa: E501

        if not line:
            if not self._event and not self._data and not self._last_event_id and self._retry is None:
                return None

            sse = ServerSentEvent(
                event=self._event,
                data=""\n"".join(self._data),
                id=self._last_event_id,
                retry=self._retry,
            )

            # NOTE: as per the SSE spec, do not reset last_event_id.
            self._event = None
            self._data = []
            self._retry = None

            return sse

        if line.startswith("":""):
            return None

        fieldname, _, value = line.partition("":"")

        if value.startswith("" ""):
            value = value[1:]

        if fieldname == ""event"":
            self._event = value
        elif fieldname == ""data"":
            self._data.append(value)
        elif fieldname == ""id"":
            if ""\0"" in value:
                pass
            else:
                self._last_event_id = value
        elif fieldname == ""retry"":
            try:
                self._retry = int(value)
            except (TypeError, ValueError):
                pass
        else:
            pass  # Field is ignored.

        return None
OS
Linux
Python version
python 3.9
Library version
openai 1.13.2
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1172","GPT-4-vision-preview does not throw error when the message content indicates one","2024-02-21T19:08:33Z","Closed issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
I'd like to know if there is a way to detect responses as the ones listed below, as the openai cliend does not throw any exceptions when these occur?
“I am unable to view images directly.”
 “Unfortunately, there seems to be an issue as the image is not visible or accessible to me at the moment.”
 “I’m unable to provide real-time analysis or generate content based on the image provided as the capabilities have been disabled.”
 “Given the lack of an actual image to analyse, (…)”
 “Since the image cannot be displayed (…)”
 “Unfortunately, I cannot assist with this request.”
 “Unfortunately, there seems to be some misunderstanding as I do not possess the ability to physically see images or any type of attachments. Therefore, I am unable to provide an analysis based on an actual image.”
Here is a discussion on the same issue:
https://community.openai.com/t/calls-to-gpt-4-vision-preview-dont-produce-errors-but-it-says-it-cant-read-images/478203/7
Please suggest a way I can handle these grasefully.
To Reproduce
N/A
Code snippets
No response
OS
Ubuntu 20.04
Python version
Python 3.10.12
Library version
openai 1.6.1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1167","feat: allow setting retry delay","2024-05-25T22:03:35Z","Closed issue","enhancement","Confirm this is a feature request for the Python library and not the underlying OpenAI API.
 This is a feature request for the Python library
Describe the feature or improvement you're requesting
Currently, setting the retry delay for the _base client is not easily exposed. End-users have easy control over the max number of retries but not the max_retry_delay and initial retry delay.
openai-python/src/openai/_base_client.py
 Lines 76 to 78 in 8ee5f33
	MAX_RETRY_DELAY, 
	DEFAULT_MAX_RETRIES, 
	INITIAL_RETRY_DELAY, 
Preferably these are exposed and easily settable similar to max_retries in the OpenAI class:

openai-python/src/openai/_client.py
 Lines 49 to 74 in 8ee5f33
	classOpenAI(SyncAPIClient): 
	completions: resources.Completions
	chat: resources.Chat
	embeddings: resources.Embeddings
	files: resources.Files
	images: resources.Images
	audio: resources.Audio
	moderations: resources.Moderations
	models: resources.Models
	fine_tuning: resources.FineTuning
	beta: resources.Beta
	with_raw_response: OpenAIWithRawResponse
	with_streaming_response: OpenAIWithStreamedResponse
	
	# client options
	api_key: str
	organization: str|None
	
	def__init__( 
	self, 
	*, 
	api_key: str|None=None, 
	organization: str|None=None, 
	base_url: str|httpx.URL|None=None, 
	timeout: Union[float, Timeout, None, NotGiven] =NOT_GIVEN, 
	max_retries: int=DEFAULT_MAX_RETRIES, 
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1166","Too many arguments in the Windows solution","2024-02-20T09:04:22Z","Open issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
I try to run the curl and grit commands, but only got the error ""-bash: cd: too many arguments""
To Reproduce
Just write
Code snippets
No response
OS
Windows
Python version
None
Library version
1.0.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1165","openai migrate fails to translate multiple except blocks with old openai.error objects","2024-02-21T15:39:25Z","Closed issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
I am running openai migrate on a codebase written for 0.28.1, and it doesn't correctly process a try/except with multiple except blocks. E.g.
try:
   [calling the completions API]
except openai.error.RateLimitError as e:
   [handle it]
except openai.error.AuthenticationError as e:
   [handle it]
[other except blocks with other openai.error.* types...]

The migrate command translates just the first except block to use the newer class (openai.RateLimitError, e.g.) and ignores the later ones.
Each time I re-run openai migrate it translates one more except block, so that is a simple workaround.
To Reproduce
Write a simple test case like the above code.
Run openai migrate
See it translates only one except block.
Code snippets
No response
OS
Linux
Python version
Python v3.10.12
Library version
openai v1.12.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1163","NotFoundError when specifying azure_deployment in AzureOpenAI client","2024-02-19T01:56:19Z","Open issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
When initializing the AzureOpenAI client with the azure_deployment parameter specified, a NotFoundError with error code 404 is raised upon calling client.beta.assistants.list(). However, omitting the azure_deployment parameter results in the expected behavior with no errors.
(autogen) ➜  autogen git:(main) ✗ python test.py
Traceback (most recent call last):
  File ""/Users/ianz/Work/autogen/test.py"", line 11, in <module>
    print(client.beta.assistants.list())
  File ""/Users/ianz/Work/miniconda3/envs/autogen/lib/python3.10/site-packages/openai/resources/beta/assistants/assistants.py"", line 270, in list
    return self._get_api_list(
  File ""/Users/ianz/Work/miniconda3/envs/autogen/lib/python3.10/site-packages/openai/_base_client.py"", line 1145, in get_api_list
    return self._request_api_list(model, page, opts)
  File ""/Users/ianz/Work/miniconda3/envs/autogen/lib/python3.10/site-packages/openai/_base_client.py"", line 990, in _request_api_list
    return self.request(page, options, stream=False)
  File ""/Users/ianz/Work/miniconda3/envs/autogen/lib/python3.10/site-packages/openai/_base_client.py"", line 856, in request
    return self._request(
  File ""/Users/ianz/Work/miniconda3/envs/autogen/lib/python3.10/site-packages/openai/_base_client.py"", line 908, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.NotFoundError: Error code: 404 - {'error': {'code': '404', 'message': 'Resource not found'}}

To Reproduce
import osfrom openai import AzureOpenAI
    
client = AzureOpenAI(
    api_key=os.getenv(""AZURE_OPENAI_API_KEY""),  
    api_version=""2024-02-15-preview"",
    azure_endpoint = os.getenv(""AZURE_OPENAI_API_BASE""),
    azure_deployment = ""gpt-4-turbo""
)

print(client.beta.assistants.list())
Code snippets
No response
OS
macOS
Python version
Python 3.10.13
Library version
1.3.7
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1160","All API Errors raised during streaming raise the same generic message (""An error occurred during streaming"")","2024-03-06T22:42:04Z","Closed issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
All API Errors raised during streaming raise the same generic message (""An error occurred during streaming"").
 See line 64 of _streaming.py.
The data[""error""] field is completely disregarded for streaming errors. Instead, it could be read to raise the correct type of error.
 This is a major quality-of-life issue with custom inference servers that seek to follow the openai standard.
To Reproduce
Spin up a custom inference server.
Have users access it via the openai Python library.
Introduce an arbitrary error in your inference server, and handle it correctly, by returning the error's code, param, and type inside of the streaming response.
Notice that the users will always get the generic ""An error occurred during streaming"" message.
Code snippets
The relevant part of the codebase uses this code snippet:

if is_mapping(data) and data.get(""error""):
    raise APIError(
        message=""An error occurred during streaming"",
        request=self.response.request,
        body=data[""error""],
    )
A different part of the codebase uses this code snippet:
raise self._make_status_error_from_response(err.response) from None

This latter type of functionality should also be used while iterating over a stream.


### OS

Ubuntu 20.04.5 LTS

### Python version

Python v.3.11.6

### Library version

openai v1.11.1

 The text was updated successfully, but these errors were encountered: 
👍2
clemlesne and lauradang reacted with thumbs up emoji
All reactions
👍2 reactions"
"https://github.com/openai/openai-python/issues/1156","Add support for token providers in the native OpenAI client","2024-02-15T08:55:33Z","Open issue","No label","Confirm this is a feature request for the Python library and not the underlying OpenAI API.
 This is a feature request for the Python library
Describe the feature or improvement you're requesting
Our project utilizes both OpenAI and Azure OpenAI APIs, which our organisation manages through an API gateway. The gateway provides its own short-lived JWTs and serves as a mediator between clients and these APIs, with some organisational logic over the top.
For Azure APIs, we use the AzureOpenAI.azure_ad_token_provider parameter to automatically refresh access tokens. Unfortunately, OpenAI's standard clients lack this feature.
As a workaround, we have written a custom class extending openai.AsyncOpenAI, with api_key defined as an @property getter to handle token refreshes. However, this workaround is not ideal due to the extra maintenance required. Ideally, OpenAI's clients would natively support token providers the same way as the Azure ones do.
Would there be interest in adding this feature? I'm willing to make this contribution myself!
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1154","Migrating to v1: How to create a fine-tuning job and stream using async client?","2024-02-14T11:51:16Z","Open issue","No label","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
In v0, astream_events is able to iterate and wait until the model is actually completed.
I've migrated it to v1 (see snippets), but fine_tuning.jobs.list_events() doesn't wait until the model is actually completed. The messages also seem to be in the reverse order.
I know that this is because it's not streaming, but I don't see a method or parameter (e.g. stream=True) that allows me to stream and wait for the events.
To Reproduce
Use AsyncClient
await fine tuning job create
try to stream and wait for all events
Code snippets
# v0 - workswith open('ai_data/model.jsonl', 'rb') as f:
            upload_resp = await openai.File.acreate(f, 'fine-tune')
file_id = upload_resp.idtune_resp = await openai.FineTune.acreate(training_file=file_id, model='babbage')
async for event in await openai.FineTune.astream_events(tune_resp.id):
    print(event.message)
print('Completed. New Model ID: ' + tune_resp.fine_tuned_model)

# v1with open('ai_data/model.jsonl', 'rb') as f:
    upload_resp = await client.files.create(file=f, purpose='fine-tune')
file_id = upload_resp.idtune_resp = await client.fine_tuning.jobs.create(training_file=file_id, model='babbage-002')
async for event in client.fine_tuning.jobs.list_events(tune_resp.id):
    print(event.message)
# Only prints the following# Sending model train request# Validating training file: file-XZpR8yio1o1Hw7wPyxfRXtYQ# Created fine-tuning job: ftjob-f9UuWfBcOOu3Vhd0jR3k65oa

print('Completed. New Model ID: ' + tune_resp.fine_tuned_model) # Error because the model is not completed yet
OS
Windows
Python version
3.11.5
Library version
1.10.6
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1153","'timestamp granularity', new featured added to AudioAPI Feb 9th not accessible","2024-02-14T02:50:06Z","Closed issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
audio.transcriptions.create() parameter 'timestamp_granularities' not recognized.
As per API changelog, parameter added to API Feb 9th:
https://platform.openai.com/docs/changelog
To Reproduce
passing timestamp_granularities=[""word""] to audio.transcriptions.create() results in
TypeError: Transcriptions.create() got an unexpected keyword argument 'timestamp_granularities'
Code snippets
---> 19 transcript = client.audio.transcriptions.create( #openai.audio.transcriptions.create(
     20   file=wavfile,
     21   model=""whisper-1"",
     22   response_format=""verbose_json"",
     23   timestamp_granularities=[""word""]
     24 )

TypeError: Transcriptions.create() got an unexpected keyword argument 'timestamp_granularities'
OS
All
Python version
3.11.4
Library version
openai-1.12.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1151","name 'openai' is not defined in openai 1.2.0","2024-02-14T02:18:31Z","Closed issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
This code should work as documented in usage https://github.com/openai/openai-python
Also i have doubt about using client with or without api key as parameter...in courses of OpenAi form DeepLearning, they show client without api key as parameter, and they prefer to use an environment variable saved in a .env file.:
client = OpenAI(
    # This is the default and can be omitted
    api_key=os.environ.get(""OPENAI_API_KEY""),
)


or
client = OpenAI()
To Reproduce
1 - Execute the snippet below from DeepLearning official short course. Reading the usage of your main github file as linked above, it should work.
2 - Gives error : name 'openai' is not defined in openai 1.2.0
Code snippets
from openai import OpenAIimport os


from dotenv import load_dotenv, find_dotenv

_ = load_dotenv(find_dotenv()) # read local .env file

openai.api_key  = os.getenv('OPENAI_API_KEY')

client = OpenAI()

def get_completion(prompt, model=""gpt-3.5-turbo""):
    messages = [{""role"": ""user"", ""content"": prompt}]
    response = client.chat.completions.create(
        model=model,
        messages=messages,
        temperature=0
    )
    return response.choices[0].message.content



def get_completion_from_messages(messages, 
                                 model=""gpt-3.5-turbo"", 
                                 temperature=0, 
                                 max_tokens=500):
    response = client.chat.completions.create(model=model,
    messages=messages,
    temperature=temperature, 
    max_tokens=max_tokens)
    return response.choices[0].message.content

messages =  [  
{'role':'system', 'content':'You are an assistant that speaks like Shakespeare.'},    
{'role':'user', 'content':'tell me a joke'},   
{'role':'assistant', 'content':'Why did the chicken cross the road'},   
{'role':'user', 'content':'I don\'t know'}  ]

response = get_completion_from_messages(messages)
print(response)


### OS

Ubuntu 22.04

### Python version

Python 3.11

### Library version

openai v1.2.0

 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1149","openai lib preventing reading of headers for an http exception","2024-02-13T11:39:11Z","Open issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
https://github.com/openai/openai-python/blob/main/src/openai/_base_client.py#L980 - raises exceptions if there's an HTTPStatusError, such as a 429, 500 etc.
Which is semi-useful, but as-implemented, it does mean the headers are unavailable for that request.
Headers contain extremely valuable information sometimes, even when there are exceptions, and the library should ideally still return the headers along with the exception somehow, so that on receiving an exception it's possible to still read the headers.
To Reproduce
try:
    stream = sync_client.chat.completions.create(.... stream=True)
except Exception as e:
    print(e)
    print(vars(e))
    print(stream)
    raise e

stream is None, e is an exception with a bubbled up error message from the api, headers not available
Code snippets
see above for code snippets.
OS
macOS
Python version
Python 3.10.12
Library version
openai 1.3.7 and above
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1147","Problem with proxy and streaming","2024-02-13T11:41:38Z","Closed issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
I try to process streaming (return customer in chat). I need to use proxy. I have problem that response does not streaming when use proxy (all responses returned after all processed, no effect of writing text)
To Reproduce
All code below, api key in ENV
Code snippets
import asynciofrom typing import Optional

from httpx import AsyncClientfrom openai import AsyncStream, AsyncOpenAIfrom openai.types.chat import ChatCompletionChunk


async def get_openai_stream_agenerator() -> AsyncStream[ChatCompletionChunk]:
    client = AsyncOpenAI(
        http_client=AsyncClient(
            # when I comment these two lines streaming is ok
            proxy=""http://localhost:8080"",  # I'm using mitmproxy with basic configuration
            verify=False,
        )
    )
    messages = [
        {""role"": ""system"", ""content"": ""Return details about asking person""},
        {""role"": ""user"", ""content"": ""Iga Świątek""},
    ]
    response: AsyncStream[ChatCompletionChunk] = await client.chat.completions.create(
        model='gpt-4-0613',
        messages=messages,
        stream=True,
    )  # type: ignore
    return response


def get_delta_argument(chunk: ChatCompletionChunk) -> Optional[str]:
    if len(chunk.choices) > 0:
        return chunk.dict()['choices'][0]['delta']['content']
    else:
        return None


async def get_response_generator() -> None:
    async for it in await get_openai_stream_agenerator():
        value = get_delta_argument(it)
        if value:
            print(value, end="""")
    print()


if __name__ == '__main__':
    asyncio.run(get_response_generator())
OS
macOS
Python version
Python v3.11.7
Library version
openai 1.12.0, httpx 0.26.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1145","usage of Completion.Create is None","2024-02-11T04:52:30Z","Closed issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
usage object is None - how could it be?
 It didn't happen before; today, that's the first time it happened.
 What is the scenario in which usage is None?
 Why was it the first time that happened?
To Reproduce
Run the code about with openai version = 1.3.3 or 1.12.0 and usage object will be none.
Code snippets
openai_response = self.client.chat.completions.create(
            model=model, messages=messages, temperature=temperature,
            max_tokens=max_tokens, top_p=top_p,
            frequency_penalty=frequency_penalty, presence_penalty=presence_penalty, stop=stop
        )

        openai_usage = openai_response.usage # Sometimes usage is None


### OS

Linux

### Python version

Python 3.9.18

### Library version

openai v1.3.3 or openai v1.12.0
Using Azure OpenAI

 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1144","Commit a7ebc26 disables Open Telemetry's httpx instrumentation in some scenarios","2024-02-09T17:29:22Z","Open issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
This commit a7ebc26, which was introduced in PR #966, for release v1.3.9, disables httpx instrumentation in some cases.
Specifically, it is disabled when openai is imported before instrumenting httpx.
 This is because opentelemetry.instrumentation.httpx .HTTPXClientInstrumentor._instrument creates subclasses of httpx.Client and httpx.AsyncClient. And then replaces the original clients with those subclasses, which adds telemetry.
 In the above commit, openai creates SyncHttpxClientWrapper and AsyncHttpxClientWrapper subclasses of httpx's clients.
That means that when we instrument first, and then import openai, the client wrappers inherit from the instrumented clients.
 When we import openai first, and then instrument httpx, the wrapper clients inherit from the original httpx clients.
Maybe this should be addressed at the opentelemetry-python-contrib side, but even so, v1.3.9 broke telemetry.
Could the change implemented in #966 be implemented in a backward-compatible way?
 If not, at the very least, the changelog should be updated, warning about this.
To Reproduce
Import openai
Use HTTPXClientInstrumentor from opentelemetry.instrumentation.httpx to instrument httpx.
Code snippets
Running this, openai httpx requests are instrumented:
from opentelemetry.instrumentation.httpx import HTTPXClientInstrumentor, _InstrumentedClientHTTPXClientInstrumentor().instrument()
from openai._base_client import SyncHttpxClientWrapperassert issubclass(SyncHttpxClientWrapper, _InstrumentedClient)
But this version is not instrumented, and the assertion fails:
from openai._base_client import SyncHttpxClientWrapperfrom opentelemetry.instrumentation.httpx import HTTPXClientInstrumentor, _InstrumentedClientHTTPXClientInstrumentor().instrument()
assert issubclass(SyncHttpxClientWrapper, _InstrumentedClient)
OS
any
Python version
any
Library version
openai from v1.3.9.0 to v1.12.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1143","AttributeError: 'OpenAI' object has no attribute 'chat'","2024-02-11T04:51:33Z","Closed issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
pip install -U openai
i met this error i think v1.12.0 some issue
To Reproduce
OS : wsl-Ubuntu
 python : 3.9.18
 Package-version : 1.12.0
Code snippets
#### Error
 bashAttributeError: 'OpenAI' object has no attribute 'chat' 
Name: openaiVersion: 1.12.0Summary: The official Python library for the openai APIHome-page: 
Author: 
Author-email: OpenAI <support@openai.com>


### OS

wsl-Ubuntu

### Python version

3.9.18

### Library version

1.12.0

 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1142","AttributeError: 'function' object has no attribute 'completion'","2024-02-12T02:14:29Z","Closed issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug

client = OpenAI(api_key = ""API-KEY"")
chat_completion = client.chat.completions.create(
    messages=[
        {
            ""role"": ""user"",
            ""content"": ""Say this is a test"",
        }
    ],
    model=""gpt-3.5-turbo"",
)

Gives an Error on Colab
 `---------------------------------------------------------------------------
 AttributeError Traceback (most recent call last)
 in <cell line: 1>()
 ----> 1 chat_completion = client.chat.completions.create(
 2 messages=[
 3 {
 4 ""role"": ""user"",
 5 ""content"": ""Say this is a test"",
AttributeError: 'function' object has no attribute 'completions'
 `
 Version: openai 1.12.0
To Reproduce
client = OpenAI(api_key = ""API-KEY"")
chat_completion = client.chat.completions.create(
    messages=[
        {
            ""role"": ""user"",
            ""content"": ""Say this is a test"",
        }
    ],
    model=""gpt-3.5-turbo"",
)

Code snippets
Already Attached
OS
linux
Python version
Python 3.11
Library version
openai v1.12.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1141","""We could not parse the JSON body of your request."" with tts-1-(hd) endpoint, text to speach","2024-02-12T06:05:43Z","Closed issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
I am receiving this error (suddenly) with openai==1.12.0 on Feb 09, 2024...worked the day before:

  File ""/Users/ajung/Dropbox/FILMORA- VIDEOS/Typesense-Onkopedia - 2/convert.py"", line 66, in <module>
    text_to_speech(content, ""de"", output_wav)
  File ""/Users/ajung/Dropbox/FILMORA- VIDEOS/Typesense-Onkopedia - 2/convert.py"", line 17, in text_to_speech
    response = client.audio.speech.create(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/ajung/Dropbox/FILMORA- VIDEOS/Typesense-Onkopedia - 2/lib/python3.12/site-packages/openai/resources/audio/speech.py"", line 81, in create
    return self._post(
           ^^^^^^^^^^^
  File ""/Users/ajung/Dropbox/FILMORA- VIDEOS/Typesense-Onkopedia - 2/lib/python3.12/site-packages/openai/_base_client.py"", line 1201, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/ajung/Dropbox/FILMORA- VIDEOS/Typesense-Onkopedia - 2/lib/python3.12/site-packages/openai/_base_client.py"", line 889, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File ""/Users/ajung/Dropbox/FILMORA- VIDEOS/Typesense-Onkopedia - 2/lib/python3.12/site-packages/openai/_base_client.py"", line 981, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': ""We could not parse the JSON body of your request. (HINT: This likely means you aren't using your HTTP library correctly. The OpenAI API expects a JSON payload, but what was sent was not valid JSON. If you have trouble figuring out how to fix this, please contact us through our help center at help.openai.com.)"", 'type': 'invalid_request_error', 'param': None, 'code': None}}

To Reproduce
import sys
from openai import OpenAI
import openai

client = OpenAI()

SPEED = 1.0
VOICE = ""echo""
MODEL=""tts-1"",
#MODEL=""tts-1-hd"",

def text_to_speech(content: str, language: str, output_wav: str):

    response = client.audio.speech.create(
        model=MODEL,
        voice=VOICE,
        speed=SPEED,
        input=content,
    )

    print(f""Saved to {output_wav}"")
    response.stream_to_file(output_wav)


text_to_speech(""hello world"", ""de"", ""output.wav"")

Code snippets
No response
OS
MacOS Soama
Python version
3.12
Library version
open 1.12.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1139","Assistants API","2024-03-04T23:43:59Z","Closed issue","No label","Confirm this is a feature request for the Python library and not the underlying OpenAI API.
 This is a feature request for the Python library
Describe the feature or improvement you're requesting
Is there a way to customize the Assistants chat response?
 Given the outcome of certain tool uses, id like the assistant to respond in a different manner.
I guess this somehow should happen in the client.beta.threads.runs.create function
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1137","openai.organization not being used","2024-02-12T10:20:31Z","Closed as not planned issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
We've recently upgraded the openai client to >1.0.0 and found that our ""organization switching"" logic stopped working.
Before 1.0.0, organization was being set by setting global openai.organization. All clients created used that as the default organization.
Post 1.0.0, the logic doesn't work.
Having the ability to set the property, but property not being used is very surprising.
To Reproduce
Snippet is below. I've used opentelemetry httpx instrumentation to intercept the request and verify that header is not being set.
Example headers after running the provided code below. Observe that there's no ""Openai-organization"" header set.
Headers({'host': 'api.openai.com', 'accept-encoding': 'gzip, deflate', 'connection': 'keep-alive', 'accept': 'application/json', 'content-type': 'application/json', 'user-agent': 'OpenAI/Python 1.3.5', 'x-stainless-lang': 'python', 'x-stainless-package-version': '1.3.5', 'x-stainless-os': 'MacOS', 'x-stainless-arch': 'arm64', 'x-stainless-runtime': 'CPython', 'x-stainless-runtime-version': '3.11.7', 'authorization': '[secure]', 'content-length': '84', 'sentry-trace': 'cf394dd2ce8b4682843e13cbf98782d1-894bb0e05b111a50', 'baggage': 'sentry-trace_id=cf394dd2ce8b4682843e13cbf98782d1,sentry-environment=development,sentry-release=unknown-development'})

Code snippets
import openai

openai.organization = ""some_id""from openai import OpenAIclient = OpenAI(
    # Defaults to os.environ.get(""OPENAI_API_KEY"")
)

chat_completion = client.chat.completions.create(
    model=""gpt-3.5-turbo"",
    messages=[{""role"": ""user"", ""content"": ""Hello world""}]
)
OS
macOs
Python version
v3.11.7
Library version
openai 1.3.5
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1135","export 'Usage' class as apart of openai.types","2024-02-13T12:57:14Z","Closed issue","No label","Confirm this is a feature request for the Python library and not the underlying OpenAI API.
 This is a feature request for the Python library
Describe the feature or improvement you're requesting
Hi there,
I'm using the library and writing tests to mock CreateEmbeddingResponse. To mock the class, I'm configuring the properties, including the usage parameter. I'd like to instantiate a 'Usage' object for this. Hence, requesting it to be exported with openai.types. Thanks!
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1134","Client Frequently hangs gives Timeout error!","2024-02-14T18:05:47Z","Closed issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
There are lot of openai.APIConnectionError when there is non english messages sent!
I stream response and have tried with timeout setting 30.0 , 60.0 and even 120.0
 OpenAI(api_key = os.getenv(""OPENAI_API_KEY""),timeout=30.0)
I have noticed there are frequent timeout error and most of the time it's when there is non english messaged but also few times in english messages. Eventually I have to set max retries to 0 and use fallback model.
To Reproduce
Essentially openai.APIConnectionError randomly. I don't even have huge concurrent traffic.
Code snippets
#I have tried will all these similar settings while setting streaming = True

client = OpenAI(api_key = os.getenv(""OPENAI_API_KEY""),timeout=30.0, max_retries=1)

client = OpenAI(api_key = os.getenv(""OPENAI_API_KEY""),timeout=60.0, max_retries=0)

client = OpenAI(api_key = os.getenv(""OPENAI_API_KEY""),timeout=80.0, max_retries=0)
My OpenAI Library version is always the latest
OS
Cloud Function
Python version
Python 3.10
Library version
v1.11.1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1131","Slower than expected performance after upgrading","2024-02-07T07:31:32Z","Open issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
I'm not absolutely certain if this is an issue with the Python library, but after upgrading from v0.28.0 to v1.10.0, we noticed a significant increase in latency (by about a factor of 4x) when requesting embeddings via an Azure OpenAI ada v2 deployment. This was confirmed in the Azure portal, where latency was indeed about 4x higher immediately after we deployed our service using the upgraded package. After downgrading back to v0.28.0, the issue resolved itself.
To Reproduce
Create an AzureOpenAI client
Request embeddings with the client.
 For reference, we send about 2-3k embedding requests per 5 minutes
Code snippets
This is how we query Azure with v0.28.0:
import openaiimport os

texts = [""this"", ""is"", ""a"", ""test""]
embedding_args = {
                ""api_type"": AZURE_API_TYPE,
                ""api_version"": AZURE_API_VERSION,
                ""api_key"": os.getenv(""AZURE_OPENAI_API_KEY""),
                ""api_base"": os.getenv(""AZURE_OPENAI_API_BASE""),
                ""deployment_id"": os.getenv(""AZURE_OPENAI_DEPLOYMENT_ID""),
                ""input"": texts
            }
res = openai.Embedding.create(**embedding_args)
With v1.10.0:
from openai import AzureOpenAIimport os

texts = [""this"", ""is"", ""a"", ""test""]
openai_client = AzureOpenAI(
                api_key=os.getenv(""AZURE_OPENAI_API_KEY""),
                api_version=AZURE_API_VERSION,
                azure_endpoint=os.getenv(""AZURE_OPENAI_API_BASE"")
            )
create_args = {
                        ""model"": os.getenv(""AZURE_OPENAI_DEPLOYMENT_ID""),
                        ""input"": texts
                    }
res = openai_client.embeddings.create(**create_args)
OS
debian:bullseye-slim
Python version
Python v3.11.7
Library version
v1.10.0
 The text was updated successfully, but these errors were encountered: 
👀1
elonzh reacted with eyes emoji
All reactions
👀1 reaction"
"https://github.com/openai/openai-python/issues/1130","Support Dall-e-3 from the command line","2024-02-07T12:50:51Z","Closed issue","CLI","Confirm this is a feature request for the Python library and not the underlying OpenAI API.
 This is a feature request for the Python library
Describe the feature or improvement you're requesting
Currently the default model for images.create is Dall-e-2.
Source: https://platform.openai.com/docs/api-reference/images/create
But the command line does not support selecting a different model.
Source:
$ openai api images.generate

usage: openai api images.generate [-h] -p PROMPT [-n NUM_IMAGES] [-s SIZE] [--response-format RESPONSE_FORMAT]
openai api images.generate: error: the following arguments are required: -p/--prompt
Additional context
openai-python/src/openai/resources/images.py
 Lines 61 to 62 in d231d1f
	 model: The model to use for image generation. Only `dall-e-2` is supported at this
	 time.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1123","Mypy typing error when treating a ChatCompletionSystemMessageParam as dict","2024-02-05T17:30:19Z","Open issue","enhancement","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
We have a list like:
messages: list[ChatCompletionMessageParam]

We then convert a message to a dict:
dict(messages[0])

Mypy does not like that:
Argument 1 to ""dict"" has incompatible type ""ChatCompletionSystemMessageParam | ChatCompletionUserMessageParam | ChatCompletionAssistantMessageParam | ChatCompletionToolMessageParam | ChatCompletionFunctionMessageParam""; expected ""SupportsKeysAndGetItem[str, str]"" [arg-type]
I'm not sure if this is a bug with mypy or with the SDK, to be honest. Or maybe there's even a better approach in our code. I see that your classes extend TypedDict, so I'd think that should be considered as SupportsKeysAndGetItem, but I'm probably missing something.
To Reproduce
See code above. You can also see our CI failing here: https://github.com/Azure-Samples/azure-search-openai-demo/actions/runs/7786990746/job/21233073297?pr=1233
Code snippets
No response
OS
MacOS
Python version
3.11
Library version
1.10.0
 The text was updated successfully, but these errors were encountered: 
👍1
fumi-sagawa reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/openai-python/issues/1119","I encountered an error: openai. error. AuthenticationError Invalid token (request id: 20240204142954xxxxxxxxxxxxxxxxxxx)","2024-02-04T18:28:52Z","Closed issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
When I call the openai API, it may be normal for the first 1-6 hours, but then as long as I send a request to gpt-3.5 turbo, an AuthenticationError Invalid token (request id: 20240204142954xxxxxxxxxxxxxxxxxxx) will occur. When I restart my django service, it returns to a normal state, which I am very puzzled about
To Reproduce
Call the openai interface
Maintain call frequency and wait for a few hours
An invalid token with an error occurred
When restarting django, everything becomes normal again
Code snippets
response = openai.ChatCompletion.create(
                    model=""gpt-3.5-turbo-1106"",
                    stream=True,
                    messages=conversation_list,
                    timeout=120
                )
                for data in response:
                    if ""content"" in data[""choices""][0][""delta""]:
                        content = data[""choices""][0][""delta""][""content""]
                        print(content, end='')
OS
linux
Python version
Python v3.9.2
Library version
openai v0.28
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1118","Bugs When using OpenAI api with Celery/redit","2024-02-04T18:53:11Z","Closed issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
Hello,
 There seem to be a bug when using the Openai API with Celery/redit shared task. When running the function bellow.
 I’m not sure exactly why. When In use langchain with Mistral api, Ollama or with any my own custom models it works really well, but it crashes when I use anything touching the openai api. I tried bypassing langchain to work directly with the openai api and I still do get this error Does someone have any idea why ?
To Reproduce
pip install Django
pip install celery
brew install redit // for macs
create a random view function that will call the task at task.py
Code snippets
in views.py
def some_view_fucntion(request):
    for x in range(2):
         some_ai_function.delay()
in task.py
@shared_taskdef some_ai_function():
    message = ""Hello, write a message about testing ""
    print(message)
    completion = openai.chat.completions.create(
        model=“gpt-3.5-turbo”,
        messages=[
            {
                 “role”: “user”,
                 “content”: message,
            },
        ],
    )
    print(completion.choices[0].message.content)
RETURNS :
ERROR/MainProcess] Process ‘ForkPoolWorker-2’ pid:86900 exited with ‘signal 5 (SIGTRAP)’
ERROR/MainProcess] Task handler raised error: WorkerLostError(‘Worker exited prematurely: signal 5 (SIGTRAP) Job: 0.’)

Traceback (most recent call last):
File"" …
/python3.11/site-packages/billiard/pool.py"", line 1264, in mark_as_worker_lost
raise WorkerLostError(
billiard.exceptions.WorkerLostError: Worker exited prematurely: signal 5 (SIGTRAP) Job: 0.

OS
macOS (m2)
Python version
Python 3.11.5
Library version
openai 1.8.0
The content you are editing has changed. Please copy your edits and refresh the page.
Tasks
BetaGive feedback
Create a work around get_architecture() for celery tasks on MacOS
Options
Convert to issue
Toggle completion
Rename
Remove
 The text was updated successfully, but these errors were encountered: 
👍1
ramsrib reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/openai-python/issues/1116","Seed","2024-02-02T04:40:13Z","Closed as not planned issue","No label","Confirm this is a feature request for the Python library and not the underlying OpenAI API.
 This is a feature request for the Python library
Describe the feature or improvement you're requesting
Hi,
I would like to inquire about the behavior of the seed value in making chat interactions deterministic. Currently, it seems that the determinism is only ""more or less"" reliable, often deviating from the expected path after the second response-answer, regardless of the model used. Could this be addressed or clarified in future updates?
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1115","TypeError: 'LegacyAPIResponse' object is not iterable","2024-02-01T14:20:51Z","Closed as not planned issue","question","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
Not able to get a streaming response because 'LegacyAPIResponse' object is not iterable.
To Reproduce
The following code:
import osimport openaifrom openai import AzureOpenAI

os.environ['AZURE_OPENAI_API_KEY'] = ""xxx""client = AzureOpenAI(
    api_version=""2023-07-01-preview"",
    azure_endpoint=""https://xxx.openai.azure.com/"",
)

stream = client.chat.completions.with_raw_response.create(
    messages=[{
        ""role"": ""user"",
        ""content"": ""sing me a song"",
    }],
    model=""gpt-35-turbo"",
    max_tokens=30,
    temperature=0.7,
    stream=True
)
print(stream)
for chunk in stream:
    print(chunk.choices[0].delta.content or """", end="""")
Results in:
<APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>
Traceback (most recent call last):
  File ""test.py"", line 27, in <module>
    for chunk in stream:
TypeError: 'LegacyAPIResponse' object is not iterable
Code snippets
No response
OS
WSL2 Ubuntu
Python version
Python v3.9.7
Library version
openai-python v1.10.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1110","Add support for locally hosted img urls","2024-02-02T04:40:43Z","Closed issue","enhancement","Confirm this is a feature request for the Python library and not the underlying OpenAI API.
 This is a feature request for the Python library
Describe the feature or improvement you're requesting
You know what I don't like doing right now. Creating a microservice and deploying it just to handle, serve and store img urls.
 OpenAi should implement a feature which allows the OpenAi vision API to be able to communicate with localhost. I know that this could be a complex task, but you guys have 100 of billions of dollar in funding. You guys could probably figure it out
Additional context
I know that we can use bas64 encoding, but it's slow.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1109","AsyncAzureClient connection hangs frequently","2024-02-14T18:09:08Z","Closed issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
something very strange: the python OpenAI AzureClient works fine, but using the AsyncAzureClient results in hanging connection ~5% of the time
To Reproduce
Run a bunch of requests (~100) to GPT-4 using AsyncAzureClient, eventually one of them will hang for 20+ minutes before timing out
Code snippets
No response
OS
macOS
Python version
Python v3.11.6
Library version
1.6.1
 The text was updated successfully, but these errors were encountered: 
👍2
CallumAtCarter and nuncaunca reacted with thumbs up emoji
All reactions
👍2 reactions"
"https://github.com/openai/openai-python/issues/1108","Can't import openai module without authentication error","2024-02-03T21:22:28Z","Closed issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
Given the following import:
def do_stuff():

    secret = ""vgtc/dev/openai-secret""
    secret_values = get_secret_json(secret)
    
    import openai

    client = openai.OpenAI(**secret_values)
The exception is raised:
""The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable""
This happens because on line 316 of openai/__init__.py it attempts to initialize client proxies (before I have requested initialization to occur)
This isn't a problem for those who use environment variables to leak their secrets to processes, but for those of us who want stricter control of our secrets than what is available by handing them over to the OS, it makes it impossible to even import the library without creating an exception to our security policy.
Here's a blog post on it:
https://www.cloudtruth.com/blog/the-pitfalls-of-using-environment-variables-for-config-and-secrets#:~:text=Developers%20often%20use%20environment%20variables,information%20poses%20significant%20security%20risks.
To Reproduce
New environment
pip install openai
import openai
See crash
Code snippets
No response
OS
Win11
Python version
3.12
Library version
openai v1.10.0""
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1107","[Assistants API] Run Step Tool call element changes type depending on run status","2024-03-03T01:10:40Z","Closed issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
When polling run steps to read tool calls, I've observed that when the run is not in status 'completed' the function object in the tool_calls array is of type dict, however, once the run is completed it becomes a regular object. This has implications for how the tool call attributes can be accessed, leading to redundant code checking the element type. I have specifically observed this with function tool calls. I haven't done any work with the other types of Assistant tools.
To Reproduce
Initiate a run that requires the use of a function
Retrieve the latest run step associated with the run
Attempt to read the tool calls when the run is of status completed versus when it's in an in_progress or requires_action state.
Code snippets
No response
OS
Windows 11
Python version
Python v3.9,3
Library version
openai v1.3.5
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1102","Functionality to adjust exponential backoff associated with max_retries option","2024-01-25T17:30:13Z","Open issue","enhancement","Confirm this is a feature request for the Python library and not the underlying OpenAI API.
 This is a feature request for the Python library
Describe the feature or improvement you're requesting
Functionality to adjust exponential backoff associated with max_retries option. Looking in the documentation it's stated that certain errors are automatically retried 2 times by default, with a short exponential backoff, if it's possible it's ideal to control the value of the exponential backoff either manually or should be dynamically associated with rate limited duration in case of a 429 error since just short exponential backoffs are not helpfull in this case!
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1101","openai.error.APIConnectionError","2024-01-25T04:23:53Z","Closed as not planned issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
Error communicating with OpenAI: HTTPSConnectionPool(host='api.openai.com', port=443): Max retries exceeded with url: /v1/completions (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x0000018360C08370>: Failed to establish a new connection: [WinError 10060] 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。'))
To Reproduce
由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。'))
Code snippets
import openaiopenai.api_key = ""sk-…""

response = openai.Completion.create(
    model='text-davinci-003',
    prompt='主题: 早餐 风\n两句话的恐怖故事:',
    temperature=0.8,
    max_tokens=120,
    top_p=1.0,
    frequency_penalty=0.5,
    presence_penalty=0.0,
)

print(response.choices[0].text)
OS
windows
Python version
python 3.8
Library version
openai v1.1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1097","Chat completion endpoint cannot remember its previous messages","2024-01-24T02:55:51Z","Closed as not planned issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug

 With this request, llm should answer my last question. But it says it doesn;t know. Is ChatPromptTemplate with chat completion endpoint supposed to remember the previous messages or I should manually added them in the latest user prompt?
 Chat completion takes a list of messages

 Response saying don't know previous message

To Reproduce
This is my code for setting up using semantic kerner which uses openai package in the end to send the request, basically it add the previous messages into the ChatPromptTemplate.
system_message = """"""
You are a technical supporter that help users' questions, you should give 1-2 sentences to explain the answer and code examples for the question based on only the following contexts.
You may find the contexts provide information for multiple potential answers, you can give up to 3 most relevant answers separated by numbers like 1. 2. 3.

Follow this pattern to answer the question:
Contexts:
- Document path_title1 : page_content1
&&&
- Document path_title2 : page_content2

Question: question

Answer: answer
""""""

prompt_template = """"""
Contexts:
{{$context}}

Question: {{$input}}
""""""

def preprocess_messages(messages : list):
    transformed_messages = []
    transformed_messages.append({""role"": ""system"", ""message"": system_message})
    for message in messages:
      role = ""assistant"" if message[""role""] == ""bot"" else message[""role""]
      transformed_message = {""role"": role, ""message"": message[""content""]}
      if (role != ""system""):
        transformed_messages.append(transformed_message)
    return transformed_messages

def create_chat_prompt_template_instance(kernel : Kernel, messages : list = []):
    req_settings = sk_oai.AzureChatRequestSettings(max_tokens=2000, temperature=0, extension_data={""chat_system_prompt"": system_message})
    req_settings.unpack_extension_data()
    config = PromptTemplateConfig(completion=req_settings)
    template = ChatPromptTemplate(prompt_template, kernel.prompt_template_engine, config)
    if (messages and len(messages) > 0):
      processed_messages = preprocess_messages(messages)
      for message in processed_messages:
        template.add_message(message[""role""], message[""message""])
    function_config = SemanticFunctionConfig(config, template)
    return kernel.register_semantic_function(""ChatBot"", ""rag_chat"", function_config)

  # simplified calling
  messages = [{""content"": ""Q1"", ""role"": ""user""}, {""content"": ""A1"", ""role"": ""bot""},{""content"": ""Q2"", ""role"": ""user""}]
  query=messages[-1]['content']
  context['context'] = combined_documents
  context['input'] = query

  previous_messages = messages[:-1]
  chat_function = create_chat_prompt_template_instance(kernel=kernel, messages=previous_messages)

  response = await kernel.run_async(chat_function, input_context=context)

Code snippets
No response
OS
Ubuntu
Python version
python3.9
Library version
openai1.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1096","when I use gpt-3.5-turbo-1106, an openai.APIStatusError occurred","2024-01-24T02:16:32Z","Closed as not planned issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
openai.APIStatusError: 
 <title>Request Header Fields Too Large</title> 
Request Header Fields Too Large
 Error parsing headers: 'limit request headers fields' 
To Reproduce
无
Code snippets
stream = OpenAI(api_key='my key').openai_engine.chat.completions.create(
                    model=""gpt-3.5-turbo"",
                    messages=messages,
                    stream=True,
                    temperature=0.1
                )
OS
linux
Python version
python3.9
Library version
openai 1.9.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1095","When using openai asynchronous request, an error occurs when defining the proxy with http_client","2024-01-23T09:51:28Z","Closed issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
When using the AsyncOpenAI object and specifying a proxy through http_client to initiate a request, an exception ""TypeError: object Response can't be used in 'await' expression"" will be thrown in openai/_base_client.py.
The following are exception details:
Traceback (most recent call last):
 File ""/Users/apple/opt/miniconda3/lib/python3.9/site-packages/openai/_base_client.py"", line 1441, in _request
 response = await self._client.send(
 TypeError: object Response can't be used in 'await' expression
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
 File ""/Users/apple/opt/miniconda3/lib/python3.9/site-packages/openai/_base_client.py"", line 1441, in _request
 response = await self._client.send(
 TypeError: object Response can't be used in 'await' expression
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
 File ""/Users/apple/opt/miniconda3/lib/python3.9/site-packages/openai/_base_client.py"", line 1441, in _request
 response = await self._client.send(
 TypeError: object Response can't be used in 'await' expression
The above exception was the direct cause of the following exception:
Traceback (most recent call last):
 File ""/Users/apple/project/myproject-aigc/test.py"", line 87, in 
 asyncio.run(main())
 File ""/Users/apple/opt/miniconda3/lib/python3.9/asyncio/runners.py"", line 44, in run
 return loop.run_until_complete(main)
 File ""/Users/apple/opt/miniconda3/lib/python3.9/asyncio/base_events.py"", line 647, in run_until_complete
 return future.result()
 File ""/Users/apple/project/space-aigc/test.py"", line 75, in main
 chat_completion = await client.chat.completions.create(
 File ""/Users/apple/opt/miniconda3/lib/python3.9/site-packages/openai/resources/chat/completions.py"", line 1300, in create
 return await self._post(
 File ""/Users/apple/opt/miniconda3/lib/python3.9/site-packages/openai/_base_client.py"", line 1705, in post
 return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
 File ""/Users/apple/opt/miniconda3/lib/python3.9/site-packages/openai/_base_client.py"", line 1407, in request
 return await self._request(
 File ""/Users/apple/opt/miniconda3/lib/python3.9/site-packages/openai/_base_client.py"", line 1461, in _request
 return await self._retry_request(
 File ""/Users/apple/opt/miniconda3/lib/python3.9/site-packages/openai/_base_client.py"", line 1530, in _retry_request
 return await self._request(
 File ""/Users/apple/opt/miniconda3/lib/python3.9/site-packages/openai/_base_client.py"", line 1461, in _request
 return await self._retry_request(
 File ""/Users/apple/opt/miniconda3/lib/python3.9/site-packages/openai/_base_client.py"", line 1530, in _retry_request
 return await self._request(
 File ""/Users/apple/opt/miniconda3/lib/python3.9/site-packages/openai/_base_client.py"", line 1471, in _request
 raise APIConnectionError(request=request) from err
 openai.APIConnectionError: Connection error.
To Reproduce
Create an AsyncOpenAI object and define the http_client parameter as the proxy address through httpx.Client
Initiate any asynchronous request
Code snippets
import asynciofrom openai import AsyncOpenAIimport httpx

client = AsyncOpenAI(
    # This is the default and can be omitted
    api_key=<api_key>,
    http_client=httpx.Client(proxies=<proxy_address>)
)

async def main() -> None:
    chat_completion = await client.chat.completions.create(
        messages=[
            {
                ""role"": ""user"",
                ""content"": ""Say this is a test"",
            }
        ],
        model=""gpt-3.5-turbo"",
    )
    print(chat_completion)

asyncio.run(main())
OS
macOS
Python version
Python v3.9.13
Library version
openai v1.9.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1091","openai_object","2024-01-24T02:21:06Z","Closed issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
I am
 from openai import openai_object
it reports the error:
 ImportError: cannot import name 'openai_object' from 'openai'
my openai version is 1.8.0
 I wonder if there exist a version problem
To Reproduce
1.from openai import openai_object
 2.ImportError: cannot import name 'openai_object' from 'openai'
Code snippets
No response
OS
Linux
Python version
Python v3.8
Library version
openai v1.8.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1089","AzureOpenAI client usage is inconsistent with the API description regarding model id","2024-02-03T00:21:51Z","Closed issue","No label","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
Following are examples of chat completions create and assistants create calls:
response = client.chat.completions.create(
 model=model,
 messages=messages
 )
assistant = client.beta.assistants.create(
 name=name,
 instructions=instructions,
 tools=tools,
 model=model,
 file_ids=file_ids
 )
In both API methods, the model parameter is specified as
 model: ID of the model to use. See the model endpoint compatibility table for details on which models work with the Chat API.
The problem is that if client has been created with AzureOpenAI() constructor,
 the required model parameter value is not actually model id but model deployment name (from Azure Portal)
This is problematic because:
Documentation asks for model id, not for deployment name
If customer lists the models using following API client.models.list(), it provides model ids that
 works OK with OpenAI client, but not with AzureOpenAI client (unless the deployment name is the same as model id)
Listing the models using client api is probably common and the values there are expected to work with client APIs.
The problem can be avoided with AzureOpenAI if model deployment name and model id are the same, but it is not
 clear for user always to do that.
To Reproduce
Create Azure OpenAI resource and create model deployment with some name which is not the same as model id(name)
Construct AzureOpenAI client
List the model ids using client.models.list() API
Use e.g. chat completion api with the model id
If the model id is not the same as model deployment name, there will be a ""model deployment not found"" error
Code snippets
No response
OS
Windows
Python version
Python v3.12.1
Library version
openai v1.7.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1082","When debug logging is enabled, api-key header is also printed","2024-01-17T12:58:06Z","Open issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
When debug logging is enabled, FinalRequestOptions is entirely dumped because of the following code:
openai-python/src/openai/_base_client.py
 Lines 436 to 437 in 3d61ed4
	iflog.isEnabledFor(logging.DEBUG): 
	log.debug(""Request options: %s"", model_dump(options, exclude_unset=True)) 
This includes the api-key header that would be great to avoid logging
To Reproduce
Enable debug logging
Get a AsyncAzureOpenAI client and execute any kind of request against the service
See the api-key getting logged as part of the headers
Code snippets
No response
OS
WSL
Python version
Python v3.9.18
Library version
openai v1.7.2
 The text was updated successfully, but these errors were encountered: 
👍2
tekumara and artdent reacted with thumbs up emoji
All reactions
👍2 reactions"
"https://github.com/openai/openai-python/issues/1081","Is there a way to print or log to a file every word that is sent to the OpenAI API, in the same format that the API receives them?","2024-01-24T02:51:12Z","Closed issue","No label","Confirm this is a feature request for the Python library and not the underlying OpenAI API.
 This is a feature request for the Python library
Describe the feature or improvement you're requesting
I would like to be able to view whatever gets sent to the OpenAI API, after it gets sent, as a .json. I would like to see what the OpenAI server sees when my messages arrive to it.
Additional context
This would help with building my own agents using existing framework.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1080","logprobs parameter support for Chat Completion API and GPT-4 model","2024-03-03T01:08:57Z","Closed issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
May you please share a documentation regarding the correct usage of logprobs parameter in Chat Completion API as well as support for the following models.
gpt-4-0613
gpt-4-1106-Preview
Currently, when using the parameter with the above models the following errors are returned by the Chat Completion API. openai version is 1.7.2
gpt-4-06-13
 BadRequestError: Error code: 400 - {'error': {'message': 'Unrecognized request argument supplied: logprobs', 'type': 'invalid_request_error', 'param': None, 'code': None}}
gpt-4-1106
 BadRequestError: Error code: 400 - {'error': {'message': ""This model does not support the 'logprobs' parameter."", 'type': 'invalid_request_error', 'param': 'logprobs', 'code': None}}
To Reproduce
Please see code snippet below.
Code snippets
from openai import AzureOpenAI
 
client = AzureOpenAI(
  azure_endpoint = 'API_ENDPOINT', 
  api_key='API_KEY',  
  api_version='2023-05-15',
  azure_deployment = 'gpt-4-1106'
)

response = client.chat.completions.create(
    model='gpt-4',
    messages=[
        {....}
    ],
    logprobs=True,
    top_logprobs=2
)
response
OS
Windows11
Python version
Python v3.11.2
Library version
openai v1.7.2
 The text was updated successfully, but these errors were encountered: 
👍1
jullybobble reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/openai-python/issues/1078","ChatCompletionToolMessageParam should have an optional name property","2024-01-16T19:41:15Z","Open issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
In the docs for parallel function calling (https://platform.openai.com/docs/guides/function-calling), the name is included along with the tool_call_id, content, and role. All the other messages have a name.
        ... # rest of the example from https://platform.openai.com/docs/guides/function-calling

        for tool_call in tool_calls:
            function_name = tool_call.function.name
            function_to_call = available_functions[function_name]
            function_args = json.loads(tool_call.function.arguments)
            function_response = function_to_call(
                location=function_args.get(""location""),
                unit=function_args.get(""unit""),
            )
            messages.append(
                {
                    ""tool_call_id"": tool_call.id,
                    ""role"": ""tool"",
                    ""name"": function_name,
                    ""content"": function_response,
                }
            )  # extend conversation with function response
        second_response = client.chat.completions.create(
            model=""gpt-3.5-turbo-1106"",
            messages=messages,
        )  # get a new response from the model where it can see the function response
        return second_response
Either the docs need an update or ChatCompletionToolMessageParam needs name (even if Optional).
To Reproduce
Run the following script through mypy
from openai.types.chat import ChatCompletionToolMessageParam

def tool_result(tool_call_id: str, name: str, content: str) -> ChatCompletionToolMessageParam:
    """"""Create a tool result message.    Args:        tool_call_id: The ID of the tool call.        name: The name of the tool.        content: The content of the message.    Returns:        A dictionary representing a tool result message.    """"""
    return {
        ""role"": ""tool"",
        ""content"": content,
        ""name"": name,
        ""tool_call_id"": tool_call_id,
    }

tool_result(""tool_pretend_id"", name=""myfunc"", content=""4"")
Code snippets
from openai.types.chat import ChatCompletionToolMessageParam

def tool_result(tool_call_id: str, name: str, content: str) -> ChatCompletionToolMessageParam:
    """"""Create a tool result message.

    Args:
        tool_call_id: The ID of the tool call.
        name: The name of the tool.
        content: The content of the message.

    Returns:
        A dictionary representing a tool result message.
    """"""
    return {
        ""role"": ""tool"",
        ""content"": content,
        ""name"": name,
        ""tool_call_id"": tool_call_id,
    }

tool_result(""tool_pretend_id"", name=""myfunc"", content=""4"")

OS
macOS
Python version
Python 3.12.1
Library version
openai v1.4.0
 The text was updated successfully, but these errors were encountered: 
👀1
kumapo reacted with eyes emoji
All reactions
👀1 reaction"
"https://github.com/openai/openai-python/issues/1074","Use pydantic models for requests instead of TypedDict","2024-01-16T17:16:51Z","Closed issue","No label","Confirm this is a feature request for the Python library and not the underlying OpenAI API.
 This is a feature request for the Python library
Describe the feature or improvement you're requesting
Hello,
 I have been using library for some time now and lately started to update from 0.x to newest (1.7.x)
 From the start we are using pydantic models for requests and responses handling, mostly due to being very type-safe but also because it works nicely with FastApi.
Let's take chat completion as example.
 Right now you are creating different dictionary type for every role, then you have one common return type which is Union of these possible values.
 Couldn't it be replaced by one pydantic model?
I am wondering why was there a choice to use TypedDict instead of pydantic models for requests objects?
 Searched the issues/PRs but couldn't find anything.
Thanks in advance for response!
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1070","APIStatusError: Error code: 307","2024-01-26T10:40:21Z","Closed issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
APIStatusError Traceback (most recent call last)
 in <cell line: 4>()
 2
 3
 ----> 4 result = client.chat.completions.create(
 5 model=""gpt-4-vision-preview"",
 6 messages=[
4 frames
/usr/local/lib/python3.10/dist-packages/openai/_base_client.py in _request(self, cast_to, options, remaining_retries, stream, stream_cls)
 947
 948 log.debug(""Re-raising status error"")
 --> 949 raise self._make_status_error_from_response(err.response) from None
 950
 951 return self._process_response(
APIStatusError: Error code: 307
My format refers to the demo and other people's submissions. Still reporting an error。
 eg:
To Reproduce
call the api
Code snippets
from openai.types import Image, ImagesResponse


result = client.chat.completions.create(
    model=""gpt-4-vision-preview"",
    messages=[
        {
            ""role"": ""user"",
            ""content"":[
              ""please description img"",
              *map(lambda x: {""image"": x, ""resize"": 768}, base64Frames[0::20]),
            ]
        },
    ],
    max_tokens=200,
)

print(result.choices[0].message.content)
OS
colab
Python version
3
Library version
1.7.2
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1069","ImportError: cannot import name 'OpenAI' from 'openai'","2024-01-24T02:18:15Z","Closed as not planned issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
Exactly this:
ImportError: cannot import name 'OpenAI' from 'openai'

Seems like is just a silly mistake that you didn't notice you had removed such an important part of the package, I had to pin my version to 1.7.1 to avoid this.
To Reproduce
Just update to the latest version (1.7.2) and then try to import using the examples described in the documentation.
Code snippets
No response
OS
Ubuntu
Python version
Python v3.10.12
Library version
openai v1.7.2
 The text was updated successfully, but these errors were encountered: 
👍1
Domxnvk reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/openai-python/issues/1064","embedding behavior inconsistency with different parameters.","2024-03-03T01:08:22Z","Closed as not planned issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
I noticed embeddings behaves differently when we set encoding_format='float' or not.
 I ran the embeddings for chain of thoughts 100 times with and without encoding_format='float' which then give me different embedding results.
If I specify encoding_format='float', all the returned embeddings have the same vector ( list of floats).
 But if I don't specify the parameter, embedding api is using base64 by default and returned me 3 different vectors although the differences among the 3 vectors are very minimal and their cosine distances are neglectable (< 1e-6).
I suspect this inconsistency is caused by base64 encoding of the embedding results which is using numpy to decode.
Reporting this issue so hope team can investigate and fix it.
context:
 openai version: 1.3.4
 python: 3.10.12
 numpy: 1.26.1
 model: text-embedding-ada-002
To Reproduce
run the following 100 times:
openai_client.embeddings.create(input='chain of thoughts', model='text-embedding-ada-002')
run the following 100 times:
openai_client.embeddings.create(input='chain of thoughts', model='text-embedding-ada-002', encoding_format='float')
Code snippets
No response
OS
macos
Python version
Python 3.10.12
Library version
1.3.4
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1063","azure_endpoint logic not appropriate for running Azure LLMs through a proxy","2024-01-13T01:28:47Z","Closed issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
I access my Azure LLM through a gateway proxy service - e.g Microsoft APIM. Its quite opinionated on the format, and I've found that the format is:
https://myserver/proxy/openai/v1/deployments/gpt-4/chat/completions
The new (as in openai v1) standard way of specifying the azure_endpoint should be azure_endpoint=""https://myserver/proxy/openai/v1/""
However, the code in https://github.com/openai/openai-python/blob/main/src/openai/lib/azure.py#L192-L195 specifically adds an /openai on the end of my azure_endpoint - this leads to:
https://myserver/proxy/openai/v1/openai/deployments/gpt-4/chat/completions --> #404
At the moment I notice L183 that I can use base_url instead, and then azure_endpoint wont work, but worth noting that langchain spits warnings that azure_endpoint is the new way forward, and you shouldnt use base_url any more, and llama-index doesnt allow you to set that at all.
All this means if you're accessing Azure LLMs through an API Gateway (as for example happens and is mandatory in heavily regulated environments), you have to do things which may be on their way out.
If you're intending to keep base_url around, and it is not deprecated, I'll raise a bug in langchain telling them to get rid of the deprecation warning.
But I think a better solution is to make the forced addition of the /openai configurable.
To Reproduce
Run this code with openai 1.7.0
import logging
 import os
 logging.basicConfig(level=logging.DEBUG, format=""%(asctime)s - %(levelname)s - %(message)s"")
from openai import AzureOpenAI
client = AzureOpenAI(
 azure_endpoint = ""https://myserver/proxy/openai/v1"",
 api_key = os.getenv(""OPENAI_API_KEY""),
 api_version = ""2023-05-15""
 )
client.chat.completions.create(
 messages = [
 {
 ""role"": ""user"", ""content"": ""whats the capital of France""
 }
 ],
 model = ""gpt-4"",
 temperature = 0.5,
 stream = False
 )
Running this will give you:
2024-01-11 17:44:52,143 - DEBUG - HTTP Request: POST https://myserver/proxy/openai/v1/openai/deployments/gpt-4/chat/completions?api-version=2023-05-15 ""403 Forbidden""
Note the additional /openai
Code snippets
No response
OS
macOS
Python version
3.11.5
Library version
openai v1.7
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1062","Mixing multithreading with async methods hangs when trying to close internal streams","2024-01-18T09:16:28Z","Closed as not planned issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
When creating multiple threads to make completion requests in parallel the script hangs. After adding some debugging logs I can see that it happens in aiter_raw when calling .aclose().
The same code with just one thread works with no issues.
To Reproduce
I've attached a script that reproduces the issue.
Code snippets
import osimport loggingimport threadingimport asynciofrom openai import AsyncAzureOpenAIfrom dotenv import load_dotenv

load_dotenv()


deployment_name = os.environ.get(""AZURE_OPENAI_DEPLOYMENT_NAME"")
client = AsyncAzureOpenAI(
    api_version=os.environ.get(""AZURE_OPENAI_API_VERSION""),
    api_key=os.environ.get(""AZURE_OPENAI_API_KEY""),
    azure_endpoint=os.environ.get(""AZURE_OPENAI_ENDPOINT""),
    azure_deployment=deployment_name,
)

texts = [""Hello"", ""Hola"", ""Bonjour"", ""Hallo"", ""Ciao"", ""Olá"", ""Namaste"", ""Salaam"", ""Zdras-tvuy-te"", ""Konnichiwa"", ""Nǐn hǎo"", ""Guten Tag"", ""Shikamoo"", ""Merhaba"", ""Sa""]

async def detect_language(text: str) -> str:
    completion = await client.chat.completions.create(
        model=""gpt-3.5-turbo-16k"",
        messages=[
            {
                ""role"": ""user"",
                ""content"": f""""""## Given this input:{text}## Do the following:## Using only the input provided by the user, you must identify the input language and provide only the two character language code and nothing else:"""""",
            },
        ],
    )
    
    if completion.choices:
        return completion.choices[0].message.content if completion.choices[0].message.content else ""Unknown""

    return ""Unknown""

def thread_function(loop, text):
    logging.info(""Thread with text '%s': starting"", text)
    asyncio.set_event_loop(loop)
    result = loop.run_until_complete(detect_language(text))
    logging.info(""Thread with text '%s': finishing: Detected language: %s"", text, result)
    return result

def main():
    threads = []

    for index, text in enumerate(texts):
        loop = asyncio.new_event_loop()
        x = threading.Thread(target=thread_function, args=(loop, text,))
        threads.append(x)
        x.start()

    for index, thread in enumerate(threads):
        logging.info(""Before joining thread %d."", index)
        thread.join()
        logging.info(""Thread %d done"", index)

if __name__ == ""__main__"":
    format = ""%(asctime)s: %(message)s""
    logging.basicConfig(format=format, level=logging.INFO,
                        datefmt=""%H:%M:%S"")

    main()
OS
Windows 11
Python version
Python 3.9
Library version
openai 1.6.1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1059","Reusing AsyncOpenAI client results in openai.APIConnectionError","2024-02-14T18:07:51Z","Closed issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
Reusing an instance of AsyncOpenAI client for multiple calls of asyncio.gather results in an openai.APIConnectionError. Retried requests (either via the openai library directly or backoff decorator) succeed, but the first try of the second use of the client always fails.
I suspect that this usage of AsyncOpenAI is not ideal, but the behavior nonetheless feels buggy. Even if the reuse of the client should fail, I'm confused understand why retries succeed.
Bizarrely, in my application all retries after the initial openai.APIConnectionError result in unending openai.APITimeoutError instead of success, but I am unable to repro this outside of the application. However, I strongly suspect that the issue is related as reusing the client solves both the initial error as well as the timeouts.
To Reproduce
Create an instance of AsyncOpenAI with no retries enabled
Use AsyncOpenAI().chat.completions.create to create a list of Future objects (any number will do)
Use asyncio.gather to get the results of the API calls
Executes steps 3+4 again - this causes the error
Code snippets
import asyncioimport openaifrom openai import AsyncOpenAIimport httpximport backoff

print(f""OpenAI version: {openai.__version__}"")

OPENAI_API_KEY = ""redacted""

api_params = {
    ""temperature"": 0.2,
    ""max_tokens"": 500,
    ""model"": ""gpt-3.5-turbo-1106"",
}

messages = [{""role"": ""user"", ""content"": ""What is the capital of Quebec?""}]


@backoff.on_exception(    backoff.expo,    (        openai.RateLimitError,        openai.APIStatusError,        openai.APIConnectionError,        openai.APIError,        openai.APITimeoutError,        openai.InternalServerError,    ),)async def create_request_retry(client, messages, api_params):
    return await client.chat.completions.create(messages=messages, **api_params)


async def create_request_no_retry(client, messages, api_params):
    return await client.chat.completions.create(messages=messages, **api_params)


# No retries, new client for each set of requests - succeedsdef succeed1():
    for i in range(2):
        client = AsyncOpenAI(
            api_key=OPENAI_API_KEY,
            timeout=10.0,
            http_client=httpx.AsyncClient(limits=httpx.Limits(max_keepalive_connections=500, max_connections=100)),
            max_retries=0,
        )
        arequests = []
        for _ in range(5):
            arequests.append(create_request_no_retry(client, messages, api_params))
        responses = asyncio.run(asyncio.gather(*arequests))
        results = [response.choices[0].message.content for response in responses]
        print(f""{i}: {results}"")


# Retry using backoff decorator, reuse client - succeedsdef succeed2():
    client = AsyncOpenAI(
        api_key=OPENAI_API_KEY,
        timeout=10.0,
        http_client=httpx.AsyncClient(limits=httpx.Limits(max_keepalive_connections=500, max_connections=100)),
        max_retries=0,
    )
    for i in range(2):
        arequests = []
        for _ in range(5):
            arequests.append(create_request_retry(client, messages, api_params))
        responses = asyncio.run(asynciogather(*arequests))
        results = [response.choices[0].message.content for response in responses]
        print(f""{i}: {results}"")


# Retry using openai library, reuse client - succeedsdef succeed3():
    client = AsyncOpenAI(
        api_key=OPENAI_API_KEY,
        timeout=10.0,
        http_client=httpx.AsyncClient(limits=httpx.Limits(max_keepalive_connections=500, max_connections=100)),
        max_retries=2,
    )
    for i in range(2):
        arequests = []
        for _ in range(5):
            arequests.append(create_request_no_retry(client, messages, api_params))
        responses = asyncio.run(asynciogather(*arequests))
        results = [response.choices[0].message.content for response in responses]
        print(f""{i}: {results}"")


# No retries, reuse client - failsdef error():
    client = AsyncOpenAI(
        api_key=OPENAI_API_KEY,
        timeout=10.0,
        http_client=httpx.AsyncClient(limits=httpx.Limits(max_keepalive_connections=500, max_connections=100)),
        max_retries=0,
    )
    for i in range(2):
        arequests = []
        for _ in range(5):
            arequests.append(create_request_no_retry(client, messages, api_params))
        responses = asyncio.run(gather(*arequests))
        results = [response.choices[0].message.content for response in responses]
        print(f""{i}: {results}"")
OS
macOS
Python version
Python 3.11.5
Library version
openai v1.6.1
 The text was updated successfully, but these errors were encountered: 
👀1
antont reacted with eyes emoji
All reactions
👀1 reaction"
"https://github.com/openai/openai-python/issues/1056","system_fingerprint is None","2024-01-10T04:08:53Z","Closed as not planned issue","documentation","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
Per the documentation here, the OpenAI API should return a system_fingerprint. However, when calling any model that is not gpt-3.5-turbo-1106 or gpt-4-1106-preview, the system_fingerprint is always None.
The link above doesn't mention any model requirement.
To Reproduce
copy and paste notebook here
 change this line to an older model:
 GPT_MODEL = ""gpt-3.5-turbo-1106""
 For example:
 GPT_MODEL = ""gpt-3.5-turbo-0613""
the system_fingerprint returned is None
Code snippets
No response
OS
Ubuntu
Python version
Python v3.10
Library version
openai v1.6.1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1055","[assistants API] How long are the validity periods of Messages and Runs? #611","2024-03-03T01:08:03Z","Closed as not planned issue","documentation,question","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
[assistants API] How long are the validity periods of Messages and Runs?
 The deletion API of Assistants and Threads, my understanding is that it will always exist if it is not actively deleted.
 As for Messages and Runs, I’m not sure how long they last.
 【assistants API】Messages、Runs的有效期是多久？
 Assistants、Threads的删除的API，我的理解是不主动删除一直存在
 而Messages、Runs的是多久不太确定
To Reproduce
Code snippets
No response
OS
Python version
Library version
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1054","Invalid port error","2024-03-03T01:06:50Z","Closed as not planned issue","No label","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
Hi, When I run the following code, one error raises, any thoughts?
from openai import OpenAI
client = OpenAI(api_key=""XXXXX"")

Raising error:
Traceback (most recent call last):                                                                                                    
  File ""/home/tiger/.local/lib/python3.9/site-packages/httpx/_urlparse.py"", line 339, in normalize_port
    port_as_int = int(port)                                                                                                           
ValueError: invalid literal for int() with base 10: ':1'                                                                              
                                                                                                                                      
During handling of the above exception, another exception occurred: 

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/tiger/.local/lib/python3.9/site-packages/openai/_client.py"", line 106, in __init__
    super().__init__(
  File ""/home/tiger/.local/lib/python3.9/site-packages/openai/_base_client.py"", line 758, in __init__
    self._client = http_client or SyncHttpxClientWrapper(
  File ""/home/tiger/.local/lib/python3.9/site-packages/httpx/_client.py"", line 682, in __init__
    self._mounts: typing.Dict[URLPattern, typing.Optional[BaseTransport]] = {
  File ""/home/tiger/.local/lib/python3.9/site-packages/httpx/_client.py"", line 683, in <dictcomp>
    URLPattern(key): None
  File ""/home/tiger/.local/lib/python3.9/site-packages/httpx/_utils.py"", line 397, in __init__
    url = URL(pattern)
  File ""/home/tiger/.local/lib/python3.9/site-packages/httpx/_urls.py"", line 113, in __init__
    self._uri_reference = urlparse(url, **kwargs)
  File ""/home/tiger/.local/lib/python3.9/site-packages/httpx/_urlparse.py"", line 246, in urlparse
    parsed_port: typing.Optional[int] = normalize_port(port, scheme)
  File ""/home/tiger/.local/lib/python3.9/site-packages/httpx/_urlparse.py"", line 341, in normalize_port
    raise InvalidURL(""Invalid port"")
httpx.InvalidURL: Invalid port

To Reproduce
from openai import OpenAI
 client = OpenAI(api_key=""XXXXX"")
Code snippets
No response
OS
unbuntu
Python version
python 3.9
Library version
openai 1.6.0
 The text was updated successfully, but these errors were encountered: 
👍1
ycjcl868 reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/openai-python/issues/1053","Return usage when streaming chat completions.","2024-01-10T04:18:37Z","Closed issue","No label","Confirm this is a feature request for the Python library and not the underlying OpenAI API.
 This is a feature request for the Python library
Describe the feature or improvement you're requesting
Is it possible to return token usage when streaming in the final chunk?
otherwise we need to also use TikToken in an application where both streaming is enabled vs disabled.
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
👍2
jackmpcollins and Lawouach reacted with thumbs up emoji
All reactions
👍2 reactions"
"https://github.com/openai/openai-python/issues/1052","Function calling using the assistants API is partially broken","2024-01-16T15:05:06Z","Closed issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
when using the assistants api, with the function calling tool enabled, and the call request is received in the step_details.tool_calls list, I get a dictionary instead of a FunctionToolCall object for function, this seems to be related to the fact that the dict/json's ""function"" sub dictionary is missing the output field, but I'm not sure.
To Reproduce
I'm using openai==1.6.1
 just run the minimalistic script I wrote to reproduce the probem.
you'll end up with the following exception:
Traceback (most recent call last):
  File ""/home/user/Documents/dev/perso/gpt-api-tests/run-test.py"", line 65, in <module>
    if (tool.type == ""function""):
        ^^^^^^^^^


Code snippets
from openai import OpenAIimport os

client = OpenAI(api_key=os.environ[""OPENAI_API_KEY""])

assistant = client.beta.assistants.create(
    name=""Doc"",
    instructions=""You are Doc, a personal assistant. You can write code and execute it to to help the user. You can also help the user using your knowledge base, you can send email with the function tool"",
    tools=[
        {""type"": ""code_interpreter""},
        {""type"": ""retrieval""},
        {""type"": ""function"", ""function"": {
            ""name"": ""send_email"",
            ""description"": ""Send an email to the recipient"",
            ""parameters"": {
                ""type"": ""object"",
                ""properties"": {
                    ""recipient"": {
                        ""type"": ""string"",
                        ""description"": ""The recipient email address""
                    },
                    ""subject"": {
                        ""type"": ""string"",
                        ""description"": ""The email subject""
                    },
                    ""body"": {
                        ""type"": ""string"",
                        ""description"": ""Body of the email""
                    }
                }
            }
        }}
    ],
    model=""gpt-4-1106-preview"",
)

thread = client.beta.threads.create()

message = client.beta.threads.messages.create(
    thread_id=thread.id,
    role=""user"",
    content=""Hello, using python, generate the first 100 fibonacci numbers. when done, send the result by email to foo@bar.com"",
)

run = client.beta.threads.runs.create(
    thread_id=thread.id,
    assistant_id=assistant.id,
)

while (run := client.beta.threads.runs.retrieve(
    thread_id=thread.id,
    run_id=run.id,
)).status in ('queued', 'in_progress'):
    run_steps = client.beta.threads.runs.steps.list(
        thread_id=thread.id,
        run_id=run.id
    )

    for step in run_steps.data:
        if (step.type != 'tool_calls'):
            continue
        if (step.step_details.type != 'tool_calls'):
            continue
        for tool in step.step_details.tool_calls:
            if (tool.type == ""function""):
                print(""function call "")



### OS

Manjaro Linux

### Python version

3.11.6

### Library version

openai-1.6.1

 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1051","Support on setting the API_BASE variable","2024-01-10T04:28:08Z","Closed issue","No label","Confirm this is a feature request for the Python library and not the underlying OpenAI API.
 This is a feature request for the Python library
Describe the feature or improvement you're requesting
Hi Openai developers!
In the new version of openai-python package (>=1.0), I find it rather inconvenient to set the OPENAI_API_BASE variable. That is to say, it is impossible to set the variable with something either like:
client = OpenAI(
    api_base = """",
    api_key = """"
)

or export the variable in the terminal like:
export OPENAI_API_BASE=
But in order versions, one can conveniently set the variable through the Python script, using something like openai.api_base=
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1045","Limit not working when listing messages","2024-01-10T04:50:30Z","Closed as not planned issue","No label","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
I'm not able to list messages of a thread filtering by a certain limit.
To Reproduce
See the code snippet below.
Code snippets
# Assuming we have 20 messages in a thread with id ""XYZ"".


raw_messages = []
async for m in assistant.async_beta_client.threads.messages.list(""XYZ"", order=""asc"", limit=1):
    print(m)
    raw_messages.append(m) 
print(len(raw_messages))

# At the end of this code, the result is 20 and the loop runs 20 times. I was expecting a single result.
OS
macOS
Python version
Python v3.11
Library version
openai v1.6.1 (but also previous versions)
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1044","joint translation and transcription","2024-01-10T04:57:25Z","Closed as not planned issue","No label","I would like to keep track of a English speech history no matter the spoken language.
Is it possible to achieve joint translation and transcription with the current whisper model API?
Thus far we have experimented with the following two approaches:
anylanguage-to-English translations API + transcriptions API
transcriptions API + GPT-4 turbo completions API for anylanguage-to-English
It would be nice to have a unified way to achieve that; am I missing something with the API as-is?
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1040","Async with tools should be improved","2024-01-03T17:00:27Z","Closed as not planned issue","bug","No description provided.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1039","Pydantic warnings at every base64 embedding call for Async Azure","2024-01-03T06:54:12Z","Open issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
I see several warnings with this:
UserWarning: Pydantic serializer warnings:
  Expected `list[float]` but got `str` - serialized value may not be as expected
  return self.__pydantic_serializer__.to_json

This is impacting LiteLLM users too: BerriAI/litellm#1187
To Reproduce
import openai

    client = openai.AsyncAzureOpenAI(
        api_key=os.environ[""AZURE_API_KEY""],
        azure_endpoint=os.environ[""AZURE_API_BASE""]
    )

    async def _test():
        response = await client.embeddings.create(
                model=""azure-embedding-model"",
                input=[""write a litellm poem""],
                encoding_format=""base64""

        )

        print(response)

        response = response.model_dump_json()

        print(response)
        import json
        response = json.loads(response)
        print(response)
    import asyncio
    asyncio.run(_test())
Code snippets
No response
OS
macOS
Python version
3.10
Library version
1.6.1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1038","system_fingerprint is None","2024-01-02T23:35:31Z","Closed issue","No label","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
Per the documentation here, the OpenAI API should return a system_fingerprint. However, when calling any model that is not gpt-3.5-turbo-1106 or gpt-4-1106-preview, the system_fingerprint is always None.
Is this working as designed? Is this parameter only supported in these two models? If so, can this requirement be documented? The link above doesn't mention this requirement and neither does the API documentation.
To Reproduce
copy and paste notebook here
change this line to an older model:
 GPT_MODEL = ""gpt-3.5-turbo-1106""
For example:
 GPT_MODEL = ""gpt-3.5-turbo-0613""
the system_fingerprint returned is None
Code snippets
No response
OS
Unix
Python version
Python v3.10
Library version
openai v1.6.1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1036","Inconsistent performance of chat api","2024-01-02T17:57:00Z","Closed as not planned issue","No label","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
When call the client.chat.completions.create(..., logprobs=True) with gpt-3.5-turbo-1106
 Sometime it will raise below error
Error code: 400 - {'error': {'message': ""This model does not support the 'logprobs' parameter."", 'type': 'invalid_request_error', 'param': 'logprobs', 'code': None}}

To Reproduce
Run the code snippets in jupyter about 5~8 times, sometime it run correctly, somtime will raise an error.
https://platform.openai.com/docs/api-reference/chat/create?lang=python
Code snippets
from openai import OpenAIclient = OpenAI()

completion = client.chat.completions.create(
  model=""gpt-3.5-turbo"",
  messages=[
    {""role"": ""user"", ""content"": ""Hello!""}
  ],
  logprobs=True,
  top_logprobs=2
)
OS
Linux
Python version
python3.9.13
Library version
openai v1.6.1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1035","openai api audio.transcriptions.create not working when response format is not json","2024-07-22T10:39:56Z","Closed issue","bug,CLI,good first issue","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
If you specify --response-format to a format other than json (like srt or vtt), the openai api audio.transcriptions.create command will fail with error
To Reproduce
Install openai python library version v1.6.1
Prepare an audio file, such as a recording saying ""hello""
Run OPENAI_API_KEY=<YOUR_API_KEY> openai api audio.transcriptions.create -f 1.m4a --response-format vtt
You can see the error message
Traceback (most recent call last):
  File ""/usr/local/bin/openai"", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File ""/usr/local/lib/python3.11/site-packages/openai/cli/_cli.py"", line 129, in main
    _main()
  File ""/usr/local/lib/python3.11/site-packages/openai/cli/_cli.py"", line 209, in _main
    parsed.func(
  File ""/usr/local/lib/python3.11/site-packages/openai/cli/_api/audio.py"", line 78, in transcribe
    print_model(model)
  File ""/usr/local/lib/python3.11/site-packages/openai/cli/_utils.py"", line 36, in print_model
    sys.stdout.write(model_json(model, indent=2) + ""\n"")
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/site-packages/openai/_compat.py"", line 125, in model_json
    return model.model_dump_json(indent=indent)
           ^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'model_dump_json'

I can also reproduce this using python:3.12 Docker image.
Code snippets
No response
OS
macOS
Python version
Python 3.11.6
Library version
v1.6.1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1033","module openai' has no attribute 'error'","2023-12-29T12:20:19Z","Closed as not planned issue","question","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
I'm using the ""sheetcopilot"" and using the openai api. Whe running the python file, I got this problem: Cause: module 'openai' has no attribute 'error'.
My used model version is: Model: gpt-3.5-turbo
 My installed openai version is: openai 1.6.1
Could you please help me solve this problem. thanks a lot!
To Reproduce
Whe running the python file, I got this problem: Cause: module 'openai' has no attribute 'error'.
Code snippets
No response
OS
wins
Python version
3.10.13
Library version
1.6.1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1030","An error occured","2023-12-29T21:29:20Z","Closed issue","question","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
i got this error named ""No error handlers are registered, logging exception.
 Traceback (most recent call last):
 File ""/data/user/0/ru.iiec.pydroid3/files/aarch64-linux-android/lib/python3.11/site-packages/telegram/ext/dispatcher.py"", line 555, in process_update
 handler.handle_update(update, self, check, context)
 File ""/data/user/0/ru.iiec.pydroid3/files/aarch64-linux-android/lib/python3.11/site-packages/telegram/ext/handler.py"", line 198, in handle_update
 return self.callback(update, context)
 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
 File """", line 17, in handle_message
 File ""/data/user/0/ru.iiec.pydroid3/files/aarch64-linux-android/lib/python3.11/site-packages/openai/lib/_old_api.py"", line 39, in call
 raise APIRemovedInV1(symbol=self._symbol)
 openai.lib._old_api.APIRemovedInV1:
You tried to access openai.Completion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.
You can run openai migrate to automatically upgrade your codebase to use the 1.0.0 interface.
Alternatively, you can pin your installation to the old version, e.g. pip install openai==0.28
A detailed migration guide is available here: #742"" and it said that the i tried to access openAI.Completion but its no longer supported in openai>=1.0.0
To Reproduce
error ""No error handlers are registered, logging exception.
 Traceback (most recent call last):
 File ""/data/user/0/ru.iiec.pydroid3/files/aarch64-linux-android/lib/python3.11/site-packages/telegram/ext/dispatcher.py"", line 555, in process_update
 handler.handle_update(update, self, check, context)
 File ""/data/user/0/ru.iiec.pydroid3/files/aarch64-linux-android/lib/python3.11/site-packages/telegram/ext/handler.py"", line 198, in handle_update
 return self.callback(update, context)
 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
 File """", line 17, in handle_message
 File ""/data/user/0/ru.iiec.pydroid3/files/aarch64-linux-android/lib/python3.11/site-packages/openai/lib/_old_api.py"", line 39, in call
 raise APIRemovedInV1(symbol=self._symbol)
 openai.lib._old_api.APIRemovedInV1:
You tried to access openai.Completion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.
You can run openai migrate to automatically upgrade your codebase to use the 1.0.0 interface.
Alternatively, you can pin your installation to the old version, e.g. pip install openai==0.28
A detailed migration guide is available here: #742""
Code snippets
import telegram.extimport openai

# Telegram bot token received from BotFathertelegram_token = 'YOUR_BOT_TOKEN'

# OpenAI GPT API keyopenai.api_key = 'YOUR_API_TOKEN'

# Initialize the Telegram botbot = telegram.Bot(token=telegram_token)

# Function to handle incoming messagesdef handle_message(update, context):
    message = update.message.text  # Get user's message
    # Process the message using GPT-3
    response = openai.Completion.create(
        engine=""text-davinci-003"",
        prompt=message,
        max_tokens=50  # Adjust the number of tokens for the length of the response
    )
    generated_text = response.choices[0].text.strip()  # Get GPT's response

    # Send the generated response back to the user
    update.message.reply_text(generated_text)

# Function to handle the /start commanddef start(update, context):
    update.message.reply_text('Hello! I am your ChatGPT bot. Send me a message, and I will respond.')

# Function to handle the /help commanddef help(update, context):
    update.message.reply_text('This bot uses AI to generate responses. Just start chatting!')

# Set command handlersupdater = telegram.ext.Updater(token=telegram_token)
updater.dispatcher.add_handler(telegram.ext.CommandHandler('start', start))
updater.dispatcher.add_handler(telegram.ext.CommandHandler('help', help))
updater.dispatcher.add_handler(telegram.ext.MessageHandler(telegram.ext.Filters.text & (~telegram.ext.Filters.command), handle_message))
updater.start_polling()
updater.idle()
OS
Android
Python version
PyDroid3
Library version
idk
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1026","Auth flow of custom http client not invoked when making a request","2023-12-29T21:31:34Z","Closed issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
When passing a custom http client to the OpenAI client, it does not properly use the custom client's auth flow.
In the client's _request method, self.custom_auth is passed to self._client.send (link to source code).
The property custom_auth however is simply defined as return None (link to source code).
Because we explicitly pass None to the auth argument of the send method of the httpx client, the client doesn't use the default value USE_CLIENT_DEFAULT for the auth argument (link to source code).
Is there a reason why we have this custom_auth property on the client? If we do not pass it to the auth argument of self._client.send, everything works as expected and the auth logic of the custom httpx client is properly invoked.
To Reproduce
openai_client = OpenAI(
    api_key=""<API_KEY>"",
    base_url=""<BASE_URL>"",
    http_client=""<HTTP_CLIENT>"",
)

completion = openai_client.chat.completions.create(
    messages=[
        {
            ""role"": ""user"",
            ""content"": ""Last name of the president of the United States"",
        }
    ],
    model=""gpt-3.5-turbo"",
)  # Does not invoke the authentication flow of the client passed using `http_client`.
Code snippets
No response
OS
MacOS 14.2
Python version
3.11.6
Library version
1.3.9
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1025","Support for real time audio streaming using chunk transfer encoding for Whisper","2023-12-26T16:48:50Z","Open issue","enhancement","Confirm this is a feature request for the Python library and not the underlying OpenAI API.
 This is a feature request for the Python library
Describe the feature or improvement you're requesting
It would be nice to start data transfer as soon as it becomes available for the real-time voice recognition.
 We already have a similar feature for tts: https://platform.openai.com/docs/guides/text-to-speech/streaming-real-time-audio
 Please note, I am not saying that a transcript should be available before the speech ended. But I would like to start the data transfer earlier.
Additional context
The HTTP supports sending files in chunks without knowing the length in advance.
 A WAV header does require the length, however 0xFFFFFFFF (i.e. max length) works fine with Whisper (I checked).
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1023","Fail to create message in assistant thread","2024-03-03T00:50:44Z","Closed as not planned issue","API-feedback,question","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
 def add_messages(self, room_id, prompt, file_ids: List[str] | NotGiven = NotGiven): thread = self.get_thread(room_id) message = self.client.beta.threads.messages.create( thread_id=thread.id, role=""user"", content=prompt, metadata={'room_id': room_id}, file_ids=file_ids, ) mlogger.info(f'[add_messages]room_id:{room_id} thread_id:{thread.id} message_id:{message.id}') return message
 I've finished my test before. But I got this error these days.
 openai.BadRequestError: Error code: 400 - {'error': {'message': ""Can't add messages to thread_jD1dj44APTovq48PCBZACOGV while a run run_KfVG8fSJg882VHYeewFQ2BHk is active."", 'type': 'invalid_request_error', 'param': None, 'code': None}}
 I'v make sure the thread was just created .
To Reproduce
get assistant from list assistant
get file ids from file list
create thread or get thread id from cache
message create with content 、file_ids 、 metadata
call run.create
see error
Code snippets
def reply(self, room_id, prompt, payload: dict):
        file_ids = self._get_file_ids(payload, room_id)
        # get file ids from redis
        try:
            self.repo.add_messages(room_id, prompt, file_ids)
            self.repo.create_run(room_id)
        except Exception as e:
            mlogger.exception(e)
        yield from self._get_answer(room_id, payload)
    def get_thread(self, room_id):
        thread_id = room_id_to_thread_id.get(room_id)
        if thread_id:
            return self.client.beta.threads.retrieve(thread_id=thread_id)
        thread = self.client.beta.threads.create()
        room_id_to_thread_id.set(key=room_id, value=thread.id)
        return thread

    def add_messages(self, room_id, prompt, file_ids: List[str] | NotGiven = NotGiven):
        thread = self.get_thread(room_id)
        message = self.client.beta.threads.messages.create(
            thread_id=thread.id,
            role=""user"",
            content=prompt,
            metadata={'room_id': room_id},
            file_ids=file_ids,
        )
        mlogger.info(f'[add_messages]room_id:{room_id} thread_id:{thread.id}  message_id:{message.id}')
        return message

    def create_run(self, room_id):
        thread = self.get_thread(room_id)
        run = self.client.beta.threads.runs.create(
            thread_id=thread.id,
            assistant_id=self.assistant.id,
            metadata={'room_id': room_id}
        )
        mlogger.info(f'[create_run]room_id:{room_id} thread_id:{thread.id} run_id:{run.id}')
        room_id_to_run_id[room_id] = run.id
        return run
OS
win11
Python version
Python 3.11.2
Library version
openai v1.6.1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1021","Addition of plotly as a dependency for installing OpenAI API","2023-12-26T18:57:47Z","Closed issue","No label","Confirm this is a feature request for the Python library and not the underlying OpenAI API.
 This is a feature request for the Python library
Describe the feature or improvement you're requesting
While working with OpenAI API's embeddings_utils module, I ran into the following error while importing the get_embeddings and get_embedding function from it.
Hence, I think plotly should be added as a dependency for installing OpenAI API.
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1006","Missing default value to logprobs in openai.types.chat.chat_completion.Choice","2023-12-23T23:25:46Z","Closed issue","enhancement","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
#980 added token logprobs to chat completions of type Optional[ChoiceLogprobs] in openai.types.chat.chat_completion.Choice and openai.types.chat.chat_completion_chunk.Choice. In the latter, the default value is set to None, while in the former it is not set. This causes backward compatibility problems with code written for versions prior to 1.5.0.
To Reproduce
Execution of the following code fails:
from openai.types.chat.chat_completion import ChatCompletionMessage, Choice

msg = ChatCompletionMessage(role=""assistant"", content="""")

Choice(
    index=0,
    finish_reason=""stop"",
    message=msg,
)
The output
----> 1 Choice(
      2     index=0,
      3     finish_reason=""stop"",
      4     message=msg,
      5 )

File /.venv-3.10/lib/python3.10/site-packages/pydantic/main.py:164, in BaseModel.__init__(__pydantic_self__, **data)
    162 # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks
    163 __tracebackhide__ = True
--> 164 __pydantic_self__.__pydantic_validator__.validate_python(data, self_instance=__pydantic_self__)

ValidationError: 1 validation error for Choice
logprobs
  Field required [type=missing, input_value={'index': 0, 'finish_reas...=None, tool_calls=None)}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.5/v/missing

Setting logprobs to None fixes the problem.
from openai.types.chat.chat_completion import ChatCompletionMessage, Choice

msg = ChatCompletionMessage(role=""assistant"", content="""")

Choice(
    index=0,
    finish_reason=""stop"",
    message=msg,
    logprobs=None # added line
)
Code snippets
see above
OS
Linux
Python version
Python 3.10.13
Library version
openai 1.6.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1005","Async client Azure Support with Streaming not working (using AsyncAzureOpenAI with stream=True)","2023-12-26T18:51:16Z","Closed issue","No label","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
import os
 import openai
 import asyncio
 from openai import AzureOpenAI, AsyncAzureOpenAI
import os
 import asyncio
 from openai import AsyncAzureOpenAI
azure_openai_client = AsyncAzureOpenAI(
 azure_endpoint = """",
 api_key=""some-key"",
 api_version=""2023-07-01-preview""
 )
async def get_response(message):
 response = await azure_openai_client.chat.completions.create(
 model = 'GPT35',
 temperature = 0.4,
 messages = [
 {""role"": ""user"", ""content"": message}
 ],
 stream=True
 )
 #print(response.model_dump_json(indent=2)) - > no response
# async for chunk in response: - **Not working**
if ""choices"" in chunk and len(chunk[""choices""]) > 0:
  print(chunk.choices[0].message.content, end="""") # chunk.choices[0].delta.content - this also not working

asyncio.run(get_response('What is chatgpt?'))
To Reproduce
import os
 import openai
 import asyncio
 from openai import AzureOpenAI, AsyncAzureOpenAI
import os
 import asyncio
 from openai import AsyncAzureOpenAI
azure_openai_client = AsyncAzureOpenAI(
 azure_endpoint = """",
 api_key=""some-key"",
 api_version=""2023-07-01-preview""
 )
async def get_response(message):
 response = await azure_openai_client.chat.completions.create(
 model = 'GPT35',
 temperature = 0.4,
 messages = [
 {""role"": ""user"", ""content"": message}
 ],
 stream=True
 )
 #print(response.model_dump_json(indent=2)) - > not response
# async for chunk in response: - Not working
if ""choices"" in chunk and len(chunk[""choices""]) > 0:
  print(chunk.choices[0].message.content, end="""") # chunk.choices[0].delta.content - this also not working

asyncio.run(get_response('What is chatgpt?'))
Code snippets
import osimport openaiimport asynciofrom openai import AzureOpenAI, AsyncAzureOpenAI

import osimport asynciofrom openai import AsyncAzureOpenAI


azure_openai_client = AsyncAzureOpenAI(
  azure_endpoint = """", 
  api_key=""some-key"",  
  api_version=""2023-07-01-preview""
)


async def get_response(message):
    response = await azure_openai_client.chat.completions.create(
        model = 'GPT35',
        temperature = 0.4,
        messages = [
            {""role"": ""user"", ""content"": message}
        ],
        stream=True
    )
    #print(response.model_dump_json(indent=2)) - > not response

    # async for chunk in response: - Not working
    if ""choices"" in chunk and len(chunk[""choices""]) > 0:
      print(chunk.choices[0].message.content, end="""") # chunk.choices[0].delta.content - this also not working


asyncio.run(get_response('What is chatgpt?'))
OS
Windows 10 Enterprise
Python version
Python 3.8.10
Library version
1.3.9 or 1.6.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1004","openai.APITimeoutError: Request timed out.","2023-12-23T00:36:23Z","Closed as not planned issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
Traceback (most recent call last):
 File ""C:/Users/14499/PycharmProjects/chatgpt.py"", line 10, in 
 stream=True,
 File ""C:\Users\14499\AppData\Roaming\Python\Python37\site-packages\openai_utils_utils.py"", line 270, in wrapper
 return func(*args, **kwargs)
 File ""C:\Users\14499\AppData\Roaming\Python\Python37\site-packages\openai\resources\chat\completions.py"", line 677, in create
 stream_cls=Stream[ChatCompletionChunk],
 File ""C:\Users\14499\AppData\Roaming\Python\Python37\site-packages\openai_base_client.py"", line 1088, in post
 return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
 File ""C:\Users\14499\AppData\Roaming\Python\Python37\site-packages\openai_base_client.py"", line 858, in request
 remaining_retries=remaining_retries,
 File ""C:\Users\14499\AppData\Roaming\Python\Python37\site-packages\openai_base_client.py"", line 890, in _request
 response_headers=None,
 File ""C:\Users\14499\AppData\Roaming\Python\Python37\site-packages\openai_base_client.py"", line 963, in _retry_request
 stream_cls=stream_cls,
 File ""C:\Users\14499\AppData\Roaming\Python\Python37\site-packages\openai_base_client.py"", line 890, in _request
 response_headers=None,
 File ""C:\Users\14499\AppData\Roaming\Python\Python37\site-packages\openai_base_client.py"", line 963, in _retry_request
 stream_cls=stream_cls,
 File ""C:\Users\14499\AppData\Roaming\Python\Python37\site-packages\openai_base_client.py"", line 893, in _request
 raise APITimeoutError(request=request) from err
 openai.APITimeoutError: Request timed out.
Process finished with exit code 1
To Reproduce
Traceback (most recent call last):
 File ""C:/Users/14499/PycharmProjects/chatgpt.py"", line 10, in 
 stream=True,
 File ""C:\Users\14499\AppData\Roaming\Python\Python37\site-packages\openai_utils_utils.py"", line 270, in wrapper
 return func(*args, **kwargs)
 File ""C:\Users\14499\AppData\Roaming\Python\Python37\site-packages\openai\resources\chat\completions.py"", line 677, in create
 stream_cls=Stream[ChatCompletionChunk],
 File ""C:\Users\14499\AppData\Roaming\Python\Python37\site-packages\openai_base_client.py"", line 1088, in post
 return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
 File ""C:\Users\14499\AppData\Roaming\Python\Python37\site-packages\openai_base_client.py"", line 858, in request
 remaining_retries=remaining_retries,
 File ""C:\Users\14499\AppData\Roaming\Python\Python37\site-packages\openai_base_client.py"", line 890, in _request
 response_headers=None,
 File ""C:\Users\14499\AppData\Roaming\Python\Python37\site-packages\openai_base_client.py"", line 963, in _retry_request
 stream_cls=stream_cls,
 File ""C:\Users\14499\AppData\Roaming\Python\Python37\site-packages\openai_base_client.py"", line 890, in _request
 response_headers=None,
 File ""C:\Users\14499\AppData\Roaming\Python\Python37\site-packages\openai_base_client.py"", line 963, in _retry_request
 stream_cls=stream_cls,
 File ""C:\Users\14499\AppData\Roaming\Python\Python37\site-packages\openai_base_client.py"", line 893, in _request
 raise APITimeoutError(request=request) from err
 openai.APITimeoutError: Request timed out.
Process finished with exit code 1
Code snippets
from openai import OpenAI

client = OpenAI(api_key='sk-xxxxx')

stream = client.chat.completions.create(
    model=""gpt-3.5-turbo"",
    messages=[{""role"": ""user"", ""content"": ""你能做什么""}],
    stream=True,
)
for chunk in stream:
    print(chunk.choices[0].delta.content or """", end="""")
OS
windows11
Python version
python3.7
Library version
1.6.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/999","Stuck on how to output max number of characters","2023-12-21T18:52:04Z","Closed issue","No label","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
What I'm trying to do:
 I'm trying to output a max number of tokens (ex:600 tokens) in the AI response from a variable input.
Example below:
 Input
 500-1000 tokens
 output
 <600 tokens
Via documentation (https://platform.openai.com/docs/api-reference/completions/create#chat/create-max_tokens)
max_tokens
The maximum number of [tokens]that can be generated in the completion.
The token count of your prompt plus max_tokens cannot exceed the model's context length.
Question:
 It looks like the only way to control the tokens used is the max_token variable however it's the total tokens (input+output). Is there a way to control the max tokens of the output?
To Reproduce
response = openai.ChatCompletion.create(
    model=""gpt-4"",
    messages=[
        {""role"": ""system"", ""content"": context_template},
        {""role"": ""user"", ""content"": prompt}
    ],
    max_tokens=600
)

Code snippets
No response
OS
macOS
Python version
3.9.6
Library version
openai=1.60
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/996","AsyncCompletions.create not recognized as coroutine by inspect.iscoroutinefunction","2023-12-23T22:41:30Z","Closed issue","No label","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
The AsyncCompletions.create method in the openai-python library is an asynchronous method that one would expect to be recognized as a coroutine by the inspect.iscoroutinefunction. However, checks against this method currently return False, suggesting that it is not being identified as an async function. This could lead to issues in asynchronous programming where the correct identification of coroutines is crucial for event loops and async frameworks to schedule and execute tasks appropriately.
To Reproduce
Install the latest version of the OpenAI Python library.
Use the inspect.iscoroutinefunction to check if AsyncCompletions.create is recognized as a coroutine.
The result of this check is False instead of the expected True.
Code snippets
import inspectimport openai

is_coroutine = inspect.iscoroutinefunction(openai.resources.AsyncCompletions.create)
print(f'Is `openai.resources.AsyncCompletions.create` a coroutine function? {is_coroutine}')
OS
Windows 11
Python version
Python v3.9.18
Library version
openai v1.5.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/990","Multiple document upload assistant update seems not working","2024-03-03T00:51:46Z","Closed issue","No label","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
I'm trying to upload multiple files on openai Assistant.
I didn't find the way to upload just one file to a set of existing file already uploaded to the assistant.
So if you want to add just one file you need to upload a list of already uploaded files plus the new one you want to upload
The problem in my case is that proceeding as above it upload just 2 files of the list.
To Reproduce
Create file n°1:
file_1 = client.files.create(
    file=open(""'Test_1.txt', ""rb""),
    purpose=""assistants""
)

Create file n°2:
file_2 = client.files.create(
    file=open(""'Test_2.txt', ""rb""),
    purpose=""assistants""
)

Create file n°3:
file_3 = client.files.create(
    file=open(""'Test_3.txt', ""rb""),
    purpose=""assistants""
)

Then we can update the assistant with 3 files:
assistant = client.beta.assistants.update(
    assistant_id=""asst_ ........ Dmo3S"",
    file_ids=[file_1.id , file_2.id, file_3.id]
)

Than if you go in the assistant you see that only 2 documents are uploaded (attached image)
 Let me know if it is a bug or I missed something.
Thanks

Code snippets
No response
OS
Windows 10
Python version
Python v3.12
Library version
openai 1.5.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/987","APIRemovedInV1: You tried to access openai.ChatCompletion, but this is no longer supported","2023-12-19T03:12:15Z","Closed issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
I'm trying to create a simple BOT however getting the below error.
 APIRemovedInV1: You tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API. You can run openai migrate to automatically upgrade your codebase to use the 1.0.0 interface. Alternatively, you can pin your installation to the old version, e.g. pip install openai==0.28 A detailed migration guide is available here: #742
Validations:
 Environment Check done
 Upgraded OpenAI Python Library
 Restarted Kernel
 Validated API key
To Reproduce
Code attached
Code snippets
# Streamlit UIst.title(""Vitamin Quiz Bot"")

# Initialize session stateif 'iteration' not in st.session_state:
    st.session_state.iteration = 0
    st.session_state.quiz_active = False

# Main loop for the quizuser_input = st.text_input(""You (Iteration {}):"".format(st.session_state.iteration))

if user_input.lower() == ""lets begin the quiz"":
    st.session_state.quiz_active = True

if st.session_state.quiz_active:
    # Generate a question
    chat_history = [
        {""role"": ""system"", ""content"": ""You are a helpful assistant.""},
        {""role"": ""user"", ""content"": ""Generate a question about vitamins and minerals for a 3rd-grade quiz.""},
    ]
    response = openai.ChatCompletion.create(
        model=""gpt-3.5-turbo"",
        messages=chat_history,
        max_tokens=50,
        temperature=0.7,
    )
    question = response['choices'][0]['message']['content'].strip()

    # Display the question
    st.text(""Bot: {}"".format(question))

    # Get user's response
    user_response = st.text_input(""You:"")

    # Evaluate the response
    if st.button(""Submit""):
        st.text(""You said: {}"".format(user_response))
OS
windows
Python version
Python 3.12.1
Library version
openai 1.5.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/975","client.chat.completions.create() not taking logprobs and top_logprobs as arguments","2023-12-19T03:35:50Z","Closed issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
I just updated to 1.4.0, and the client.chat.completions.create() in python library is not taking logprobs or top_logprobs as arguments but these arguments are already enabled if I access them through http request.
To Reproduce
To reproduce:
from openai import OpenAI
client = OpenAI()

completion = client.chat.completions.create(
  model=""gpt-3.5-turbo"",
  messages=[
    {""role"": ""system"", ""content"": ""You are a helpful assistant.""},
    {""role"": ""user"", ""content"": ""Hello!""}
  ],
 logprobs=True,
)

Then it will return:
TypeError: Completions.create() got an unexpected keyword argument 'logprobs'
But you can use:
import requests

url = ""https://api.openai.com/v1/chat/completions""
headers = {
    ""Content-Type"": ""application/json"",
    ""Authorization"": f""Bearer {OPENAI_API_KEY}""
}
data = {
    ""model"": ""gpt-3.5-turbo"",
    ""messages"": [
        {
            ""role"": ""system"",
            ""content"": ""You are a helpful assistant.""
        },
        {
            ""role"": ""user"",
            ""content"": ""Hello!""
        }
    ],
    'logprobs': True,
}

response = requests.post(url, headers=headers, json=data)

And you will get a response with logprobs.
Code snippets
No response
OS
macOS
Python version
Python v3.11.5
Library version
openai v1.4.0
 The text was updated successfully, but these errors were encountered: 
👍2
TortillasAlfred and joschkabraun reacted with thumbs up emoji
All reactions
👍2 reactions"
"https://github.com/openai/openai-python/issues/971","Async API server starts throwing errors and works fine after restart","2023-12-23T23:29:16Z","Closed issue","No label","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
All the requests to the open ai end point start throwing timeouts. This changed after doing two things
Upgrading to the new GPT_3_5_TURBO_1106 model as well as using the async client.
I reverted the GPT_3_5_TURBO_1106 change but the issue is still happening
I am using openai = ""^1.3.9""
To Reproduce
Use a combination of above + Fastify
 After some number of requests we start getting timeouts
Code snippets
No response
OS
macOs
Python version
3.11.5
Library version
1.3.9
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/969","Interrupt streaming for chat completions using the API like ChatGPT","2023-12-15T02:07:58Z","Closed issue","No label","Confirm this is a feature request for the Python library and not the underlying OpenAI API.
 This is a feature request for the Python library
Describe the feature or improvement you're requesting
Hello, I saw this thread on the forums here to allow for the ability to stop or interrupt the generation using the chat assistants API. There does not seem like there is anything in the docs to allow to work. Is it possible this can be implemented in the future to save tokens and generations on streaming responses?
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/965","Error with the openai library","2023-12-15T03:21:00Z","Closed issue","No label","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
I tried a simple chat requestion and throws this error.
 I have installed pip 3 and python 3.11
 I have updated the environment variables with my API KEY
import os
 from openai import OpenAI
 #from openai.types import FunctionDefinition, FunctionParameters
client = OpenAI(
 # This is the default and can be omittedpip uninstall openai
 api_key=os.environ.get(""OPENAI_API_KEY""),
 )
chat_completion = client.chat.completions.create(
 model=""gpt-3.5-turbo"",
 messages = [
 { ""role"": ""user"", ""content"": ""Write a tagline for an ice cream shop."" }
 ]
 )
 print(chat_completion['choices'][0]['message']['content'])
ERROR
Traceback (most recent call last):
 File ""c:\Users\desmo\Desktop\chatAttempt1.py"", line 10, in 
 chat_completion = client.chat.completions.create(
 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
 File ""C:\Users\desmo\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\openai_utils_utils.py"", line 303, in wrapper
 return func(*args, **kwargs)
 ^^^^^^^^^^^^^^^^^^^^^
 File ""C:\Users\desmo\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\openai\resources\chat\completions.py"", line 598, in create
 return self._post(
 ^^^^^^^^^^^
 File ""C:\Users\desmo\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\openai_base_client.py"", line 1086, in post
 return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
 File ""C:\Users\desmo\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\openai_base_client.py"", line 846, in request
 return self._request(
 ^^^^^^^^^^^^^^
 File ""C:\Users\desmo\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\openai_base_client.py"", line 884, in _request
 return self._retry_request(
 ^^^^^^^^^^^^^^^^^^^^
 File ""C:\Users\desmo\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\openai_base_client.py"", line 956, in _retry_request
 return self._request(
 ^^^^^^^^^^^^^^
 File ""C:\Users\desmo\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\openai_base_client.py"", line 884, in _request
 return self._retry_request(
 ^^^^^^^^^^^^^^^^^^^^
 File ""C:\Users\desmo\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\openai_base_client.py"", line 956, in _retry_request
 return self._request(
 ^^^^^^^^^^^^^^
 File ""C:\Users\desmo\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\openai_base_client.py"", line 898, in _request
 raise self._make_status_error_from_response(err.response) from None
 openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}
To Reproduce
Just run the code it will show an error
Code snippets
import osfrom openai import OpenAI#from openai.types import FunctionDefinition, FunctionParameters

client = OpenAI(
    # This is the default and can be omittedpip uninstall openai
    api_key=os.environ.get(""OPENAI_API_KEY""),
)

chat_completion = client.chat.completions.create(
    model=""gpt-3.5-turbo"",
  messages = [
      { ""role"": ""user"", ""content"": ""Write a tagline for an ice cream shop."" }
  ]
)
print(chat_completion['choices'][0]['message']['content'])
OS
Windows 11
Python version
python 3.11
Library version
1.3.8
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/961","Lack of API key will cause openai.OpenAIError when using ""LM Studio"" REST API","2023-12-10T20:27:27Z","Open issue","No label","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
Summary: When running ""LM Studio"" for local AI models, I cannot use the openai package to interact with the REST API, because it does not require an API key.
Solution:
I was able to fix this by commenting out the below lines in ./openai/_client.py>OpenAI>__init__#L92:
        if api_key is None:
            raise OpenAIError(
                ""The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable""
            )
To Reproduce
Reproduce:
Use ""LM Studio"" to start a REST API locally
Run the below code:
import seleniumimport osimport openaifrom openai import OpenAI

client = OpenAI(
    base_url='http://localhost:1234/v1'
)

# start LM Studio, rest API?

# TODO: The 'openai.api_base' option isn't read in the client API. You will need to pass it when you instantiate the client, e.g. 'OpenAI(api_base=""http://localhost:1234/v1"")'# openai.api_base = ""http://localhost:1234/v1"" # point to the local server
 # no need for an API key

completion = client.chat.completions.create(model=""local-model"", # this field is currently unusedmessages=[
  {""role"": ""system"", ""content"": ""Always answer in rhymes.""},
  {""role"": ""user"", ""content"": ""Introduce yourself.""}
])

print(completion.choices[0].message)
Receive this error:
 src  python .\test.py
Traceback (most recent call last):
  File ""C:\Users\henry\.conda\envs\america-elections-2024\Lib\site-packages\httpcore\_exceptions.py"", line 10, in map_exceptions
    yield
  File ""C:\Users\henry\.conda\envs\america-elections-2024\Lib\site-packages\httpcore\_sync\http11.py"", line 142, in _send_request_headers
    event = h11.Request(
            ^^^^^^^^^^^^
  File ""C:\Users\henry\.conda\envs\america-elections-2024\Lib\site-packages\h11\_events.py"", line 96, in __init__
    self, ""headers"", normalize_and_validate(headers, _parsed=_parsed)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\henry\.conda\envs\america-elections-2024\Lib\site-packages\h11\_headers.py"", line 164, in normalize_and_validate
    validate(_field_value_re, value, ""Illegal header value {!r}"", value)
  File ""C:\Users\henry\.conda\envs\america-elections-2024\Lib\site-packages\h11\_util.py"", line 91, in validate
    raise LocalProtocolError(msg)
h11._util.LocalProtocolError: Illegal header value b'Bearer '

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""C:\Users\henry\.conda\envs\america-elections-2024\Lib\site-packages\httpx\_transports\default.py"", line 66, in map_httpcore_exceptions
    yield
  File ""C:\Users\henry\.conda\envs\america-elections-2024\Lib\site-packages\httpx\_transports\default.py"", line 228, in handle_request
    resp = self._pool.handle_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\henry\.conda\envs\america-elections-2024\Lib\site-packages\httpcore\_sync\connection_pool.py"", line 268, in handle_request
    raise exc
  File ""C:\Users\henry\.conda\envs\america-elections-2024\Lib\site-packages\httpcore\_sync\connection_pool.py"", line 251, in handle_request
    response = connection.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\henry\.conda\envs\america-elections-2024\Lib\site-packages\httpcore\_sync\connection.py"", line 103, in handle_request
    return self._connection.handle_request(request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\henry\.conda\envs\america-elections-2024\Lib\site-packages\httpcore\_sync\http11.py"", line 133, in handle_request
    raise exc
  File ""C:\Users\henry\.conda\envs\america-elections-2024\Lib\site-packages\httpcore\_sync\http11.py"", line 92, in handle_request
    self._send_request_headers(**kwargs)
  File ""C:\Users\henry\.conda\envs\america-elections-2024\Lib\site-packages\httpcore\_sync\http11.py"", line 141, in _send_request_headers
    with map_exceptions({h11.LocalProtocolError: LocalProtocolError}):
  File ""C:\Users\henry\.conda\envs\america-elections-2024\Lib\contextlib.py"", line 155, in __exit__
    self.gen.throw(value)
  File ""C:\Users\henry\.conda\envs\america-elections-2024\Lib\site-packages\httpcore\_exceptions.py"", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.LocalProtocolError: Illegal header value b'Bearer '

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""C:\Users\henry\.conda\envs\america-elections-2024\Lib\site-packages\openai\_base_client.py"", line 872, in _request
    response = self._client.send(
               ^^^^^^^^^^^^^^^^^^
  File ""C:\Users\henry\.conda\envs\america-elections-2024\Lib\site-packages\httpx\_client.py"", line 901, in send
    response = self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\henry\.conda\envs\america-elections-2024\Lib\site-packages\httpx\_client.py"", line 929, in _send_handling_auth
    response = self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\henry\.conda\envs\america-elections-2024\Lib\site-packages\httpx\_client.py"", line 966, in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\henry\.conda\envs\america-elections-2024\Lib\site-packages\httpx\_client.py"", line 1002, in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\henry\.conda\envs\america-elections-2024\Lib\site-packages\httpx\_transports\default.py"", line 227, in handle_request
    with map_httpcore_exceptions():
  File ""C:\Users\henry\.conda\envs\america-elections-2024\Lib\contextlib.py"", line 155, in __exit__
    self.gen.throw(value)
  File ""C:\Users\henry\.conda\envs\america-elections-2024\Lib\site-packages\httpx\_transports\default.py"", line 83, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.LocalProtocolError: Illegal header value b'Bearer '

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\henry\.conda\envs\america-elections-2024\Lib\site-packages\httpcore\_exceptions.py"", line 10, in map_exceptions
    yield
  File ""C:\Users\henry\.conda\envs\america-elections-2024\Lib\site-packages\httpcore\_sync\http11.py"", line 142, in _send_request_headers
    event = h11.Request(
            ^^^^^^^^^^^^
  File ""C:\Users\henry\.conda\envs\america-elections-2024\Lib\site-packages\h11\_events.py"", line 96, in __init__
    self, ""headers"", normalize_and_validate(headers, _parsed=_parsed)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\henry\.conda\envs\america-elections-2024\Lib\site-packages\h11\_headers.py"", line 164, in normalize_and_validate
    validate(_field_value_re, value, ""Illegal header value {!r}"", value)
  File ""C:\Users\henry\.conda\envs\america-elections-2024\Lib\site-packages\h11\_util.py"", line 91, in validate
    raise LocalProtocolError(msg)
h11._util.LocalProtocolError: Illegal header value b'Bearer '

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""C:\Users\henry\.conda\envs\america-elections-2024\Lib\site-packages\httpx\_transports\default.py"", line 66, in map_httpcore_exceptions
    yield
  File ""C:\Users\henry\.conda\envs\america-elections-2024\Lib\site-packages\httpx\_transports\default.py"", line 228, in handle_request
    resp = self._pool.handle_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\henry\.conda\envs\america-elections-2024\Lib\site-packages\httpcore\_sync\connection_pool.py"", line 268, in handle_request
    raise exc
  File ""C:\Users\henry\.conda\envs\america-elections-2024\Lib\site-packages\httpcore\_sync\connection_pool.py"", line 251, in handle_request
    response = connection.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\henry\.conda\envs\america-elections-2024\Lib\site-packages\httpcore\_sync\connection.py"", line 103, in handle_request
    return self._connection.handle_request(request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\henry\.conda\envs\america-elections-2024\Lib\site-packages\httpcore\_sync\http11.py"", line 133, in handle_request
    raise exc
  File ""C:\Users\henry\.conda\envs\america-elections-2024\Lib\site-packages\httpcore\_sync\http11.py"", line 92, in handle_request
    self._send_request_headers(**kwargs)
  File ""C:\Users\henry\.conda\envs\america-elections-2024\Lib\site-packages\httpcore\_sync\http11.py"", line 141, in _send_request_headers
    with map_exceptions({h11.LocalProtocolError: LocalProtocolError}):
  File ""C:\Users\henry\.conda\envs\america-elections-2024\Lib\contextlib.py"", line 155, in __exit__
    self.gen.throw(value)
  File ""C:\Users\henry\.conda\envs\america-elections-2024\Lib\site-packages\httpcore\_exceptions.py"", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.LocalProtocolError: Illegal header value b'Bearer '

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""C:\Users\henry\.conda\envs\america-elections-2024\Lib\site-packages\openai\_base_client.py"", line 872, in _request
    response = self._client.send(
               ^^^^^^^^^^^^^^^^^^
  File ""C:\Users\henry\.conda\envs\america-elections-2024\Lib\site-packages\httpx\_client.py"", line 901, in send
    response = self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\henry\.conda\envs\america-elections-2024\Lib\site-packages\httpx\_client.py"", line 929, in _send_handling_auth
    response = self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\henry\.conda\envs\america-elections-2024\Lib\site-packages\httpx\_client.py"", line 966, in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\henry\.conda\envs\america-elections-2024\Lib\site-packages\httpx\_client.py"", line 1002, in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\henry\.conda\envs\america-elections-2024\Lib\site-packages\httpx\_transports\default.py"", line 227, in handle_request
    with map_httpcore_exceptions():
  File ""C:\Users\henry\.conda\envs\america-elections-2024\Lib\contextlib.py"", line 155, in __exit__
    self.gen.throw(value)
  File ""C:\Users\henry\.conda\envs\america-elections-2024\Lib\site-packages\httpx\_transports\default.py"", line 83, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.LocalProtocolError: Illegal header value b'Bearer '

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\henry\.conda\envs\america-elections-2024\Lib\site-packages\httpcore\_exceptions.py"", line 10, in map_exceptions
    yield
  File ""C:\Users\henry\.conda\envs\america-elections-2024\Lib\site-packages\httpcore\_sync\http11.py"", line 142, in _send_request_headers
    event = h11.Request(
            ^^^^^^^^^^^^
  File ""C:\Users\henry\.conda\envs\america-elections-2024\Lib\site-packages\h11\_events.py"", line 96, in __init__
    self, ""headers"", normalize_and_validate(headers, _parsed=_parsed)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\henry\.conda\envs\america-elections-2024\Lib\site-packages\h11\_headers.py"", line 164, in normalize_and_validate
    validate(_field_value_re, value, ""Illegal header value {!r}"", value)
  File ""C:\Users\henry\.conda\envs\america-elections-2024\Lib\site-packages\h11\_util.py"", line 91, in validate
    raise LocalProtocolError(msg)
h11._util.LocalProtocolError: Illegal header value b'Bearer '

Code snippets
No response
OS
Windows 10.0.22621 Build 22621
Python version
Python 3.12.0 | packaged by Anaconda, Inc. | (main, Oct 2 2023, 17:20:38) [MSC v.1916 64 bit (AMD64)] on win32
Library version
openai v1.3.8
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/960","Issue with custom transport","2023-12-09T20:12:17Z","Closed issue","No label","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
So, I'm trying to resolve pyodide/pyodide#4292
With these fixes in place: urllib3/urllib3#3195, encode/httpx#2994, and the custom transport feature in the library the solution is more feasible.
To Reproduce
Testing environment: https://jupyter.org/try-jupyter/lab/
import micropipawait micropip.install('https://raw.githubusercontent.com/psymbio/pyodide_wheels/main/multidict/multidict-4.7.6-py3-none-any.whl', keep_going=True)
await micropip.install('https://raw.githubusercontent.com/psymbio/pyodide_wheels/main/frozenlist/frozenlist-1.4.0-py3-none-any.whl', keep_going=True)
await micropip.install('https://raw.githubusercontent.com/psymbio/pyodide_wheels/main/aiohttp/aiohttp-4.0.0a2.dev0-py3-none-any.whl', keep_going=True)
await micropip.install('https://raw.githubusercontent.com/psymbio/pyodide_wheels/main/openai/openai-1.3.7-py3-none-any.whl', keep_going=True)
await micropip.install('https://raw.githubusercontent.com/psymbio/pyodide_wheels/main/urllib3/urllib3-2.1.0-py3-none-any.whl', keep_going=True)
await micropip.install(""ssl"")
import sslawait micropip.install(""httpx"", keep_going=True)
import httpxawait micropip.install('https://raw.githubusercontent.com/psymbio/pyodide_wheels/main/urllib3/urllib3-2.1.0-py3-none-any.whl', keep_going=True)
import urllib3urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
import json

class URLLib3Transport(httpx.BaseTransport):
    def __init__(self):
        self.pool = urllib3.PoolManager()

    def handle_request(self, request: httpx.Request):
        urllib3_response = self.pool.request('GET', str(request.url))  # Convert httpx.URL to string
        content = json.loads(urllib3_response.data.decode('utf-8'))  # Decode the data and load as JSON
        stream = httpx.ByteStream(json.dumps(content).encode(""utf-8""))  # Convert back to JSON and encode
        headers = [(b""content-type"", b""application/json"")]
        return httpx.Response(200, headers=headers, stream=stream)

client = httpx.Client(transport=URLLib3Transport())
from openai import OpenAI

openai_client = OpenAI(
    base_url=""https://api.openai.com/v1"",
    api_key=""xxx"",
    http_client=client
)
response = openai_client.chat.completions.with_raw_response.create(
    messages=[{
        ""role"": ""user"",
        ""content"": ""Say this is a test"",
    }],
    model=""gpt-3.5-turbo"",
)
completion = response.parse()
print(completion)
But I get this output:
ChatCompletion(id=None, choices=None, created=None, model=None, object=None, system_fingerprint=None, usage=None, error={'message': ""You didn't provide an API key. You need to provide your API key in an Authorization header using Bearer auth (i.e. Authorization: Bearer YOUR_KEY), or as the password field (with blank username) if you're accessing the API from your browser and are prompted for a username and password. You can obtain an API key from https://platform.openai.com/account/api-keys."", 'type': 'invalid_request_error', 'param': None, 'code': None})

I have provided the API key - so what am I doing wrong here?
Code snippets
No response
OS
Pyodide
Python version
Python v3.11.3
Library version
openai v1.3.7
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/958","Broken link on docx","2023-12-09T00:06:24Z","Closed as not planned issue","documentation","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
https://platform.openai.com/docs/guides/text-generation/managing-tokens
The hyper link is broke. 404.
To Reproduce
Click The ChatML documentation explains how messages are converted into tokens by the OpenAI API, and may be useful for writing your own function.
Code snippets
No response
OS
Windows
Python version
NA
Library version
NA
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/957","Feature: support for ""retry-after-ms"" HTTP header variant","2024-01-17T16:12:32Z","Closed issue","Azure,enhancement","Confirm this is a feature request for the Python library and not the underlying OpenAI API.
 This is a feature request for the Python library
Describe the feature or improvement you're requesting
Feature request: add support for the millisecond-precision retry-after-ms variant of the standard retry-after response header, using its value as a higher-resolution first selection when present that falls back to the lower-resolution standard when not present.
openai-python's retry header handling is cleanly done in _base_client.py and parses the standard retry-after header, which provides second-resolution guidance on how long a client should wait before initiating a retry.
Some services, including Azure OpenAI and particularly in the context of provisioned customers, can provide a retry-after-ms header in addition to retry-after. This millisecond-resolution variant is primarily valuable when retry behavior is being used to efficiently control traffic of service-to-service calls within a topology that often has delays that can be well under a single whole second.
As a reference/comparison, Azure's SDKs use a precedence order of three retry headers, e.g. as per here in the azure-sdk-for-js core logic:
If the retry-after-ms header key is present, use its value as the number of milliseconds to delay
Else, if the x-ms-retry-after-ms header key is present, instead use its value as the number of milliseconds to delay
Else, if the retry-after header key is present, use its value as the number of whole seconds to delay
Else, fall back to standard fallback heuristics to calculate a retry delay
openai-python already uses a float value from retry-after as the input into time.sleep(), so this superficially looks like a fairly straightforward addition:
retry_after = float(retry_header)
Conceptually, this would just be a float(retry_ms_header) / 1000 style of thing.
Thank you!
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/955","Example code snippet references an incorrect default timeout value","2023-12-15T03:05:25Z","Closed issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
Example code snippet references an incorrect default timeout value of 60 seconds:
openai-python/README.md
 Line 362 in e369566
	# default is 60s
When it's actually 10 minutes:

openai-python/src/openai/_constants.py
 Lines 8 to 9 in e369566
	# default timeout is 10 minutes
	DEFAULT_TIMEOUT=httpx.Timeout(timeout=600.0, connect=5.0) 
I suggest either update code snippet comment to reflect an actual value or remove comment from the code snippet entirely.
To Reproduce
Note timeout on the README.md 
openai-python/README.md
 Line 362 in e369566
	# default is 60s
Note timeout in the _constants.py 
openai-python/src/openai/_constants.py
 Lines 8 to 9 in e369566
	# default timeout is 10 minutes
	DEFAULT_TIMEOUT=httpx.Timeout(timeout=600.0, connect=5.0) 
See the difference
Code snippets
> # default timeout is 10 minutes> DEFAULT_TIMEOUT = httpx.Timeout(timeout=600.0, connect=5.0)


vs 
> # default is 60s
OS
Any
Python version
Any
Library version
openai v1.3.7
 The text was updated successfully, but these errors were encountered: 
👍1
slyapustin reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/openai-python/issues/953","response_format={""type"": ""json_object""} leads to abnormal output","2023-12-08T14:49:39Z","Closed as not planned issue","No label","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
The expected model returns a list-type JSON task. After adding the following parameter, the square brackets disappeared: response_format={""type"": ""json_object""}
To Reproduce
The expected model was supposed to return a list-type JSON, for example: [{""A""},{""B""}]
Add the parameter response_format={""type"": ""json_object""} in completions.
Unexpected changes occurred in the model output, where the square brackets disappeared and it became like this: {""A""}
Code snippets
response = client.chat.completions.with_raw_response.create(
            model=model,
            response_format={""type"": ""json_object""},
            max_tokens=4096,
            temperature=0,
            messages=messages
        )
OS
win11
Python version
Python v3.11.6
Library version
openai v1.3.7
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/952","Make the OpenAI API model names and info available under an enum","2024-07-12T13:14:05Z","Closed issue","enhancement","Confirm this is a feature request for the Python library and not the underlying OpenAI API.
 This is a feature request for the Python library
Describe the feature or improvement you're requesting
It would greatly help to have the names and several fields of interest (like max tokens) of the available OpenAI models under some sort of enum.
This would greatly help with the model validation and accessing the fields of interest. For now, if we want a list of available models, we need to declare it ourselves (see code below).
from dataclasses import dataclass


@dataclassclass ChatGPTModel:
    name: str
    max_tokens: int


CHAT_GPT_MODELS = {
    ""gpt-3.5-turbo"": ChatGPTModel(name=""gpt-3.5-turbo"", max_tokens=4000),
    ""gpt-4"": ChatGPTModel(name=""gpt-4"", max_tokens=8000),
}
Additional context
If needed, I can add this change myself (however some guidance would be necessary).
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/948","body in APIError is never assigned","2023-12-08T10:03:16Z","Closed issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
In the APIError class, there is a body class variable. Based on the comment:
    """"The API response body.

    If the API responded with a valid JSON structure then this property will be the
    decoded result.

    If it isn't a valid JSON structure then this will be the raw response.

    If there was no response associated with this error then it will be `None`.
    """"""

This class variable should contain the body of the response. However in the __init__ function, the body variable is not assigned and kept.
To Reproduce
Simply error.body will result in:
AttributeError: 'XxxxxxError' object has no attribute 'body'
Code snippets
No response
OS
OS agnostic
Python version
Python version agnostic
Library version
Lib version agnostic
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/942","Weird interaction change with OpenAI Client and files","2023-12-08T04:54:55Z","Closed issue","No label","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
I had a webapp running which was using the openai==0.27.7 I recently was working on revamping it and switched to the new openai client. I thought it'll be just plug and play with the new Interface like how GPT works. I tried changing my approach a few times but kept getting weird file format errors everytime. Not an issue on the file end since the same one works on the older api for the same.
 Totally an issue on the new client
To Reproduce
` def transcribe(self):
 # transcript_text = openai.Audio.transcribe(""whisper-1"", self.audio_file)[""text""]
 transcript_text = client.audio.transcriptions.create(
 model = ""whisper-1"",
 file = self.audio_file
 )[""text""]
 self.transcript = transcript_text
    return transcript_text`

@app.post(""/transcribe_audio"") async def transcribe_audio(audio_file: UploadFile = File(...)): if audio_file.filename.endswith(("".mp3"", "".wav"", ""m4a"", ""mp4"", ""mpeg"", ""mpga"", ""oga"", ""ogg"", ""webm"")): audio_query = AudioQuery(audio_file) transcript_text = audio_query.transcribe() return {""transcript"": transcript_text} else: raise HTTPException(status_code=400, detail=""Invalid file type. Please upload a file in a supported format"") 
Code snippets
@app.post(""/transcribe_audio"")async def transcribe_audio(audio_file: UploadFile = File(...)):
    if audio_file.filename.endswith(("".mp3"", "".wav"")):
        file_name = os.path.splitext(os.path.basename(audio_file.filename))[0]
        audio_query = AudioQuery(audio_file, file_name)
        
        transcript_text = audio_query.transcribe()
        return {""transcript"": transcript_text}
    else:
        raise HTTPException(status_code=400, detail=""Invalid file type. Please upload a .mp3 or .wav file"")


    def transcribe(self):
        transcript_text = fetch_transcription(self.file_name)

        if transcript_text:
            self.transcript = transcript_text
        else:
            openai.api_key = API_KEY
            transcript_text = openai.Audio.transcribe(""whisper-1"", self.audio_file)[""text""]
            cache_transcription(self.file_name, transcript_text)
            self.transcript = transcript_text
OS
Windows
Python version
Python v3.11.4
Library version
openai v1.0.1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/940","typing issues","2023-12-07T10:22:35Z","Closed as not planned issue","question","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
typing errors occur such as:
Type of ""response"" is partially unknown
  Type of ""response"" is ""Unknown | ChatCompletion""Pylance[reportUnknownVariableType](https://github.com/microsoft/pyright/blob/main/docs/configuration.md#reportUnknownVariableType)

To Reproduce
from openai import AsyncOpenAIfrom openai.types.chat import ChatCompletion

...

        response: ChatCompletion = await client.chat.completions.create(
            model=""gpt-4-1106-preview"",
            messages=[
                {
                    ""role"": ""user"",
                    ""content"": data,
                },
            ],
            functions=check_charges_functions,
            function_call=""auto"",
            request_timeout=30,
        )
        response_message = response.choices[0].message
Code snippets
No response
OS
macOS
Python version
v3.12
Library version
v1.3.7
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/938","For events of the same openai job, the id returned each time the list_events method is called may be different.","2023-12-08T22:59:36Z","Closed issue","Azure","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
When I call the list_events method to get the job log, the ID of the same log returned by two repeated calls is sometimes different, which leads to the error ""Given after argument was not found in items."" when I use the get_next_page method to get the next page.

Called twice with the same parameters, some jobs have different IDs.
To Reproduce
from openai import AzureOpenAI

client=AzureOpenAI(
  azure_ad_token=token,
  azure_endpoint=endpoint,
  api_version=api_version
)

events = client.fine_tuning.jobs.list_events(fine_tuning_job_id=job_id, limit=10)
events.get_next_page()
Code snippets
from openai import AzureOpenAI

client=AzureOpenAI(
  azure_ad_token=token,
  azure_endpoint=endpoint,
  api_version=api_version
)
client.fine_tuning.jobs.list_events(fine_tuning_job_id=job_id, limit=10)
OS
macOS
Python version
Python v3.10.13
Library version
openai v1.3.5
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/937","Handling of TPM limit errors for Azure (x-rate-limit-reset-tokens)","2023-12-21T18:49:16Z","Closed issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
We've been noticing an increasing number of TPM limit errors when calling an Azure-hosted model via the library. We have a couple of retries configured but these do not help. The reason seems to be that recently the Azure API stopped returning the Retry-After header in case of limit errors and now return x-rate-limit-reset-tokens. The library currently only knows how to handle Retry-After.
To Reproduce
Force a token limit error on an Azure hosted model
Observe the response headers. Example
[2023-12-03 18:48:27.180] DEBUG worker_pool_8 [httpcore.http11.trace:45] receive_response_headers.complete return_value=(b'HTTP/1.1', 429, b'Too Many Requests', [(b'Content-Length', b'329'), (b'Content-Type', b'application/json'), (b'x-rate-limit-reset-tokens', b'55'), (b'apim-request-id', b'<uuid>'), (b'Strict-Transport-Security', b'max-age=31536000; includeSubDomains; preload'), (b'x-content-type-options', b'nosniff'), (b'policy-id', b'DeploymentRatelimit-Token'), (b'x-ms-region', b'West US'), (b'x-ratelimit-remaining-requests', b'52'), (b'Date', b'Sun, 03 Dec 2023 18:48:27 GMT')])

The retry-after header is no longer there and instead the x-rate-limit-reset-tokens is returned.
Code snippets
No response
OS
macOS
Python version
Python 3.12
Library version
openai 1.3.6
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/936","How do I know which account an OpenAI token is associated with ?","2024-03-03T00:52:25Z","Closed as not planned issue","No label","Confirm this is a feature request for the Python library and not the underlying OpenAI API.
 This is a feature request for the Python library
Describe the feature or improvement you're requesting
Hello 👋🏻
This isn't really a bug, but rather a question. I have several tokens to access OpenAI. I know how to find out whether they're valid or not, but I can't see how to find out which account is associated with them.
How can I get this information ? Does OpenAI Python API library support this ?
Thank you for your reply
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/933","OSError: [Errno 24] Too many open files during retry","2023-12-15T00:58:48Z","Closed issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
Our retry logic was successful before migration to openai v1.0.0. However, now the retry fails as described below.
After a call to openai's API chat.completions.create fails with a Connection Error, our code will catch the exception and retry. However, the retry fails with the following error:
  File ""<our code>.py"", line xxx, in <our_method1>
  File ""/opt/conda/envs/pbap_prompt/lib/python3.10/site-packages/nest_asyncio.py"", line 35, in run
  File ""/opt/conda/envs/pbap_prompt/lib/python3.10/site-packages/nest_asyncio.py"", line 90, in run_until_complete
  File ""/opt/conda/envs/pbap_prompt/lib/python3.10/asyncio/futures.py"", line 201, in result
  File ""/opt/conda/envs/pbap_prompt/lib/python3.10/asyncio/tasks.py"", line 232, in __step
  File ""<our code>.py"", line xxx, in <our_method2>
  File ""/opt/conda/envs/pbap_prompt/lib/python3.10/site-packages/openai/_client.py"", line 317, in __init__
  File ""/opt/conda/envs/pbap_prompt/lib/python3.10/site-packages/openai/_base_client.py"", line 1190, in __init__
  File ""/opt/conda/envs/pbap_prompt/lib/python3.10/site-packages/httpx/_client.py"", line 1397, in __init__
  File ""/opt/conda/envs/pbap_prompt/lib/python3.10/site-packages/httpx/_client.py"", line 1445, in _init_transport
  File ""/opt/conda/envs/pbap_prompt/lib/python3.10/site-packages/httpx/_transports/default.py"", line 272, in __init__
  File ""/opt/conda/envs/pbap_prompt/lib/python3.10/site-packages/httpx/_config.py"", line 51, in create_ssl_context
  File ""/opt/conda/envs/pbap_prompt/lib/python3.10/site-packages/httpx/_config.py"", line 75, in __init__
  File ""/opt/conda/envs/pbap_prompt/lib/python3.10/site-packages/httpx/_config.py"", line 87, in load_ssl_context
  File ""/opt/conda/envs/pbap_prompt/lib/python3.10/site-packages/httpx/_config.py"", line 145, in load_ssl_context_verify
OSError: [Errno 24] Too many open files

Here is a snippet of our methods:
method1:
        try:
            outputs = asyncio.run(
                                method2
             )

method2:
          AsyncOpenAI(
            api_key=xxx,
            organization=xxx,
            max_retries=0,
        ).chat.completions.create(
                    model=model,
                    messages=input_data,
                    max_tokens=max_new_tokens,
                    temperature=temperature,
                    timeout=timeout,
                )
            )

To Reproduce
create openai async client and call chat.completions.create
openai returns error such as Connection Error
catch error and create new openai async client and call chat.completions.create again
receive error OSError: [Errno 24] Too many open files
Code snippets
See above
OS
Unix Ubuntu
Python version
Python v3.10.0
Library version
openai 1.3.5
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/932","Add openai.embeddings_utils to the list of ###Removed","2023-12-05T22:41:15Z","Closed issue","No label","Originally posted by @rattrayalex in https://github.com/openai/openai-python/discussions/742_
 We have released a new major version of our SDK, and we recommend upgrading promptly. 
It's a total rewrite of the library, so many things have changed, but we've made upgrading easy with a code migration script and detailed docs below. It was extensively beta tested prior to release.
Getting started
pip install --upgrade openai

What's changed
Auto-retry with backoff if there's an error
Proper types (for mypy/pyright/editors)
You can now instantiate a client, instead of using a global default.
Switch to explicit client instantiation
Weights and Biases CLI will now be included in their own package
Migration guide
For Azure OpenAI users, see Microsoft's Azure-specific migration guide.
Automatic migration with grit
You can automatically migrate your codebase using grit, either online or with the following CLI command on Mac or Linux:
openai migrate

The grit binary executes entirely locally with AST-based transforms.
Be sure to audit its changes: we suggest ensuring you have a clean working tree beforehand, and running git add --patch afterwards. Note that grit.io also offers opt-in automatic fixes powered by AI.
Automatic migration with grit on Windows
To use grit to migrate your code on Windows, you will need to use Windows Subsystem for Linux (WSL). Installing WSL is quick and easy, and you do not need to keep using Linux once the command is done.
Here's a step-by-step guide for setting up and using WSL for this purpose:
Open a PowerShell or Command Prompt as an administrator and run wsl --install.
Restart your computer.
Open the WSL application.
In the WSL terminal, cd into the appropriate directory (e.g., cd /mnt/c/Users/Myself/my/code/) and then run the following commands: 
curl -fsSL https://docs.grit.io/install | bash
grit install
grit apply openai
Then, you can close WSL and go back to using Windows.
Automatic migration with grit in Jupyter Notebooks
If your Jupyter notebooks are not in source control, they will be more difficult to migrate. You may want to copy each cell into grit's web interface, and paste the output back in.
If you need to migrate in a way that preserves use of the module-level client instead of instantiated clients, you can use the openai_global grit migration instead.
Initialization
# oldimport openai

openai.api_key = os.environ['OPENAI_API_KEY']

# newfrom openai import OpenAI

client = OpenAI(
  api_key=os.environ['OPENAI_API_KEY'],  # this is also the default, it can be omitted
)
Responses
Response objects are now pydantic models and no longer conform to the dictionary shape. However you can easily convert them to a dictionary with model.model_dump().
# beforeimport jsonimport openai

completion = openai.Completion.create(model='curie')
print(completion['choices'][0]['text'])
print(completion.get('usage'))
print(json.dumps(completion, indent=2))

# afterfrom openai import OpenAI

client = OpenAI()

completion = client.completions.create(model='curie')
print(completion.choices[0].text)
print(dict(completion).get('usage'))
print(completion.model_dump_json(indent=2))
Async client
We do not support calling asynchronous methods in the module-level client, instead you will have to instantiate an async client.
The rest of the API is exactly the same as the synchronous client.
# oldimport openai

completion = openai.ChatCompletion.acreate(model=""gpt-3.5-turbo"", messages=[{""role"": ""user"", ""content"": ""Hello world""}])

# newfrom openai import AsyncOpenAI

client = AsyncOpenAI()
completion = await client.chat.completions.create(model=""gpt-3.5-turbo"", messages=[{""role"": ""user"", ""content"": ""Hello world""}])
Module client
[!IMPORTANT]
 We highly recommend instantiating client instances instead of relying on the global client.
We also expose a global client instance that is accessible in a similar fashion to versions prior to v1.
import openai

# optional; defaults to `os.environ['OPENAI_API_KEY']`openai.api_key = '...'

# all client options can be configured just like the `OpenAI` instantiation counterpartopenai.base_url = ""https://...""openai.default_headers = {""x-foo"": ""true""}

completion = openai.chat.completions.create(
    model=""gpt-4"",
    messages=[
        {
            ""role"": ""user"",
            ""content"": ""How do I output all files in a directory using Python?"",
        },
    ],
)
print(completion.choices[0].message.content)
The API is the exact same as the standard client instance based API.
This is intended to be used within REPLs or notebooks for faster iteration, not in application code.
We recommend that you always instantiate a client (e.g., with client = OpenAI()) in application code because:
It can be difficult to reason about where client options are configured
It's not possible to change certain client options without potentially causing race conditions
It's harder to mock for testing purposes
It's not possible to control cleanup of network connections
Pagination
All list() methods that support pagination in the API now support automatic iteration, for example:
from openai import OpenAI

client = OpenAI()

for job in client.fine_tuning.jobs.list(limit=1):
    print(job)
Previously you would have to explicitly call a .auto_paging_iter() method instead.
 See the README for more details.
Azure OpenAI
To use this library with Azure OpenAI, use the AzureOpenAI class instead of the OpenAI class.
A more comprehensive Azure-specific migration guide is available on the Microsoft website.
[!IMPORTANT]
 The Azure API shape differs from the core API shape which means that the static types for responses / params
 won't always be correct.
from openai import AzureOpenAI

# gets the API Key from environment variable AZURE_OPENAI_API_KEYclient = AzureOpenAI(
    # https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#rest-api-versioning
    api_version=""2023-07-01-preview"",
    # https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/create-resource?pivots=web-portal#create-a-resource
    azure_endpoint=""https://example-endpoint.openai.azure.com"",
)

completion = client.chat.completions.create(
    model=""deployment-name"",  # e.g. gpt-35-instant
    messages=[
        {
            ""role"": ""user"",
            ""content"": ""How do I output all files in a directory using Python?"",
        },
    ],
)
print(completion.model_dump_json(indent=2))
In addition to the options provided in the base OpenAI client, the following options are provided:
azure_endpoint
azure_deployment
api_version
azure_ad_token
azure_ad_token_provider
An example of using the client with Azure Active Directory can be found here.
All name changes
Note: all a* methods have been removed; the async client must be used instead.
openai.api_base -> openai.base_url
openai.proxy -> openai.proxies (docs)
openai.InvalidRequestError -> openai.BadRequestError
openai.Audio.transcribe() -> client.audio.transcriptions.create()
openai.Audio.translate() -> client.audio.translations.create()
openai.ChatCompletion.create() -> client.chat.completions.create()
openai.Completion.create() -> client.completions.create()
openai.Edit.create() -> client.edits.create()
openai.Embedding.create() -> client.embeddings.create()
openai.File.create() -> client.files.create()
openai.File.list() -> client.files.list()
openai.File.retrieve() -> client.files.retrieve()
openai.File.download() -> client.files.retrieve_content()
openai.FineTune.cancel() -> client.fine_tunes.cancel()
openai.FineTune.list() -> client.fine_tunes.list()
openai.FineTune.list_events() -> client.fine_tunes.list_events()
openai.FineTune.stream_events() -> client.fine_tunes.list_events(stream=True)
openai.FineTune.retrieve() -> client.fine_tunes.retrieve()
openai.FineTune.delete() -> client.fine_tunes.delete()
openai.FineTune.create() -> client.fine_tunes.create()
openai.FineTuningJob.create() -> client.fine_tuning.jobs.create()
openai.FineTuningJob.cancel() -> client.fine_tuning.jobs.cancel()
openai.FineTuningJob.delete() -> client.fine_tuning.jobs.create()
openai.FineTuningJob.retrieve() -> client.fine_tuning.jobs.retrieve()
openai.FineTuningJob.list() -> client.fine_tuning.jobs.list()
openai.FineTuningJob.list_events() -> client.fine_tuning.jobs.list_events()
openai.Image.create() -> client.images.generate()
openai.Image.create_variation() -> client.images.create_variation()
openai.Image.create_edit() -> client.images.edit()
openai.Model.list() -> client.models.list()
openai.Model.delete() -> client.models.delete()
openai.Model.retrieve() -> client.models.retrieve()
openai.Moderation.create() -> client.moderations.create()
openai.api_resources -> openai.resources
Removed
openai.api_key_path
openai.app_info
openai.debug
openai.log
openai.OpenAIError
openai.Audio.transcribe_raw()
openai.Audio.translate_raw()
openai.ErrorObject
openai.Customer
openai.api_version
openai.verify_ssl_certs
openai.api_type
openai.enable_telemetry
openai.ca_bundle_path
openai.requestssession (we now use httpx)
openai.aiosession (we now use httpx)
openai.Deployment (only used for Azure)
openai.Engine
openai.File.find_matching_files()
I propose to update migration guide to reflect that openai.embeddings_utils has been removed from the v1.0 and above. To achieve this add the following to ###Removed:
openai.embeddings_utils
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/930","Delete thread message... It seems the API supports delete message but python library doesn't have it implemented","2023-12-05T22:43:25Z","Closed issue","No label","Confirm this is a feature request for the Python library and not the underlying OpenAI API.
 This is a feature request for the Python library
Describe the feature or improvement you're requesting
The need for this feature came because with the newer model where input is 120k tokens, when the conversation gets longer and longer, one message might cost up to $1.2+ So users might want to cleanup the thread conversation and leave only desired number of messages in history...
 I've already implemented this feature in my fork below, and would be very happy if we also have it in the official repo as well.
 Link to my fork commit: 6c40896
Additional context
Also some related discussion here: https://community.openai.com/t/how-to-limit-the-number-of-messages-or-tokens-that-are-persisted-in-a-thread-to-maintain-context-in-open-ai-assistants/531814
 The text was updated successfully, but these errors were encountered: 
👍1
andrebadini reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/openai-python/issues/926","Function Calling with AzureOpenAI","2024-02-15T15:07:04Z","Closed issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
I am attempting to utilize function calling with the chat completions API using AzureOpenAI, referring to the example given here - https://platform.openai.com/docs/guides/function-calling
While standard chat completions work as expected, I am encountering an error specific to function calling.
NotFoundError: Error code: 404 - {'error': {'message': 'Unrecognized request argument supplied: tools', 'type': 'invalid_request_error', 'param': None, 'code': None}}
Relevant code snippets are attached for reference.
It's worth noting that the code works seamlessly when initializing the client with OpenAI but encounters issues with AzureOpenAI as outlined above
To Reproduce
Initialize client with AzureOpenAI
Define set of functions
Call the chat completion with the user query and a set of functions
Code snippets
from openai import AzureOpenAI

client = AzureOpenAI(
    api_key=""API_KEY"",
    api_version=""API_VERSION,
    azure_endpoint=""API_ENDPOINT""
)

tools = [
    {
        ""type"": ""function"",
        ""function"": {
            ""name"": ""get_current_weather"",
            ""description"": ""Get the current weather in a given location"",
            ""parameters"": {
                ""type"": ""object"",
                ""properties"": {
                    ""location"": {
                        ""type"": ""string"",
                        ""description"": ""The city and state, e.g. San Francisco, CA"",
                    },
                    ""unit"": {""type"": ""string"", ""enum"": [""celsius"", ""fahrenheit""]},
                },
                ""required"": [""location""],
            },
        },
    } 
]

response = client.chat.completions.create(
    model=""DEPLOYMENT_NAME"",
    messages=[{""role"": ""user"", ""content"": ""What's the weather like in San Francisco, Tokyo, and Paris?""}],
    tools=tools,
    tool_choice=""auto""
)
OS
macOS
Python version
Python v3.9.16
Library version
openai v1.3.7
 The text was updated successfully, but these errors were encountered: 
❤️1
AnjaliKeshri reacted with heart emoji
All reactions
❤️1 reaction"
"https://github.com/openai/openai-python/issues/925","Easy way to get a context window for a model","2023-12-05T22:44:37Z","Closed issue","No label","Confirm this is a feature request for the Python library and not the underlying OpenAI API.
 This is a feature request for the Python library
Describe the feature or improvement you're requesting
The current version of openai-python (v1.3.7) does not contain any method to get the size of context window for a model used for a chat completion. It might cause some overload or underutilization of a model.
Definitely, some model names are clear enough (like gpt-4-32k) but the majority of them must be manually checked using the correspondent tables at https://platform.openai.com/docs/models.
It would be great to fix this inconsistency by any or all of these measures:
Include the size of the context window in the name of each model.
Improve the model object (https://platform.openai.com/docs/api-reference/models/object) by adding the size of context window.
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/923","error info of “Processed 0 files and found 0 matches after type “grit apply openai” and press return on my WSL terminal","2023-12-04T02:19:58Z","Closed issue","No label","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
Hi, I follow your step by step process on WSL for windows user, first 2 step are right , I am sure grit are installed. but when I type "" grit apply openai "" , it shows ""Processed 0 files and found 0 matches"". I think migration doesn't work because I rerun my Python program still with the error running.
Normally I am not a Linux user. I type grit help, I find ""Usage: grit apply [OPTIONS] <PATTERN_OR_WORKFLOW> [PATHS]..."". I guess I didn't make PATH right. I need to apply ""openai"" in correct location. But I can't find ""openai"" anywhere in Linux directory. Could you help to solve my problem-that is-how to write correct path for openai. Can WSL linux command access the windows directory?
""openai.exe"" in my python directory is ""C:\Users\Felix SUN\PycharmProjects\pythonProject1\venv\Scripts"". and ""C:\Users\Felix SUN\PycharmProjects\pythonProject1\venv\Lib\site-packages"" is my openai SDK.
please help me to kick start my Python with openai.
To Reproduce
error like message in my WSL:
timekeeper already present, skipping
 hojosi@DESKTOP-AMM5O49:~/.grit/bin$ grit apply openai
 Processed 0 files and found 0 matches
error message of my python :
 ""C:\Users\Felix SUN\PycharmProjects\pythonProject1\venv\Scripts\python.exe"" ""C:\Users\Felix SUN\PycharmProjects\pythonProject1\venv\Lib\snippet.py""
 Traceback (most recent call last):
 File ""C:\Users\Felix SUN\PycharmProjects\pythonProject1\venv\Lib\snippet.py"", line 5, in 
 openai.ChatCompletion.create(
 File ""C:\Users\Felix SUN\PycharmProjects\pythonProject1\venv\Lib\site-packages\openai\lib_old_api.py"", line 39, in call
 raise APIRemovedInV1(symbol=self._symbol)
 openai.lib._old_api.APIRemovedInV1:
You tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.
You can run openai migrate to automatically upgrade your codebase to use the 1.0.0 interface.
Alternatively, you can pin your installation to the old version, e.g. pip install openai==0.28
A detailed migration guide is available here: #742
Process finished with exit code 1
Code snippets
import openai

openai.api_key = 'my key here'

openai.ChatCompletion.create(
  model=""gpt-3.5-turbo"",
  messages=[
        {""role"": ""system"", ""content"": ""You are a helpful assistant.""},
        {""role"": ""user"", ""content"": ""Who won the world series in 2020?""},
        {""role"": ""assistant"", ""content"": ""The Los Angeles Dodgers won the World Series in 2020.""},
        {""role"": ""user"", ""content"": ""Where was it played?""}
    ]
)
OS
windows 11
Python version
Python 3.12
Library version
openai 1.3.7
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/921","Anyio>4 support","2023-12-04T10:38:08Z","Closed issue","enhancement","Confirm this is a feature request for the Python library and not the underlying OpenAI API.
 This is a feature request for the Python library
Describe the feature or improvement you're requesting
The project depends on anyio>=3.5.0,<4, however I would like to use it in a project that requires anyio==4.1.0
Additional context
https://pypi.org/project/anyio/#history

openai-python/pyproject.toml
 Line 14 in e369566
	""anyio>=3.5.0, <4"", 
 The text was updated successfully, but these errors were encountered: 
👍1
inspite reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/openai-python/issues/918","404 here..","2023-12-01T23:47:32Z","Closed issue","No label","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
https://github.com/openai/openai-python/blob/main/chatml.md. 404
To Reproduce
https://github.com/openai/openai-python/blob/main/chatml.md
Code snippets
ddd
OS
Mac
Python version
na
Library version
na
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/916","Custom openai like sever nolonger work for new openai api","2024-03-03T00:53:15Z","Closed as not planned issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
I have acustomedz openai like server
@app.post(""/v1/chat/completions"", response_model=ChatCompletionResponse)
async def create_chat_completion(request: ChatCompletionRequest):
    print('got request')

This works for old openai,
but after upgrade, my custom server no longer receive the request now.
To Reproduce
my request code:
import openai
import argparse
from openai import OpenAI

parser = argparse.ArgumentParser()
parser.add_argument(""--ip"", type=str, default='127.0.0.1')
parser.add_argument(""--port"", type=str, default=80)
args = parser.parse_args()

local_ip = args.ip

base_url = 'http://127.0.0.1:8080'
model = ""qwen-v2""

tts = False
stream_mode = True

if tts:
    client = OpenAI(api_key=api_key)
    response = client.audio.speech.create(
        model='tts-1', # ""tts-1"",""tts-1-hd""
        voice='fable', # 'alloy', 'echo', 'fable', 'onyx', 'nova', 'shimmer'
        input='hell，你好，请你介绍一下你自己',
    )
else:
    client = OpenAI(api_key='sk34fgerg', base_url=base_url)
    if stream_mode:
        questions = [
        '你好，请介绍一下你自己，以及北京的有哪些小吃。',
        '你是谁',
        '请列举出8部好看的电影'
        ]
        for qs in questions:
            print(f'问: {qs}')
            response = client.chat.completions.create(
                model=""qwen-v2"",
                messages=[
                    {""role"": ""user"", ""content"": qs}
                ],
                temperature=0.95,
                stream=True
            )
            for chunk in response:
                if hasattr(chunk.choices[0].delta, ""content""):
                    print(chunk.choices[0].delta.content, end="""", flush=True)
                print()
    else:
        pass

Code snippets
No response
OS
macOS
Python version
3.10
Library version
1.3.6
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/913","base_url of OpenAI Client Instance Cannot Be Modified Due to Missing Update in Setter Method","2023-12-01T18:08:54Z","Closed issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
I encountered an issue where the base_url of an OpenAI client instance couldn't be modified. This is due to the setter method of base_url not updating self._base_url.
Possible Solution
I suggest modifying the setter method for base_url to ensure self._base_url is also updated. Additionally, incorporate handling for the trailing slash of the URL, as done in the init method:
@base_url.setterdef base_url(self, url: URL | str) -> None:
    url = url if isinstance(url, URL) else URL(url)
    url = self._enforce_trailing_slash(url)
    self._base_url = url
    self._client.base_url = url
To Reproduce
Set up an OpenAI client instance with initial parameters including base_url.
Attempt to modifybase_url.
Observe that self._base_url within the client instance remains unchanged, indicating that the modification has not been applied.
Code snippets
import os

from openai import OpenAI

client = OpenAI(
    api_key=os.getenv(""OPENAI_API_KEY""),
    base_url=""https://api.openai.com/v1"",
)
print(client.base_url)
client.base_url = ""https://newurl.example.com/v1""print(client.base_url)
OS
Windows11
Python version
Python 3.12.0
Library version
1.3.6
 The text was updated successfully, but these errors were encountered: 
👍1
wdzhwsh4067 reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/openai-python/issues/911","Fix typing errors from mypy when calling completion","2023-11-30T20:55:44Z","Closed as not planned issue","question","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
Mypy doesn't seem to play very well with the type annotations that the SDK has generated. I can see that OpenAI uses Stainless and that might be the issue in itself, but as it stands the type annotations aren't very well documented.
See below for the exact use case and how to reproduce.
I'd propose either updating the annotations to be more semantically correct, or updating the documentation and usage if the idea is to keep the annotations more strict.
Option 1 seems like a better option until a full migration to stricter types can be reached, likely in a major version.
To Reproduce
When calling completion like so
client = AsyncOpenAI(api_key=<API_KEY>)
messages = [
    {""role"": ""user"", ""content"": ""some test message""}
]
completion = await client.chat.completions.create(
    model=""gpt-3.5-turbo-0613"", # Actual model doesn't matter here
    messages=messages # Mypy shows a type mismatch error here
)
I did a little bit of digging and found that the messages arg in chat.completions.create is annotated as ChatCompletionMessageParam, however the method is smart enough to take and serialize plain dicts as well. I haven't seen the rest of the endpoints, but my guess is that this annotation issue is present throughout.
I can get around it by using the appropriate types instead. So, the following works
from openai.types.chat import ChatCompletionMessageParam, ChatCompletionUserMessageParammessages: list[ChatCompletionMessageParam] = [
    ChatCompletionUserMessageParam(role=""user"", content=""some test message"")
]
# This seems to satisfy mypy
Code snippets
No response
OS
Linux
Python version
3.12
Library version
v1.3.6
 The text was updated successfully, but these errors were encountered: 
👍3
nathan-chappell, willemvdb42, and jhaansireddyms reacted with thumbs up emoji
All reactions
👍3 reactions"
"https://github.com/openai/openai-python/issues/910","Support for Azure OpenAI extentions","2023-12-01T07:40:11Z","Closed issue","No label","Confirm this is a feature request for the Python library and not the underlying OpenAI API.
 This is a feature request for the Python library
Describe the feature or improvement you're requesting
Azure OpenAI also has additional endpoints for extentions, especially for the ""on your data"" functionality, would be great if this package also supports that feature, it is described here: https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#completions-extensions it mostly consists of a slightly different endpoint and additional fields in the request, and the output adds a enhancement field with citations, latest version: https://github.com/Azure/azure-rest-api-specs/blob/main/specification/cognitiveservices/data-plane/AzureOpenAI/inference/preview/2023-12-01-preview/inference.yaml
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
👍1
moonbox3 reacted with thumbs up emoji🚀1
SimonJoel224 reacted with rocket emoji
All reactions
👍1 reaction
🚀1 reaction"
"https://github.com/openai/openai-python/issues/906","RecursionError in openai\_utils\_proxy.py","2024-01-16T16:59:37Z","Closed issue","No label","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
A RecursionError is encountered when debugging a Python script that includes import openai in PyCharm.
The specific error occurs within the openai library's _utils_proxy.py file, involving the get_proxied and getattr methods.
The error message is as follows:
 File ""<venv_path>\Lib\site-packages\openai\_utils\_proxy.py"", line 45, in __get_proxied__
    proxied = self.__proxied
  File ""<venv_path>\Lib\site-packages\openai\_utils\_proxy.py"", line 22, in __getattr__
    return getattr(self.__get_proxied__(), attr)
  ...
RecursionError: maximum recursion depth exceeded

This error is triggered upon initiating the PyCharm debugger to debug the script.
stack trace:
...
getattr, _proxy.py:22
get_proxied, _proxy.py:45
getattr, _proxy.py:22
get_proxied, _proxy.py:45
getattr, _proxy.py:22
get_proxied, _proxy.py:45
class, _proxy.py:39
init, _proxy.py:18
 <module>, numpy_proxy.py:29
 <module>, init.py:1
 <module>, embeddings.py:14
 <module>, init.py:20
 <module>, _client.py:12
 <module>, init.py:11
 <module>, temp.py:1
To Reproduce
In PyCharm, create and open a Python script that imports the OpenAI library (import openai).
Configure PyCharm for debugging the script.
Start the debugger in PyCharm. The command executed by PyCharm will be similar to:
<python_executable> -X pycache_prefix=<cache_directory> <pydevd_path> --multiprocess --qt-support --client <client_ip> --port <port> --file test.py
Observe the RecursionError during the debugging process when the OpenAI library is imported.
PyCharm 2023.3 EAP (Professional Edition)
Code snippets
import openai

print(""1"")
OS
Windows11
Python version
Python 3.12.0
Library version
1.3.6
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/903","Fix code example for Streaming section in Readme","2023-11-30T02:24:58Z","Closed issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
There are a couple of errors in the examples provided for the streaming section.
First Issue
 This is the example code, the last line should be print(chunk.choices[0].delta.content)
from openai import OpenAI

client = OpenAI()

stream = client.chat.completions.create(
    model=""gpt-4"",
    messages=[{""role"": ""user"", ""content"": ""Say this is a test""}],
    stream=True,
)
for chunk in stream:
    if chunk.choices[0].delta.content is not None:
        print(part.choices[0].delta.content)
Second Issue
 This is the example code, the last line should be print(chunk.choices[0].delta.content)
from openai import AsyncOpenAI

client = AsyncOpenAI()

stream = await client.chat.completions.create(
    prompt=""Say this is a test"",
    messages=[{""role"": ""user"", ""content"": ""Say this is a test""}],
    stream=True,
)
async for chunk in stream:
    if chunk.choices[0].delta.content is not None:
        print(part.choices[0].delta.content)
To Reproduce
If you run the snippet provided, there are None values as result
from openai import OpenAI

client = OpenAI()

stream = client.chat.completions.create(
    model=""gpt-4"",
    messages=[{""role"": ""user"", ""content"": ""Say this is a test""}],
    stream=True,
)
for chunk in stream:
    if chunk.choices[0].delta.content is not None:
        print(part.choices[0].delta.content)+
Result:
 None
 None
 None
 None
 None
 None
Code snippets
No response
OS
linux
Python version
Python 3.10.12
Library version
openai v1.3.3
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/901","@rattrayalex @RobertCraigie - hey there - Keiji from Azure OpenAI here!","2023-11-29T19:09:20Z","Closed as not planned issue","No label","@rattrayalex@RobertCraigie - hey there - Keiji from Azure OpenAI here!
We have written a more extended Azure OpenAI migration guide in our documentation: https://aka.ms/oai/v1-python-migration
Would it be possible to add it to the main body of this Guide?
Originally posted by @gojira in #742 (comment)
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/896","tab completion in ipython crashes interpreter","2023-12-03T02:41:16Z","Closed issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
When using tab completion on openai in interactive mode, my interpreter shows a big traceback and crashes ipython.
Traceback (most recent call last):
  File ""/path/lib/python3.10/site-packages/IPython/core/completer.py"", line 3171, in _complete
    result = matcher(context)
  File ""/path/lib/python3.10/site-packages/IPython/core/completer.py"", line 2707, in custom_completer_matcher
    matches = self.dispatch_custom_completer(context.token) or []
  File ""/path/lib/python3.10/site-packages/IPython/core/completer.py"", line 2747, in dispatch_custom_completer
    res = c(event)
  File ""/path/lib/python3.10/site-packages/IPython/core/completerlib.py"", line 272, in module_completer
    return module_completion(event.line)
  File ""/path/lib/python3.10/site-packages/IPython/core/completerlib.py"", line 249, in module_completion
    completion_list = try_import('.'.join(mod[:-1]), True)
  File ""/path/lib/python3.10/site-packages/IPython/core/completerlib.py"", line 183, in try_import
    completions.extend( [attr for attr in dir(m) if
  File ""/path/lib/python3.10/site-packages/IPython/core/completerlib.py"", line 184, in <listcomp>
    is_importable(m, attr, only_modules)])
  File ""/path/lib/python3.10/site-packages/IPython/core/completerlib.py"", line 153, in is_importable
    return inspect.ismodule(getattr(module, attr))
  File ""/.pyenv/versions/3.10.8/lib/python3.10/inspect.py"", line 189, in ismodule
    return isinstance(object, types.ModuleType)
  File ""/path/lib/python3.10/site-packages/openai/_utils/_proxy.py"", line 39, in __class__
    return self.__get_proxied__().__class__
  File ""/path/lib/python3.10/site-packages/openai/_utils/_proxy.py"", line 43, in __get_proxied__
    return self.__load__()
  File ""/path/lib/python3.10/site-packages/openai/lib/_old_api.py"", line 33, in __load__
    raise APIRemovedInV1(symbol=self._symbol)
openai.lib._old_api.APIRemovedInV1: 

You tried to access openai.Audio, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.

You can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. 

Alternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`

A detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742

To Reproduce
Open an interactive ipython session.
type from openai and press your tab key.
Code snippets
No response
OS
macOS
Python version
v3.10.8
Library version
openai v1.3.6
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/895","Azure OpenAI methods for embeddings and chat completions don't have a deployment parameter","2023-11-30T00:13:37Z","Closed issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
The Azure AsyncAzureOpenAI class has a single parameter to set the deployment name azure_deployment.
A common situation is to have an embeddings model deployment and a chat model deployment. In the calls for embeddings.create() and chat.completions.create() there is no parameter to set the deployment name.
What this means is you need to create multiple clients for each deployment you want to use.
To Reproduce
openai_client = AsyncAzureOpenAI(
            api_version=""2023-07-01-preview"",
            azure_endpoint=f""https://{AZURE_OPENAI_SERVICE}.openai.azure.com"",
            azure_ad_token_provider=token_provider,
            organization=OPENAI_ORGANIZATION,
            azure_deployment='?', # A single parameter for the whole client?
        )

# No parameter for the chat deployment name?await openai_client.chat.completions.create(
            model='gpt-35-turbo',
            messages=messages,
            temperature=0.0,
            max_tokens=100,
            n=1,
            functions=functions,
            function_call=""auto"",
        )
# No deployment name parameter?await openai_client.embeddings.create(model=embedding_model, input=query_text)
Without the deployment name being correct, calls to the embeddings API give an error message
Code snippets
No response
OS
macOS
Python version
Python 3.11.1
Library version
1.3.6
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/888","Calling OpenAI async no longer works after latest update","2023-11-28T00:35:22Z","Closed issue","No label","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
Trying to use OpenAI asyncrhonously via aiohttp. This worked in the last version of the API (like 2 or 3 weeks ago), why isn't it working now?
import openai
 from aiohttp import ClientSession
 self.session = ClientSession()
 openai.aiosession.set(self.session)
Exception: AttributeError: module 'openai' has no attribute 'aiosession'
To Reproduce
Run the code above
Code snippets
import openaifrom aiohttp import ClientSessionself.session = ClientSession()
openai.aiosession.set(self.session)
OS
Windows 11
Python version
Python 3.9
Library version
openai v1.3.5
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/887","response_format error","2023-11-28T04:53:24Z","Closed issue","No label","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
completion = client.chat.completions.create(
    messages=[
        {
            ""role"": ""user"",
            ""content"": ""Can you generate an example json object describing a fruit?"",
        }
    ],
    model=""gpt-3.5-turbo"",
    response_format={""type"": ""json_object""},
)

---------------------------------------------------------------------------
BadRequestError                           Traceback (most recent call last)
Cell In[5], line 1
----> 1 completion = client.chat.completions.create(
      2     messages=[
      3         {
      4             ""role"": ""user"",
      5             ""content"": ""Can you generate an example json object describing a fruit?"",
      6         }
      7     ],
      8     model=""gpt-3.5-turbo"",
      9     response_format={""type"": ""json_object""},
     10 )

File ~/anaconda3/envs/llama/lib/python3.10/site-packages/openai/_utils/_utils.py:299, in required_args.<locals>.inner.<locals>.wrapper(*args, **kwargs)
    297             msg = f""Missing required argument: {quote(missing[0])}""
    298     raise TypeError(msg)
--> 299 return func(*args, **kwargs)

File ~/anaconda3/envs/llama/lib/python3.10/site-packages/openai/resources/chat/completions.py:598, in Completions.create(self, messages, model, frequency_penalty, function_call, functions, logit_bias, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_p, user, extra_headers, extra_query, extra_body, timeout)
    551 @required_args([""messages"", ""model""], [""messages"", ""model"", ""stream""])
    552 def create(
    553     self,
   (...)
    596     timeout: float | httpx.Timeout | None | NotGiven = NOT_GIVEN,
    597 ) -> ChatCompletion | Stream[ChatCompletionChunk]:
--> 598     return self._post(
    599         ""/chat/completions"",
    600         body=maybe_transform(
    601             {
    602                 ""messages"": messages,
    603                 ""model"": model,
    604                 ""frequency_penalty"": frequency_penalty,
    605                 ""function_call"": function_call,
    606                 ""functions"": functions,
    607                 ""logit_bias"": logit_bias,
    608                 ""max_tokens"": max_tokens,
    609                 ""n"": n,
    610                 ""presence_penalty"": presence_penalty,
    611                 ""response_format"": response_format,
    612                 ""seed"": seed,
    613                 ""stop"": stop,
    614                 ""stream"": stream,
    615                 ""temperature"": temperature,
    616                 ""tool_choice"": tool_choice,
    617                 ""tools"": tools,
    618                 ""top_p"": top_p,
    619                 ""user"": user,
    620             },
    621             completion_create_params.CompletionCreateParams,
    622         ),
    623         options=make_request_options(
    624             extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout
    625         ),
    626         cast_to=ChatCompletion,
    627         stream=stream or False,
    628         stream_cls=Stream[ChatCompletionChunk],
    629     )

File ~/anaconda3/envs/llama/lib/python3.10/site-packages/openai/_base_client.py:1063, in SyncAPIClient.post(self, path, cast_to, body, options, files, stream, stream_cls)
   1049 def post(
   1050     self,
   1051     path: str,
   (...)
   1058     stream_cls: type[_StreamT] | None = None,
   1059 ) -> ResponseT | _StreamT:
   1060     opts = FinalRequestOptions.construct(
   1061         method=""post"", url=path, json_data=body, files=to_httpx_files(files), **options
   1062     )
-> 1063     return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))

File ~/anaconda3/envs/llama/lib/python3.10/site-packages/openai/_base_client.py:842, in SyncAPIClient.request(self, cast_to, options, remaining_retries, stream, stream_cls)
    833 def request(
    834     self,
    835     cast_to: Type[ResponseT],
   (...)
    840     stream_cls: type[_StreamT] | None = None,
    841 ) -> ResponseT | _StreamT:
--> 842     return self._request(
    843         cast_to=cast_to,
    844         options=options,
    845         stream=stream,
    846         stream_cls=stream_cls,
    847         remaining_retries=remaining_retries,
    848     )

File ~/anaconda3/envs/llama/lib/python3.10/site-packages/openai/_base_client.py:885, in SyncAPIClient._request(self, cast_to, options, remaining_retries, stream, stream_cls)
    882     # If the response is streamed then we need to explicitly read the response
    883     # to completion before attempting to access the response text.
    884     err.response.read()
--> 885     raise self._make_status_error_from_response(err.response) from None
    886 except httpx.TimeoutException as err:
    887     if retries > 0:

BadRequestError: Error code: 400 - {'error': {'message': ""Invalid parameter: 'response_format' of type 'json_object' is not supported with this model."", 'type': 'invalid_request_error', 'param': 'response_format', 'code': None}}

To Reproduce
run the code block
error
Code snippets
completion = client.chat.completions.create(
    messages=[
        {
            ""role"": ""user"",
            ""content"": ""Can you generate an example json object describing a fruit?"",
        }
    ],
    model=""gpt-3.5-turbo"",
    response_format={""type"": ""json_object""},
)
OS
22.04.1-Ubuntu
Python version
3.10.13
Library version
1.3.5
 The text was updated successfully, but these errors were encountered: 
👍1
tennox reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/openai-python/issues/886","AuthenticationError with Incorrect API Key","2023-11-27T09:58:59Z","Closed as not planned issue","question","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
When attempting to run the provided code, an openai.AuthenticationError is raised with the error message stating, ""Incorrect API key provided."" The error code is 401.
To Reproduce
Traceback (most recent call last):
 File ""d:\Chatbot_AzureAI_CSV_Project\open.py"", line 23, in 
 asyncio.run(main())
... (rest of the traceback)
openai.AuthenticationError: Error code: 401 - {'error': {'message': 'Incorrect API key provided: 9********************p. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Code snippets
import asynciofrom openai import AsyncOpenAI

client = AsyncOpenAI(
    api_key=""***************************"",  # <-- Replace with the correct API key
)

async def main() -> None:
    chat_completion = await client.chat.completions.create(
        messages=[
            {
                ""role"": ""user"",
                ""content"": ""Say this is a test"",
            }
        ],
        model=""gpt-3.5-turbo"",
    )

asyncio.run(main())
OS
Windows
Python version
3.11.0
Library version
Latest
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/885","Assistant API tokens usage","2023-11-27T14:22:46Z","Closed as not planned issue","No label","Confirm this is a feature request for the Python library and not the underlying OpenAI API.
 This is a feature request for the Python library
Describe the feature or improvement you're requesting
There is not usage when retrieving run/message/assistant using AssistantAPI, not like ChatCompletion.
 Wanted to know maybe there is a way to get the cost/usage or you are working on that.
Thank you very much! :)
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/883","Error from openai import OpenAI","2023-12-04T02:38:31Z","Closed as not planned issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
1.py"", line 1, in 
 from openai import OpenAI
 ImportError: cannot import name 'OpenAI' from 'openai' (C:\Python311\Lib\site-packages\openai_init_.py)
from openai import OpenAI
client = OpenAI(
 # defaults to os.environ.get(""OPENAI_API_KEY"")
 api_key=""yxv9HaaUBM3Pz"",
 )
chat_completion = client.chat.completions.create(
 messages=[
 {
 ""role"": ""user"",
 ""content"": ""Say this is a test"",
 }
 ],
 model=""gpt-3.5-turbo"",
 )
To Reproduce
Win 10 x 64
 python 3.11
from openai import OpenAI
client = OpenAI(
 # defaults to os.environ.get(""OPENAI_API_KEY"")
 api_key=""yxv9HaaUBM3Pz"",
 )
chat_completion = client.chat.completions.create(
 messages=[
 {
 ""role"": ""user"",
 ""content"": ""Say this is a test"",
 }
 ],
 model=""gpt-3.5-turbo"",
 )
Code snippets
from openai import OpenAI

client = OpenAI(
    # defaults to os.environ.get(""OPENAI_API_KEY"")
    api_key=""yxv9HaaUBM3Pz"",
)

chat_completion = client.chat.completions.create(
    messages=[
        {
            ""role"": ""user"",
            ""content"": ""Say this is a test"",
        }
    ],
    model=""gpt-3.5-turbo"",
)
OS
win 10 x 64
Python version
python 3.11.4
Library version
open 10.1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/882","Enhanced Chat History Search","2023-11-25T22:01:33Z","Closed as not planned issue","No label","Confirm this is a feature request for the Python library and not the underlying OpenAI API.
 This is a feature request for the Python library
Describe the feature or improvement you're requesting
Problem:
Currently, users are unable to search through their chat history, making it difficult to locate specific information. This necessitates scrolling through the entire conversation history, which can be time-consuming and inefficient.
Proposed Solution:
Implement a search functionality within the chat history, similar to that of WhatsApp. This feature would allow users to quickly and easily find the information they need by simply entering keywords or phrases.
Benefits:
Enhanced User Experience: The ability to search chat history would significantly improve the user experience by making it easier to navigate and locate specific information.
Improved Efficiency: By eliminating the need to manually scroll through entire chat histories, users would save time and effort, leading to increased efficiency.
Reduced Frustration: The ability to quickly find the desired information would reduce frustration and improve overall user satisfaction.
Implementation Details:
The search functionality should be integrated seamlessly into the existing chat interface. Users should be able to access the search bar from any point within the chat history. The search results should be displayed in a clear and organized manner, highlighting the most relevant matches.
If this seems a relevant feature, shall I give this a try?
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/881","CI pipeline is not running Unit tests","2023-11-25T22:02:40Z","Closed issue","No label","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
CI pipeline is not running Unit tests.
To Reproduce
1 . Open a PULL request against the main branch.
 2. See that tests do not run => Is clear when observing the ci.yml file
Code snippets
No response
OS
ANY
Python version
ANY
Library version
ANY
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/879","[Security][PIP] [Github] Setup dependabot in the repo","2023-11-25T22:04:53Z","Closed issue","No label","Confirm this is a feature request for the Python library and not the underlying OpenAI API.
 This is a feature request for the Python library
Describe the feature or improvement you're requesting
Setup Dependentbot in the repo, to have a daily scan for possible outdated dependencies, which represents a vulnerability risk.
 Assignee: @omonimus1
 PR presented:
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/874","Unclear how to use the new AsyncClient at scale - memory leaks or timeouts","2023-11-23T22:22:55Z","Closed issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
I have a containerized FastAPI server.
When I create new clients per request, I see a big memory leak. This is the same as Memory leak in the chat completion create method #820.
When I create a global AsyncClient that is reused across requests, I see regular httpx.PoolTimeouts. Similar to httpx.PoolTimeout occurs frequently with SyncClient #821, except mine is async
As a result, its not clear how to use the new library at scale. It seems like the optimal solution would be to manage a pool of AsyncClients and scale them up/down as QPS goes up/down. Looking for suggestions/advice on how to handle this. Thanks!
To Reproduce
Use this as the client:
def get_openai_client() -> AsyncOpenAI:
    if not hasattr(get_openai_client, ""client""):
        get_openai_client.client = AsyncOpenAI(
            api_key=""..."",
        )
    return get_openai_client.client

And then send a bunch of requests/see timeouts.
Code snippets
No response
OS
Linux
Python version
3.11
Library version
1.3.4
 The text was updated successfully, but these errors were encountered: 
👍3
josephzhang23, brian-goo, and karfly reacted with thumbs up emoji
All reactions
👍3 reactions"
"https://github.com/openai/openai-python/issues/873","peer closed connection without sending complete message body occurred when use different session to send msg at same time","2023-11-23T16:56:29Z","Closed issue","No label","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
background:
I used llama2 as backend for inferenceing
use chanlit to start a frontend for user.
 And I created two users, then I logined with user in different browser.
 If one session was inferencing now, and I send msg in another session, it will raise error as above.
And if I changed ""openai_api_base"" to openai api endpoint, everything works.
if I changed ""openai_api_base"" to my local api endpoint(for example llama2 service endpoint), it will raise error.
From above 3 and 4 seems like my local endpoint is borken right? But I've tried use concurrent to send multiple requests at a same time, everything works. (my endpoints based on llama.cpp)
So I suspect if there is some differences between openai official endpoint with local endpoint, especially when use openai library to send multiple completions request to local endpoint.
Many thx
To Reproduce
start a local llama2 service which based on llama.cpp and llama-cpp-python
start a frontend service based on chainlit
open two webpage in different browsers
to start to chat on frontend at same time, the later one will interrupt former one.

2023-11-24 00:19:39 - Your app is available at http://localhost:8001
2023-11-24 00:19:47 - HTTP Request: POST http://localhost:8000/v1/chat/completions ""HTTP/1.1 200 OK""
2023-11-24 00:19:49 - Error in LangchainTracer.on_llm_error callback: IndexError('list index out of range')
2023-11-24 00:19:49 - peer closed connection without sending complete message body (incomplete chunked read)
Traceback (most recent call last):
  File ""/Users/vince/miniconda3/envs/python310env/lib/python3.10/site-packages/httpcore/_exceptions.py"", line 10, in map_exceptions
    yield
  File ""/Users/vince/miniconda3/envs/python310env/lib/python3.10/site-packages/httpcore/_async/http11.py"", line 209, in _receive_event
    event = self._h11_state.next_event()
  File ""/Users/vince/miniconda3/envs/python310env/lib/python3.10/site-packages/h11/_connection.py"", line 469, in next_event
    event = self._extract_next_receive_event()
  File ""/Users/vince/miniconda3/envs/python310env/lib/python3.10/site-packages/h11/_connection.py"", line 419, in _extract_next_receive_event
    event = self._reader.read_eof()  # type: ignore[attr-defined]
  File ""/Users/vince/miniconda3/envs/python310env/lib/python3.10/site-packages/h11/_readers.py"", line 204, in read_eof
    raise RemoteProtocolError(
h11._util.RemoteProtocolError: peer closed connection without sending complete message body (incomplete chunked read)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""/Users/vince/miniconda3/envs/python310env/lib/python3.10/site-packages/httpx/_transports/default.py"", line 66, in map_httpcore_exceptions
    yield
  File ""/Users/vince/miniconda3/envs/python310env/lib/python3.10/site-packages/httpx/_transports/default.py"", line 249, in __aiter__
    async for part in self._httpcore_stream:
  File ""/Users/vince/miniconda3/envs/python310env/lib/python3.10/site-packages/httpcore/_async/connection_pool.py"", line 361, in __aiter__
    async for part in self._stream:
  File ""/Users/vince/miniconda3/envs/python310env/lib/python3.10/site-packages/httpcore/_async/http11.py"", line 337, in __aiter__
    raise exc
  File ""/Users/vince/miniconda3/envs/python310env/lib/python3.10/site-packages/httpcore/_async/http11.py"", line 329, in __aiter__
    async for chunk in self._connection._receive_response_body(**kwargs):
  File ""/Users/vince/miniconda3/envs/python310env/lib/python3.10/site-packages/httpcore/_async/http11.py"", line 198, in _receive_response_body
    event = await self._receive_event(timeout=timeout)
  File ""/Users/vince/miniconda3/envs/python310env/lib/python3.10/site-packages/httpcore/_async/http11.py"", line 208, in _receive_event
    with map_exceptions({h11.RemoteProtocolError: RemoteProtocolError}):
  File ""/Users/vince/miniconda3/envs/python310env/lib/python3.10/contextlib.py"", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File ""/Users/vince/miniconda3/envs/python310env/lib/python3.10/site-packages/httpcore/_exceptions.py"", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.RemoteProtocolError: peer closed connection without sending complete message body (incomplete chunked read)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""/Users/vince/miniconda3/envs/python310env/lib/python3.10/site-packages/chainlit/utils.py"", line 39, in wrapper
    return await user_function(**params_values)
  File ""/Users/vince/Downloads/myworkspace/llm/llm/chainlit_test/profile/app.py"", line 92, in on_message
    async for chunk in runnable.astream(
  File ""/Users/vince/miniconda3/envs/python310env/lib/python3.10/site-packages/langchain/schema/runnable/base.py"", line 1802, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
  File ""/Users/vince/miniconda3/envs/python310env/lib/python3.10/site-packages/langchain/schema/runnable/base.py"", line 1788, in atransform
    async for chunk in self._atransform_stream_with_config(
  File ""/Users/vince/miniconda3/envs/python310env/lib/python3.10/site-packages/langchain/schema/runnable/base.py"", line 1158, in _atransform_stream_with_config
    async for chunk in iterator:
  File ""/Users/vince/miniconda3/envs/python310env/lib/python3.10/site-packages/langchain/schema/runnable/base.py"", line 1761, in _atransform
    async for output in final_pipeline:
  File ""/Users/vince/miniconda3/envs/python310env/lib/python3.10/site-packages/langchain/schema/output_parser.py"", line 343, in atransform
    async for chunk in self._atransform_stream_with_config(
  File ""/Users/vince/miniconda3/envs/python310env/lib/python3.10/site-packages/langchain/schema/runnable/base.py"", line 1158, in _atransform_stream_with_config
    async for chunk in iterator:
  File ""/Users/vince/miniconda3/envs/python310env/lib/python3.10/site-packages/langchain/schema/output_parser.py"", line 321, in _atransform
    async for chunk in input:
  File ""/Users/vince/miniconda3/envs/python310env/lib/python3.10/site-packages/langchain/utils/aiter.py"", line 97, in tee_peer
    item = await iterator.__anext__()
  File ""/Users/vince/miniconda3/envs/python310env/lib/python3.10/site-packages/langchain/schema/runnable/base.py"", line 683, in atransform
    async for output in self.astream(final, config, **kwargs):
  File ""/Users/vince/miniconda3/envs/python310env/lib/python3.10/site-packages/langchain/chat_models/base.py"", line 273, in astream
    raise e
  File ""/Users/vince/miniconda3/envs/python310env/lib/python3.10/site-packages/langchain/chat_models/base.py"", line 262, in astream
    async for chunk in self._astream(
  File ""/Users/vince/miniconda3/envs/python310env/lib/python3.10/site-packages/langchain/chat_models/openai.py"", line 469, in _astream
    async for chunk in await acompletion_with_retry(
  File ""/Users/vince/miniconda3/envs/python310env/lib/python3.10/site-packages/openai/_streaming.py"", line 94, in __aiter__
    async for item in self._iterator:
  File ""/Users/vince/miniconda3/envs/python310env/lib/python3.10/site-packages/openai/_streaming.py"", line 123, in __stream__
    async for sse in iterator:
  File ""/Users/vince/miniconda3/envs/python310env/lib/python3.10/site-packages/openai/_streaming.py"", line 98, in _iter_events
    async for sse in self._decoder.aiter(self.response.aiter_lines()):
  File ""/Users/vince/miniconda3/envs/python310env/lib/python3.10/site-packages/openai/_streaming.py"", line 190, in aiter
    async for line in iterator:
  File ""/Users/vince/miniconda3/envs/python310env/lib/python3.10/site-packages/httpx/_models.py"", line 966, in aiter_lines
    async for text in self.aiter_text():
  File ""/Users/vince/miniconda3/envs/python310env/lib/python3.10/site-packages/httpx/_models.py"", line 953, in aiter_text
    async for byte_content in self.aiter_bytes():
  File ""/Users/vince/miniconda3/envs/python310env/lib/python3.10/site-packages/httpx/_models.py"", line 932, in aiter_bytes
    async for raw_bytes in self.aiter_raw():
  File ""/Users/vince/miniconda3/envs/python310env/lib/python3.10/site-packages/httpx/_models.py"", line 990, in aiter_raw
    async for raw_stream_bytes in self.stream:
  File ""/Users/vince/miniconda3/envs/python310env/lib/python3.10/site-packages/httpx/_client.py"", line 146, in __aiter__
    async for chunk in self._stream:
  File ""/Users/vince/miniconda3/envs/python310env/lib/python3.10/site-packages/httpx/_transports/default.py"", line 248, in __aiter__
    with map_httpcore_exceptions():
  File ""/Users/vince/miniconda3/envs/python310env/lib/python3.10/contextlib.py"", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File ""/Users/vince/miniconda3/envs/python310env/lib/python3.10/site-packages/httpx/_transports/default.py"", line 83, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: peer closed connection without sending complete message body (incomplete chunked read)
2023-11-24 00:19:49 - HTTP Request: POST http://localhost:8000/v1/chat/completions ""HTTP/1.1 200 OK""

Code snippets
import jsonfrom typing import Optional

import chainlit as clfrom langchain.chat_models import ChatOpenAIfrom langchain.prompts import ChatPromptTemplatefrom langchain.schema import StrOutputParserfrom langchain.schema.runnable import Runnablefrom langchain.schema.runnable.config import RunnableConfigimport platform

url = 'xxxx'  # local service endpointurl = 'openai endpoint'


@cl.set_chat_profilesasync def chat_profile(current_user: cl.AppUser):
    if ""admin_user"" not in current_user.tags:
        # Default to 3.5 when not admin
        return [
            cl.ChatProfile(
                name=""GPT-3.5"",
                markdown_description=""The underlying LLM model is **GPT-3.5**."",
                icon=""https://picsum.photos/200"",
            )
        ]

    return [
        cl.ChatProfile(
            name=""GPT-3.5"",
            markdown_description=""The underlying LLM model is **GPT-3.5**."",
            icon=""https://picsum.photos/200"",
        ),
        cl.ChatProfile(
            name=""GPT-4"",
            markdown_description=""The underlying LLM model is **GPT-4**."",
            icon=""https://picsum.photos/250"",
        ),
    ]


@cl.password_auth_callbackdef auth_callback(username: str, password: str) -> Optional[cl.AppUser]:
    auth = None
    with open('authentication.json', 'r') as f:
        auth = json.load(f)
    if (username, password) == (""admin"", ""admin""):
        return cl.AppUser(
            username=""admin"", role=""ADMIN"", provider=""credentials"", tags=[""admin_user""]
        )
    elif username == auth.get(username, None):
        return cl.AppUser(
            username=username, role=""USER"", provider=""credentials"", tags=[""regular_user""]
        )
    else:
        return None


@cl.on_chat_startasync def on_chat_start():
    app_user = cl.user_session.get(""user"")
    chat_profile = cl.user_session.get(""chat_profile"")
    await cl.Message(
        content=f""starting chat with {app_user.username} using the {chat_profile} chat profile""
    ).send()

    model = ChatOpenAI(temperature=0.0, openai_api_base=url, openai_api_key=""xxx"", max_tokens=2048, streaming=True)
    prompt = ChatPromptTemplate.from_messages(
        [
            (
                ""system"",
                # ""You're a very knowledgeable historian who provides accurate and eloquent answers to historical questions."",
                ""You're a Linux expert who provides accurate and eloquent answers to Linux system related questions. Only answer the question about Linux""
            ),
            (""human"", ""{question}""),
        ]
    )
    runnable = prompt | model | StrOutputParser()
    cl.user_session.set(app_user.username, runnable)


@cl.on_messageasync def on_message(message: cl.Message):
    runnable = cl.user_session.get(message.author)  # type: Runnable
    print(message.author, message.content)
    msg = cl.Message(content="""")

    async for chunk in runnable.astream(
            {""question"": message.content},
            config=RunnableConfig(callbacks=[cl.LangchainCallbackHandler()]),
    ):
        await msg.stream_token(chunk)
OS
macos
Python version
python 3.10
Library version
openai 1.3.5
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/872","In openai V0.27.0 version, there is a field called response_ms in the API call return, but it cannot be found after 1.0.0. Is there an alternative way to obtain the call duration?","2023-11-23T17:09:04Z","Closed issue","enhancement","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
In openai V0.27.0 version, there is a field called response_ms in the API call return, but it cannot be found after 1.0.0. Is there an alternative way to obtain the call duration?
ERROR messageAttributeError: 'ChatCompletion' object has no attribute 'response_ms'
Thank you very much
To Reproduce
client = openai.AzureOpenAI(
    azure_endpoint=endpoint,
    api_key=api_key,
    api_version=""2023-09-01-preview""
)

    completion = client.chat.completions.create(
        model=deployment,
        messages=messages,
        max_tokens=4096,
        temperature=0,
    )

response_time = completion .response_ms

Code snippets
client = openai.AzureOpenAI(
        azure_endpoint=endpoint,
        api_key=api_key,
        api_version=""2023-09-01-preview""
    )

        completion = client.chat.completions.create(
            model=deployment,
            messages=messages,
            max_tokens=4096,
            temperature=0,
        )

    response_time = completion .response_ms

ERROR messageAttributeError: 'ChatCompletion' object has no attribute 'response_ms'
OS
win11
Python version
Python v3.11.6
Library version
openai v1.3.5
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/871","Allow filename to be passed as a parameter into files.create","2023-11-23T15:56:46Z","Closed issue","No label","Confirm this is a feature request for the Python library and not the underlying OpenAI API.
 This is a feature request for the Python library
Describe the feature or improvement you're requesting
Right now, you can only pass a FileTypes file object into the files.create function. Therefore, when doing
with open(""some_file.txt"", ""r"") as f:
        file = f.read()
    openai_file = openai.files.create(file=file, purpose=""assistants"")
there is no way of passing a filename into the request (altough the endpoint accepts one, the library provides no way of passing one manually).
 A workaround I found is
response = openai.files.create(file=Path(file_path), purpose=""assistants"")
but I think many people don't know about it, and you cannot provide a different filename than the one in the file system.
I can look into the library and make a PR myself if the feature sounds good:)
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
👍1
Adam-D-Lewis reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/openai-python/issues/870","Pylint complains when using Stream[ChatCompletionChunk]: E1133 not-an-iterable","2023-11-23T11:47:54Z","Closed as not planned issue","question","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
I´m constructing a chat completion response:
query_response: Stream[ChatCompletionChunk] = client.chat.completions.create(
            ...
            stream=True,
        )

Then, when iterating over the chunks, it works but Pylint raises an error:
        for chunk in query_response:
            choice = chunk.choices[0]

Error:
Non-iterable value query_response is used in an iterating context
To Reproduce
Create a chat completion with streaming enabled
Read chunks by iterating over the Stream
Run pylint
Code snippets
No response
OS
macOS
Python version
3.11.5
Library version
1.3.5
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/868","Embeddings with openAI ""text-embedding-ada-002"" are not deterministic","2023-11-23T20:39:19Z","Closed issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
I'm not sure if this is an issue with the python library or part of the should be reported as part of the developer community.
 Reproducing the embeddings from the same input produce different embeddings.
To Reproduce
>>> from openai import OpenAI
>>> client = OpenAI(api_key=""..."")
>>> for i in range(50):
...     client.embeddings.create(model=""text-embedding-ada-002"", input=""hello"").data[0].embedding[:5]
... 
[-0.025058425962924957, -0.019388560205698013, -0.027781018987298012, -0.030979404225945473, -0.024688364937901497]
[-0.025058425962924957, -0.019388560205698013, -0.027781018987298012, -0.030979404225945473, -0.024688364937901497]
[-0.025058425962924957, -0.019388560205698013, -0.027781018987298012, -0.030979404225945473, -0.024688364937901497]
[-0.025058425962924957, -0.019388560205698013, -0.027781018987298012, -0.030979404225945473, -0.024688364937901497]
[-0.025058425962924957, -0.019388560205698013, -0.027781018987298012, -0.030979404225945473, -0.024688364937901497]
[-0.025058425962924957, -0.019388560205698013, -0.027781018987298012, -0.030979404225945473, -0.024688364937901497]
[-0.025058425962924957, -0.019388560205698013, -0.027781018987298012, -0.030979404225945473, -0.024688364937901497]
[-0.025058425962924957, -0.019388560205698013, -0.027781018987298012, -0.030979404225945473, -0.024688364937901497]
[-0.025058425962924957, -0.019388560205698013, -0.027781018987298012, -0.030979404225945473, -0.024688364937901497]
[-0.025058425962924957, -0.019388560205698013, -0.027781018987298012, -0.030979404225945473, -0.024688364937901497]
[-0.025058425962924957, -0.019388560205698013, -0.027781018987298012, -0.030979404225945473, -0.024688364937901497]
[-0.025058425962924957, -0.019388560205698013, -0.027781018987298012, -0.030979404225945473, -0.024688364937901497]
[-0.025058425962924957, -0.019388560205698013, -0.027781018987298012, -0.030979404225945473, -0.024688364937901497]
[-0.024994507431983948, -0.019366780295968056, -0.027768738567829132, -0.031097816303372383, -0.02462460845708847]
[-0.025058425962924957, -0.019388560205698013, -0.027781018987298012, -0.030979404225945473, -0.024688364937901497]
[-0.0250103659927845, -0.01939525455236435, -0.027798103168606758, -0.030995413661003113, -0.024706488475203514]
[-0.025058425962924957, -0.019388560205698013, -0.027781018987298012, -0.030979404225945473, -0.024688364937901497]
[-0.025058425962924957, -0.019388560205698013, -0.027781018987298012, -0.030979404225945473, -0.024688364937901497]
[-0.025058425962924957, -0.019388560205698013, -0.027781018987298012, -0.030979404225945473, -0.024688364937901497]
[-0.025058425962924957, -0.019388560205698013, -0.027781018987298012, -0.030979404225945473, -0.024688364937901497]
[-0.025058425962924957, -0.019388560205698013, -0.027781018987298012, -0.030979404225945473, -0.024688364937901497]
[-0.0250103659927845, -0.01939525455236435, -0.027798103168606758, -0.030995413661003113, -0.024706488475203514]
[-0.025058425962924957, -0.019388560205698013, -0.027781018987298012, -0.030979404225945473, -0.024688364937901497]
[-0.025058425962924957, -0.019388560205698013, -0.027781018987298012, -0.030979404225945473, -0.024688364937901497]
[-0.025058425962924957, -0.019388560205698013, -0.027781018987298012, -0.030979404225945473, -0.024688364937901497]
[-0.025058425962924957, -0.019388560205698013, -0.027781018987298012, -0.030979404225945473, -0.024688364937901497]
[-0.025058425962924957, -0.019388560205698013, -0.027781018987298012, -0.030979404225945473, -0.024688364937901497]
[-0.025048330426216125, -0.019377758726477623, -0.027810918167233467, -0.0310361385345459, -0.02466500550508499]
[-0.025058425962924957, -0.019388560205698013, -0.027781018987298012, -0.030979404225945473, -0.024688364937901497]
[-0.025058425962924957, -0.019388560205698013, -0.027781018987298012, -0.030979404225945473, -0.024688364937901497]
[-0.025058425962924957, -0.019388560205698013, -0.027781018987298012, -0.030979404225945473, -0.024688364937901497]
[-0.025058425962924957, -0.019388560205698013, -0.027781018987298012, -0.030979404225945473, -0.024688364937901497]
[-0.025058425962924957, -0.019388560205698013, -0.027781018987298012, -0.030979404225945473, -0.024688364937901497]
[-0.025058425962924957, -0.019388560205698013, -0.027781018987298012, -0.030979404225945473, -0.024688364937901497]
[-0.025058425962924957, -0.019388560205698013, -0.027781018987298012, -0.030979404225945473, -0.024688364937901497]
[-0.025058425962924957, -0.019388560205698013, -0.027781018987298012, -0.030979404225945473, -0.024688364937901497]
[-0.025058425962924957, -0.019388560205698013, -0.027781018987298012, -0.030979404225945473, -0.024688364937901497]
[-0.025058425962924957, -0.019388560205698013, -0.027781018987298012, -0.030979404225945473, -0.024688364937901497]
[-0.025058425962924957, -0.019388560205698013, -0.027781018987298012, -0.030979404225945473, -0.024688364937901497]
[-0.025058425962924957, -0.019388560205698013, -0.027781018987298012, -0.030979404225945473, -0.024688364937901497]
[-0.025058425962924957, -0.019388560205698013, -0.027781018987298012, -0.030979404225945473, -0.024688364937901497]
[-0.02502652257680893, -0.019331468269228935, -0.027801373973488808, -0.031051915138959885, -0.02469618245959282]
[-0.025058425962924957, -0.019388560205698013, -0.027781018987298012, -0.030979404225945473, -0.024688364937901497]
[-0.025058425962924957, -0.019388560205698013, -0.027781018987298012, -0.030979404225945473, -0.024688364937901497]
[-0.025058425962924957, -0.019388560205698013, -0.027781018987298012, -0.030979404225945473, -0.024688364937901497]
[-0.025058425962924957, -0.019388560205698013, -0.027781018987298012, -0.030979404225945473, -0.024688364937901497]
[-0.025058425962924957, -0.019388560205698013, -0.027781018987298012, -0.030979404225945473, -0.024688364937901497]
[-0.025058425962924957, -0.019388560205698013, -0.027781018987298012, -0.030979404225945473, -0.024688364937901497]
[-0.025058425962924957, -0.019388560205698013, -0.027781018987298012, -0.030979404225945473, -0.024688364937901497]
[-0.025058425962924957, -0.019388560205698013, -0.027781018987298012, -0.030979404225945473, -0.024688364937901497]

Code snippets
No response
OS
MacOS
Python version
Python 3.10.10
Library version
openai v1.3.5
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/865","Memory leak when using with_options() to configure options per request","2023-12-08T18:57:53Z","Closed issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
Using with_options() to configure options per request introduces a memory leak.
How to determine it's a memory leak?
The following code calls the completion API 10 times. On each iteration, it takes a snapshot of traces of memory blocks using tracemalloc, compares it to the snapshot of the previous iteration, and prints the top 2 differences. The memory required by the openai library increases on every iteration. In particular, the following lines have the most increases (using sync client):
.../env/lib/python3.11/site-packages/openai/_response.py:227: size=321 KiB (+26.0 KiB), count=2681 (+175), average=123 B
.../env/lib/python3.11/site-packages/openai/_response.py:226: size=214 KiB (+19.4 KiB), count=1830 (+165), average=120 B

import os
import tracemalloc
from openai import OpenAI

tracemalloc.start()

client = OpenAI(api_key=os.getenv(""OPENAI_API_KEY""))

snapshot1 = tracemalloc.take_snapshot()
for _ in range(10):
    client.with_options(max_retries=5).chat.completions.create(
        messages=[
            {
                ""role"": ""user"",
                ""content"": ""How can I get the name of the current day in Node.js?"",
            }
        ],
        model=""gpt-3.5-turbo"",
    )

    snapshot2 = tracemalloc.take_snapshot()
    top_stats = snapshot2.compare_to(snapshot1, 'lineno')
    print(""[ Top 2 differences ]"")
    for stat in top_stats[:2]:
        print(stat)
    
    snapshot1 = snapshot2

Potential cause
Looking at the library code, when using with_options(), a new client is created on every request (sync client and async client).
To Reproduce
Use with_options() and make multiple API calls.
Code snippets
No response
OS
macOS
Python version
Python v3.11.5
Library version
openai v1.3.3
 The text was updated successfully, but these errors were encountered: 
👍1
adhsu reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/openai-python/issues/864","TTS streaming does not work","2024-03-03T00:55:17Z","Closed issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
When following the documentation on how to use client.audio.speech.create(), the returned response has a method called stream_to_file(file_path) which explains that when used, it should stream the content of the audio file as it's being created. This does not seem to work. I've used a rather large text input that generates a 3.5 minute sound file and the file is only created once the whole request is completed.
To Reproduce
Utilize the following code and replace the text input with a decently large amount of text.
from pathlib import Pathfrom openai import OpenAIclient = OpenAI()

speech_file_path = Path(__file__).parent / ""speech.mp3""response = client.audio.speech.create(
    model=""tts-1"",
    voice=""alloy"",
    input=""""""        <Decently large bit of text here>    """"""
)

response.stream_to_file(speech_file_path)
Notice that when the script is run that the speech.mp3 file is only ever created after the request is fully completed.
Code snippets
No response
OS
macOS
Python version
Python 3.11.6
Library version
openai v1.2.4
 The text was updated successfully, but these errors were encountered: 
👍5
Shackless, rattrayalex, panique, ArthurAsenheimer, and leyuan reacted with thumbs up emoji
All reactions
👍5 reactions"
"https://github.com/openai/openai-python/issues/863","Why, my msg can be used, but the tools case cannot be passed","2023-11-22T14:49:10Z","Closed as not planned issue","API-feedback","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
(openai) /mnt/workspace/project/Eagent> python 1.py
 Traceback (most recent call last):
 File ""/mnt/workspace/project/Eagent/1.py"", line 94, in 
 print(run_conversation())
 File ""/mnt/workspace/project/Eagent/1.py"", line 55, in run_conversation
 response = client.chat.completions.create(
 File ""/home/pai/lib/python3.9/site-packages/openai/_utils/_utils.py"", line 299, in wrapper
 return func(*args, **kwargs)
 File ""/home/pai/lib/python3.9/site-packages/openai/resources/chat/completions.py"", line 598, in create
 return self._post(
 File ""/home/pai/lib/python3.9/site-packages/openai/_base_client.py"", line 1063, in post
 return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
 File ""/home/pai/lib/python3.9/site-packages/openai/_base_client.py"", line 842, in request
 return self._request(
 File ""/home/pai/lib/python3.9/site-packages/openai/_base_client.py"", line 885, in _request
 raise self._make_status_error_from_response(err.response) from None
 openai.NotFoundError: Error code: 404 - {'error': {'message': 'Unrecognized request argument supplied: tools (request id: 202311220623247350366364AkoGmo5)', 'type': 'invalid_request_error', 'param': '', 'code': None}}
To Reproduce
Fill in ak and run the code
Code snippets
from openai import OpenAIfrom openai import api_versionimport jsonimport os 
api_base = ""https://vp01.glyph.cyou:8443/v1""api_key = ""xxxx""model_name = ""gpt-3.5-turbo-1106""# model_name = ""gpt-4""# gpt-3.5-turbo、 gpt-3.5-turbo-16k、gpt-4、gpt-4-32k、gpt-3.5-turbo-1106、gpt-4-1106-preview

os.environ[""OPENAI_API_BASE""] = api_baseos.environ[""OPENAI_API_KEY""] = api_keyclient = OpenAI(
    api_key=api_key,
    base_url=api_base
)
# client = OpenAI()

# Example dummy function hard coded to return the same weather# In production, this could be your backend API or an external APIdef get_current_weather(location, unit=""fahrenheit""):
    """"""Get the current weather in a given location""""""
    if ""tokyo"" in location.lower():
        return json.dumps({""location"": ""Tokyo"", ""temperature"": ""10"", ""unit"": ""celsius""})
    elif ""san francisco"" in location.lower():
        return json.dumps({""location"": ""San Francisco"", ""temperature"": ""72"", ""unit"": ""fahrenheit""})
    elif ""paris"" in location.lower():
        return json.dumps({""location"": ""Paris"", ""temperature"": ""22"", ""unit"": ""celsius""})
    else:
        return json.dumps({""location"": location, ""temperature"": ""unknown""})

def run_conversation():
    # Step 1: send the conversation and available functions to the model
    messages = [{""role"": ""user"", ""content"": ""What's the weather like in San Francisco, Tokyo, and Paris?""}]
    tools = [
        {
            ""type"": ""function"",
            ""function"": {
                ""name"": ""get_current_weather"",
                ""description"": ""Get the current weather in a given location"",
                ""parameters"": {
                    ""type"": ""object"",
                    ""properties"": {
                        ""location"": {
                            ""type"": ""string"",
                            ""description"": ""The city and state, e.g. San Francisco, CA"",
                        },
                        ""unit"": {""type"": ""string"", ""enum"": [""celsius"", ""fahrenheit""]},
                    },
                    ""required"": [""location""],
                },
            },
        }
    ]
    response = client.chat.completions.create(
        model=model_name,
        messages=messages,
        tools=tools,
        # tool_choice=""auto"",  # auto is default, but we'll be explicit
    )
    response_message = response.choices[0].message
    print(response_message.model_dump_json(indent=4))
    tool_calls = response_message.tool_calls
    # Step 2: check if the model wanted to call a function
    if tool_calls:
        # Step 3: call the function
        # Note: the JSON response may not always be valid; be sure to handle errors
        available_functions = {
            ""get_current_weather"": get_current_weather,
        }  # only one function in this example, but you can have multiple
        messages.append(response_message)  # extend conversation with assistant's reply
        # Step 4: send the info for each function call and function response to the model
        for tool_call in tool_calls:
            function_name = tool_call.function.name
            function_to_call = available_functions[function_name]
            function_args = json.loads(tool_call.function.arguments)
            function_response = function_to_call(
                location=function_args.get(""location""),
                unit=function_args.get(""unit""),
            )
            messages.append(
                {
                    ""tool_call_id"": tool_call.id,
                    ""role"": ""tool"",
                    ""name"": function_name,
                    ""content"": function_response,
                }
            )  # extend conversation with function response
        second_response = client.chat.completions.create(
            model=""gpt-3.5-turbo-1106"",
            messages=messages,
        )  # get a new response from the model where it can see the function response
        return second_responseprint(run_conversation())
OS
macos
Python version
v3.10
Library version
openai v1.34.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/862","test","2023-11-21T22:52:15Z","Closed issue","No label","Confirm this is a feature request for the Python library and not the underlying OpenAI API.
 This is a feature request for the Python library
Describe the feature or improvement you're requesting
na
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/861","New bug in Thread","2023-11-22T00:03:51Z","Closed issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
I had great code working last night (Nov 20, 2023). It had been working well for a week. Around 11:00 pm it stopped returning the expected result and would only return the prompt I typed in. I then ran your example: examples/assistant.py
I was using openai==1.3.2 when the error appeared. I upgraded to openai==1.3.4, but this did not resolve anything.
Your code in examples/assistant.py will now not complete. Openai never completes. Please advise and let us know what has changed in the last 12 hours.
To Reproduce
python assistant.py
checking assistant status.
 in progress...
 in progress...
 in progress...
 in progress...
 in progress...
 in progress...
 in progress...
 in progress...
 in progress...
 in progress...
 in progress...
 in progress...
 in progress...
 in progress...
 in progress...
 in progress...
 in progress...
 in progress...
 in progress...
 in progress...
 in progress...
 in progress...
 in progress...
 in progress...
 in progress...
 in progress...
 in progress...
 in progress...
 in progress...
 in progress...
 in progress...
 in progress...
 in progress...
 in progress...
 in progress...
 in progress...
 in progress...
 in progress...
 in progress...
 in progress...
 in progress...
 in progress...
 in progress...
 in progress...
 in progress...
 in progress...
 in progress...
 in progress...
 in progress...
 in progress...
 in progress...
 in progress...
 in progress...
 in progress...
 in progress...
 in progress...
 in progress...
 in progress...
 in progress...
 in progress...
 in progress...
 in progress...
 in progress...
 in progress...
 in progress...
 in progress...
 in progress...
 in progress...
 in progress...
 in progress...
 in progress...
 in progress...
 in progress...
 in progress...
 in progress...
 in progress...
 in progress...
 in progress...
 in progress...
 in progress...
 in progress...
 in progress...
 in progress...
 in progress...
 in progress...
 in progress...
 in progress...
 in progress...
 in progress...
 in progress...
 in progress...
 in progress...
 in progress...
 in progress...
 in progress...
 in progress...
 in progress...
 in progress...
 in progress...
 in progress...
 in progress...
 in progress...
 in progress...
 in progress...
 in progress...
 in progress...
 in progress...
 in progress...
 in progress...
 in progress...
 in progress...
 in progress...
 in progress...
 in progress...
 in progress...
 in progress...
 in progress...
 in progress...
 in progress...
 in progress...
Code snippets
No response
OS
Windows 10
Python version
Python 3.12.0
Library version
openai==1.3.2
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/857","json format causes infinite ""\n \n \n \n"" in response","2023-11-21T14:11:14Z","Closed as not planned issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
I am trying to use json format to get json response, it worked well when I give same short example, but when I use production data to test it, prompt_token = 2966, then it start to response with all ""\n \n \n \n"", till max token. Then, I use the same prompt without response_format, it worked well though it's not a json object.
(test with a another shorter user message still failed, is that the ""#"" matter?)
To Reproduce
tsg = """"""
# TSG for debugging openai
## 1. install openai
### 1.1. install openai with pip
### 1.2. install openai with conda
## 2. import openai
### 2.1. import openai in python
### 2.2. import openai in jupyter notebook
## 3. use openai
### 3.1. use openai in python
### 3.2. use openai in jupyter notebook
""""""
messages = [
                {
                    ""role"": ""system"",
                    ""content"": 
                        """"""
                        You are an expert of reading troubeshooting guidance.
                        The users will provide you a troubleshooting guide in markdown format, which consists of several steps.
                        You need to break down the document into steps based on the text semantics and markdown structure and return JSON format
                        Note that: (1) Ignore the document title if it does not indicate a step. (2) Only do text slicing from front to back, can't lose any content of the step. (3) Maintain the original text in task_description without any summarization or abbreviation. (4) Don't lose the prefix and serial number of the title displayed in the document. (5) If the step itself has a title in document, the task_title should use the original content.
                        You will respond with the list of steps as a JSON object. Here's an example of your output format: 
                        [{
                            ""task_title"": """",
                            ""task_description"": """",
                        },
                        {
                            ""task_title"": """",
                            ""task_description"": """",
                        }].
                        Here is an example of the input markdown document:
                            # Troubleshooting guide for buying a puppy
                            ## 1. know what puppy you want
                            ### 1.1. you could surf the internet to find the puppy you want
                            ### 1.2. visit friends who have puppies to see if you like them
                            ## 2. buy healthy puppies
                            ### 2.1. you could go to puppy selling websites to find healthy puppies, if you prefer buying puppies online, please go to step 3 for more information
                            ### 2.2. you could go to pet stores to find healthy puppies
                            ## 3. buy puppies online
                            here is a list of puppy selling websites: www.happydog.com, www.puppy.com, www.puppylove.com
                        Here is an example of the output json object:
                        [{
                            ""task_title"": ""1. know what puppy you want"",
                            ""task_description"": ""### 1.1. you could surf the internet to find the puppy you want\n### 1.2. visit friends who have puppies to see if you like them""
                        },
                        {
                            ""task_title"": ""2. buy healthy puppies"",
                            ""task_description"": ""### 2.1. you could go to puppy selling websites to find healthy puppies, if you prefer buying puppies online, please go to step 3 for more information\n### 2.2. you could go to pet stores to find healthy puppies""
                        },
                        {
                            ""task_title"": ""3. buy puppies online"",
                            ""task_description"": ""here is a list of puppy selling websites: www.happydog.com, www.puppy.com, www.puppylove.com""
                        }
                        ]
                        """"""
                },
                {
                    ""role"": ""user"", 
                    ""content"": tsg
                }
            ]

response = llm.client.chat.completions.create(
            model = llm.engine,
            messages = messages,
            response_format={""type"": ""json_object""},
            temperature = 0,
        )

Code snippets
No response
OS
win11
Python version
3.10.12
Library version
1.3.3
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/850","consistent seed not generating reproducible responses","2023-11-21T00:51:32Z","Closed as not planned issue","API-feedback,bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
When I call gpt-4-turbo streaming with a constant seed (1234), There is still variations in the output, espeically as the prompt and answer get longer. Is there any way to avoid this or somehow force it to be even more deterministic?
To Reproduce
code below, problem occurs specifically with in-debth thought provoking prompts.
Code snippets
client = AsyncOpenAI(timeout=30)

async def send_openai_request(prompt, engine=""gpt-4-1106-preview""):
    try:
        stream = await client.chat.completions.create(
            messages=[{""role"": ""user"", ""content"": prompt}],
            stream=True,
            model=engine,
            seed=1234,
            temperature=0.0001,
        )
        collected_messages = []

        async for part in stream:
            print(part.choices[0].delta.content or """")
            collected_messages.append(part.choices[0].delta.content or """")

        all_messages = ''.join(collected_messages)
        return all_messages


### OS

macOS

### Python version

Python 3.10.12

### Library version

openai 1.3.3

 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/848","403 error on post request","2023-11-18T23:40:19Z","Closed as not planned issue","No label","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
i have error 403 on my discord bot, using library
 i dont use VPN or proxy
2023-11-18 17:38:35.096 INFO: HTTP Request: POST https://api.openai.com/v1/chat/completions ""HTTP/1.1 403 Forbidden""
2023-11-18 17:38:35.098 ERROR: Ignoring exception in on_message
Traceback (most recent call last):
  File ""/usr/local/lib/python3.11/dist-packages/discord/client.py"", line 409, in _run_event
    await coro(*args, **kwargs)
  File ""/root/gpt-discord/gpt-discord.py"", line 85, in on_message
    async for part in await openai_client.chat.completions.create(model=os.environ[""GPT_MODEL""], messages=msgs, max_tokens=MAX_COMPLETION_TOKENS, stream=True):
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/dist-packages/openai/resources/chat/completions.py"", line 1191, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/dist-packages/openai/_base_client.py"", line 1480, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/dist-packages/openai/_base_client.py"", line 1275, in request
    return await self._request(
           ^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/dist-packages/openai/_base_client.py"", line 1318, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.PermissionDeniedError: <!DOCTYPE html>
<!--[if lt IE 7]> <html class=""no-js ie6 oldie"" lang=""en-US""> <![endif]-->
<!--[if IE 7]>    <html class=""no-js ie7 oldie"" lang=""en-US""> <![endif]-->
<!--[if IE 8]>    <html class=""no-js ie8 oldie"" lang=""en-US""> <![endif]-->
<!--[if gt IE 8]><!--> <html class=""no-js"" lang=""en-US""> <!--<![endif]-->
<head>
<title>Attention Required! | Cloudflare</title>
<meta charset=""UTF-8"" />
<meta http-equiv=""Content-Type"" content=""text/html; charset=UTF-8"" />
<meta http-equiv=""X-UA-Compatible"" content=""IE=Edge"" />
<meta name=""robots"" content=""noindex, nofollow"" />
<meta name=""viewport"" content=""width=device-width,initial-scale=1"" />
<link rel=""stylesheet"" id=""cf_styles-css"" href=""/cdn-cgi/styles/cf.errors.css"" />
<!--[if lt IE 9]><link rel=""stylesheet"" id='cf_styles-ie-css' href=""/cdn-cgi/styles/cf.errors.ie.css"" /><![endif]-->
<style>body{margin:0;padding:0}</style>


<!--[if gte IE 10]><!-->
<script>
  if (!navigator.cookieEnabled) {
    window.addEventListener('DOMContentLoaded', function () {
      var cookieEl = document.getElementById('cookie-alert');
      cookieEl.style.display = 'block';
    })
  }
</script>
<!--<![endif]-->


</head>
<body>
  <div id=""cf-wrapper"">
    <div class=""cf-alert cf-alert-error cf-cookie-error"" id=""cookie-alert"" data-translate=""enable_cookies"">Please enable cookies.</div>    <div id=""cf-error-details"" class=""cf-error-details-wrapper"">
      <div class=""cf-wrapper cf-header cf-error-overview"">
        <h1 data-translate=""block_headline"">Sorry, you have been blocked</h1>
        <h2 class=""cf-subheadline""><span data-translate=""unable_to_access"">You are unable to access</span> api.openai.com</h2>
      </div><!-- /.header -->

      <div class=""cf-section cf-highlight"">
        <div class=""cf-wrapper"">
          <div class=""cf-screenshot-container cf-screenshot-full"">
            
              <span class=""cf-no-screenshot error""></span>
            
          </div>
        </div>
      </div><!-- /.captcha-container -->

      <div class=""cf-section cf-wrapper"">
        <div class=""cf-columns two"">
          <div class=""cf-column"">
            <h2 data-translate=""blocked_why_headline"">Why have I been blocked?</h2>

            <p data-translate=""blocked_why_detail"">This website is using a security service to protect itself from online attacks. The action you just performed triggered the security solution. There are several actions that could trigger this block including submitting a certain word or phrase, a SQL command or malformed data.</p>
          </div>

          <div class=""cf-column"">
            <h2 data-translate=""blocked_resolve_headline"">What can I do to resolve this?</h2>

            <p data-translate=""blocked_resolve_detail"">You can email the site owner to let them know you were blocked. Please include what you were doing when this page came up and the Cloudflare Ray ID found at the bottom of this page.</p>
          </div>
        </div>
      </div><!-- /.section -->

      <div class=""cf-error-footer cf-wrapper w-240 lg:w-full py-10 sm:py-4 sm:px-8 mx-auto text-center sm:text-left border-solid border-0 border-t border-gray-300"">
  <p class=""text-13"">
    <span class=""cf-footer-item sm:block sm:mb-1"">Cloudflare Ray ID: <strong class=""font-semibold"">8280f53d0de016a3</strong></span>
    <span class=""cf-footer-separator sm:hidden"">•</span>
    <span id=""cf-footer-item-ip"" class=""cf-footer-item hidden sm:block sm:mb-1"">
      Your IP:
      <button type=""button"" id=""cf-footer-ip-reveal"" class=""cf-footer-ip-reveal-btn"">Click to reveal</button>
      <span class=""hidden"" id=""cf-footer-ip"">194.87.252.3</span>
      <span class=""cf-footer-separator sm:hidden"">•</span>
    </span>
    <span class=""cf-footer-item sm:block sm:mb-1""><span>Performance & security by</span> <a rel=""noopener noreferrer"" href=""https://www.cloudflare.com/5xx-error-landing"" id=""brand_link"" target=""_blank"">Cloudflare</a></span>
    
  </p>
  <script>(function(){function d(){var b=a.getElementById(""cf-footer-item-ip""),c=a.getElementById(""cf-footer-ip-reveal"");b&&""classList""in b&&(b.classList.remove(""hidden""),c.addEventListener(""click"",function(){c.classList.add(""hidden"");a.getElementById(""cf-footer-ip"").classList.remove(""hidden"")}))}var a=document;document.addEventListener&&a.addEventListener(""DOMContentLoaded"",d)})();</script>
</div><!-- /.error-footer -->


    </div><!-- /#cf-error-details -->
  </div><!-- /#cf-wrapper -->

  <script>
  window._cf_translation = {};
  
  
</script>

</body>
</html>

To Reproduce
start bot
send request (post)
i have a error
Code snippets
No response
OS
ubuntu 22.04
Python version
3.11.6
Library version
1.2.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/845","Keep Conversation History","2023-11-18T22:58:08Z","Closed issue","No label","Confirm this is a feature request for the Python library and not the underlying OpenAI API.
 This is a feature request for the Python library
Describe the feature or improvement you're requesting
When using the API, it should be the case that conversation history is kept. While one can do this manually (by providing the history in the prompt), this costs additional API tokens...
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/843","Add shell auto completion for different shell, like bash, zsh, fish, powershell","2023-11-17T05:26:09Z","Open issue","enhancement,good first issue","Confirm this is a feature request for the Python library and not the underlying OpenAI API.
 This is a feature request for the Python library
Describe the feature or improvement you're requesting
I would like to have shell auto completion support to cli in this python library, which will make cli a more useful tool to play around with openai.
Other examples:
GitHub CLI: https://cli.github.com/manual/gh_completion
Stripe CLI: https://stripe.com/docs/stripe-cli/autocomplete
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/840","Missing module openai.error","2023-11-16T20:31:15Z","Closed as not planned issue","question","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
I am using library prompt2model, and its demo shows that import openai.error
While then there is a Error ModuleNotFoundError: No module named 'openai.error'
My question is whether openai python have deleted the library openai.error? Is there any alternative code here?
To Reproduce
I think there is a general problem of Module missing. Maybe try as following
1.In terminal, pip install openai
 2. In any file, try import openai.error
Alternatively, when I did this in terminal
pip install prompt2model
python prompt2model_demo.py
Code snippets
""""""Tools for accessing API-based models.""""""

from __future__ import annotations  # noqa FI58

import asyncioimport jsonimport loggingimport time

import aiolimiterimport litellm.utilsimport openaiimport openai.errorimport tiktokenfrom aiohttp import ClientSessionfrom litellm import acompletion, completionfrom tqdm.asyncio import tqdm_asyncio

# Note that litellm converts all API errors into openai errors,# so openai errors are valid even when using other services.API_ERRORS = (
    openai.error.APIError,
    openai.error.Timeout,
    openai.error.RateLimitError,
    openai.error.ServiceUnavailableError,
    openai.error.InvalidRequestError,
    json.decoder.JSONDecodeError,
    AssertionError,
)

ERROR_ERRORS_TO_MESSAGES = {
    openai.error.InvalidRequestError: ""API Invalid Request: Prompt was filtered"",
    openai.error.RateLimitError: ""API rate limit exceeded. Sleeping for 10 seconds."",
    openai.error.APIConnectionError: ""Error Communicating with API"",
    openai.error.Timeout: ""API Timeout Error: API Timeout"",
    openai.error.ServiceUnavailableError: ""API service unavailable error: {e}"",
    openai.error.APIError: ""API error: {e}"",
}
OS
Mac OS
Python version
Python v3.9
Library version
openai v1.3.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/837","Client objects are not pickle-able for multiprocess programs","2023-11-16T20:29:49Z","Closed as not planned issue","No label","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
I am developing a program that employs multiprocessing to enhance efficiency through parallel execution. Python relies on the pickle module for object serialization and inter-process communication. It would be nice if we let all classes in the openai package be pickle-able。
To Reproduce
Create an OpenAI or AsyncOpenAI client.
Serialize the client using the pickle module.
Code snippets
import pickle

from openai import OpenAI, AsyncOpenAI


client = OpenAI(api_key='sk-...')
async_client = AsyncOpenAI(api_key='sk-...')

pickle.dumps(client)  # TypeError: cannot pickle '_thread.RLock' objectpickle.dumps(async_client)  # TypeError: cannot pickle '_thread.RLock' object
OS
macOS
Python version
Python 3.11.6
Library version
openai v1.3.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/835","When using with Gevent in Flask_Sock it errors out: specifically, the monkey.patch_all() line casues some sort of error to occur.","2024-03-03T00:56:08Z","Closed issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
When using with Gevent in Flask_Sock it errors out: specifically, the monkey.patch_all() line casues some sort of error to occur.
To Reproduce
When using with Gevent in Flask_Sock it errors out: specifically, the monkey.patch_all() line casues some sort of error to occur.
Code snippets
if __name__ == '__main__':
    app.logger.setLevel(logging.DEBUG)
    from gevent import monkey
    monkey.patch_all()

    from gevent.pywsgi import WSGIServer
    server=WSGIServer(('127.0.0.1', 5000), app)
    print(""Server listening on: http://localhost:"" + str(HTTP_SERVER_PORT))
    server.serve_forever()
(venv) C:\Users\lukas\OneDrive\Desktop\Fuckaroundy\TwilioBidirectionalStream>python app.pyC:\Users\lukas\OneDrive\Desktop\Fuckaroundy\TwilioBidirectionalStream\app.py:4: DeprecationWarning: 'audioop' is deprecated and slated for removal in Python 3.13
  import audioopTraceback (most recent call last):
  File ""C:\Users\lukas\OneDrive\Desktop\Fuckaroundy\TwilioBidirectionalStream\app.py"", line 247, in <module>
    monkey.patch_all()
  File ""C:\Users\lukas\OneDrive\Desktop\Fuckaroundy\TwilioBidirectionalStream\venv\Lib\site-packages\gevent\monkey.py"", line 1219, in patch_all
    patch_thread(Event=Event, _warnings=_warnings)
  File ""C:\Users\lukas\OneDrive\Desktop\Fuckaroundy\TwilioBidirectionalStream\venv\Lib\site-packages\gevent\monkey.py"", line 187, in ignores
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\lukas\OneDrive\Desktop\Fuckaroundy\TwilioBidirectionalStream\venv\Lib\site-packages\gevent\monkey.py"", line 749, in patch_thread
    _patch_existing_locks(threading_mod)
  File ""C:\Users\lukas\OneDrive\Desktop\Fuckaroundy\TwilioBidirectionalStream\venv\Lib\site-packages\gevent\monkey.py"", line 630, in _patch_existing_locks  
    if isinstance(o, rlock_type):
       ^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\lukas\OneDrive\Desktop\Fuckaroundy\TwilioBidirectionalStream\venv\Lib\site-packages\openai\_utils\_proxy.py"", line 39, in __class__        
    return self.__get_proxied__().__class__
           ^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\lukas\OneDrive\Desktop\Fuckaroundy\TwilioBidirectionalStream\venv\Lib\site-packages\openai\_utils\_proxy.py"", line 43, in __get_proxied__  
    return self.__load__()
           ^^^^^^^^^^^^^^^
  File ""C:\Users\lukas\OneDrive\Desktop\Fuckaroundy\TwilioBidirectionalStream\venv\Lib\site-packages\openai\lib\_old_api.py"", line 33, in __load__
    raise APIRemovedInV1(symbol=self._symbol)
openai.lib._old_api.APIRemovedInV1:

You tried to access openai.Edit, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.  

You can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface.

Alternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`

A detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742


(venv) C:\Users\lukas\OneDrive\Desktop\Fuckaroundy\TwilioBidirectionalStream>python app.pyC:\Users\lukas\OneDrive\Desktop\Fuckaroundy\TwilioBidirectionalStream\app.py:4: DeprecationWarning: 'audioop' is deprecated and slated for removal in Python 3.13
  import audioopTraceback (most recent call last):
  File ""C:\Users\lukas\OneDrive\Desktop\Fuckaroundy\TwilioBidirectionalStream\app.py"", line 247, in <module>
    monkey.patch_all()
  File ""C:\Users\lukas\OneDrive\Desktop\Fuckaroundy\TwilioBidirectionalStream\venv\Lib\site-packages\gevent\monkey.py"", line 1219, in patch_all
    patch_thread(Event=Event, _warnings=_warnings)
  File ""C:\Users\lukas\OneDrive\Desktop\Fuckaroundy\TwilioBidirectionalStream\venv\Lib\site-packages\gevent\monkey.py"", line 187, in ignores
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\lukas\OneDrive\Desktop\Fuckaroundy\TwilioBidirectionalStream\venv\Lib\site-packages\gevent\monkey.py"", line 749, in patch_thread
    _patch_existing_locks(threading_mod)
  File ""C:\Users\lukas\OneDrive\Desktop\Fuckaroundy\TwilioBidirectionalStream\venv\Lib\site-packages\gevent\monkey.py"", line 630, in _patch_existing_locks  
    if isinstance(o, rlock_type):
       ^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\lukas\OneDrive\Desktop\Fuckaroundy\TwilioBidirectionalStream\venv\Lib\site-packages\openai\_utils\_proxy.py"", line 39, in __class__        
    return self.__get_proxied__().__class__
           ^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\lukas\OneDrive\Desktop\Fuckaroundy\TwilioBidirectionalStream\venv\Lib\site-packages\openai\_utils\_proxy.py"", line 43, in __get_proxied__  
    return self.__load__()
           ^^^^^^^^^^^^^^^
  File ""C:\Users\lukas\OneDrive\Desktop\Fuckaroundy\TwilioBidirectionalStream\venv\Lib\site-packages\openai\lib\_old_api.py"", line 33, in __load__
    raise APIRemovedInV1(symbol=self._symbol)
openai.lib._old_api.APIRemovedInV1:

You tried to access openai.Edit, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.  

You can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface.

Alternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`

A detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742
OS
Windows
Python version
Python v3.11.3
Library version
openai 1.3.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/828","Can't import openai","2023-11-15T16:40:33Z","Closed as not planned issue","question","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
openai module is installed, yet it gives the following error
To Reproduce
pip install openai
Start a REPL session, try to import openai
Code snippets
No response
OS
Ubuntu 22.04.3 LTS
Python version
Python 3.10.12
Library version
openai 1.2.4
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/827","Adding license classifier","2023-12-02T09:13:37Z","Closed issue","No label","Confirm this is a feature request for the Python library and not the underlying OpenAI API.
 This is a feature request for the Python library
Describe the feature or improvement you're requesting
See #826
It is important that Python packages have proper classifiers added. That way we can make use of PyPI's metadata to detect the license of the package easily.
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/825","use openai.proxies to set proxy doesn't work","2023-11-16T03:23:33Z","Closed issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
before v1 I can use openai.proxy to set proxy, but after v1 I can't use it any more. At first I thought it would be good again after change openai.proxy to openai.proxies, but not working either.
To Reproduce
openai.proxies = {...proxy setting}
Code snippets
No response
OS
ubuntu20.04
Python version
python 3.9.17
Library version
1.2.4
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/822","add list method for threads object","2023-11-16T03:29:47Z","Closed as not planned issue","No label","Confirm this is a feature request for the Python library and not the underlying OpenAI API.
 This is a feature request for the Python library
Describe the feature or improvement you're requesting
It seems that there's an undocumented GET endpoint for the threads in the underlying OpenAI API that can list all the treads. It's convenient for developers to see all the threads since this can't be done in the Assistants page in the playgound site.
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/821","httpx.PoolTimeout occurs frequently with SyncClient","2023-11-15T01:04:21Z","Open issue","enhancement","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
httpx.PoolTimeout occurs frequently with SyncClient
Recently, we noticed a high number of timeouts. Many requests were getting stuck on the default timeout of 600.
 This was before we migrated.
 We migrated to v1.2.3 to try to mitigate this but the requests were still getting stuck in timeout.
 We have managed to mitigate this a little bit by setting the timeout to 30 seconds and retrying (without our own retry library since the OpenAI retries don't appear to have jitter or exp backoff and were causing problems at scale)
 Now we are getting httpx.PoolTimeout when using the SyncClient. This is causing downstream issues since tasks start to pile up and we just get tons of httpx.PoolTimeout.
I think we will consider using a custom http client, though I noticed this requests being stuck in timeout on the old version of the api as well... which was our original motivation to migrate...
In case it helps this is in a production app doing about 3-6 OpenAI requests per second and seems to line up with busier traffic moments.
To Reproduce
Use SyncClient
Make 3-6 requests per second to ChatCompletions endpoint
Get httpx.PoolTimeouts
Code snippets
No response
OS
ubuntu
Python version
Python v3.10.8
Library version
OpenAI v1.2.4
 The text was updated successfully, but these errors were encountered: 
👍1
josephzhang23 reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/openai-python/issues/820","Memory leak in the chat completion create method","2023-12-15T01:44:43Z","Closed issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
Calling the create method on completions introduces a memory leak, according to tracemalloc.
Example:
client = OpenAI(MY_KEY)
client.chat.completions.create(
    messages=[
        {
            ""role"": ""user"",
            ""content"": ""Can you write a poem?"",
        }
    ],
    model=""gpt-3.5-turbo""
)
How to determine it's a memory leak?
I use tracemalloc with my flask application:
@blueprint.route(""/admin/sys/stats"")def admin_sys_stats():
    snapshot = tracemalloc.take_snapshot()
    top_stats = snapshot.statistics('lineno')

    from openai import OpenAI

    client = OpenAI(KEY)
    client.chat.completions.create(
        messages=[
            {
                ""role"": ""user"",
                ""content"": ""Can you write a poem?"",
            }
        ],
        model=""gpt-3.5-turbo""
    )

    stats = """"
    for stat in top_stats[:1000]:
        if grep in str(stat):
            stats += str(stat) + ""\n""

    return f""<pre>{stats}</pre>"", 200
When running this endpoint multiple times, one line is at the very top (which means it's the most expensive one):
\venv\Lib\site-packages\openai\_response.py:227: size=103 KiB, count=1050, average=100 B

When I refresh, the size increases. Of course, in a production environment, the numbers get high a lot quicker.
To Reproduce
There's no one way to prove there's a memory leak.
 But what I did was:
Setup a flask application
Create the route provided in the bug description
Hit the route multiple times, you'll see an increase in the size of the object
Code snippets
No response
OS
Linux, macOS
Python version
Python v3.11.2
Library version
openai v1.2.4
 The text was updated successfully, but these errors were encountered: 
👍1
tofulim reacted with thumbs up emoji👀2
antont and sudokohi reacted with eyes emoji
All reactions
👍1 reaction
👀2 reactions"
"https://github.com/openai/openai-python/issues/819","Assistant API - Code Interpreter Image File Creation (files.retrieve_content)","2023-11-14T21:52:53Z","Closed as not planned issue","API-feedback,enhancement","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
when using the API to write and download a local file using files.retrieve_content it fails to create the file correctly.
To reproduce this error, try asking an assistant to create a graph for you. It will use the tool Code Interpreter and will create the file which will be returned in the thread messages. This file_id can then be used to view the file via the Playground. However, if you want to render this file in the application you are building, you can not.
To Reproduce
Run the below code on google colab
Code snippets
!pip install --upgrade openai
!openai migrate

from openai import OpenAI

client = OpenAI(api_key='sk-...')
file = client.files.create(
    file=open(""balancesheet.pdf"", ""rb""),
    purpose='assistants'
)


assistant = client.beta.assistants.create(
  name=""Data Visualizer"",
  description=""You are great at creating beautiful data visualizations. You analyze data present in .pdf files, understand trends, and come up with data visualizations relevant to those trends. You also share a brief text summary of the trends observed."",
  model=""gpt-4-1106-preview"",
  tools=[{""type"": ""code_interpreter""},{""type"": ""retrieval""}],
  file_ids=[file.id]
)

thread = client.beta.threads.create()
client.beta.threads.messages.create(
    thread_id=thread.id,
    role=""user"",
    content=""Can you please give me a simple example of a scatter plot""
)

run = client.beta.threads.runs.create(
  thread_id=thread.id,
  assistant_id=assistant.id
)

messages = client.beta.threads.messages.list(
  thread_id=thread.id
)

messages


content = client.files.retrieve_content(file.id)


Error message:
BadRequestError                           Traceback (most recent call last)
<ipython-input-17-23ff13466060> in <cell line: 1>()
----> 1 content = client.files.retrieve_content(file.id)

4 frames/usr/local/lib/python3.10/dist-packages/openai/_base_client.py in _request(self, cast_to, options, remaining_retries, stream, stream_cls)
    875             # to completion before attempting to access the response text.
    876             err.response.read()
--> 877             raise self._make_status_error_from_response(err.response) from None
    878         except httpx.TimeoutException as err:
    879             if retries > 0:

BadRequestError: Error code: 400 - {'error': {'message': 'Not allowed to download files of purpose: assistants', 'type': 'invalid_request_error', 'param': None, 'code': None}}
OS
Google Colab
Python version
Python 3.10.12
Library version
openai 1.2.4
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/818","How to get usage in streaming response?","2023-11-16T03:40:29Z","Closed as not planned issue","No label","Confirm this is a feature request for the Python library and not the underlying OpenAI API.
 This is a feature request for the Python library
Describe the feature or improvement you're requesting
We can use print(dict(completion).get('usage')) to get the token usage of the request, but in streaming mode, there is no way to get the token usage. It would be great if there was a way to retrieve token usage in streaming mode as well.
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
👍4
hansh0801, patillacode, jaehakchang-dc, and rr- reacted with thumbs up emoji
All reactions
👍4 reactions"
"https://github.com/openai/openai-python/issues/816","Error ""cannot import name 'TypeAliasType' from 'typing_extensions'"" when importing openai just after installation","2023-12-23T23:26:27Z","Closed issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
ImportError                               Traceback (most recent call last)
Cell In[38], line 1
----> 1 import openai
      2 import os
      4 opai.api_key = os.getenv(""OPENAI_API_KEY"")

File D:\Anaconda\envs\py38\lib\site-packages\openai\__init__.py:8
      5 import os as _os
      6 from typing_extensions import override
----> 8 from . import types
      9 from ._types import NoneType, Transport, ProxiesTypes
     10 from ._utils import file_from_path

File D:\Anaconda\envs\py38\lib\site-packages\openai\types\__init__.py:5
      1 # File generated from our OpenAPI spec by Stainless.
      3 from __future__ import annotations
----> 5 from .edit import Edit as Edit
      6 from .image import Image as Image
      7 from .model import Model as Model

File D:\Anaconda\envs\py38\lib\site-packages\openai\types\edit.py:6
      3 from typing import List
      4 from typing_extensions import Literal
----> 6 from .._models import BaseModel
      7 from .completion_usage import CompletionUsage
      9 __all__ = [""Edit"", ""Choice""]

File D:\Anaconda\envs\py38\lib\site-packages\openai\_models.py:20
     18 import pydantic
     19 import pydantic.generics
---> 20 from pydantic.fields import FieldInfo
     22 from ._types import (
     23     Body,
     24     IncEx,
   (...)
     31     HttpxRequestFiles,
     32 )
     33 from ._utils import (
     34     is_list,
     35     is_given,
   (...)
     39     strip_not_given,
     40 )

File D:\Anaconda\envs\py38\lib\site-packages\pydantic\fields.py:24
     21 from pydantic_core import PydanticUndefined
     22 from typing_extensions import Literal, Unpack
---> 24 from . import types
     25 from ._internal import _decorators, _fields, _generics, _internal_dataclass, _repr, _typing_extra, _utils
     26 from .config import JsonDict

File D:\Anaconda\envs\py38\lib\site-packages\pydantic\types.py:33
     31 from annotated_types import BaseMetadata, MaxLen, MinLen
     32 from pydantic_core import CoreSchema, PydanticCustomError, core_schema
---> 33 from typing_extensions import Annotated, Literal, Protocol, TypeAlias, TypeAliasType, deprecated
     35 from ._internal import (
     36     _core_utils,
     37     _fields,
   (...)
     41     _validators,
     42 )
     43 from ._migration import getattr_migration

ImportError: cannot import name 'TypeAliasType' from 'typing_extensions' (D:\Anaconda\envs\py38\lib\site-packages\typing_extensions.py)

To Reproduce
import openai
then error happens
 the typing_extensions version is 4.8.0(latest)
Code snippets
No response
OS
windows
Python version
Python 3.8.16
Library version
openai v 1.2.4
 The text was updated successfully, but these errors were encountered: 
👍4
rlin88, VibhuAg, Marwen94, and TaIos reacted with thumbs up emoji
All reactions
👍4 reactions"
"https://github.com/openai/openai-python/issues/815","Support for Pyodide","2023-12-09T16:30:37Z","Closed issue","No label","Confirm this is a feature request for the Python library and not the underlying OpenAI API.
 This is a feature request for the Python library
Describe the feature or improvement you're requesting
import micropipawait micropip.install('openai', keep_going=True)
await micropip.install(""ssl"")
import openaifrom openai import OpenAI

client = OpenAI(
    api_key=""API KEY"",
)

chat_completion = client.chat.completions.create(
    messages=[
        {
            ""role"": ""user"",
            ""content"": ""Say this is a test"",
        }
    ],
    model=""gpt-3.5-turbo"",
)
Running this results in the following error:
    OSError                                   Traceback (most recent call last)
File /lib/python3.11/site-packages/httpcore/_exceptions.py:10, in map_exceptions(map)
    9 try:
---> 10     yield
    11 except Exception as exc:  # noqa: PIE786

File /lib/python3.11/site-packages/httpcore/_backends/sync.py:206, in SyncBackend.connect_tcp(self, host, port, timeout, local_address, socket_options)
    205 with map_exceptions(exc_map):
--> 206     sock = socket.create_connection(
    207         address,
    208         timeout,
    209         source_address=source_address,
    210     )
    211     for option in socket_options:

File /lib/python311.zip/socket.py:851, in create_connection(address, timeout, source_address, all_errors)
    850 if not all_errors:
--> 851     raise exceptions[0]
    852 raise ExceptionGroup(""create_connection failed"", exceptions)

File /lib/python311.zip/socket.py:836, in create_connection(address, timeout, source_address, all_errors)
    835     sock.bind(source_address)
--> 836 sock.connect(sa)
    837 # Break explicitly a reference cycle

OSError: [Errno 23] Host is unreachable

The above exception was the direct cause of the following exception:

ConnectError                              Traceback (most recent call last)
File /lib/python3.11/site-packages/httpx/_transports/default.py:66, in map_httpcore_exceptions()
    65 try:
---> 66     yield
    67 except Exception as exc:

File /lib/python3.11/site-packages/httpx/_transports/default.py:228, in HTTPTransport.handle_request(self, request)
    227 with map_httpcore_exceptions():
--> 228     resp = self._pool.handle_request(req)
    230 assert isinstance(resp.stream, typing.Iterable)

File /lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:268, in ConnectionPool.handle_request(self, request)
    267         self.response_closed(status)
--> 268     raise exc
    269 else:

File /lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:251, in ConnectionPool.handle_request(self, request)
    250 try:
--> 251     response = connection.handle_request(request)
    252 except ConnectionNotAvailable:
    253     # The ConnectionNotAvailable exception is a special case, that
    254     # indicates we need to retry the request on a new connection.
(...)
    258     # might end up as an HTTP/2 connection, but which actually ends
    259     # up as HTTP/1.1.

File /lib/python3.11/site-packages/httpcore/_sync/connection.py:99, in HTTPConnection.handle_request(self, request)
    98         self._connect_failed = True---> 99         raise exc
    100 elif not self._connection.is_available():

File /lib/python3.11/site-packages/httpcore/_sync/connection.py:76, in HTTPConnection.handle_request(self, request)
    75 try:
---> 76     stream = self._connect(request)
    78     ssl_object = stream.get_extra_info(""ssl_object"")

File /lib/python3.11/site-packages/httpcore/_sync/connection.py:124, in HTTPConnection._connect(self, request)
    123 with Trace(""connect_tcp"", logger, request, kwargs) as trace:
--> 124     stream = self._network_backend.connect_tcp(**kwargs)
    125     trace.return_value = stream

File /lib/python3.11/site-packages/httpcore/_backends/sync.py:205, in SyncBackend.connect_tcp(self, host, port, timeout, local_address, socket_options)
    200 exc_map: ExceptionMapping = {
    201     socket.timeout: ConnectTimeout,
    202     OSError: ConnectError,
    203 }
--> 205 with map_exceptions(exc_map):
    206     sock = socket.create_connection(
    207         address,
    208         timeout,
    209         source_address=source_address,
    210     )

File /lib/python311.zip/contextlib.py:155, in _GeneratorContextManager.__exit__(self, typ, value, traceback)
    154 try:
--> 155     self.gen.throw(typ, value, traceback)
    156 except StopIteration as exc:
    157     # Suppress StopIteration *unless* it's the same exception that
    158     # was passed to throw().  This prevents a StopIteration
    159     # raised inside the ""with"" statement from being suppressed.

File /lib/python3.11/site-packages/httpcore/_exceptions.py:14, in map_exceptions(map)
    13     if isinstance(exc, from_exc):
---> 14         raise to_exc(exc) from exc
    15 raise

ConnectError: [Errno 23] Host is unreachable

The above exception was the direct cause of the following exception:

ConnectError                              Traceback (most recent call last)
File /lib/python3.11/site-packages/openai/_base_client.py:858, in SyncAPIClient._request(self, cast_to, options, remaining_retries, stream, stream_cls)
    857 try:
--> 858     response = self._client.send(request, auth=self.custom_auth, stream=stream)
    859     log.debug(
    860         'HTTP Request: %s %s ""%i %s""', request.method, request.url, response.status_code, response.reason_phrase
    861     )

File /lib/python3.11/site-packages/httpx/_client.py:901, in Client.send(self, request, stream, auth, follow_redirects)
    899 auth = self._build_request_auth(request, auth)
--> 901 response = self._send_handling_auth(
    902     request,
    903     auth=auth,
    904     follow_redirects=follow_redirects,
    905     history=[],
    906 )
    907 try:

File /lib/python3.11/site-packages/httpx/_client.py:929, in Client._send_handling_auth(self, request, auth, follow_redirects, history)
    928 while True:
--> 929     response = self._send_handling_redirects(
    930         request,
    931         follow_redirects=follow_redirects,
    932         history=history,
    933     )
    934     try:

File /lib/python3.11/site-packages/httpx/_client.py:966, in Client._send_handling_redirects(self, request, follow_redirects, history)
    964     hook(request)
--> 966 response = self._send_single_request(request)
    967 try:

File /lib/python3.11/site-packages/httpx/_client.py:1002, in Client._send_single_request(self, request)
1001 with request_context(request=request):
-> 1002     response = transport.handle_request(request)
1004 assert isinstance(response.stream, SyncByteStream)

File /lib/python3.11/site-packages/httpx/_transports/default.py:227, in HTTPTransport.handle_request(self, request)
    215 req = httpcore.Request(
    216     method=request.method,
    217     url=httpcore.URL(
(...)
    225     extensions=request.extensions,
    226 )
--> 227 with map_httpcore_exceptions():
    228     resp = self._pool.handle_request(req)

File /lib/python311.zip/contextlib.py:155, in _GeneratorContextManager.__exit__(self, typ, value, traceback)
    154 try:
--> 155     self.gen.throw(typ, value, traceback)
    156 except StopIteration as exc:
    157     # Suppress StopIteration *unless* it's the same exception that
    158     # was passed to throw().  This prevents a StopIteration
    159     # raised inside the ""with"" statement from being suppressed.

File /lib/python3.11/site-packages/httpx/_transports/default.py:83, in map_httpcore_exceptions()
    82 message = str(exc)
---> 83 raise mapped_exc(message) from exc

ConnectError: [Errno 23] Host is unreachable

During handling of the above exception, another exception occurred:

OSError                                   Traceback (most recent call last)
File /lib/python3.11/site-packages/httpcore/_exceptions.py:10, in map_exceptions(map)
    9 try:
---> 10     yield
    11 except Exception as exc:  # noqa: PIE786

File /lib/python3.11/site-packages/httpcore/_backends/sync.py:206, in SyncBackend.connect_tcp(self, host, port, timeout, local_address, socket_options)
    205 with map_exceptions(exc_map):
--> 206     sock = socket.create_connection(
    207         address,
    208         timeout,
    209         source_address=source_address,
    210     )
    211     for option in socket_options:

File /lib/python311.zip/socket.py:851, in create_connection(address, timeout, source_address, all_errors)
    850 if not all_errors:
--> 851     raise exceptions[0]
    852 raise ExceptionGroup(""create_connection failed"", exceptions)

File /lib/python311.zip/socket.py:836, in create_connection(address, timeout, source_address, all_errors)
    835     sock.bind(source_address)
--> 836 sock.connect(sa)
    837 # Break explicitly a reference cycle

OSError: [Errno 23] Host is unreachable

The above exception was the direct cause of the following exception:

ConnectError                              Traceback (most recent call last)
File /lib/python3.11/site-packages/httpx/_transports/default.py:66, in map_httpcore_exceptions()
    65 try:
---> 66     yield
    67 except Exception as exc:

File /lib/python3.11/site-packages/httpx/_transports/default.py:228, in HTTPTransport.handle_request(self, request)
    227 with map_httpcore_exceptions():
--> 228     resp = self._pool.handle_request(req)
    230 assert isinstance(resp.stream, typing.Iterable)

File /lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:268, in ConnectionPool.handle_request(self, request)
    267         self.response_closed(status)
--> 268     raise exc
    269 else:

File /lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:251, in ConnectionPool.handle_request(self, request)
    250 try:
--> 251     response = connection.handle_request(request)
    252 except ConnectionNotAvailable:
    253     # The ConnectionNotAvailable exception is a special case, that
    254     # indicates we need to retry the request on a new connection.
(...)
    258     # might end up as an HTTP/2 connection, but which actually ends
    259     # up as HTTP/1.1.

File /lib/python3.11/site-packages/httpcore/_sync/connection.py:99, in HTTPConnection.handle_request(self, request)
    98         self._connect_failed = True---> 99         raise exc
    100 elif not self._connection.is_available():

File /lib/python3.11/site-packages/httpcore/_sync/connection.py:76, in HTTPConnection.handle_request(self, request)
    75 try:
---> 76     stream = self._connect(request)
    78     ssl_object = stream.get_extra_info(""ssl_object"")

File /lib/python3.11/site-packages/httpcore/_sync/connection.py:124, in HTTPConnection._connect(self, request)
    123 with Trace(""connect_tcp"", logger, request, kwargs) as trace:
--> 124     stream = self._network_backend.connect_tcp(**kwargs)
    125     trace.return_value = stream

File /lib/python3.11/site-packages/httpcore/_backends/sync.py:205, in SyncBackend.connect_tcp(self, host, port, timeout, local_address, socket_options)
    200 exc_map: ExceptionMapping = {
    201     socket.timeout: ConnectTimeout,
    202     OSError: ConnectError,
    203 }
--> 205 with map_exceptions(exc_map):
    206     sock = socket.create_connection(
    207         address,
    208         timeout,
    209         source_address=source_address,
    210     )

File /lib/python311.zip/contextlib.py:155, in _GeneratorContextManager.__exit__(self, typ, value, traceback)
    154 try:
--> 155     self.gen.throw(typ, value, traceback)
    156 except StopIteration as exc:
    157     # Suppress StopIteration *unless* it's the same exception that
    158     # was passed to throw().  This prevents a StopIteration
    159     # raised inside the ""with"" statement from being suppressed.

File /lib/python3.11/site-packages/httpcore/_exceptions.py:14, in map_exceptions(map)
    13     if isinstance(exc, from_exc):
---> 14         raise to_exc(exc) from exc
    15 raise

ConnectError: [Errno 23] Host is unreachable

The above exception was the direct cause of the following exception:

ConnectError                              Traceback (most recent call last)
File /lib/python3.11/site-packages/openai/_base_client.py:858, in SyncAPIClient._request(self, cast_to, options, remaining_retries, stream, stream_cls)
    857 try:
--> 858     response = self._client.send(request, auth=self.custom_auth, stream=stream)
    859     log.debug(
    860         'HTTP Request: %s %s ""%i %s""', request.method, request.url, response.status_code, response.reason_phrase
    861     )

File /lib/python3.11/site-packages/httpx/_client.py:901, in Client.send(self, request, stream, auth, follow_redirects)
    899 auth = self._build_request_auth(request, auth)
--> 901 response = self._send_handling_auth(
    902     request,
    903     auth=auth,
    904     follow_redirects=follow_redirects,
    905     history=[],
    906 )
    907 try:

File /lib/python3.11/site-packages/httpx/_client.py:929, in Client._send_handling_auth(self, request, auth, follow_redirects, history)
    928 while True:
--> 929     response = self._send_handling_redirects(
    930         request,
    931         follow_redirects=follow_redirects,
    932         history=history,
    933     )
    934     try:

File /lib/python3.11/site-packages/httpx/_client.py:966, in Client._send_handling_redirects(self, request, follow_redirects, history)
    964     hook(request)
--> 966 response = self._send_single_request(request)
    967 try:

File /lib/python3.11/site-packages/httpx/_client.py:1002, in Client._send_single_request(self, request)
1001 with request_context(request=request):
-> 1002     response = transport.handle_request(request)
1004 assert isinstance(response.stream, SyncByteStream)

File /lib/python3.11/site-packages/httpx/_transports/default.py:227, in HTTPTransport.handle_request(self, request)
    215 req = httpcore.Request(
    216     method=request.method,
    217     url=httpcore.URL(
(...)
    225     extensions=request.extensions,
    226 )
--> 227 with map_httpcore_exceptions():
    228     resp = self._pool.handle_request(req)

File /lib/python311.zip/contextlib.py:155, in _GeneratorContextManager.__exit__(self, typ, value, traceback)
    154 try:
--> 155     self.gen.throw(typ, value, traceback)
    156 except StopIteration as exc:
    157     # Suppress StopIteration *unless* it's the same exception that
    158     # was passed to throw().  This prevents a StopIteration
    159     # raised inside the ""with"" statement from being suppressed.

File /lib/python3.11/site-packages/httpx/_transports/default.py:83, in map_httpcore_exceptions()
    82 message = str(exc)
---> 83 raise mapped_exc(message) from exc

ConnectError: [Errno 23] Host is unreachable

During handling of the above exception, another exception occurred:

OSError                                   Traceback (most recent call last)
File /lib/python3.11/site-packages/httpcore/_exceptions.py:10, in map_exceptions(map)
    9 try:
---> 10     yield
    11 except Exception as exc:  # noqa: PIE786

File /lib/python3.11/site-packages/httpcore/_backends/sync.py:206, in SyncBackend.connect_tcp(self, host, port, timeout, local_address, socket_options)
    205 with map_exceptions(exc_map):
--> 206     sock = socket.create_connection(
    207         address,
    208         timeout,
    209         source_address=source_address,
    210     )
    211     for option in socket_options:

File /lib/python311.zip/socket.py:851, in create_connection(address, timeout, source_address, all_errors)
    850 if not all_errors:
--> 851     raise exceptions[0]
    852 raise ExceptionGroup(""create_connection failed"", exceptions)

File /lib/python311.zip/socket.py:836, in create_connection(address, timeout, source_address, all_errors)
    835     sock.bind(source_address)
--> 836 sock.connect(sa)
    837 # Break explicitly a reference cycle

OSError: [Errno 23] Host is unreachable

The above exception was the direct cause of the following exception:

ConnectError                              Traceback (most recent call last)
File /lib/python3.11/site-packages/httpx/_transports/default.py:66, in map_httpcore_exceptions()
    65 try:
---> 66     yield
    67 except Exception as exc:

File /lib/python3.11/site-packages/httpx/_transports/default.py:228, in HTTPTransport.handle_request(self, request)
    227 with map_httpcore_exceptions():
--> 228     resp = self._pool.handle_request(req)
    230 assert isinstance(resp.stream, typing.Iterable)

File /lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:268, in ConnectionPool.handle_request(self, request)
    267         self.response_closed(status)
--> 268     raise exc
    269 else:

File /lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:251, in ConnectionPool.handle_request(self, request)
    250 try:
--> 251     response = connection.handle_request(request)
    252 except ConnectionNotAvailable:
    253     # The ConnectionNotAvailable exception is a special case, that
    254     # indicates we need to retry the request on a new connection.
(...)
    258     # might end up as an HTTP/2 connection, but which actually ends
    259     # up as HTTP/1.1.

File /lib/python3.11/site-packages/httpcore/_sync/connection.py:99, in HTTPConnection.handle_request(self, request)
    98         self._connect_failed = True---> 99         raise exc
    100 elif not self._connection.is_available():

File /lib/python3.11/site-packages/httpcore/_sync/connection.py:76, in HTTPConnection.handle_request(self, request)
    75 try:
---> 76     stream = self._connect(request)
    78     ssl_object = stream.get_extra_info(""ssl_object"")

File /lib/python3.11/site-packages/httpcore/_sync/connection.py:124, in HTTPConnection._connect(self, request)
    123 with Trace(""connect_tcp"", logger, request, kwargs) as trace:
--> 124     stream = self._network_backend.connect_tcp(**kwargs)
    125     trace.return_value = stream

File /lib/python3.11/site-packages/httpcore/_backends/sync.py:205, in SyncBackend.connect_tcp(self, host, port, timeout, local_address, socket_options)
    200 exc_map: ExceptionMapping = {
    201     socket.timeout: ConnectTimeout,
    202     OSError: ConnectError,
    203 }
--> 205 with map_exceptions(exc_map):
    206     sock = socket.create_connection(
    207         address,
    208         timeout,
    209         source_address=source_address,
    210     )

File /lib/python311.zip/contextlib.py:155, in _GeneratorContextManager.__exit__(self, typ, value, traceback)
    154 try:
--> 155     self.gen.throw(typ, value, traceback)
    156 except StopIteration as exc:
    157     # Suppress StopIteration *unless* it's the same exception that
    158     # was passed to throw().  This prevents a StopIteration
    159     # raised inside the ""with"" statement from being suppressed.

File /lib/python3.11/site-packages/httpcore/_exceptions.py:14, in map_exceptions(map)
    13     if isinstance(exc, from_exc):
---> 14         raise to_exc(exc) from exc
    15 raise

ConnectError: [Errno 23] Host is unreachable

The above exception was the direct cause of the following exception:

ConnectError                              Traceback (most recent call last)
File /lib/python3.11/site-packages/openai/_base_client.py:858, in SyncAPIClient._request(self, cast_to, options, remaining_retries, stream, stream_cls)
    857 try:
--> 858     response = self._client.send(request, auth=self.custom_auth, stream=stream)
    859     log.debug(
    860         'HTTP Request: %s %s ""%i %s""', request.method, request.url, response.status_code, response.reason_phrase
    861     )

File /lib/python3.11/site-packages/httpx/_client.py:901, in Client.send(self, request, stream, auth, follow_redirects)
    899 auth = self._build_request_auth(request, auth)
--> 901 response = self._send_handling_auth(
    902     request,
    903     auth=auth,
    904     follow_redirects=follow_redirects,
    905     history=[],
    906 )
    907 try:

File /lib/python3.11/site-packages/httpx/_client.py:929, in Client._send_handling_auth(self, request, auth, follow_redirects, history)
    928 while True:
--> 929     response = self._send_handling_redirects(
    930         request,
    931         follow_redirects=follow_redirects,
    932         history=history,
    933     )
    934     try:

File /lib/python3.11/site-packages/httpx/_client.py:966, in Client._send_handling_redirects(self, request, follow_redirects, history)
    964     hook(request)
--> 966 response = self._send_single_request(request)
    967 try:

File /lib/python3.11/site-packages/httpx/_client.py:1002, in Client._send_single_request(self, request)
1001 with request_context(request=request):
-> 1002     response = transport.handle_request(request)
1004 assert isinstance(response.stream, SyncByteStream)

File /lib/python3.11/site-packages/httpx/_transports/default.py:227, in HTTPTransport.handle_request(self, request)
    215 req = httpcore.Request(
    216     method=request.method,
    217     url=httpcore.URL(
(...)
    225     extensions=request.extensions,
    226 )
--> 227 with map_httpcore_exceptions():
    228     resp = self._pool.handle_request(req)

File /lib/python311.zip/contextlib.py:155, in _GeneratorContextManager.__exit__(self, typ, value, traceback)
    154 try:
--> 155     self.gen.throw(typ, value, traceback)
    156 except StopIteration as exc:
    157     # Suppress StopIteration *unless* it's the same exception that
    158     # was passed to throw().  This prevents a StopIteration
    159     # raised inside the ""with"" statement from being suppressed.

File /lib/python3.11/site-packages/httpx/_transports/default.py:83, in map_httpcore_exceptions()
    82 message = str(exc)
---> 83 raise mapped_exc(message) from exc

ConnectError: [Errno 23] Host is unreachable

The above exception was the direct cause of the following exception:

APIConnectionError                        Traceback (most recent call last)
Cell In[7], line 11
    5 from openai import OpenAI
    7 client = OpenAI(
    8     api_key=""API KEY"",
    9 )
---> 11 chat_completion = client.chat.completions.create(
    12     messages=[
    13         {
    14             ""role"": ""user"",
    15             ""content"": ""Say this is a test"",
    16         }
    17     ],
    18     model=""gpt-3.5-turbo"",
    19 )

File /lib/python3.11/site-packages/openai/_utils/_utils.py:299, in required_args.<locals>.inner.<locals>.wrapper(*args, **kwargs)
    297             msg = f""Missing required argument: {quote(missing[0])}""
    298     raise TypeError(msg)
--> 299 return func(*args, **kwargs)

File /lib/python3.11/site-packages/openai/resources/chat/completions.py:594, in Completions.create(self, messages, model, frequency_penalty, function_call, functions, logit_bias, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_p, user, extra_headers, extra_query, extra_body, timeout)
    548 @required_args([""messages"", ""model""], [""messages"", ""model"", ""stream""])
    549 def create(
    550     self,
(...)
    592     timeout: float | httpx.Timeout | None | NotGiven = NOT_GIVEN,
    593 ) -> ChatCompletion | Stream[ChatCompletionChunk]:
--> 594     return self._post(
    595         ""/chat/completions"",
    596         body=maybe_transform(
    597             {
    598                 ""messages"": messages,
    599                 ""model"": model,
    600                 ""frequency_penalty"": frequency_penalty,
    601                 ""function_call"": function_call,
    602                 ""functions"": functions,
    603                 ""logit_bias"": logit_bias,
    604                 ""max_tokens"": max_tokens,
    605                 ""n"": n,
    606                 ""presence_penalty"": presence_penalty,
    607                 ""response_format"": response_format,
    608                 ""seed"": seed,
    609                 ""stop"": stop,
    610                 ""stream"": stream,
    611                 ""temperature"": temperature,
    612                 ""tool_choice"": tool_choice,
    613                 ""tools"": tools,
    614                 ""top_p"": top_p,
    615                 ""user"": user,
    616             },
    617             completion_create_params.CompletionCreateParams,
    618         ),
    619         options=make_request_options(
    620             extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout
    621         ),
    622         cast_to=ChatCompletion,
    623         stream=stream or False,
    624         stream_cls=Stream[ChatCompletionChunk],
    625     )

File /lib/python3.11/site-packages/openai/_base_client.py:1055, in SyncAPIClient.post(self, path, cast_to, body, options, files, stream, stream_cls)
1041 def post(
1042     self,
1043     path: str,
(...)
1050     stream_cls: type[_StreamT] | None = None,
1051 ) -> ResponseT | _StreamT:
1052     opts = FinalRequestOptions.construct(
1053         method=""post"", url=path, json_data=body, files=to_httpx_files(files), **options1054     )
-> 1055     return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))

File /lib/python3.11/site-packages/openai/_base_client.py:834, in SyncAPIClient.request(self, cast_to, options, remaining_retries, stream, stream_cls)
    825 def request(
    826     self,
    827     cast_to: Type[ResponseT],
(...)
    832     stream_cls: type[_StreamT] | None = None,
    833 ) -> ResponseT | _StreamT:
--> 834     return self._request(
    835         cast_to=cast_to,
    836         options=options,
    837         stream=stream,
    838         stream_cls=stream_cls,
    839         remaining_retries=remaining_retries,
    840     )

File /lib/python3.11/site-packages/openai/_base_client.py:890, in SyncAPIClient._request(self, cast_to, options, remaining_retries, stream, stream_cls)
    888 except Exception as err:
    889     if retries > 0:
--> 890         return self._retry_request(
    891             options,
    892             cast_to,
    893             retries,
    894             stream=stream,
    895             stream_cls=stream_cls,
    896         )
    897     raise APIConnectionError(request=request) from err
    899 return self._process_response(
    900     cast_to=cast_to,
    901     options=options,
(...)
    904     stream_cls=stream_cls,
    905 )

File /lib/python3.11/site-packages/openai/_base_client.py:925, in SyncAPIClient._retry_request(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)
    921 # In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a
    922 # different thread if necessary.
    923 time.sleep(timeout)
--> 925 return self._request(
    926     options=options,
    927     cast_to=cast_to,
    928     remaining_retries=remaining,
    929     stream=stream,
    930     stream_cls=stream_cls,
    931 )

File /lib/python3.11/site-packages/openai/_base_client.py:890, in SyncAPIClient._request(self, cast_to, options, remaining_retries, stream, stream_cls)
    888 except Exception as err:
    889     if retries > 0:
--> 890         return self._retry_request(
    891             options,
    892             cast_to,
    893             retries,
    894             stream=stream,
    895             stream_cls=stream_cls,
    896         )
    897     raise APIConnectionError(request=request) from err
    899 return self._process_response(
    900     cast_to=cast_to,
    901     options=options,
(...)
    904     stream_cls=stream_cls,
    905 )

File /lib/python3.11/site-packages/openai/_base_client.py:925, in SyncAPIClient._retry_request(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)
    921 # In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a
    922 # different thread if necessary.
    923 time.sleep(timeout)
--> 925 return self._request(
    926     options=options,
    927     cast_to=cast_to,
    928     remaining_retries=remaining,
    929     stream=stream,
    930     stream_cls=stream_cls,
    931 )

File /lib/python3.11/site-packages/openai/_base_client.py:897, in SyncAPIClient._request(self, cast_to, options, remaining_retries, stream, stream_cls)
    889     if retries > 0:
    890         return self._retry_request(
    891             options,
    892             cast_to,
(...)
    895             stream_cls=stream_cls,
    896         )
--> 897     raise APIConnectionError(request=request) from err
    899 return self._process_response(
    900     cast_to=cast_to,
    901     options=options,
(...)
    904     stream_cls=stream_cls,
    905 )

APIConnectionError: Connection error.
Is there any possible way to support openai to run in Pyodide?
Also, mentioning pyodide/pyodide#4292 for other possible solutions to work with the library on Pyodide.
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/809","Improve behaviour for read timeouts","2023-11-13T19:43:01Z","Open issue","enhancement","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
The fix for #769 involved adding retries for httpx.ReadTimeout exceptions which works but as the default timeout is 10 minutes, this can quickly rack up time spent in API requests.
We should figure out a better solution here.
To Reproduce
N/A
Code snippets
No response
OS
N/A
Python version
N/A
Library version
v1.2.4
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/801","Add copy thread API","2023-11-14T03:48:45Z","Closed issue","No label","Confirm this is a feature request for the Python library and not the underlying OpenAI API.
 This is a feature request for the Python library
Describe the feature or improvement you're requesting
It seems that there is not copy thread API. I think it's a common scenario to use it.
A possible design is as follows:
from openai import OpenAI

original_thread_id = ""xxx""client = OpenAI()
thread = client.beta.threads.copy(thread_id=original_thread_id)
I want to hear your thoughts.
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/800","The chatCompletion operation does not work with the specified model, gpt-35-turbo-instruct.","2023-11-14T02:38:49Z","Closed issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
Can someone please help me? The code is error when it tried to use the gpt-35-turbo-instruct for Azure services. When I used the gpt-35-turbo it works fine.
 BadRequestError: Error code: 400 - {'error': {'code': 'OperationNotSupported', 'message': 'The chatCompletion operation does not work with the specified model, gpt-35-turbo-instruct. Please choose different model and try again. You can learn more about which models can be used with each operation here: https://go.microsoft.com/fwlink/?linkid=2197993.'}}
To Reproduce
Install Requirements
langchain==0.0.335
azure-search-documents==11.4.0b8
torch==2.1.0
openai==1.2.3
azure-identity==1.15.0

Execute Code, The deployment name is set to gpt-35-turbo-instruct
Code snippets
azure_openai_model = AzureChatOpenAI(
    azure_deployment=os.getenv(AZURE_MODEL_DEPLOYMENT_NAME),
    azure_endpoint=os.getenv(AZURE_OPENAI_API_ENDPOINT),
    openai_api_key=os.getenv(OPENAI_API_KEY),
)
print(azure_openai_model.predict(""Count 1 to 10""))
OS
Linux Ubuntu 22.04
Python version
3.9.13
Library version
openai v1.2.3
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/799","ChatCompletion Not Accessible via openai.types in init.py","2023-11-14T03:51:27Z","Closed issue","No label","Confirm this is a feature request for the Python library and not the underlying OpenAI API.
 This is a feature request for the Python library
Describe the feature or improvement you're requesting
Environment:
openai-python version: Most recent version
 Python version: 3.11.2
 Operating System: Win 11
Description:
 I have encountered an issue where ChatCompletion cannot be accessed via openai.types as one would expect from the module's init.py. This is causing type hinting and direct import issues for developers trying to use type hints with the openai client for chat-based operations.
Steps to Reproduce:
Install the latest version of openai-python client.
Attempt to import ChatCompletion using the following code snippet:
from openai.types import ChatCompletion
Observe the ImportError that occurs.
Expected behavior:
 I expected to be able to access all types relevant to chat completions directly through openai.types for consistency and convenience when using type hints.
Actual behavior:
 An ImportError is raised when attempting to import ChatCompletion from openai.types, suggesting that it is not exported properly in init.py.
Possible Fix:
 Ensure that ChatCompletion and other relevant types are accessible through openai.types by adjusting the init.py file accordingly.
Additional context
Access to consistent and correct type hints is crucial for developing robust applications with proper static code analysis. The current inability to import ChatCompletion as expected hinders this process and could lead to less type-safe code.
Thank you for looking into this matter. I am looking forward to the fix.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/798","Dynamic function call callback possibility","2023-11-12T22:53:11Z","Closed issue","No label","Confirm this is a feature request for the Python library and not the underlying OpenAI API.
 This is a feature request for the Python library
Describe the feature or improvement you're requesting
I have a use case where the list of functions that could be used by is dynamic (I get the list programmatically from an external API).
 I can populate the list of functions list easily with the API response JSON, no problem.
 The problem is with the actual ""callback"" functions: I cannot provide them ""dynamically"". In my use case, if a function is identified and should be called, then I would need to call a generic API that I created that implements the function logic.
I can call this API with the name of the function and the parameters. So it's not a local Python function but something on an external server. But the list of these functions should be always dynamic, and be provided by an external API.
I could use something like the following:
def func_call_proxy(**data):
 # function name
 func_name = inspect.currentframe().f_code.co_name
 # params and values
 for key, value in data.items():
 print(""{} is {}"".format(key,value))
But the actual function name here is of course always the same. Maybe I am not that adept with Python, I thought about using lambda functions, but there does not seem to be a way to realize my use case of dynamic functions. If this is the case, then it would be nice to be able to support it via e.g. providing a proxy function that receivers the name of the function first and arguments.
Thanks!
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/797","Tool invocation via API","2023-11-14T03:56:15Z","Closed issue","No label","Confirm this is a feature request for the Python library and not the underlying OpenAI API.
 This is a feature request for the Python library
Describe the feature or improvement you're requesting
Can we have tools invoked by the API so we can have control over access:
Example
Assuming you've already uploaded your files and have their IDs
file_ids = [""file-123abc"", ""file-456def""]

response = openai.Tool(""retriever"").create(
    file_ids=file_ids,
    query=""Find information about GPT-3 capabilities""
)

# For Code Interpreter
code_snippet = """"""
with open('my_uploaded_file.txt', 'r') as file:
    data = file.read()
    # Process data...
""""""
response = openai.Tool(""code-interpreter"").create(
    code=code_snippet,
    file_id=""file-123abc""
)

This would remove the need for local sandboxed environments to run the tools. In a work around we can instruct through inference to run a tool but its not ideal as context is misused for data parameterization.
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
👀1
antont reacted with eyes emoji
All reactions
👀1 reaction"
"https://github.com/openai/openai-python/issues/792","Streaming Completions for Assistant Thread Output","2023-11-14T03:58:59Z","Closed issue","No label","Confirm this is a feature request for the Python library and not the underlying OpenAI API.
 This is a feature request for the Python library
Describe the feature or improvement you're requesting
Add streaming functionality to assistant messages.
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/791","openai==1.2.3: segmentation fault with async","2024-03-03T00:58:27Z","Closed issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
seg fault when using async and running openai integration tests with the latest version on windows
running this
python -q -X faulthandler test.py

gets this:
ChatCompletion(id='chatcmpl-8JvRae0P4EOZRkIJuGQQMFBQ6lRpf', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='Hello! How can I assist you today?', role='a
ssistant', function_call=None, tool_calls=None))], created=1699759738, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=
9, prompt_tokens=18, total_tokens=27))
Windows fatal exception: access violation

Current thread 0x000032c8 (most recent call first):
  Garbage-collecting
  File ""C:\Users\erik\.pyenv\pyenv-win\versions\3.11.6\Lib\asyncio\proactor_events.py"", line 82 in __repr__
  File ""C:\Users\erik\.pyenv\pyenv-win\versions\3.11.6\Lib\asyncio\proactor_events.py"", line 116 in __del__
Segmentation fault

i updated to 3.11.6 jut to see if that helped... nope
i don't normally have issues with async code, and i have unit tests and code passing fine. only the openai lib has this issue.
fortunately it only fails at exit, so i can keep on keeping on for now without worrying too much
To Reproduce
using windows pyenv python 3.11.5 or 3.11.6, use the async openai client to make a single chat completion in a pytest test.
it works, but then your program segfaults
all other httpx async/rest/tests all work, even respx live tests, etc.
only when i do the integration test with openai does it fail
Code snippets
example of failure


from openai import AsyncOpenAIimport asyncio

async def x():
    model=""gpt-3.5-turbo""
    client = AsyncOpenAI()
    messages = [
        {
            ""role"": ""system"",
            ""content"": ""You are a helpful assistant.""
        },
        {
            ""role"": ""user"",
            ""content"": ""Hello""
        },
    ]

    response = await client.chat.completions.create(
        model=model,
        messages=messages,
        max_tokens=500
    )

    return response

print(asyncio.run(x()))


### OS

windows

### Python version

3.11.5

### Library version

openai==1.2.3

 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/787","UnicodeEncodeError: 'ascii' codec can't encode character '\u201d' in position 59: ordinal not in range(128)","2023-12-19T03:11:39Z","Closed issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
I run the code from docs but receive the error
I find the cause of the bug is text load from .env have some special character.
from dotenv import load_dotenv
load_dotenv()
import os
openai.api_key = os.getenv(""OPENAI_API_KEY"")

and solved with
client = OpenAI(
      api_key=""sk....""
)

Seem have something weird when load .env file
 My .env file is and I still use it for other code. But dont know this version receive error
I use new .venv of python 3.10.12 only install python-dotenv
To Reproduce
!pip install python-dotenv
 !pip install openai
RUN the code snip set
I still can print
print(u""\u201d"")
Code snippets
import openaifrom openai import OpenAI

from dotenv import load_dotenvload_dotenv()
import osopenai.api_key = os.getenv(""OPENAI_API_KEY"")

client = OpenAI()
response = client.chat.completions.create(
  model=""gpt-3.5-turbo"",
    response_format={ ""type"": ""json_object"" },

  messages=[
    {""role"": ""system"", ""content"": ""You are 3translator to translate the input text to destination language the input is the text nees to translate. First: Detect the language of the input text Second: Translate the input text from the detected language in first step to the Vietnamese language Third: Return output only the translated text not add up the description""},
    {""role"": ""user"", ""content"": ""this is a cat""},
  ]
)
OS
ubuntu 22.04.3
Python version
3.10.12
Library version
openai 1.2.3
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/786","Better (Azure OpenAI’s content management policy) exception management","2023-11-14T06:17:19Z","Closed issue","Azure,enhancement","Confirm this is a feature request for the Python library and not the underlying OpenAI API.
 This is a feature request for the Python library
Describe the feature or improvement you're requesting
Something between a feature request and better Python module exception management.
When using an OpenAI (chat) create completion API endpoint through an Azure deployment sometime I got an UNEXPECTED exception raised by the Azure content filtering component.
As far as I understand, Azure ""prepend"" ANY OpenAI completion model deployment by a content filter on top.
┌────────────────────────────────────────────────────────────┐
│                                                            │
│ Completion API Request with prompt containing the sentence │
│                                                            │
│ “vorrei vedere la cosa più bella ad Ercolano. Qual’è?”     │
│                                                            │
└─────────────────────────────┬──────────────────────────────┘
                              │
                              │            InvalidRequestError
                              │                   ▲
                              │                   │
                              │                   │
                              │                   │
               ┌──────────────┴──────────────┐    │
               │                             │    │
               │   (Python) OpenAI module    │    │
               │                             │    │
               └──────────────┬──────────────┘    │
                              │                   │
                              │                   │
               ┌──────────────▼──────────────┐    │
               │                             │    │
               │    Azure Content Filter     │    │
               │                             │    │
               └──────────────┬──────────────┘    │
                              │                   │
               exception      ▼                   │
               prompt triggering Azure OpenAI’s   │
               content management policy          │
                                                  │
                              │                   │
               ┌──────────────▼──────────────┐    │
               │                             │    │
               │                             │    │
               │   Azure OpenAI Deployment   │    │
               │    (Any Model Behind)       │    │
               │                             │    │
               │                             │    │
               │                             │    │
               └──────────────┬──────────────┘    │
                              │                   │
                              │                   │
                              └───────────────────┘

When testing a chatbot (made with Azure OpenAI completions, e.g. a deployment using a GPT3.5-Turbo policy) in Italian language, I got an exception
              except openai.error.InvalidRequestError as e:
                  # Handle invalid request error, e.g. validate parameters or log
                  # WARNING: The response was filtered due to the prompt triggering Azure OpenAI’s content management policy.
                  sys.stderr.write(f""OpenAI API request was invalid: {e}"")
                  return ""Non ho capito bene. Cosa intendi?""
OpenAI API request was invalid: The response was filtered due to the prompt triggering Azure OpenAI’s content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766
I experienced that sometime the Azure content filter component UNEXPECTEDLY trigger an exception with a content category management violation.
As rightly stated in the closed issue #331 (comment) that problem is apparently NOT an OpenAI model issue but a maybe WRONG triggering due to a not correct behavior of the Azure filter management. So I already opened an issue to Azure.
Nevertheless, when concerning the Python module engineering I'm a bit perplexed about the fact openAI python SDK raises an InvalidRequestError for an exception NOT related to the OpenAI but raised by an external component (in this case the Azure Filtering component).
I didn't inspected the code, but I think the specific error due to the Azure filter management component could reserve a specific error AzureContentManagementPolicy instead of a generic and misleading InvalidRequestError (sse documentation: reading documentation here: https://help.openai.com/en/articles/6897213-openai-library-error-types-guidance).
Thanks
 Giorgio
Additional context
$ py --version
Python 3.11.6

$ pip show openai | grep Version
Version: 0.28.1

 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/785","Redirects aren't working anymore on version 1.x (with private api)","2023-11-13T21:58:05Z","Closed issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
So, I am running a private API, that is/was compatible with the openai python library.
 In one of my endpoints, I am redirecting to another endpoint, to give model specific rate limits with slowapi.
return RedirectResponse(url=""/v1/chat/completions/llama-70b-chat"") <- this works just fine with version 0.28 and you get no response status code failure, but on the newest version, its raising an error saying response code was 307.
To Reproduce
Make a openai library compatble fastapi server, with one endpoint redirecting to another.
call the first endpoint with library version 0.28 and with the newest version 1.x
you can see that the endpoint works on older versions, as they seem to allow redirects. But on newer versions it throws you the response code failure
Code snippets
# FASTAPI EXAMPLEimport time

import fastapifrom starlette.responses import RedirectResponse

app = fastapi.FastAPI()

@app.post(""/v1/chat/completions"")def chat_compl():
    return RedirectResponse(url=""/v1/chat/completions/new"")

@app.post(""/v1/chat/completions/new"")def chat_compl():
    return {""id"": f""chatcmpl-fag4646utg49a"",
            'object': 'chat.completion',
            'created': int(time.time()),
            'model': f'test',
            'choices': [
                {'index': 0, 'message':
                    {'role': 'assistant',
                     'content': ""This is a test.""},
                 'finish_reason': 'stop'}],
            'usage': {'prompt_tokens': 17, 'completion_tokens': 16, 'total_tokens': 33}}


if __name__ == ""__main__"":
    import uvicorn
    uvicorn.run(app, host=""0.0.0.0"", port=8000)


# CLIENT EXAMPLE VERSION 0.28 (working)import openai

openai.api_base = ""http://localhost:8000/v1""openai.api_key = ""Some random text""

response = openai.ChatCompletion.create(
    model=""test"",
    messages=[
        {""role"": ""system"", ""content"": ""Doesnt matter what you write here""},
        {""role"": ""user"", ""content"": ""Doesnt matter what you write here""}
    ])

print(response)


# CLIENT EXAMPLE VERSION 1.x (Not working)import osimport openaifrom openai import OpenAI

client = OpenAI(base_url=""http://localhost:8000/v1"", api_key=""some random text"")

response = client.chat.completions.create(
    model=""test"",
    messages=[
        {""role"": ""system"", ""content"": ""Doesnt matter what you write here""},
        {""role"": ""user"", ""content"": ""Doesnt matter what you write here""}
    ]
)

print(response)
OS
Arch Linux
Python version
Python 3.11.5
Library version
openai v1.x
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/784","AttributeError: module 'openai' has no attribute 'api_base'.","2023-11-11T09:38:40Z","Closed as not planned issue","question","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
Hello, I'm encountering an issue with my program EasyTranslator.
if len(self.api_proxy) == 0:
   print(""-"" * 3)
   print(f""\033[1;32mOpenAI API proxy not detected, currently using the api address: {openai.api_base}\033[0m"") 
else:
   self.api_proxy_url = self.api_proxy + ""/v1""
   openai.api_base = os.environ.get(""OPENAI_API_BASE"", self.api_proxy_url)
   print(""-"" * 3)
   print(f""\033[1;32mUsing OpenAI API proxy, the proxy address is: {openai.api_base}\033[0m"")
The program consistently throws an error:
AttributeError: module 'openai' has no attribute 'api_base'. Did you mean: 'api_type'?
I also noticed that this attribute is indeed missing, as shown in the image below:

Could you please clarify if there's another attribute used in place of 'api_base,' or if it's deprecated?
 Additionally, I'm using Azure to call the openai API's openai.api_base.
 How should I address this problem? Thank you very much!
To Reproduce
git clone https://github.com/artwalker/EasyTranslator.git
cd EasyTranslator
pip install -r preconditions.txt
python easy_translator.py ./book/profile.txt
Code snippets
No response
OS
win 11
Python version
Python 3.12.0
Library version
openai v1.2.3
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/779","ValueError for missing api_version when calling with_options() on an (Async)AzureOpenAI client instance","2023-11-28T15:48:35Z","Closed issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
Calling with_options() on an AzureOpenAI or AsyncAzureOpenAI client instance throws a ValueError exception:
ValueError: Must provide either the api_version argument or the OPENAI_API_VERSION environment variable
To Reproduce
Initialize an AzureOpenAI client and call with_options() on it.
Code snippets
from openai import AsyncAzureOpenAI

client = AsyncAzureOpenAI(
    azure_endpoint="""",
    api_key=""mykey"",
    api_version=""2023-07-01-preview"",
)

copied_client = client.with_options(timeout=30)
OS
ubuntu 22.04
Python version
v3.10.12
Library version
opeanai v1.1.2
 The text was updated successfully, but these errors were encountered: 
👍1
dlebech reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/openai-python/issues/777","None is not of type ‘object’","2024-05-13T01:16:06Z","Closed as not planned issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
When calling functions with no input arguments it gives this error
response = openai_client.chat.completions.create(timeout=10,
  File ""/usr/local/lib/python3.9/site-packages/openai/_utils/_utils.py"", line 299, in wrapper
    return func(*args, **kwargs)
  File ""/usr/local/lib/python3.9/site-packages/openai/resources/chat/completions.py"", line 556, in create
    return self._post(
  File ""/usr/local/lib/python3.9/site-packages/openai/_base_client.py"", line 1055, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File ""/usr/local/lib/python3.9/site-packages/openai/_base_client.py"", line 834, in request
    return self._request(
  File ""/usr/local/lib/python3.9/site-packages/openai/_base_client.py"", line 877, in _request
    raise self._make_status_error_from_response(err.response) from Noneopenai.OpenAIError: Error code: 400 - {'error': {'message': ""None is not of type 'object' - 'messages.2.function_call'"", 'type': 'invalid_request_error', 'param': None, 'code': None}}
This used to work before the recent updates.
I also posted this bug in the community forum since it is related to the API https://community.openai.com/t/none-is-not-of-type-object/488873
To Reproduce
An example of the function I try to call:
tools = [
  {
      ""type"": ""function"",
      ""function"": {
          ""name"":  ""escalate_to_manager"",
          ""description"": ""Contact the manager when there is nothing else that can be done"",
          ""parameters"": {
              ""type"": ""object"",
              ""properties"": {},
              ""required"": [],
          },
      }
  }
]
And I am calling the chat completion normally:
response = client.chat.completions.create(timeout=10,
                                         model=""gpt-3.5-turbo-1106"",
                                         messages=messages,
                                         tools=tools,
                                         tool_choice=""auto"",
                                         temperature=0,
                                         )
Code snippets
No response
OS
debian bullseye
Python version
python 3.9
Library version
openai v1.1.1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/774","TypeError: 'ChatCompletionMessage' object is not subscriptable","2023-11-10T15:07:09Z","Closed as not planned issue","question","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
content_with_padding_02 = ""\u200B\n\n"" + response_02.choices[0].message['content']

TypeError: 'ChatCompletionMessage' object is not subscriptable
To Reproduce
content_with_padding_02 = ""\u200B\n\n"" + response_02.choices[0].message['content']

TypeError: 'ChatCompletionMessage' object is not subscriptable
Code snippets
content_with_padding_02 = ""\u200B\n\n"" + response_02.choices[0].message['content']

TypeError: 'ChatCompletionMessage' object is not subscriptable
OS
macOS
Python version
3.11.4
Library version
1.0.1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/770","Response Format verbose_json in Audio Transcription with V1 Library","2023-11-11T02:26:02Z","Closed issue","enhancement","Confirm this is a feature request for the Python library and not the underlying OpenAI API.
 This is a feature request for the Python library
Describe the feature or improvement you're requesting
There is a few response formats documented in the OpenAI documentation, but it seems that the V1 library only supports for the response of response format json.
It is fine when using the V0 library as it will return an entire response body. However, since V1 library return a Pydantic model, and the Transcription model only has a text property. Even if I request the transcript in verbose_json format, the extra properties (e.g. duration, language) are dumped after parsing to a Pydantic model.
It should be able to add the extra properties to the Pydantic model. Can the team work on it, please?
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/769","Constant timeouts after multiple calls with async","2023-12-04T02:35:12Z","Closed issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
Constant timeouts after multiple asynchronous calls. It was discovered when using the Llama_Index framework that when calls are made to this library through the openai-python client wrapped with async, constant timeouts begin. If you do this without async or with asynchrony, but on the old version like 0.28, then there are no problems.
To Reproduce
Several calls in a row, for example, to embeddings that are wrapped with asynс.
Code snippets
No response
OS
ubuntu
Python version
Python 3.11.4
Library version
v1.2.0 and newer
 The text was updated successfully, but these errors were encountered: 
👍6
rubber-duck, mpmisko, linchpinlin, sourabhdesai, HerrIvan, and mingot reacted with thumbs up emoji
All reactions
👍6 reactions"
"https://github.com/openai/openai-python/issues/767","Hope to cancel ""required_args"" mecanism. Upgraded openai-python (v0->v1) refuses argumment ""repetition_penalty""","2023-11-10T08:27:56Z","Closed as not planned issue","question","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
I notice that about 2 weeks ago vllm has merged pr about adding an argument repetition_penalty aligned with huggingface. (refer to pr 1424)
However, openai-python also graded to v1 several days ago. it refuses the unknown argument repetition_penalty. The following is the detail error.
 xxx/examples/openai_chatcompletion_client.py"", line 72, in <module> chat_completion = client.chat.completions.create( File ""/opt/conda/lib/python3.10/site-packages/openai/_utils/_utils.py"", line 299, in wrapper return func(*args, **kwargs) TypeError: Completions.create() got an unexpected keyword argument 'repetition_penalty'
This conflict makes that we can not depend openai library for open-source models and self-defined arguments (much more than repetition_penalty). This library has been factually used in many API-based model deployment library like fastchat and vllm.
I really hope that the required_args can be removed. At least, we can modify its behaviors without modifying the source code of openai-python.
To Reproduce
git clone vllm and setup it.
Make some request using vllm and openai, add arguments repetition_penalty
The error happens.
Code snippets
No response
OS
Linux
Python version
Python v3.10
Library version
openai v1.2.2
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/763","The connection is not returned to the httpx pool when using a stream","2023-11-10T22:46:00Z","Closed issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
The connections is not returned to the pool when using a stream
In this case (see ""To reproduce""), the error occurs in venv/lib/python3.11/site-packages/httpcore/_async/connection_pool.py in the wait_for_connection() method of the RequestStatus() class
And if after each end of reading the stream you call AsyncStream.response.aclose(), then everything will work correctly
To Reproduce
Initialize an AsyncOpenAI object with a maximum number of connections equal to 2
Send 2 requests (chat.completions) with stream=True and read the entire stream
Send a third request
We will receive an APITimeoutError error
Code snippets
import asyncio

import httpxfrom openai import AsyncOpenAI


async def stream(client: AsyncOpenAI):
    response = await client.chat.completions.create(
        messages=[
            {'role': 'system', 'content': 'You are helpful assistant'},
            {'role': 'user', 'content': 'Hello'}
        ],
        model='gpt-4',
        stream=True,
        timeout=30,
    )
    result = ''
    async for message in response:
        result += message.choices[0].delta.content or """"


async def main():
    client = AsyncOpenAI(
        api_key='',
        http_client=httpx.AsyncClient(
            limits=httpx.Limits(
                max_connections=2,
            )
        )
    )
    for _ in range(3):
        await stream(client)


if __name__ == '__main__':
    asyncio.run(main())
OS
macOS, Ubuntu 22.04
Python version
Python v3.11.6
Library version
openai 1.2.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/762","Default timeout is ten minutes not 60 seconds.","2024-03-03T00:59:13Z","Closed issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
The code comment in README.md says timeout is 60s which conflicts with the prior sentence saying it's ten minutes. According to _constants.DEFAULT_TIMEOUT, ten minutes is the correct value.
(ten minutes seems like a lot!)
To Reproduce
Read the README.md
Code snippets
No response
OS
any
Python version
any
Library version
1.0 I guess
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/758","cannot import name 'OpenAI' from 'openai' v1.2.0","2023-11-09T19:10:28Z","Closed as not planned issue","question","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
ImportError: cannot import name 'OpenAI' from 'openai'
The not working code was taken from the official documentation.
To Reproduce
from openai import OpenAI
Code snippets
# There are many following errorsfrom pathlib import Path

from openai import OpenAI# ImportError: cannot import name 'OpenAI' from 'openai'

client = OpenAI()
# NameError: name 'OpenAI' is not defined

speech_file_path = Path(__file__).parent / ""speech.mp3""response = client.audio.speech.create(
# AttributeError: module 'client' has no attribute 'audio'# response = client.Audio.speech.create(# AttributeError: type object 'Audio' has no attribute 'speech'
  model=""tts-1"",
  voice=""alloy"",
  input=""Today is a wonderful day to build something people love!""
)

response.stream_to_file(speech_file_path)
OS
Windows 10
Python version
Python v3.10.7
Library version
openai v1.2.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/757","Get request headers data","2023-11-09T17:27:44Z","Closed as not planned issue","No label","Confirm this is a feature request for the Python library and not the underlying OpenAI API.
 This is a feature request for the Python library
Describe the feature or improvement you're requesting
When sending a request to openAI through this library in removes a lot of valuable metadata that comes back via headers, keeping those headers as part of the response would be great.
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/752","AzureOpenAI error :(","2023-11-09T14:48:15Z","Closed as not planned issue","question","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
I'm getting this error:
 ImportError: cannot import name 'AzureOpenAI' from 'openai' (/usr/local/lib/python3.10/dist-packages/openai/init.py)
 while import ing lbrary, 0.28.1 version of openai is being used.
 Whay should i do?
To Reproduce
imported library openai and AzureOpenAI
didn't run gave error.
Code snippets
No response
OS
macos
Python version
3.10
Library version
0.28.1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/751","ImportError: cannot import name 'override' from 'typing_extensions' with openai==1.2.0","2023-11-10T12:34:53Z","Closed as not planned issue","No label","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
ImportError Traceback (most recent call last)
 File , line 4
 2 from langchain.embeddings import OpenAIEmbeddings, AzureOpenAIEmbeddings
 3 #import openai
 ----> 4 from openai import AzureOpenAI
 5 def utils_embedding(input_chunk, OPENAI_API_KEY, deployment_str = ""xxxxx, api_base = ""xxxx""):
 7 OPENAI_API_KEY = XXX)
File /local_disk0/.ephemeral_nfs/envs/pythonEnv-853491b7-ec2b-411d-9c68-bdb8f2c9309a/lib/python3.10/site-packages/openai/init.py:6
 3 from future import annotations
 5 import os as _os
 ----> 6 from typing_extensions import override
 8 from . import types
 9 from ._types import NoneType, Transport, ProxiesTypes
ImportError: cannot import name 'override' from 'typing_extensions' (/databricks/python/lib/python3.10/site-packages/typing_extensions.py)
To Reproduce
import openai
Code snippets
No response
OS
windows
Python version
python v3.10.12
Library version
openai v1.2.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/750","Implement retrieving information about all threads without providing a thread id","2023-11-10T03:06:46Z","Closed issue","No label","Confirm this is a feature request for the Python library and not the underlying OpenAI API.
 This is a feature request for the Python library
Describe the feature or improvement you're requesting
Please implement fetching all the thread ids available from the python library.
That would be similar to calling the following, which works as of 2023-11-09 9:16 UTC:
r = httpx.get(
    ""https://api.openai.com/v1/threads"",
    headers={
        ""Authorization"": ""Bearer "" + api_key,
        ""Content-Type"": ""application/json"",
        ""OpenAI-Beta"": ""assistants=v1"",
    },
)
The existing OpenAI.beta.threads.retrieve requires a specific thread id, and calling it with an empty id produces #749.
At this time this endpoint does not seem to be documented. See https://platform.openai.com/docs/api-reference/threads/getThread
Additional context
See issue #749 for an actual bug triggered by trying to retrieve information about all the threads using the existing
OpenAI.beta.threads.retrieve.
 The text was updated successfully, but these errors were encountered: 
👍2
Tarolrr and ashbuilds reacted with thumbs up emoji
All reactions
👍2 reactions"
"https://github.com/openai/openai-python/issues/749","Threads retrieve request not properly validated","2024-03-25T16:52:49Z","Closed issue","enhancement","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
OpenAI.beta.threads.retrieve does not validate the ""thread_id"" input.
Invoking
t = client.beta.threads.retrieve(thread_id="""")
Throws ""APIStatusError"" with the following message:
Exception has occurred: APIStatusError
Error code: 307
httpx.HTTPStatusError: Redirect response '307 Temporary Redirect' for url 'https://api.openai.com/v1/threads/'
Redirect location: 'http://api.openai.com/v1/threads'

The expected behavior would be to fail validation at the library level. Another possible behavior would be to return a list with all the threads, as returned by the https://api.openai.com/v1/threads endpoint, but in that case it would be cleaner to implement something like client.beta.threads.retrieve_all()
To Reproduce
Run the following code:
from openai import OpenAI

client = OpenAI(api_key=""sk-*****"")

t = client.beta.threads.retrieve(thread_id="""")

print(t)
This was tested with version 1.2.0 of the python library on 2023-11-09 9:16 UTC.
Code snippets
No response
OS
Ubuntu 22.04 Linux
Python version
Python 3.11.5 (conda)
Library version
openai v1.2.0
 The text was updated successfully, but these errors were encountered: 
👍2
Tarolrr and ashbuilds reacted with thumbs up emoji
All reactions
👍2 reactions"
"https://github.com/openai/openai-python/issues/745","Add back support for OPENAI_API_BASE environment variable","2023-11-25T22:04:11Z","Closed issue","No label","Confirm this is a feature request for the Python library and not the underlying OpenAI API.
 This is a feature request for the Python library
Describe the feature or improvement you're requesting
For my environment I need to configure this library via environment variables. I use to be able to define the base URL via OPENAI_API_BASE but no longer can. I am also not able to define a base_url and a AZURE_OPENAI_ENDPOINT at the same time.
I understand v1 now exposes the ability for set base_url with _httpx.URL, but after reading the httpx documentation, it does not seem immediately clear how to do so via environment variables.
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
👍2
harryfyodor and jmikedupont2 reacted with thumbs up emoji
All reactions
👍2 reactions"
"https://github.com/openai/openai-python/issues/744","Pydantic serializer warnings when dumping a completion response with logprobs","2023-11-09T01:27:42Z","Closed issue","bug","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
Dumping a completion response from a request with logprobs set raises this warning:
python3.11/site-packages/pydantic/main.py:308: UserWarning: Pydantic serializer warnings:
  Expected `int` but got `float` - serialized value may not be as expected
  Expected `int` but got `float` - serialized value may not be as expected
  Expected `int` but got `float` - serialized value may not be as expected
  return self.__pydantic_serializer__.to_python(

I confirmed that changing this type annotation to
    top_logprobs: Optional[List[Dict[str, float]]] = None
removes the warnings.
To Reproduce
Install openai>=1.0.0
In a completions API call, set logprobs>=1
Dump the response using .model_dump(), as advised in the migration guide
Code snippets
import openai

client = openai.OpenAI()

# Set logprobs >= 1response = client.completions.create(
    model=""curie"", prompt=""test"", max_tokens=3, logprobs=1
)

response_as_dict = response.model_dump()
# raises a UserWarning
To ignore the warnings, the user currently has to do this:
import warnings

with warnings.catch_warnings():
    warnings.filterwarnings(
        ""ignore"",
        category=UserWarning,
        message=""Pydantic serializer warnings"",
    )
    response_as_dict = response.model_dump()
OS
macOS
Python version
Python v3.11.5
Library version
openai v1.1.2
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/740","Passing headers with the Azure OpenAI client in 1.1.1","2023-11-09T13:56:23Z","Closed issue","question","Confirm this is an issue with the Python library and not an underlying OpenAI API
 This is an issue with the Python library
Describe the bug
To be clear, I’m not certain this is a bug, but I’m uncertain how to do this now.
For extra security layers, I am exposing Azure OpenAI via the Azure API Management service. When I make a call this way, I pass a bearer JWT token and Apim subscription key as headers. I had no problem doing this on lower versions like 0.28.1, but now I don’t know how to do this.
To Reproduce
Attempting to call Azure OpenAI with 1.1.1. The call does not successfully connect. After glancing through the source Python code, I thought maybe sticking the headers as default_headers in the AzureOpenAI client would work, but it does not.
# Setting the auth headersauth_headers = {
    ‘Ocp-Apim-Subscription-Key’: my_apim_sub_key,
    ‘Authorization’: f’Bearer {my_jwt_token}’
}

# Setting the Azure OpenAI clientazure_openai_client = AzureOpenAI(
   api_key = ‘null’,
   api_version = ‘2023-07-01-preview’,
   azure_endpoint = my_special_url,
   default_headers = auth_headers
)

# Calling Azure OpenAIopenai_response = azure_openai_client.chat.completions.create(
    model = ‘gpt-35-turbo-16k’,
    messages = my_messages
)
Here’s the way I’m doing it on 0.28.1, and this still works just fine. As you can see, I’m passing the auth headers at the time of the actual API call here.
import openai

# Setting the auth headersauth_headers = {
    ‘Ocp-Apim-Subscription-Key’: my_apim_sub_key,
    ‘Authorization’: f’Bearer {my_jwt_token}’
}

# Setting config information on OpenAIopenai.api_url = my_special_urlopenai.api_version = ‘2023-07-01-preview’
openai.api_key = ‘null’

# Calling Azure OpenAIopenai_response = openai.ChatCompletion.create(
    engine = ‘gpt-35-turbo-16k’,
    messages = my_messages,
    Headers = auth_headers
)
What might I be doing wrong?
Code snippets
No response
OS
macOS
Python version
Python v3.10
Library version
1.1.1
 The text was updated successfully, but these errors were encountered: 
👍1
mahzy reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/openai-python/issues/735","client.audio.transcriptions.create takes one positional argument but 3 were given","2023-11-08T18:40:48Z","Closed as not planned issue","question","I was just trying to update my code with latest openai. But i get this response. I just pass model, file and prompt as i did in previous versions.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/734","JSON errors not decoded into APIError correctly","2023-11-09T21:49:32Z","Closed issue","bug","The following code does not work as expected since the release of 1.0:
import openaiclient = openai.Client(api_key='...')
file = open('demo.wav', 'rb')
try:
    client.audio.transcriptions.create(model=""whisper-1"", file=file)
except openai.BadRequestError as e:
    print(e.code)
    print(e.message)
This prints
None
Error code: 400 - {'error': {'message': 'Audio file is too short. Minimum audio length is 0.1 seconds.', 'type': 'invalid_request_error', 'param': 'file', 'code': 'audio_too_short'}}

so it appears e.code is not getting set correctly to the expected value of audio_too_short. Similarly param and type are not getting decoded correctly from the error response.
Here is a 0s WAV file that can be used to reproduce the issue: demo.wav.zip
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/731","AzureChatOpenAI missing ?","2023-11-08T13:45:23Z","Closed as not planned issue","No label","I have started seeing this error since Monday when using the latest openai package:
ValidationError: 1 validation error for AzureChatOpenAI rootopenai has no ChatCompletion attribute, this is likely due to an old version of the openai package. Try upgrading it with pip install --upgrade openai. (type=value_error)
I got a similar error when using ChatOpenAI as well.
But if I revert back to older version, e.g. 0.28.1, it all works fine.
 Can someone look into this ?
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/730","Ai","2023-11-08T12:53:33Z","Closed as not planned issue","No label","No description provided.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/727","Uploading JSON to Files API returns invalid file format","2023-11-10T03:00:14Z","Closed issue","API-feedback,bug","Upload to the files endpoint with a JSON file throws an error
Code:
from openai import OpenAI

client = OpenAI()
file = client.files.create(
    file=open(""example_1.json"", ""rb""),
    # Can either be fine-tuned or assistant
    purpose=""assistants"",
)
Stacktrace:
ile [~/anaconda3/lib/python3.10/site-packages/openai/resources/files.py:88](https://file+.vscode-resource.vscode-cdn.net/Users/vashishtmadhavan/Documents/playground/~/anaconda3/lib/python3.10/site-packages/openai/resources/files.py:88), in Files.create(self, file, purpose, extra_headers, extra_query, extra_body, timeout)
     82 if files:
     83     # It should be noted that the actual Content-Type header that will be
     84     # sent to the server will contain a `boundary` parameter, e.g.
     85     # multipart/form-data; boundary=---abc--
     86     extra_headers = {""Content-Type"": ""multipart/form-data"", **(extra_headers or {})}
---> 88 return self._post(
     89     ""[/files](https://file+.vscode-resource.vscode-cdn.net/files)"",
     90     body=maybe_transform(body, file_create_params.FileCreateParams),
     91     files=files,
     92     options=make_request_options(
     93         extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout
     94     ),
     95     cast_to=FileObject,
     96 )

File [~/anaconda3/lib/python3.10/site-packages/openai/_base_client.py:1055](https://file+.vscode-resource.vscode-cdn.net/Users/vashishtmadhavan/Documents/playground/~/anaconda3/lib/python3.10/site-packages/openai/_base_client.py:1055), in SyncAPIClient.post(self, path, cast_to, body, options, files, stream, stream_cls)
   1041 def post(
   1042     self,
   1043     path: str,
   (...)
   1050     stream_cls: type[_StreamT] | None = None,
   1051 ) -> ResponseT | _StreamT:
   1052     opts = FinalRequestOptions.construct(
   1053         method=""post"", url=path, json_data=body, files=to_httpx_files(files), **options
   1054     )
-> 1055     return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))

File [~/anaconda3/lib/python3.10/site-packages/openai/_base_client.py:834](https://file+.vscode-resource.vscode-cdn.net/Users/vashishtmadhavan/Documents/playground/~/anaconda3/lib/python3.10/site-packages/openai/_base_client.py:834), in SyncAPIClient.request(self, cast_to, options, remaining_retries, stream, stream_cls)
    825 def request(
    826     self,
    827     cast_to: Type[ResponseT],
   (...)
    832     stream_cls: type[_StreamT] | None = None,
    833 ) -> ResponseT | _StreamT:
--> 834     return self._request(
    835         cast_to=cast_to,
    836         options=options,
    837         stream=stream,
    838         stream_cls=stream_cls,
    839         remaining_retries=remaining_retries,
    840     )

File [~/anaconda3/lib/python3.10/site-packages/openai/_base_client.py:877](https://file+.vscode-resource.vscode-cdn.net/Users/vashishtmadhavan/Documents/playground/~/anaconda3/lib/python3.10/site-packages/openai/_base_client.py:877), in SyncAPIClient._request(self, cast_to, options, remaining_retries, stream, stream_cls)
    874     # If the response is streamed then we need to explicitly read the response
    875     # to completion before attempting to access the response text.
    876     err.response.read()
--> 877     raise self._make_status_error_from_response(err.response) from None
    878 except httpx.TimeoutException as err:
    879     if retries > 0:

BadRequestError: Error code: 400 - {'error': {'message': ""Invalid file format. Supported formats: ['c', 'cpp', 'csv', 'docx', 'html', 'java', 'json', 'md', 'pdf', 'php', 'pptx', 'py', 'rb', 'tex', 'txt', 'css', 'jpeg', 'jpg', 'js', 'gif', 'png', 'tar', 'ts', 'xlsx', 'xml', 'zip']"", 'type': 'invalid_request_error', 'param': None, 'code': None}}

Here is the example file: example_1.json
 The text was updated successfully, but these errors were encountered: 
👍2
TomasVotruba and illusions-LYY reacted with thumbs up emoji
All reactions
👍2 reactions"
"https://github.com/openai/openai-python/issues/726","Support for generate function's json object","2023-11-09T00:06:51Z","Closed issue","No label","For using func_call, hoping there is a parse tool to genenrate a json object of funtion's description. Like that:
Write an utils function with docstring
def foo(a: str, b: int = 0) -> None:
    """"""""    Description for funcion    Args:        a(str): argument a        b(int): argument b    """"""""
Parse the foo
from openai.xxx import parse

parse(foo)
will get the output
{
  ""type"": ""function"",
  ""function"": {
     ""name"": ""foo"",
     ""description"": ""Description for funcion"",
     ""parameters"": {
        ""type"": ""object"",
        ""properties"": {
           ""a"": {""type"": ""str"", ""description"": ""argument a""},
           ""b"": {""type"": ""int"", ""description"": ""argument b""},
         },
         ""required"": [""a""],
      },
    },
}
Maybe can use the inspect to get the argmument name and annotions, use func.__doc__ and re to extract the descriptions of function and argmuents, or even call GPT to do that.
 If this is ok, mayby I could try to do it.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/723","TypeError: object ImagesResponse can't be used in 'await' expression","2023-11-07T21:54:14Z","Closed as not planned issue","No label","With AsyncOpenAI, openai.images.generate does not work correctly when it is used with await, but it works without await. This seems to work perfectly in NodeJS, but not here in Python.
Example code:
response = await openai.images.generate(
    model=""dall-e-3"",
    prompt=prompt,
    n=1,
    quality=""hd"",
    response_format=""b64_json"",
    size=(""792x1024""
)

And here is the traceback:
Traceback (most recent call last):
  File ""app.py"", line 83, in dall_e
    response = await openai.images.generate(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: object ImagesResponse can't be used in 'await' expression

By removing await, it works correctly, but it is obviously not running asynchronously.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/722","Error: module 'openai' has no attribute 'completion'","2023-11-07T19:58:08Z","Closed as not planned issue","No label","It was working before the update. Are there any alternatives to initialize the process?
def analyze_text_with_gpt4(text):
    response = openai.ChatCompletion.create(
        model=""gpt-4-1106-preview"",
        messages=[
            {""role"": ""system"", ""content"": ""You are a helpful research assistant.""},
            {""role"": ""user"", ""content"": f""....: {text}""}
        ],
    )
    return response['choices'][0]['message']['content']
Except for switching back to the v0.28.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/721","async client Azure Support","2023-11-07T19:59:08Z","Closed as not planned issue","No label","will the async client be updated to support azure?
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/720","Issue with system_fingerprint Returning as ""None"" in OpenAI API Response","2023-11-10T03:03:50Z","Closed issue","No label","tried to make the completion output consistent while it's working and its consistent to some degree, but when I checked the API response the system_fingerprint=None was coming ....
example API response,
ChatCompletion(id='chatcmpl-8IKPGoyfgVAYPTQ270zYHV5MuwCKE', choices=[Choice(finish_reason='length', index=0, message=ChatCompletionMessage(content=""Once upon a time, in a not-so-distant future, humanity had set its sights on the red planet: Mars. Excitement filled the hearts of scientists, explorers, and dreamers alike as they prepared for a groundbreaking journey into the unknown.\n\nAmong them was Captain Amelia Carter, a seasoned astronaut known for her unwavering determination and unwavering spirit. She had been selected to lead the crew of the Ares-1, the first manned mission to Mars. Alongside her were five exceptional individuals, each with their own expertise and a shared passion for discovery.\n\nAs the day of departure drew near, the crew underwent rigorous training, preparing themselves physically and mentally for the challenges that lay ahead. They studied the Martian landscape, practiced living in confined spaces, and simulated emergency situations. They became a tight-knit family, relying on each other's strengths and supporting one another through the journey that lay ahead.\n\nFinally, the day arrived. The Ares-1 stood tall on the"", role='assistant', function_call=None, tool_calls=None))], created=1699379038, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=200, prompt_tokens=31, total_tokens=231))

code-piece to replicate
import asynciofrom openai import OpenAIimport pprintimport difflibfrom IPython.display import display, HTML

GPT_MODEL = ""gpt-3.5-turbo""client = OpenAI(
    api_key="""",
)


async def get_chat_response(system_message: str, user_request: str, seed: int = None):
    try:
        messages = [
            {""role"": ""system"", ""content"": system_message},
            {""role"": ""user"", ""content"": user_request},
        ]

        response = client.chat.completions.create(
            model=GPT_MODEL,
            messages=messages,
            seed=seed,
            max_tokens=200,
            n=1,
            temperature=1,
        )
        response_content = response.choices[0].message.content
        system_fingerprint = response.system_fingerprint
        prompt_tokens = response.usage.prompt_tokens
        completion_tokens = response.usage.total_tokens - response.usage.prompt_tokens

        table = f""""""        <table>        <tr><th>Response</th><td>{response_content}</td></tr>        <tr><th>System Fingerprint</th><td>{system_fingerprint}</td></tr>        <tr><th>Number of prompt tokens</th><td>{prompt_tokens}</td></tr>        <tr><th>Number of completion tokens</th><td>{completion_tokens}</td></tr>        </table>        """"""
        # Print the HTML table content
        display(HTML(table))


        return response_content
    except Exception as e:
        print(f""An error occurred: {e}"")
        return None


# This function compares two responses and displays the differences in a table.# Deletions are highlighted in red and additions are highlighted in green.# If no differences are found, it prints ""No differences found.""


def compare_responses(previous_response: str, response: str):
    d = difflib.Differ()
    diff = d.compare(previous_response.splitlines(), response.splitlines())

    diff_table = ""<table>""
    diff_exists = False

    for line in diff:
        if line.startswith(""- ""):
            diff_table += f""<tr style='color: red;'><td>{line}</td></tr>""
            diff_exists = True
        elif line.startswith(""+ ""):
            diff_table += f""<tr style='color: green;'><td>{line}</td></tr>""
            diff_exists = True
        else:
            diff_table += f""<tr><td>{line}</td></tr>""

    diff_table += ""</table>""

    if diff_exists:
        display(HTML(diff_table))
    else:
        print(""No differences found."")
        
system_message = ""You are a creative writing companion, aiding me in crafting captivating stories.""user_request = ""Compose a thrilling short story about a time-traveling detective who solves perplexing mysteries in the streets of Victorian London, blending historical elements with a touch of the supernatural.""

prev_response = await get_chat_response(
        system_message=system_message, user_request=user_request
    )

response = await get_chat_response(
        system_message=system_message, user_request=user_request
    )
    # The function compare_responses is then called with the two responses as arguments.
    # This function will compare the two responses and display the differences in a table.
    # If no differences are found, it will print ""No differences found.""compare_responses(prev_response, response)
version detail ,
Name: openai
Version: 1.1.1
Summary: Client library for the openai API
Home-page: 
Author: 
Author-email: OpenAI <[support@openai.com](mailto:support@openai.com)>
License: 
Location: /usr/local/lib/python3.10/dist-packages
Requires: anyio, distro, httpx, pydantic, tqdm, typing-extensions
Required-by: llmx

 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/717","AttributeError: type object 'OpenAI' has no attribute 'beta'","2023-11-07T16:46:12Z","Closed as not planned issue","No label","I use openai version is 1.1.1
 When I code like this:
from openai import Client as client
 self.client = client.beta.assistants.create(
            name=name,
            instructions=""This is a test assistant"",
            tools=[{""type"": ""code_interpreter""}],
            model=""gpt-3.5-turbo-110""
        )


it happens :AttributeError: type object 'OpenAI' has no attribute 'beta'
 The text was updated successfully, but these errors were encountered: 
👍1
ConnectingDNA-Depositary reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/openai-python/issues/716","disregard","2023-11-07T16:31:39Z","Closed issue","No label","disregard
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/715","convert_to_openai_object in new API","2023-11-07T16:43:36Z","Closed as not planned issue","No label","This function was quite useful for unit testing - was this moved to another location / under a different name? I cannot find similar functionality in the new API.
Thanks in advance.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/714","AttributeError: 'httpcore' module has no attribute 'UnsupportedProtocol' when importing openai","2023-11-07T16:00:07Z","Closed issue","question","Issue description
When attempting to import the openai Python package, an AttributeError is raised, indicating that the httpcore module lacks the attribute 'UnsupportedProtocol'.
Environment
Python 3.10.12
httpcore 1.0.1
Databricks Runtime 13.1
Spark 3.4.0
Trouble Shooting
Checked for updates on both openai and httpcore packages.
Attempted to reinstall both packages.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/712","How can I use Async with Azure?","2023-11-07T15:30:53Z","Closed as not planned issue","question","Just now I'm updating from 0.28.1 to the latest version and migrating. I am currently using await openai.ChatCompletion.acreate. I understand in migrating that I need to instantiate a Client, however there doesn't appear to be an Async client for Azure, only the standard AzureOpenAI() that doesn't appear to support Async.
Any guidance would be much appreciated - if this is a case of it not being implemented yet then I would just keep using 0.28.1 until there is Async support for Azure.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/710","Type incompatibility between ToolAssistantToolsFunctionFunction and Function","2023-11-09T01:29:42Z","Closed issue","enhancement","Function from types/chat/completion_create_params.py is not compatible with ToolAssistantToolsFunctionFunction from types/beta/assistant_create_param because description is not a required field.
I have existing code that's emitting a Function schema and using the type. Now that I'm transitioning some work over to assistants, I'd like to use the same function schema generation I've had before. For the moment I'll probably switch to using ToolAssistantToolsFunctionFunction, however these types may need some alignment.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/708","Not able to achieve Reproducible outputs with the seed parameter (SDK version 1.1.1)","2023-11-10T10:41:46Z","Closed issue","No label","Hi 👋,
I've noticed the exciting new capability for achieving Reproducible outputs.
 Nevertheless, I'm encountering varying results when using identical input parameters and getting the same system fingerprint.
Am I missing something?
Python code to reproduce this issue (might need a few runs)
import openai
from openai.types.chat import ChatCompletionUserMessageParam

print(openai.__version__)
print()

for i in range(5):
    message = ChatCompletionUserMessageParam(
        content=(
            ""Imagine something completely random and crazy! (approximately 100 words)""
        ),
        role=""user"",
    )
    response = openai.chat.completions.create(
        messages=[message], model=""gpt-3.5-turbo-1106"", temperature=0, seed=42
    )
    print(response.system_fingerprint, response.choices[0].message.content)

Versions
OpenAI SDK 1.1.1
 Python version 3.11.5
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/707","AsssitantDeleted Typo in spec -> generated file","2023-11-07T15:36:14Z","Closed issue","bug","While working with the types from openai.types.beta I noticed that there's one too many s in AssistantDeleted:
https://github.com/openai/openai-python/blob/e0aafc6c1a45334ac889fe3e54957d309c3af93f/src/openai/types/beta/asssitant_deleted.py#L10C11-L10C11
I'd make a PR, but this looks like its autogenerated from an OpenAPI Schema.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/705","chat-create-response_format - response_format={""type"": ""json_object""} does not follow other default types for that param","2023-11-07T09:25:15Z","Closed as not planned issue","No label","Most are Typing.Literal, not Dict. This is especially confusing given that the default param is a str and the documentation seems to imply the type is str. Instead it requires response_format={""type"": ""json_object""} and {""type"": ""text""}
openai.chat.completions.create(
            model=model_type,
            messages=prompts,
            response_format=""json_object"" 
        )

produces:
openai.BadRequestError: Error code: 400 - 
{'error': {'message': 
""'json_object' is not of type 'object' - 'response_format'"", 
'type': 'invalid_request_error', 'param': None, 'code': None}}

I haven't dug deeper; perhaps this has a reason I am missing, such as it is detected as dict, so the api ""knows"" the type.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/704","ChatCompletion with Azure","2023-11-07T07:42:31Z","Closed as not planned issue","question","openai has no ChatCompletion attribute, this is likely due to an old version of the openai package. Try upgrading it with pip install --upgrade openai. (type=value_error)
Keep getting this error, although yesterday downgraded openai version it worked today it showes the same error with downgraded and upgraded model as well!
 What should I do ?
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/703","The official example for Function Calling doesn't work with SDK version 1.1.1","2023-11-11T02:24:43Z","Closed issue","No label","Expected behavior
The ""Example with one function called in parallel"" code from the documentation should correctly show how to use the function call feature.
Actual behavior
I get BadRequestError: Error code: 400 - {'error': {'message': ""'content' is a required property - 'messages.1'"", 'type': 'invalid_request_error', 'param': None, 'code': None}} while running the code.
Stack trace:
BadRequestError                           Traceback (most recent call last)
[<ipython-input-73-3a60881757d5>](https://localhost:8080/#) in <cell line: 77>()
     75         )  # get a new response from the model where it can see the function response
     76         return second_response
---> 77 print(run_conversation())

5 frames
[<ipython-input-73-3a60881757d5>](https://localhost:8080/#) in run_conversation()
     70                 }
     71             )  # extend conversation with function response
---> 72         second_response = openai.chat.completions.create(
     73             model=""gpt-3.5-turbo-1106"",
     74             messages=messages,

[/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py](https://localhost:8080/#) in wrapper(*args, **kwargs)
    297                         msg = f""Missing required argument: {quote(missing[0])}""
    298                 raise TypeError(msg)
--> 299             return func(*args, **kwargs)
    300 
    301         return wrapper  # type: ignore

[/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py](https://localhost:8080/#) in create(self, messages, model, frequency_penalty, function_call, functions, logit_bias, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_p, user, extra_headers, extra_query, extra_body, timeout)
    554         timeout: float | httpx.Timeout | None | NotGiven = NOT_GIVEN,
    555     ) -> ChatCompletion | Stream[ChatCompletionChunk]:
--> 556         return self._post(
    557             ""/chat/completions"",
    558             body=maybe_transform(

[/usr/local/lib/python3.10/dist-packages/openai/_base_client.py](https://localhost:8080/#) in post(self, path, cast_to, body, options, files, stream, stream_cls)
   1053             method=""post"", url=path, json_data=body, files=to_httpx_files(files), **options
   1054         )
-> 1055         return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
   1056 
   1057     def patch(

[/usr/local/lib/python3.10/dist-packages/openai/_base_client.py](https://localhost:8080/#) in request(self, cast_to, options, remaining_retries, stream, stream_cls)
    832         stream_cls: type[_StreamT] | None = None,
    833     ) -> ResponseT | _StreamT:
--> 834         return self._request(
    835             cast_to=cast_to,
    836             options=options,

[/usr/local/lib/python3.10/dist-packages/openai/_base_client.py](https://localhost:8080/#) in _request(self, cast_to, options, remaining_retries, stream, stream_cls)
    875             # to completion before attempting to access the response text.
    876             err.response.read()
--> 877             raise self._make_status_error_from_response(err.response) from None
    878         except httpx.TimeoutException as err:
    879             if retries > 0:

Versions
OpenAI SDK: 1.1.1
 Python: 3.10.12
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/701","Cannot import OpenAI from openai","2023-11-07T02:35:31Z","Closed as not planned issue","No label","The pypi page shows this example
from openai import OpenAI

client = OpenAI(
    # defaults to os.environ.get(""OPENAI_API_KEY"")
    api_key=""My API Key"",
)

chat_completion = client.chat.completions.create(
    messages=[
        {
            ""role"": ""user"",
            ""content"": ""Say this is a test"",
        }
    ],
    model=""gpt-3.5-turbo"",
)

Similar examples using openai.OpenAI can be found in official docs for Dalle-3:
from openai import OpenAI
client = OpenAI()

response = client.images.generate(
  model=""dall-e-3"",
  prompt=""a white siamese cat"",
  size=""1024x1024"",
  quality=""standard"",
  n=1,
)

image_url = response.data[0].url

However, this code will cause the following error (with openai==1.1.1):
 ImportError: cannot import name 'OpenAI' from 'openai'
Tested this on both local Windows and a Databricks notebook.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/700","Using Azure OpenAI with AAD example broken link in OpenAI v1 Release Notes Discussion","2023-11-07T09:35:02Z","Closed issue","bug,documentation","#631 links to https://github.com/openai/openai-python/blob/v1/examples/azure_ad.py
Fix: Should be https://github.com/openai/openai-python/blob/main/examples/azure_ad.py
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/699","client.files.retrieve_content only returns strings, and not bytes/binary data","2023-11-09T19:07:59Z","Closed issue","No label","When I try to download/retrieve a binary file (eg. PNG image) created by an assistant, it get automatically cast to a string, so it can't be correctly parsed/displayed.
Eg:
ret_file = client.files.retrieve_content('file-XXX')
ret_file[:10]
# '�PNG\r\n\x1a\n\x00\x00'
There doesn't seem to be a clean way in the API to retrieve a file as raw bytes from what I can see.
This is important for code interpreter scenarios where the agent returns binary files that need to be rendered, like Images.
For others having this issue, you can just request the files directly using requests like:
import requestsfrom io import BytesIO

file_id = 'file-XXXX'headers = {
    'Authorization': f""Bearer {os.environ['OPENAI_API_KEY']}""
}
response = requests.get(f'https://api.openai.com/v1/files/{file_id}/content', headers=headers)
Image.open(BytesIO(response.content))
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/692","CRITICAL BUG: images.generate does not work on Azure OpenAI","2024-01-10T04:38:53Z","Closed issue","No label","openai_api_version = ""2023-09-01-preview""
        client = AzureOpenAI(azure_endpoint = openai_api_base, api_key=openai_api_key, api_version=openai_api_version)

        response = client.images.generate(
            prompt=prompt,
            size=size,
            n=n
        )

Fails with:
{'error': {'code': '404', 'message': 'Resource not found'}}
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/691","Official Documentation Example for Function Calling Does Not Work Post Major Update","2023-11-06T22:42:06Z","Closed issue","No label","Issue Description
Following the recent major update to the OpenAI library, the function calling example provided in the official documentation no longer works.
Steps to Reproduce
Visit the official OpenAI documentation on function calling with Chat models here.
Attempt to run the provided example.
Expected Behavior
The function calling example should execute without errors, demonstrating how to utilize the feature with the latest library version.
Actual Behavior
When attempting to run the function calling example from the official documentation, the following error occurs: [insert error message]
 The text was updated successfully, but these errors were encountered: 
👀1
adriangalilea reacted with eyes emoji
All reactions
👀1 reaction"
"https://github.com/openai/openai-python/issues/690","TypeError: ConnectionPool.__init__() got an unexpected keyword argument 'socket_options'","2023-11-06T22:08:58Z","Closed issue","No label","Code
import os
from openai import OpenAI

client = OpenAI(
    api_key=settings.OPEN_AI_API_KEY,
    organization='org-xxxx'
)
client.models.list()


Stack Trace
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
Cell In[3], line 6
      2 from openai import OpenAI
      4 # os.environ['OPENAI_API_KEY'] = settings.OPEN_AI_API_KEY
----> 6 client = OpenAI(
      7     api_key=settings.OPEN_AI_API_KEY,
      8     organization='org-xxxx'
      9 )
     10 client.models.list()
     12 # from django.conf import settings
     13 # from openai import OpenAI
     14 # client = OpenAI(organization = settings.OPEN_AI_API_KEY)
   (...)
     17 
     18 # client.models.list()

File /usr/local/lib/python3.11/site-packages/openai/_client.py:105, in OpenAI.__init__(self, api_key, organization, base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)
    102 if base_url is None:
    103     base_url = f""https://api.openai.com/v1""
--> 105 super().__init__(
    106     version=__version__,
    107     base_url=base_url,
    108     max_retries=max_retries,
    109     timeout=timeout,
    110     http_client=http_client,
    111     custom_headers=default_headers,
    112     custom_query=default_query,
    113     _strict_response_validation=_strict_response_validation,
    114 )
    116 self._default_stream_cls = Stream
    118 self.completions = resources.Completions(self)

File /usr/local/lib/python3.11/site-packages/openai/_base_client.py:738, in SyncAPIClient.__init__(self, version, base_url, max_retries, timeout, transport, proxies, limits, http_client, custom_headers, custom_query, _strict_response_validation)
    723         timeout = DEFAULT_TIMEOUT
    725 super().__init__(
    726     version=version,
    727     limits=limits,
   (...)
    736     _strict_response_validation=_strict_response_validation,
    737 )
--> 738 self._client = http_client or httpx.Client(
    739     base_url=base_url,
    740     # cast to a valid type because mypy doesn't understand our type narrowing
    741     timeout=cast(Timeout, timeout),
    742     proxies=proxies,
    743     transport=transport,
    744     limits=limits,
    745 )
    746 self._has_custom_http_client = bool(http_client)

File /usr/local/lib/python3.11/site-packages/httpx/_client.py:672, in Client.__init__(self, auth, params, headers, cookies, verify, cert, http1, http2, proxies, mounts, timeout, follow_redirects, limits, max_redirects, event_hooks, base_url, transport, app, trust_env, default_encoding)
    669 allow_env_proxies = trust_env and app is None and transport is None
    670 proxy_map = self._get_proxy_map(proxies, allow_env_proxies)
--> 672 self._transport = self._init_transport(
    673     verify=verify,
    674     cert=cert,
    675     http1=http1,
    676     http2=http2,
    677     limits=limits,
    678     transport=transport,
    679     app=app,
    680     trust_env=trust_env,
    681 )
    682 self._mounts: typing.Dict[URLPattern, typing.Optional[BaseTransport]] = {
    683     URLPattern(key): None
    684     if proxy is None
   (...)
    694     for key, proxy in proxy_map.items()
    695 }
    696 if mounts is not None:

File /usr/local/lib/python3.11/site-packages/httpx/_client.py:720, in Client._init_transport(self, verify, cert, http1, http2, limits, transport, app, trust_env)
    717 if app is not None:
    718     return WSGITransport(app=app)
--> 720 return HTTPTransport(
    721     verify=verify,
    722     cert=cert,
    723     http1=http1,
    724     http2=http2,
    725     limits=limits,
    726     trust_env=trust_env,
    727 )

File /usr/local/lib/python3.11/site-packages/httpx/_transports/default.py:136, in HTTPTransport.__init__(self, verify, cert, http1, http2, limits, trust_env, proxy, uds, local_address, retries, socket_options)
    133 ssl_context = create_ssl_context(verify=verify, cert=cert, trust_env=trust_env)
    135 if proxy is None:
--> 136     self._pool = httpcore.ConnectionPool(
    137         ssl_context=ssl_context,
    138         max_connections=limits.max_connections,
    139         max_keepalive_connections=limits.max_keepalive_connections,
    140         keepalive_expiry=limits.keepalive_expiry,
    141         http1=http1,
    142         http2=http2,
    143         uds=uds,
    144         local_address=local_address,
    145         retries=retries,
    146         socket_options=socket_options,
    147     )
    148 elif proxy.url.scheme in (""http"", ""https""):
    149     self._pool = httpcore.HTTPProxy(
    150         proxy_url=httpcore.URL(
    151             scheme=proxy.url.raw_scheme,
   (...)
    165         socket_options=socket_options,
    166     )

TypeError: ConnectionPool.__init__() got an unexpected keyword argument 'socket_options'

 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/688","OpenAI client lacks Completions in version 1.1.0","2023-11-06T21:48:30Z","Closed issue","No label","With version 1.1.0 installed to pyenv virtualenv and python 3.11.2 :
>>> from langchain.llms import OpenAI
>>> llm = OpenAI()
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/lifehedging/.pyenv/versions/myenv/lib/python3.11/site-packages/langchain/load/serializable.py"", line 97, in __init__
    super().__init__(**kwargs)
  File ""/Users/lifehedging/.pyenv/versions/myenv/lib/python3.11/site-packages/pydantic/v1/main.py"", line 339, in __init__
    values, fields_set, validation_error = validate_model(__pydantic_self__.__class__, data)
                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/lifehedging/.pyenv/versions/myenv/lib/python3.11/site-packages/pydantic/v1/main.py"", line 1102, in validate_model
    values = validator(cls_, values)
             ^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/lifehedging/.pyenv/versions/myenv/lib/python3.11/site-packages/langchain/llms/openai.py"", line 266, in validate_environment
    values[""client""] = openai.Completion
                       ^^^^^^^^^^^^^^^^^
AttributeError: module 'openai' has no attribute 'Completion'. Did you mean: 'completions'?

Or this issue is for langchain if openai has changed the interface.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/687","gpt-4-1106-preview only has 4096 tokens?","2023-11-06T22:25:15Z","Closed as not planned issue","question","I got this error while trying to generate with gpt-4-1106-preview:
Generating with model: gpt-4-1106-preview ...
 Error while generating: Error code: 400 - {'error': {'message': 'max_tokens is too large: 5500. This model supports at most 4096 completion tokens, whereas you provided 5500.', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': None}}
This seems unexpected, since max_tokens is supposed to have a much higher ceiling than that.
Anyone now what could be happening? Thanks.
 The text was updated successfully, but these errors were encountered: 
👍8
batjko, sangonzal, Charlie059, glenn-jocher, blacklee123, hi-i-m-GTooth, sabaimran, and ch-j reacted with thumbs up emoji👀5
mike-luabase, sledmonkey, glenn-jocher, blacklee123, and AndromedaPerseus reacted with eyes emoji
All reactions
👍8 reactions
👀5 reactions"
"https://github.com/openai/openai-python/issues/686","Assistant API url Invalid","2023-11-07T05:32:27Z","Closed issue","No label","Code
Error
openai version - 1.1.0
 The text was updated successfully, but these errors were encountered: 
👍5
mehmetkoca, brandonwatts, kdcokenny, ryanzola, and Jeremaiha reacted with thumbs up emoji
All reactions
👍5 reactions"
"https://github.com/openai/openai-python/issues/685","Missing OpenAI object","2023-11-06T21:38:52Z","Closed issue","question","Problem
from django.conf import settings
from openai import OpenAI
client = OpenAI()

client.models.list()

Error
---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
Cell In[6], line 2
      1 from django.conf import settings
----> 2 from openai import OpenAI
      3 client = OpenAI()
      5 client.models.list()

ImportError: cannot import name 'OpenAI' from 'openai' (/usr/local/lib/python3.11/site-packages/openai/__init__.py)

Docs
https://platform.openai.com/docs/api-reference/models/list
Version
pip freeze | grep openai
openai==1.1.0

 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/684","openai 1.0.1 is missing model and other things seems behind the docs","2023-11-06T20:59:34Z","Closed issue","No label","Hey,
openai 1.0.1 is missing model and other things seems behind
response = self.client.images.generate(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: Images.generate() got an unexpected keyword argument 'model'

Main docs page mentions model can be used
https://platform.openai.com/docs/guides/images/generations
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/683","AttributeError: 'Audio' object has no attribute 'speech' in 1.0.1","2023-11-06T20:10:13Z","Closed as not planned issue","No label","AttributeError: 'Audio' object has no attribute 'speech'
Traceback (most recent call last):
 File ""/Users/xxx/src/jshare-llm-demo/openai-demo/tts.py"", line 12, in 
 response = client.audio.speech.create(
 AttributeError: 'Audio' object has no attribute 'speech'
from pathlib import Path
from openai import OpenAI

client = OpenAI(
    # defaults to os.environ.get(""OPENAI_API_KEY"")
    api_key=""xxxxxx"",
)

speech_file_path = Path(__file__).parent / ""speech.mp3""
response = client.audio.speech.create(
  model=""tts-1"",
  voice=""alloy"",
  input=""Today is a wonderful day to build something people love!""
)

response.stream_to_file(speech_file_path)

=============================
OS: mac OS
 python: 3.9.*
 openai: 1.0.1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/681","Missing client.audio.speech?","2023-11-06T18:56:52Z","Closed issue","No label","The example at https://platform.openai.com/docs/guides/text-to-speech/quick-start doesn't work. Not sure if you forgot to push an update to this repo?
dave@mbp openai-test-1 % python3 main.py
Traceback (most recent call last):
  File ""/Users/dave/Work/openai-test-1/main.py"", line 10, in <module>
    response = client.audio.speech.create(
               ^^^^^^^^^^^^^^^^^^^
AttributeError: 'Audio' object has no attribute 'speech'

 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/680","Getting ChatCompletion error via Azure OpenAI","2023-11-06T18:02:06Z","Closed as not planned issue","question","openai has no ChatCompletion attribute, this is likely due to an old version of the openai package. Try upgrading it with pip install --upgrade openai. (type=value_error)
What is that what should i do ? (FYI upgrading openai doesn't work out)
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/676","v1.0 drops embeddings_util.py breaking semantic text search","2023-11-06T13:55:17Z","Closed issue","No label","Describe the bug
The previous version of the OpenAI Python library contained embeddings_utils.py which provided functions like cosine_similarity which are used for semantic text search with embeddings. Without this functionality existing code including OpenAI's cookbook example: https://cookbook.openai.com/examples/semantic_text_search_using_embeddings will fail due to this dependency.
Are there plans to add this support back-in or should we just create our own cosine_similarity function based on the one that was present in embeddings_utils:
def cosine_similarity(a, b):
    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))
To Reproduce
Cookbook example cannot be converted to use v1.0 without removing the dependency on embeddings_utils.pyhttps://cookbook.openai.com/examples/semantic_text_search_using_embeddings
Code snippets
from openai.embeddings_utils import get_embedding, cosine_similarity

# search through the reviews for a specific productdef search_reviews(df, product_description, n=3, pprint=True):
    product_embedding = get_embedding(
        product_description,
        engine=""text-embedding-ada-002""
    )
    df[""similarity""] = df.embedding.apply(lambda x: cosine_similarity(x, product_embedding))

    results = (
        df.sort_values(""similarity"", ascending=False)
        .head(n)
        .combined.str.replace(""Title: "", """")
        .str.replace(""; Content:"", "": "")
    )
    if pprint:
        for r in results:
            print(r[:200])
            print()
    return results


results = search_reviews(df, ""delicious beans"", n=3)
OS
Windows
Python version
Python v3.10.11
Library version
openai-python==1.0.0rc2
 The text was updated successfully, but these errors were encountered: 
👍5
trangdata, fareshan, jerpint, lordofmetis, and davidgilbertson reacted with thumbs up emoji
All reactions
👍5 reactions"
"https://github.com/openai/openai-python/issues/675","timeout param is not handled correctly, resulting in no timeout being set","2023-11-06T16:37:51Z","Closed issue","bug,fixed in v1","Describe the bug
Passing a timeout to openai.ChatCompletion.create doesn't do anything. Likely due to he incorrect logic here: https://github.com/openai/openai-python/blob/main/openai/api_resources/abstract/engine_api_resource.py#L101
 If pass 3, according to this I will have no timeout.
To Reproduce
call a completion api with a small timeout
Code snippets
No response
OS
any
Python version
any
Library version
v0.28.1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/674","Images too different","2023-11-06T19:04:06Z","Closed as not planned issue","API-feedback","Describe the bug
Im using dall-e 3 api but images with same prompt on chatgpt generate too much different images.
Prompt: ""cartoon style + king davi from bible""
From Python:

From ChatGPT:

What im doing wrong?
To Reproduce
In python call api with prompt ""cartoon style + king davi from bible"".
In chatgpt use prompt ""cartoon style + king davi from bible"".
Code snippets
No response
OS
macOS
Python version
3.9
Library version
latest
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/673","Not able to build the wheel for v1","2023-11-03T20:44:42Z","Closed as not planned issue","No label","Describe the bug
I'm not able to build the wheel for v1 because of the error: project.license must be valid exactly by one definition.
To Reproduce
Clone and checkout to the new version.
git clone https://github.com/openai/openai-python.git
cd openai-python/
git checkout origin/v1

Create a setup.py file:
vim setup.py

Paste the following:
from setuptools import setup

setup()

Build the wheel
python3 setup.py bdist_wheel

Error:
configuration error: `project.license` must be valid exactly by one definition (2 matches found):

    - keys:
        'file': {type: string}
      required: ['file']
    - keys:
        'text': {type: string}
      required: ['text']

DESCRIPTION:
    `Project license <https://peps.python.org/pep-0621/#license>`_.

GIVEN VALUE:
    ""Apache-2.0""

OFFENDING RULE: 'oneOf'

DEFINITION:
    {
        ""oneOf"": [
            {
                ""properties"": {
                    ""file"": {
                        ""type"": ""string"",
                        ""$$description"": [
                            ""Relative path to the file (UTF-8) which contains the license for the"",
                            ""project.""
                        ]
                    }
                },
                ""required"": [
                    ""file""
                ]
            },
            {
                ""properties"": {
                    ""text"": {
                        ""type"": ""string"",
                        ""$$description"": [
                            ""The license of the project whose meaning is that of the"",
                            ""`License field from the core metadata"",
                            ""<https://packaging.python.org/specifications/core-metadata/#license>`_.""
                        ]
                    }
                },
                ""required"": [
                    ""text""
                ]
            }
        ]
    }
Traceback (most recent call last):
  File ""/home/user_name/projects/openai-python/setup.py"", line 3, in <module>
    setup()
  File ""/home/user_name/anaconda3/lib/python3.9/site-packages/setuptools/__init__.py"", line 107, in setup
    return distutils.core.setup(**attrs)
  File ""/home/user_name/anaconda3/lib/python3.9/site-packages/setuptools/_distutils/core.py"", line 159, in setup
    dist.parse_config_files()
  File ""/home/user_name/anaconda3/lib/python3.9/site-packages/setuptools/dist.py"", line 908, in parse_config_files
    pyprojecttoml.apply_configuration(self, filename, ignore_option_errors)
  File ""/home/user_name/anaconda3/lib/python3.9/site-packages/setuptools/config/pyprojecttoml.py"", line 66, in apply_configuration
    config = read_configuration(filepath, True, ignore_option_errors, dist)
  File ""/home/user_name/anaconda3/lib/python3.9/site-packages/setuptools/config/pyprojecttoml.py"", line 129, in read_configuration
    validate(subset, filepath)
  File ""/home/user_name/anaconda3/lib/python3.9/site-packages/setuptools/config/pyprojecttoml.py"", line 55, in validate
    raise ValueError(f""{error}\n{summary}"") from None
ValueError: invalid pyproject.toml config: `project.license`.
configuration error: `project.license` must be valid exactly by one definition (2 matches found):

    - keys:
        'file': {type: string}
      required: ['file']
    - keys:
        'text': {type: string}
      required: ['text']

Code snippets
No response
OS
WSL2 Ubuntu
Python version
Python v3.9.7
Library version
openai-python v1.0.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/671","Assistant not using function response","2023-11-10T03:24:58Z","Closed issue","API-feedback,bug","Describe the bug
When my assistant gets the output from a function with long text it doesn't use that output to response to the user. Instead it makes reference to it like if the final user could also see the response of the function when it is actually internal. Here's an example:

{
    {
      ""role"": ""assistant"",
      ""function_call"": {
        ""name"": ""answer_question"",
        ""arguments"": {
          ""question"": ""what happened in catalonia during ww2""
        }
      },
      ""content"": null
    },
    {
      ""role"": ""function"",
      ""name"": ""answer_question"",
      ""content"": ""During World War II, Catalonia was part of Spain under the rule of Francisco Franco. Although Spain declared neutrality, Catalonia was affected by the conflict. Italy and Germany had some interest in Catalonia before the war, but their attempts to establish a fascist movement failed. Barcelona was bombed by Italian planes supporting the Nationalist side in the Spanish Civil War. Catalan individuals, such as Joan Pujol and Josep Trueta, played roles in the Allied side, with Pujol acting as a double agent and Trueta organizing medical services. Some Catalans also fought on the Soviet side in the Eastern Front.""
    },
    {
      ""role"": ""assistant"",
      ""content"": ""Thank you for your question about Catalonia during WWII. Now, can you tell me the time period during which World War II took place?""
    }
}

To Reproduce
Run the script copied below
Code snippets
import openaiimport json
​
openai.api_key = ""YOUR_API_KEY""
​
​
# Example dummy function hard coded to return the same answer to the example questiondef answer_question(question):
    """"""Get the answer to the student's question""""""
    answer = ""During World War II, Catalonia was part of Spain under the rule of Francisco Franco. Although Spain declared neutrality, Catalonia was affected by the conflict. Italy and Germany had some interest in Catalonia before the war, but their attempts to establish a fascist movement failed. Barcelona was bombed by Italian planes supporting the Nationalist side in the Spanish Civil War. Catalan individuals, such as Joan Pujol and Josep Trueta, played roles in the Allied side, with Pujol acting as a double agent and Trueta organizing medical services. Some Catalans also fought on the Soviet side in the Eastern Front.""
    return answer
​
​
def run_conversation():
    messages = [
        {
            ""role"": ""system"",
            ""content"": ""I want you to act as a history teacher who has to ask some questions to his students.\nQuestions: \n- When did WWII take place?\n- Which countries were the principal belligerents?\nYour primary focus is asking the provided questions but if the student asks any history questions, you will try to answer them using the provided functions.\n""
        },
        {
            ""role"": ""assistant"",
            ""content"": ""Can I ask you some history questions?""
        },
        {
            ""role"": ""user"",
            ""content"": ""what happened in Catalonia during WW2""
        }
    ]
    functions = [
       {
           ""name"": ""answer_question"",
           ""description"": ""Answers a question from the student that is outside the context of the agent's prompt"",
           ""parameters"": {
               ""type"": ""object"",
               ""properties"": {
                   ""question"": {
                       ""description"": ""The question the student is asking"",
                       ""type"": ""string""
                   }
               },
               ""required"": [""question""]
           }
       }
   ]
    response = openai.ChatCompletion.create(
        model=""gpt-4"",
        messages=messages,
        functions=functions,
        function_call=""auto"",
    )
    response_message = response[""choices""][0][""message""]
​
    if response_message.get(""function_call""):
        available_functions = {
            ""answer_question"": answer_question,
        }
        function_name = response_message[""function_call""][""name""]
        function_to_call = available_functions[function_name]
        function_args = json.loads(response_message[""function_call""][""arguments""])
        function_response = function_to_call(
            question=function_args.get(""question""),
        )
​
        messages.append(response_message)
        messages.append(
            {
                ""role"": ""function"",
                ""name"": function_name,
                ""content"": function_response,
            }
        )
        second_response = openai.ChatCompletion.create(
            model=""gpt-4"",
            messages=messages,
        )
        return second_response
​
​
print(run_conversation())


### OS

macOS

### Python version

Python v3.9.*

### Library version

openai-python v0.27.*

 The text was updated successfully, but these errors were encountered: 
👍5
atibaup, david-gomey, jorgeas80, JaimeFLandbot, and osama-nehme reacted with thumbs up emoji
All reactions
👍5 reactions"
"https://github.com/openai/openai-python/issues/670","GPT4V - API","2023-11-06T22:05:58Z","Closed issue","No label","Describe the feature or improvement you're requesting
I am requesting GPT4V API support. The ability to provide image input with the purpose of multimodality.
This feature has been out for a little bit now with ChatGPT Plus mobile and web. Even if this is not on the V1 release roadmap, it would be nice to understand timeline.
Any information on this functionality would be great. Thanks
 The text was updated successfully, but these errors were encountered: 
👍4
toxoplasmotic, Magnetar99, nazfoxway, and jianglongye reacted with thumbs up emoji
All reactions
👍4 reactions"
"https://github.com/openai/openai-python/issues/668","Chat GPT Eduaction (Moodle)","2023-11-03T22:25:23Z","Closed as not planned issue","enhancement","Describe the feature or improvement you're requesting
Moodle, Quality Matters, and other online templates are nowadays provided to create MOOCs, the problem is that generating the template involves time and advanced IT expertise (Yaml, JSON), in addition to domain expertise.
 It would be very useful to have an implementation in Python or similar that allows through chatgpt and raising the theme of the course, generate the template to load in moodle, which meets the highest international quality standards, which serves as a basis for the teacher to worry only to load the template and begin to customize the content.
 This would help to accelerate the speed with which knowledge is shared, going one step further.
Additional context
https://docs.moodle.org/403/en/Course_restore
https://www.qualitymatters.org/qa-resources/rubric-standards
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/667","Connection Not Closing Properly When Using GPT API with Stream","2023-11-03T22:24:34Z","Closed issue","bug,fixed in v1","Describe the bug
I am experiencing a connection issue when utilizing OpenAI's GPT API with the streaming feature in my application. When I close the stream using the method I've implemented, the connection to OpenAI does not terminate correctly. After a certain duration, a timeout error is thrown by the discarded thread. The error message I receive is:
CRITICAL - Request timed out: HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read timeout=600)
...
openai.error.Timeout: Request timed out: HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read timeout=600)

To Reproduce
Initiate the GPT API with streaming using the code snippet provided.
def gpt(prompt: str):
    for chunk in openai.ChatCompletion.create(
        model=""gpt-3.5-turbo"",
        messages=[{""role"": 'user', ""content"": prompt}],
        stream=True
    ):
        if (text_chunk := chunk['choices'][0]['delta'].get('content'))  is not None:
            yield text_chunk

In specific scenarios, discard the API response and close the stream.
gpt_stream = write(prompt)
for text_chunk in gpt_stream:
	if discarded:
		gpt_stream.close()
		return;
	... continue

Wait for a certain period.
Observe the aforementioned timeout error.
Code snippets
No response
OS
ubuntu 22.04
Python version
3.9
Library version
0.27.8
 The text was updated successfully, but these errors were encountered: 
👍3
andy2mrqz, rishabhRsinghvi, and ijwfly reacted with thumbs up emoji
All reactions
👍3 reactions"
"https://github.com/openai/openai-python/issues/666","Invalid URL 'gpt-3.5-turbo/fine_tuning/jobs': No scheme supplied. Perhaps you meant https://gpt-3.5-turbo/fine_tuning/jobs?","2023-11-10T03:29:25Z","Closed issue","bug","Describe the bug
When using fine tuning I uploaded a file with success and when creating a job using the resulting job id I get this error:
Error communicating with OpenAI: Invalid URL 'gpt-3.5-turbo/fine_tuning/jobs': No scheme supplied. Perhaps you meant https://gpt-3.5-turbo/fine_tuning/jobs?
To Reproduce
This is the python code I'm running:
response = openai.FineTuningJob.create(training_file_id, model)
for model I'm using ""gpt-3.5-turbo""
I'm using openai library version 0.28.1
Code snippets
No response
OS
linux
Python version
3.11
Library version
0.28.1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/665","Consider switching to httpx as the HTTP client library","2023-11-02T14:58:11Z","Closed issue","fixed in v1","Describe the feature or improvement you're requesting
Currently, this project uses requests and aiohttp to provide synchronous and asynchronous interfaces, respectively. The httpx library, in contrast, is an actively maintained project that supports both with a consistent interface. On top of that, it works with the alternative trio async event loop implementation.
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
👍7
floriandotorg, paolodina, rubber-duck, jamie-certn, Imccccc, ijwfly, and radovanZRasa reacted with thumbs up emoji
All reactions
👍7 reactions"
"https://github.com/openai/openai-python/issues/663","JSONDecodeError when trying to stream chat completion","2023-10-25T20:47:23Z","Closed issue","bug,fixed in v1","Describe the bug
After sending a request to chat completion API using stream=True, sometimes, the api returns a single response with two chunks inside of it. For example:
b'{""id"": ""chatcmpl-8BnB0NvhQxY0szkmjMdM5px67189w"", ""object"": ""chat.completion.chunk"", ""created"": ""2023-10-20T17:02:14+00:00"", ""model"": ""gpt-4"", ""choices"": [{""delta"": {""content"": ""\'s""}, ""index"": 0}]}{""id"": ""chatcmpl-8BnB0NvhQxY0szkmjMdM5px67189w"", ""object"": ""chat.completion.chunk"", ""created"": ""2023-10-20T17:02:14+00:00"", ""model"": ""gpt-4"", ""choices"": [{""delta"": {""content"": "" language""}, ""index"": 0}]}'

My expectation was to get one chunk per response:
b'{""id"": ""chatcmpl-8BnB0NvhQxY0szkmjMdM5px67189w"", ""object"": ""chat.completion.chunk"", ""created"": ""2023-10-20T17:02:14+00:00"", ""model"": ""gpt-4"", ""choices"": [{""delta"": {""content"": ""\'s""}, ""index"": 0}]}'

b'{""id"": ""chatcmpl-8BnB0NvhQxY0szkmjMdM5px67189w"", ""object"": ""chat.completion.chunk"", ""created"": ""2023-10-20T17:02:14+00:00"", ""model"": ""gpt-4"", ""choices"": [{""delta"": {""content"": "" language""}, ""index"": 0}]}'

To Reproduce
Send a request to chat completion with stream=True
Consume each chunk of the stream
Check the contents of each chunk
One of the chunks will have two payloads inside of it
Code snippets
No response
OS
Linux
Python version
3.10.11
Library version
openai v0.27.10
 The text was updated successfully, but these errors were encountered: 
👍1
ercanucan reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/openai-python/issues/660","openai.error.InvalidRequestError: Must provide an 'engine' or 'deployment_id' parameter to create a <class 'openai.api_resources.chat_completion.ChatCompletion'>","2023-11-10T03:39:16Z","Closed issue","bug","Describe the bug
I seem unable to consume my Azure OpenAI API. See the code below.
I'm getting the following error message:
""openai.error.InvalidRequestError: Must provide an 'engine' or 'deployment_id' parameter to create a <class 'openai.api_resources.chat_completion.ChatCompletion'>""
To Reproduce
Deploy a resource on Azure OpenAI Studio
Copy the API keys etc.
Copy&Paste the code snippet
Code snippets
import openaiimport osimport load_dotenv

'''AZURE_OPENAI_RESOURCE = 'AI_Sandbox'AZURE_OPENAI_MODEL = 'gpt-3.5-turbo'AZURE_OPENAI_KEY = ''AZURE_OPENAI_ENDPOINT = 'https://{resource}.openai.azure.com/'AZURE_OPENAI_MODEL_NAME = 'gpt-35-turbo'AZURE_DEPLOYMENT_NAME = 'GPT-35-TURBO''''

# AOAI Integration SettingsAZURE_OPENAI_RESOURCE = os.environ.get(""AZURE_OPENAI_RESOURCE"")
AZURE_OPENAI_MODEL = os.environ.get(""AZURE_OPENAI_MODEL"")
AZURE_OPENAI_MODEL_NAME = os.environ.get(""AZURE_OPENAI_MODEL_NAME"")
AZURE_OPENAI_ENDPOINT = os.environ.get(""AZURE_OPENAI_ENDPOINT"")
AZURE_OPENAI_KEY = os.environ.get(""AZURE_OPENAI_KEY"")
AZURE_DEPLOYMENT_NAME = os.environ.get(""AZURE_DEPLOYMENT_NAME"")

base_url = AZURE_OPENAI_ENDPOINT if AZURE_OPENAI_ENDPOINT else f""https://{AZURE_OPENAI_RESOURCE}.openai.azure.com/""
        
class OpenAIService:
    def __init__(self):
        openai.api_key = AZURE_OPENAI_KEY
        openai.api_type = ""azure""
        openai.api_base = base_url
        openai.api_version = ""2023-03-15-preview""

    def list_models(self):
        return openai.Model.list()

    def prompt(self, prompt):
        return openai.ChatCompletion.create(
            engine=AZURE_OPENAI_MODEL_NAME,
            deployment_id=AZURE_DEPLOYMENT_NAME,
            messages=[{""role"": ""user"", ""content"": prompt}],
            temperature=0,
            timeout=60,
        )
OS
macOS
Python version
Python 3.11
Library version
openai
 The text was updated successfully, but these errors were encountered: 
👍1
magedhelmy1 reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/openai-python/issues/658","Error in fine tune","2023-10-23T01:32:24Z","Closed issue","bug","Describe the bug
when I do !openai tools fine_tunes.prepare_data --file data/prepared_data.csv --quiet, it works successfully.
But when I do !openai api fine_tunes.create --training_file data/prepared_data_prepared.jsonl --model curie --suffix ""ultraman"", It report an error:

Found potentially duplicated files with name 'prepared_data_prepared.jsonl', purpose 'fine-tune' and size 446199 bytes
file-hK8jbDzaub5ORlsodyrgQUo8
Enter file ID to reuse an already uploaded file, or an empty string to upload this file anyway: file-hK8jbDzaub5ORlsodyrgQUo8
Reusing already uploaded file: file-hK8jbDzaub5ORlsodyrgQUo8
Created fine-tune: ft-dUygZsOEXNv50BJuvDHA3vKE
Streaming events until fine-tuning is complete...

(Ctrl-C will interrupt the stream, but not cancel the fine-tune)
[2023-10-17 09:08:41] Created fine-tune: ft-dUygZsOEXNv50BJuvDHA3vKE

Stream interrupted (client disconnected).
To resume the stream, run:

  openai api fine_tunes.follow -i ft-dUygZsOEXNv50BJuvDHA3vKE

then I !openai api fine_tunes.follow -i ft-dUygZsOEXNv50BJuvDHA3vKE,
 it returns
[2023-10-17 09:08:41] Created fine-tune: ft-dUygZsOEXNv50BJuvDHA3vKE
[2023-10-17 09:13:37] Fine-tune costs $3.67
[2023-10-17 09:13:37] Fine-tune enqueued. Queue number: 0

Stream interrupted (client disconnected).
To resume the stream, run:

  openai api fine_tunes.follow -i ft-dUygZsOEXNv50BJuvDHA3vKE

To Reproduce
Stream interrupted (client disconnected).
 To resume the stream, run:
openai api fine_tunes.follow -i ft-dUygZsOEXNv50BJuvDHA3vKE
Code snippets
No response
OS
colab
Python version
Python 3.10
Library version
openai 0.28.1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/657","Update","2023-10-16T19:36:29Z","Closed as not planned issue","No label","Describe the feature or improvement you're requesting
Update your openai pip envarment.
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/656","openai.error.APIError: Invalid response object from API？","2023-11-10T03:28:05Z","Closed issue","bug","Describe the bug
openai.api_base = ""http://localhost:10860/v1/embeddings""

def test_embedding():
    embedding = openai.Embedding.create(model =model,input=""hello"")
    print(embedding)
    print(embedding[""data""][0][""embedding""].shape)
    print(embedding[""data""][0][""embedding""])

To Reproduce
Traceback (most recent call last):
 File ""C:\Users\loong.conda\envs\nlp\lib\site-packages\openai\api_requestor.py"", line 403, in handle_error_response
 error_data = resp[""error""]
 KeyError: 'error'
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
 File ""D:\llm\NLP\aquila_openai_api_vllm.py"", line 54, in 
 test_embedding()
 File ""D:\llm\NLP\aquila_openai_api_vllm.py"", line 25, in test_embedding
 embedding = openai.Embedding.create(model =model,input=""你好"")
 File ""C:\Users\loong.conda\envs\nlp\lib\site-packages\openai\api_resources\embedding.py"", line 33, in create
 response = super().create(*args, **kwargs)
 File ""C:\Users\loong.conda\envs\nlp\lib\site-packages\openai\api_resources\abstract\engine_api_resource.py"", line 153, in create
 response, _, api_key = requestor.request(
 File ""C:\Users\loong.conda\envs\nlp\lib\site-packages\openai\api_requestor.py"", line 298, in request
 resp, got_stream = self._interpret_response(result, stream)
 File ""C:\Users\loong.conda\envs\nlp\lib\site-packages\openai\api_requestor.py"", line 707, in _interpret_response
 self._interpret_response_line(
 File ""C:\Users\loong.conda\envs\nlp\lib\site-packages\openai\api_requestor.py"", line 772, in _interpret_response_line
 raise self.handle_error_response(
 File ""C:\Users\loong.conda\envs\nlp\lib\site-packages\openai\api_requestor.py"", line 405, in handle_error_response
 raise error.APIError(
 openai.error.APIError: Invalid response object from API: '{""detail"":""Not Found""}' (HTTP response code was 404)
Code snippets
No response
OS
centos
Python version
3.10
Library version
0.28.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/655","openai.error.APIError: Invalid response object from API？","2023-10-14T23:44:26Z","Closed as not planned issue","No label","Describe the bug
openai.api_base = ""http://localhost:10860/v1/embeddings""

def test_embedding():
    embedding = openai.Embedding.create(model =model,input=""hello"")
    print(embedding)
    print(embedding[""data""][0][""embedding""].shape)
    print(embedding[""data""][0][""embedding""])

To Reproduce
Traceback (most recent call last):
 File ""C:\Users\loong.conda\envs\nlp\lib\site-packages\openai\api_requestor.py"", line 403, in handle_error_response
 error_data = resp[""error""]
 KeyError: 'error'
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
 File ""D:\llm\NLP\aquila_openai_api_vllm.py"", line 54, in 
 test_embedding()
 File ""D:\llm\NLP\aquila_openai_api_vllm.py"", line 25, in test_embedding
 embedding = openai.Embedding.create(model =model,input=""你好"")
 File ""C:\Users\loong.conda\envs\nlp\lib\site-packages\openai\api_resources\embedding.py"", line 33, in create
 response = super().create(*args, **kwargs)
 File ""C:\Users\loong.conda\envs\nlp\lib\site-packages\openai\api_resources\abstract\engine_api_resource.py"", line 153, in create
 response, _, api_key = requestor.request(
 File ""C:\Users\loong.conda\envs\nlp\lib\site-packages\openai\api_requestor.py"", line 298, in request
 resp, got_stream = self._interpret_response(result, stream)
 File ""C:\Users\loong.conda\envs\nlp\lib\site-packages\openai\api_requestor.py"", line 707, in _interpret_response
 self._interpret_response_line(
 File ""C:\Users\loong.conda\envs\nlp\lib\site-packages\openai\api_requestor.py"", line 772, in _interpret_response_line
 raise self.handle_error_response(
 File ""C:\Users\loong.conda\envs\nlp\lib\site-packages\openai\api_requestor.py"", line 405, in handle_error_response
 raise error.APIError(
 openai.error.APIError: Invalid response object from API: '{""detail"":""Not Found""}' (HTTP response code was 404)
Code snippets
No response
OS
centos
Python version
3.10
Library version
0.28.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/654","Unable to use Python 3.12 due to aiohttp dependency unable to build wheel.","2023-10-14T23:42:13Z","Closed as not planned issue","fixed in v1","Describe the bug
I am sure that aiohttp will catch up, but meanwhile, I figure I would through this on your radar for potential work around.
To Reproduce
Running Python 3.12.0
 pip install openai
Code snippets
No response
OS
any
Python version
3.12.0
Library version
current
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/652","v1.0.0b2: Types not working correctly when Literals not used for stream parameter for completions.create","2023-10-17T16:19:23Z","Closed issue","bug","Describe the bug
When stream is a bool variable, typing does not select the correct overloaded function implementation in client.chat.completions.create. This results in the following error:
Argument of type ""bool"" cannot be assigned to parameter ""stream"" of type ""Literal[True]"" in function ""create""
  ""bool"" cannot be assigned to type ""Literal[True]""Pylance[reportGeneralTypeIssues](https://github.com/microsoft/pyright/blob/main/docs/configuration.md#reportGeneralTypeIssues)

Passing in a Literal works as expected, and so does 'unpacking' the dynamic value into Literals.
It appears that typing does not select the overloaded function correctly that contains the relevant types (line 624 in Async Completions):
stream: Optional[Literal[False]] | Literal[True] | NotGiven = NOT_GIVEN,

To Reproduce
The following code doesn't work:
client = AsyncOpenAI()
stream = True if 0 == 0 else False

await client.chat.completions.create(
    model=request.model,
    messages=[
        {
            ""role"": ""user"",
            ""content"": """",
        },
    ],
    stream=stream,
)

This code works correctly:
client = AsyncOpenAI()
stream = True if 0 == 0 else False
if (stream == True):
    await client.chat.completions.create(
        model=request.model,
        messages=[
            {
                ""role"": ""user"",
                ""content"": """",
            },
        ],
        stream=stream,
    )
else:
    await client.chat.completions.create(
        model=request.model,
        messages=[
            {
                ""role"": ""user"",
                ""content"": """",
            },
        ],
        stream=stream,
    )

Code snippets
No response
OS
macOS
Python version
3.9.13
Library version
1.0.0b2
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/650","JSONDecodeError when streaming chunks for ChatCompletion","2023-11-10T03:36:11Z","Closed issue","bug,fixed in v1","Describe the bug
openai.ChatCompletion.acreate(..., stream=True) seems to fail with a JSONDecodeError when it's called such that:
the prompt alone doesn't exceed the rate limit
the generated output + prompt together exceeds the rate limit
Example:
Suppose my rate limit is 10000 tokens/min, and my prompt is 8000 tokens.
If the generated output is 2000+ tokens, then I hit my rate limit before my response is finished streaming
The first 2000 tokens stream successfully, but after I hit my rate limit, the OpenAI server streams a rate limit exception message to me
This rate limit exception message is parsed incorrectly, triggering JSONDecodeError
I believe this is parsed incorrectly because the OpenAI Python library parses each line in the streamed output separately and assumes each line is JSON. However, the rate limit exception message is pretty-printed JSON and includes internal newlines, so this call to JSON.loads fails.
This is the line in the streamed output that triggered the error: ""{\""rate_limit_usage\"": {\
Error below:
HTTP code 200 from API (""{\""rate_limit_usage\"": {\)
Traceback (most recent call last):
  File "".venv/lib/python3.10/site-packages/openai/api_requestor.py"", line 765, in _interpret_response_line
    data = json.loads(rbody)
  File ""/Users/xxx/.pyenv/versions/3.10.11/lib/python3.10/json/__init__.py"", line 346, in loads
    return _default_decoder.decode(s)
  File ""/Users/xxx/.pyenv/versions/3.10.11/lib/python3.10/json/decoder.py"", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File ""/Users/xxx/.pyenv/versions/3.10.11/lib/python3.10/json/decoder.py"", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
json.decoder.JSONDecodeError: Unterminated string starting at: line 1 column 1 (char 0)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  ...
  File "".venv/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py"", line 232, in <genexpr>
    return (
  File "".venv/lib/python3.10/site-packages/openai/api_requestor.py"", line 396, in wrap_resp
    async for r in resp:
  File "".venv/lib/python3.10/site-packages/openai/api_requestor.py"", line 725, in <genexpr>
    self._interpret_response_line(
  File "".venv/lib/python3.10/site-packages/openai/api_requestor.py"", line 767, in _interpret_response_line
    raise error.APIError(
openai.error.APIError: HTTP code 200 from API (""{\""rate_limit_usage\"": {\)

To Reproduce
Call openai.ChatCompletion.acreate(..., stream=True) with a prompt that doesn't exceed the rate limit, but whose generated output + prompt exceeds the rate limit.
Code snippets
No response
OS
macOS
Python version
Python v3.10.11
Library version
openai-python v0.28.1
 The text was updated successfully, but these errors were encountered: 
👍5
andrew-christianson, qxcv, vinvcn, codeslaw, and karfly reacted with thumbs up emoji
All reactions
👍5 reactions"
"https://github.com/openai/openai-python/issues/649","Streaming chunk generator: incomplete JSON","2023-10-13T18:55:14Z","Closed issue","bug","Describe the bug
When iterating through chunks in a chat completion stream response, the generator is crashing due to an incomplete JSON expression:
File ""/data/data/com.termux/files/usr/lib/python3.11/site-packages/openai/api_requestor.py"", line 765, in _interpret_response_line
 data = json.loads(rbody)
 ^^^^^^^^^^^^^^^^^
(Pdb) p rbody
 '""{\""rate_limit_usage\"": {\'
It looks like it should potentially collect more responses and concatenate them before trying to do a json.loads decode. Or alternatively the server should provide the entire JSON object in the same response. This issue appears to have come into existence today as I had no issues yesterday.
To Reproduce
import openai, os
 openai.api_key = os.environ['OPENAI_API_KEY']
params = {'model': 'gpt-4', 'messages': [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': 'tell me three arbitrary words'}], 'stream': True}
response = openai.ChatCompletion.create(**params)
for chunk in response:
 print(chunk)
Code snippets
import openai, osopenai.api_key = os.environ['OPENAI_API_KEY']

params = {'model': 'gpt-4', 'messages': [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': 'tell me three arbitrary words'}], 'stream': True}

response = openai.ChatCompletion.create(**params)

for chunk in response:
    print(chunk)
OS
android/termux
Python version
python 3.11.5
Library version
openai 0.28.1
 The text was updated successfully, but these errors were encountered: 
👍2
dayunet2009 and qxcv reacted with thumbs up emoji
All reactions
👍2 reactions"
"https://github.com/openai/openai-python/issues/648","[REPOST of #455 (with corrections to grammar, behavior/attitude and more recent errors) - unfairly/unreasonably closed] openai.error.InvalidRequestError: Invalid URL (POST /v1/chat/engines/gpt-3.5-turbo-0613/chat/completions)","2023-10-12T08:22:59Z","Closed issue","fixed in v1","Describe the bug
The module doesn't work! My issue got closed with absolutely NO WARNING/REASON too. :(
Error:
Traceback (most recent call last):
  File ""C:\Users\SonicandTailsCD\Desktop\openai_script.py"", line 14, in <module>

    response = openai.ChatCompletion.create(
  File ""C:\Program Files\Python38\lib\site-packages\openai\api_resources\chat_co
mpletion.py"", line 25, in create
    return super().create(*args, **kwargs)
  File ""C:\Program Files\Python38\lib\site-packages\openai\api_resources\abstrac
t\engine_api_resource.py"", line 153, in create
    response, _, api_key = requestor.request(
  File ""C:\Program Files\Python38\lib\site-packages\openai\api_requestor.py"", li
ne 230, in request
    resp, got_stream = self._interpret_response(result, stream)
  File ""C:\Program Files\Python38\lib\site-packages\openai\api_requestor.py"", li
ne 624, in _interpret_response
    self._interpret_response_line(
  File ""C:\Program Files\Python38\lib\site-packages\openai\api_requestor.py"", li
ne 687, in _interpret_response_line
    raise self.handle_error_response(
openai.error.InvalidRequestError: Invalid URL (POST /v1/chat/engines/gpt-3.5-tur
bo-0613/chat/completions)
To Reproduce
Simply create a new openai script. Then run it.
Code snippets
Sure.
import openai

openai.organization = *censored*openai.api_key = *censored*

messages = [
    {
        ""role"": ""system"",
        ""content"": ""Hi, ChatGPT! So just to test something, say \""This is a test!\"" please :)""
    }
]

response = openai.ChatCompletion.create(
    engine=""gpt-3.5-turbo"",
    messages=messages,
    max_tokens=2000,
    temperature=0.7,
    top_p=1,
    n=1
)

print(response)
OS
Windows 7 x64 (Multiboot)
Python version
Python v3.8.10
Library version
openai-python 27.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/646","Describe the bug report that this github project has cracked","2023-10-10T15:06:07Z","Closed issue","bug","Describe the bug
Describe the bug
 report that this github project has cracked the web request protocol https://github.com/zhile-io/pandora
To Reproduce
 report that this github project has cracked the web request protocol https://github.com/zhile-io/pandora
Code snippets
 report that this github project has cracked the web request protocol https://github.com/zhile-io/pandora
To Reproduce
Describe the bug
 report that this github project has cracked the web request protocol https://github.com/zhile-io/pandora
To Reproduce
 report that this github project has cracked the web request protocol https://github.com/zhile-io/pandora
Code snippets
 report that this github project has cracked the web request protocol https://github.com/zhile-io/pandora
Code snippets
No response
OS
mac
Python version
3.1
Library version
0.265
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/645","Cannot install openai because ""Failed building wheel for aiohttp""","2023-10-11T11:11:13Z","Closed as not planned issue","fixed in v1","Describe the bug
I yesterday freshly install Python, and now i want to install openai. The installation always fails at building the wheel for aiohttp. Im not a big programmer, so i dont know what is wrong here (I did remove my user name for this post)
To Reproduce
Install Python (newest version)
Install openai
Expected: Succesfully Installing openai
Code snippets
pip3.12 install --upgrade openaiCollecting openai
  Obtaining dependency information for openai from https://files.pythonhosted.org/packages/1e/9f/385c25502f437686e4aa715969e5eaf5c2cb5e5ffa7c5cdd52f3c6ae967a/openai-0.28.1-py3-none-any.whl.metadata
  Using cached openai-0.28.1-py3-none-any.whl.metadata (11 kB)
Requirement already satisfied: requests>=2.20 in c:\users\\appdata\local\programs\python\python312\lib\site-packages (from openai) (2.31.0)
Collecting tqdm (from openai)
  Obtaining dependency information for tqdm from https://files.pythonhosted.org/packages/00/e5/f12a80907d0884e6dff9c16d0c0114d81b8cd07dc3ae54c5e962cc83037e/tqdm-4.66.1-py3-none-any.whl.metadata
  Using cached tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)
Collecting aiohttp (from openai)
  Using cached aiohttp-3.8.6.tar.gz (7.4 MB)
  Installing build dependencies ... done
  Getting requirements to build wheel ... done
  Installing backend dependencies ... done
  Preparing metadata (pyproject.toml) ... doneRequirement already satisfied: charset-normalizer<4,>=2 in c:\users\\appdata\local\programs\python\python312\lib\site-packages (from requests>=2.20->openai) (3.3.0)
Requirement already satisfied: idna<4,>=2.5 in c:\users\\appdata\local\programs\python\python312\lib\site-packages (from requests>=2.20->openai) (3.4)
Requirement already satisfied: urllib3<3,>=1.21.1 in c:\users\\appdata\local\programs\python\python312\lib\site-packages (from requests>=2.20->openai) (2.0.6)
Requirement already satisfied: certifi>=2017.4.17 in c:\users\\appdata\local\programs\python\python312\lib\site-packages (from requests>=2.20->openai) (2023.7.22)
Collecting attrs>=17.3.0 (from aiohttp->openai)
  Using cached attrs-23.1.0-py3-none-any.whl (61 kB)
Collecting multidict<7.0,>=4.5 (from aiohttp->openai)
  Using cached multidict-6.0.4-cp312-cp312-win_amd64.whlCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->openai)
  Obtaining dependency information for async-timeout<5.0,>=4.0.0a3 from https://files.pythonhosted.org/packages/a7/fa/e01228c2938de91d47b307831c62ab9e4001e747789d0b05baf779a6488c/async_timeout-4.0.3-py3-none-any.whl.metadata 
  Using cached async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)
Collecting yarl<2.0,>=1.0 (from aiohttp->openai)
  Using cached yarl-1.9.2-cp312-cp312-win_amd64.whlCollecting frozenlist>=1.1.1 (from aiohttp->openai)
  Using cached frozenlist-1.4.0-cp312-cp312-win_amd64.whlCollecting aiosignal>=1.1.2 (from aiohttp->openai)
  Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)
Requirement already satisfied: colorama in c:\users\\appdata\local\programs\python\python312\lib\site-packages (from tqdm->openai) (0.4.6)
Using cached openai-0.28.1-py3-none-any.whl (76 kB)
Using cached tqdm-4.66.1-py3-none-any.whl (78 kB)
Using cached async_timeout-4.0.3-py3-none-any.whl (5.7 kB)
Building wheels for collected packages: aiohttp
  Building wheel for aiohttp (pyproject.toml) ... error
  error: subprocess-exited-with-error

  × Building wheel for aiohttp (pyproject.toml) did not run successfully.
  │ exit code: 1
  ╰─> [110 lines of output]
      *********************
      * Accelerated build *
      *********************
      running bdist_wheel
      running build
      running build_py
      creating build
      creating build\lib.win-amd64-cpython-312
      creating build\lib.win-amd64-cpython-312\aiohttp
      copying aiohttp\abc.py -> build\lib.win-amd64-cpython-312\aiohttp
      copying aiohttp\base_protocol.py -> build\lib.win-amd64-cpython-312\aiohttp
      copying aiohttp\client.py -> build\lib.win-amd64-cpython-312\aiohttp
      copying aiohttp\client_exceptions.py -> build\lib.win-amd64-cpython-312\aiohttp
      copying aiohttp\client_proto.py -> build\lib.win-amd64-cpython-312\aiohttp
      copying aiohttp\client_reqrep.py -> build\lib.win-amd64-cpython-312\aiohttp
      copying aiohttp\client_ws.py -> build\lib.win-amd64-cpython-312\aiohttp
      copying aiohttp\connector.py -> build\lib.win-amd64-cpython-312\aiohttp
      copying aiohttp\cookiejar.py -> build\lib.win-amd64-cpython-312\aiohttp
      copying aiohttp\formdata.py -> build\lib.win-amd64-cpython-312\aiohttp
      copying aiohttp\hdrs.py -> build\lib.win-amd64-cpython-312\aiohttp
      copying aiohttp\helpers.py -> build\lib.win-amd64-cpython-312\aiohttp
      copying aiohttp\http.py -> build\lib.win-amd64-cpython-312\aiohttp
      copying aiohttp\http_exceptions.py -> build\lib.win-amd64-cpython-312\aiohttp
      copying aiohttp\http_parser.py -> build\lib.win-amd64-cpython-312\aiohttp
      copying aiohttp\http_websocket.py -> build\lib.win-amd64-cpython-312\aiohttp
      copying aiohttp\http_writer.py -> build\lib.win-amd64-cpython-312\aiohttp
      copying aiohttp\locks.py -> build\lib.win-amd64-cpython-312\aiohttp
      copying aiohttp\log.py -> build\lib.win-amd64-cpython-312\aiohttp
      copying aiohttp\multipart.py -> build\lib.win-amd64-cpython-312\aiohttp
      copying aiohttp\payload.py -> build\lib.win-amd64-cpython-312\aiohttp
      copying aiohttp\payload_streamer.py -> build\lib.win-amd64-cpython-312\aiohttp
      copying aiohttp\pytest_plugin.py -> build\lib.win-amd64-cpython-312\aiohttp
      copying aiohttp\resolver.py -> build\lib.win-amd64-cpython-312\aiohttp
      copying aiohttp\streams.py -> build\lib.win-amd64-cpython-312\aiohttp
      copying aiohttp\tcp_helpers.py -> build\lib.win-amd64-cpython-312\aiohttp
      copying aiohttp\test_utils.py -> build\lib.win-amd64-cpython-312\aiohttp
      copying aiohttp\tracing.py -> build\lib.win-amd64-cpython-312\aiohttp
      copying aiohttp\typedefs.py -> build\lib.win-amd64-cpython-312\aiohttp
      copying aiohttp\web.py -> build\lib.win-amd64-cpython-312\aiohttp
      copying aiohttp\web_app.py -> build\lib.win-amd64-cpython-312\aiohttp
      copying aiohttp\web_exceptions.py -> build\lib.win-amd64-cpython-312\aiohttp
      copying aiohttp\web_fileresponse.py -> build\lib.win-amd64-cpython-312\aiohttp
      copying aiohttp\web_log.py -> build\lib.win-amd64-cpython-312\aiohttp
      copying aiohttp\web_middlewares.py -> build\lib.win-amd64-cpython-312\aiohttp
      copying aiohttp\web_protocol.py -> build\lib.win-amd64-cpython-312\aiohttp
      copying aiohttp\web_request.py -> build\lib.win-amd64-cpython-312\aiohttp
      copying aiohttp\web_response.py -> build\lib.win-amd64-cpython-312\aiohttp
      copying aiohttp\web_routedef.py -> build\lib.win-amd64-cpython-312\aiohttp
      copying aiohttp\web_runner.py -> build\lib.win-amd64-cpython-312\aiohttp
      copying aiohttp\web_server.py -> build\lib.win-amd64-cpython-312\aiohttp
      copying aiohttp\web_urldispatcher.py -> build\lib.win-amd64-cpython-312\aiohttp
      copying aiohttp\web_ws.py -> build\lib.win-amd64-cpython-312\aiohttp
      copying aiohttp\worker.py -> build\lib.win-amd64-cpython-312\aiohttp
      copying aiohttp\__init__.py -> build\lib.win-amd64-cpython-312\aiohttp
      running egg_info
      writing aiohttp.egg-info\PKG-INFO
      writing dependency_links to aiohttp.egg-info\dependency_links.txt
      writing requirements to aiohttp.egg-info\requires.txt
      writing top-level names to aiohttp.egg-info\top_level.txt
      reading manifest file 'aiohttp.egg-info\SOURCES.txt'
      reading manifest template 'MANIFEST.in'
      warning: no files found matching 'aiohttp' anywhere in distribution
      warning: no previously-included files matching '*.pyc' found anywhere in distribution
      warning: no previously-included files matching '*.pyd' found anywhere in distribution
      warning: no previously-included files matching '*.so' found anywhere in distribution
      warning: no previously-included files matching '*.lib' found anywhere in distribution
      warning: no previously-included files matching '*.dll' found anywhere in distribution
      warning: no previously-included files matching '*.a' found anywhere in distribution
      warning: no previously-included files matching '*.obj' found anywhere in distribution
      warning: no previously-included files found matching 'aiohttp\*.html'
      no previously-included directories found matching 'docs\_build'
      adding license file 'LICENSE.txt'
      writing manifest file 'aiohttp.egg-info\SOURCES.txt'
      copying aiohttp\_cparser.pxd -> build\lib.win-amd64-cpython-312\aiohttp
      copying aiohttp\_find_header.pxd -> build\lib.win-amd64-cpython-312\aiohttp
      copying aiohttp\_headers.pxi -> build\lib.win-amd64-cpython-312\aiohttp
      copying aiohttp\_helpers.pyi -> build\lib.win-amd64-cpython-312\aiohttp
      copying aiohttp\_helpers.pyx -> build\lib.win-amd64-cpython-312\aiohttp
      copying aiohttp\_http_parser.pyx -> build\lib.win-amd64-cpython-312\aiohttp
      copying aiohttp\_http_writer.pyx -> build\lib.win-amd64-cpython-312\aiohttp
      copying aiohttp\_websocket.pyx -> build\lib.win-amd64-cpython-312\aiohttp
      copying aiohttp\py.typed -> build\lib.win-amd64-cpython-312\aiohttp
      creating build\lib.win-amd64-cpython-312\aiohttp\.hash
      copying aiohttp\.hash\_cparser.pxd.hash -> build\lib.win-amd64-cpython-312\aiohttp\.hash
      copying aiohttp\.hash\_find_header.pxd.hash -> build\lib.win-amd64-cpython-312\aiohttp\.hash
      copying aiohttp\.hash\_helpers.pyi.hash -> build\lib.win-amd64-cpython-312\aiohttp\.hash
      copying aiohttp\.hash\_helpers.pyx.hash -> build\lib.win-amd64-cpython-312\aiohttp\.hash
      copying aiohttp\.hash\_http_parser.pyx.hash -> build\lib.win-amd64-cpython-312\aiohttp\.hash
      copying aiohttp\.hash\_http_writer.pyx.hash -> build\lib.win-amd64-cpython-312\aiohttp\.hash
      copying aiohttp\.hash\_websocket.pyx.hash -> build\lib.win-amd64-cpython-312\aiohttp\.hash
      copying aiohttp\.hash\hdrs.py.hash -> build\lib.win-amd64-cpython-312\aiohttp\.hash
      running build_ext
      building 'aiohttp._websocket' extension
      creating build\temp.win-amd64-cpython-312
      creating build\temp.win-amd64-cpython-312\Release
      creating build\temp.win-amd64-cpython-312\Release\aiohttp
      ""C:\Program Files (x86)\Microsoft Visual Studio\2022\BuildTools\VC\Tools\MSVC\14.37.32822\bin\HostX86\x64\cl.exe"" /c /nologo /O2 /W3 /GL /DNDEBUG /MD -IC:\Users\\AppData\Local\Programs\Python\Python312\include -IC:\Users\\AppData\Local\Programs\Python\Python312\Include ""-IC:\Program Files (x86)\Microsoft Visual Studio\2022\BuildTools\VC\Tools\MSVC\14.37.32822\include"" ""-IC:\Program Files (x86)\Microsoft Visual Studio\2022\BuildTools\VC\Auxiliary\VS\include"" ""-IC:\Program Files (x86)\Windows Kits\10\include\10.0.22621.0\ucrt"" ""-IC:\Program Files (x86)\Windows Kits\10\\include\10.0.22621.0\\um"" ""-IC:\Program Files (x86)\Windows Kits\10\\include\10.0.22621.0\\shared"" ""-IC:\Program Files (x86)\Windows Kits\10\\include\10.0.22621.0\\winrt"" ""-IC:\Program Files (x86)\Windows Kits\10\\include\10.0.22621.0\\cppwinrt"" /Tcaiohttp/_websocket.c /Fobuild\temp.win-amd64-cpython-312\Release\aiohttp/_websocket.obj
      _websocket.c
      aiohttp/_websocket.c(1475): warning C4996: 'Py_OptimizeFlag': deprecated in 3.12
      aiohttp/_websocket.c(3042): error C2039: 'ob_digit': is not a member of '_longobject'
      C:\Users\\AppData\Local\Programs\Python\Python312\include\cpython/longintrepr.h(87): note: see declaration of '_longobject'
      aiohttp/_websocket.c(3097): error C2039: 'ob_digit': is not a member of '_longobject'
      C:\Users\\AppData\Local\Programs\Python\Python312\include\cpython/longintrepr.h(87): note: see declaration of '_longobject'
      aiohttp/_websocket.c(3238): error C2039: 'ob_digit': is not a member of '_longobject'
      C:\Users\\AppData\Local\Programs\Python\Python312\include\cpython/longintrepr.h(87): note: see declaration of '_longobject'
      aiohttp/_websocket.c(3293): error C2039: 'ob_digit': is not a member of '_longobject'
      C:\Users\\AppData\Local\Programs\Python\Python312\include\cpython/longintrepr.h(87): note: see declaration of '_longobject'
      aiohttp/_websocket.c(3744): error C2039: 'ob_digit': is not a member of '_longobject'
      C:\Users\\AppData\Local\Programs\Python\Python312\include\cpython/longintrepr.h(87): note: see declaration of '_longobject'
      error: command 'C:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Tools\\MSVC\\14.37.32822\\bin\\HostX86\\x64\\cl.exe' failed with exit code 2
      [end of output]

  note: This error originates from a subprocess, and is likely not a problem with pip.
  ERROR: Failed building wheel for aiohttpFailed to build aiohttpERROR: Could not build wheels for aiohttp, which is required to install pyproject.toml-based projects
OS
Windows 11
Python version
Python v3.12
Library version
Newest
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/644","report that this github project has cracked the web request protocol","2023-10-18T10:13:13Z","Closed as not planned issue","invalid","Describe the bug
report that this github project has cracked the web request protocol https://github.com/zhile-io/pandora
To Reproduce
report that this github project has cracked the web request protocol https://github.com/zhile-io/pandora
Code snippets
report that this github project has cracked the web request protocol https://github.com/zhile-io/pandora
OS
1
Python version
1
Library version
1
 The text was updated successfully, but these errors were encountered: 
👎2
bweber-rebellion and dgellow reacted with thumbs down emoji
All reactions
👎2 reactions"
"https://github.com/openai/openai-python/issues/640","ERROR: Failed building wheel for aiohttp","2023-10-07T19:48:38Z","Closed issue","bug,fixed in v1","Describe the bug
I got this error ""ERROR: Failed building wheel for aiohttp"" when I tried to do a pip3 install open ai.
 macOs Sonoma
 MacBook Pro M2 Max.
To Reproduce
pip3 install openai.
Code snippets
Building wheels for collected packages: aiohttp
  Building wheel for aiohttp (pyproject.toml) ... error
  error: subprocess-exited-with-error

  × Building wheel for aiohttp (pyproject.toml) did not run successfully.
  │ exit code: 1
  ╰─> [188 lines of output]
      *********************
      * Accelerated build *
      *********************
      running bdist_wheel
      running build
      running build_py
      creating build
      creating build/lib.macosx-10.9-universal2-cpython-312
      creating build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aiohttp/web_ws.py -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aiohttp/worker.py -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aiohttp/multipart.py -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aiohttp/web_response.py -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aiohttp/client_ws.py -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aiohttp/test_utils.py -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aiohttp/tracing.py -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aiohttp/web_exceptions.py -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aiohttp/web_middlewares.py -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aiohttp/web.py -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aiohttp/http_exceptions.py -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aiohttp/web_app.py -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aiohttp/streams.py -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aiohttp/web_protocol.py -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aiohttp/log.py -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aiohttp/client.py -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aiohttp/web_urldispatcher.py -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aiohttp/web_request.py -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aiohttp/http_websocket.py -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aiohttp/client_proto.py -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aiohttp/locks.py -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aiohttp/__init__.py -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aiohttp/web_runner.py -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aiohttp/web_server.py -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aiohttp/base_protocol.py -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aiohttp/payload.py -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aiohttp/client_reqrep.py -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aiohttp/http.py -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aiohttp/web_log.py -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aiohttp/resolver.py -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aiohttp/formdata.py -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aiohttp/payload_streamer.py -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aiohttp/web_routedef.py -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aiohttp/connector.py -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aiohttp/client_exceptions.py -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aiohttp/typedefs.py -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aiohttp/hdrs.py -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aiohttp/web_fileresponse.py -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aiohttp/http_writer.py -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aiohttp/tcp_helpers.py -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aiohttp/helpers.py -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aiohttp/http_parser.py -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aiohttp/cookiejar.py -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aiohttp/pytest_plugin.py -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aiohttp/abc.py -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      running egg_info
      writing aiohttp.egg-info/PKG-INFO
      writing dependency_links to aiohttp.egg-info/dependency_links.txt
      writing requirements to aiohttp.egg-info/requires.txt
      writing top-level names to aiohttp.egg-info/top_level.txt
      reading manifest file 'aiohttp.egg-info/SOURCES.txt'
      reading manifest template 'MANIFEST.in'
      warning: no files found matching 'aiohttp' anywhere in distribution
      warning: no previously-included files matching '*.pyc' found anywhere in distribution
      warning: no previously-included files matching '*.pyd' found anywhere in distribution
      warning: no previously-included files matching '*.so' found anywhere in distribution
      warning: no previously-included files matching '*.lib' found anywhere in distribution
      warning: no previously-included files matching '*.dll' found anywhere in distribution
      warning: no previously-included files matching '*.a' found anywhere in distribution
      warning: no previously-included files matching '*.obj' found anywhere in distribution
      warning: no previously-included files found matching 'aiohttp/*.html'
      no previously-included directories found matching 'docs/_build'
      adding license file 'LICENSE.txt'
      writing manifest file 'aiohttp.egg-info/SOURCES.txt'
      copying aiohttp/_cparser.pxd -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aiohttp/_find_header.pxd -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aiohttp/_headers.pxi -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aiohttp/_helpers.pyi -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aiohttp/_helpers.pyx -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aiohttp/_http_parser.pyx -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aiohttp/_http_writer.pyx -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aiohttp/_websocket.pyx -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aiohttp/py.typed -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      creating build/lib.macosx-10.9-universal2-cpython-312/aiohttp/.hash
      copying aiohttp/.hash/_cparser.pxd.hash -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp/.hash
      copying aiohttp/.hash/_find_header.pxd.hash -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp/.hash
      copying aiohttp/.hash/_helpers.pyi.hash -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp/.hash
      copying aiohttp/.hash/_helpers.pyx.hash -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp/.hash
      copying aiohttp/.hash/_http_parser.pyx.hash -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp/.hash
      copying aiohttp/.hash/_http_writer.pyx.hash -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp/.hash
      copying aiohttp/.hash/_websocket.pyx.hash -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp/.hash
      copying aiohttp/.hash/hdrs.py.hash -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp/.hash
      running build_ext
      building 'aiohttp._websocket' extension
      creating build/temp.macosx-10.9-universal2-cpython-312
      creating build/temp.macosx-10.9-universal2-cpython-312/aiohttp
      clang -fno-strict-overflow -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -O3 -Wall -arch arm64 -arch x86_64 -g -I/Library/Frameworks/Python.framework/Versions/3.12/include/python3.12 -c aiohttp/_websocket.c -o build/temp.macosx-10.9-universal2-cpython-312/aiohttp/_websocket.o
      aiohttp/_websocket.c:1475:17: warning: 'Py_OptimizeFlag' is deprecated [-Wdeprecated-declarations]
        if (unlikely(!Py_OptimizeFlag)) {
                      ^
      /Library/Frameworks/Python.framework/Versions/3.12/include/python3.12/cpython/pydebug.h:13:1: note: 'Py_OptimizeFlag' has been explicitly marked deprecated here
      Py_DEPRECATED(3.12) PyAPI_DATA(int) Py_OptimizeFlag;
      ^
      /Library/Frameworks/Python.framework/Versions/3.12/include/python3.12/pyport.h:317:54: note: expanded from macro 'Py_DEPRECATED'
      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))
                                                           ^
      aiohttp/_websocket.c:2680:27: warning: 'ma_version_tag' is deprecated [-Wdeprecated-declarations]
          return likely(dict) ? __PYX_GET_DICT_VERSION(dict) : 0;
                                ^
      aiohttp/_websocket.c:1118:65: note: expanded from macro '__PYX_GET_DICT_VERSION'
      #define __PYX_GET_DICT_VERSION(dict)  (((PyDictObject*)(dict))->ma_version_tag)
                                                                      ^
      /Library/Frameworks/Python.framework/Versions/3.12/include/python3.12/cpython/dictobject.h:22:5: note: 'ma_version_tag' has been explicitly marked deprecated here
          Py_DEPRECATED(3.12) uint64_t ma_version_tag;
          ^
      /Library/Frameworks/Python.framework/Versions/3.12/include/python3.12/pyport.h:317:54: note: expanded from macro 'Py_DEPRECATED'
      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))
                                                           ^
      aiohttp/_websocket.c:2692:36: warning: 'ma_version_tag' is deprecated [-Wdeprecated-declarations]
          return (dictptr && *dictptr) ? __PYX_GET_DICT_VERSION(*dictptr) : 0;
                                         ^
      aiohttp/_websocket.c:1118:65: note: expanded from macro '__PYX_GET_DICT_VERSION'
      #define __PYX_GET_DICT_VERSION(dict)  (((PyDictObject*)(dict))->ma_version_tag)
                                                                      ^
      /Library/Frameworks/Python.framework/Versions/3.12/include/python3.12/cpython/dictobject.h:22:5: note: 'ma_version_tag' has been explicitly marked deprecated here
          Py_DEPRECATED(3.12) uint64_t ma_version_tag;
          ^
      /Library/Frameworks/Python.framework/Versions/3.12/include/python3.12/pyport.h:317:54: note: expanded from macro 'Py_DEPRECATED'
      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))
                                                           ^
      aiohttp/_websocket.c:2696:56: warning: 'ma_version_tag' is deprecated [-Wdeprecated-declarations]
          if (unlikely(!dict) || unlikely(tp_dict_version != __PYX_GET_DICT_VERSION(dict)))
                                                             ^
      aiohttp/_websocket.c:1118:65: note: expanded from macro '__PYX_GET_DICT_VERSION'
      #define __PYX_GET_DICT_VERSION(dict)  (((PyDictObject*)(dict))->ma_version_tag)
                                                                      ^
      /Library/Frameworks/Python.framework/Versions/3.12/include/python3.12/cpython/dictobject.h:22:5: note: 'ma_version_tag' has been explicitly marked deprecated here
          Py_DEPRECATED(3.12) uint64_t ma_version_tag;
          ^
      /Library/Frameworks/Python.framework/Versions/3.12/include/python3.12/pyport.h:317:54: note: expanded from macro 'Py_DEPRECATED'
      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))
                                                           ^
      aiohttp/_websocket.c:2741:9: warning: 'ma_version_tag' is deprecated [-Wdeprecated-declarations]
              __PYX_PY_DICT_LOOKUP_IF_MODIFIED(
              ^
      aiohttp/_websocket.c:1125:16: note: expanded from macro '__PYX_PY_DICT_LOOKUP_IF_MODIFIED'
          if (likely(__PYX_GET_DICT_VERSION(DICT) == __pyx_dict_version)) {\
                     ^
      aiohttp/_websocket.c:1118:65: note: expanded from macro '__PYX_GET_DICT_VERSION'
      #define __PYX_GET_DICT_VERSION(dict)  (((PyDictObject*)(dict))->ma_version_tag)
                                                                      ^
      /Library/Frameworks/Python.framework/Versions/3.12/include/python3.12/cpython/dictobject.h:22:5: note: 'ma_version_tag' has been explicitly marked deprecated here
          Py_DEPRECATED(3.12) uint64_t ma_version_tag;
          ^
      /Library/Frameworks/Python.framework/Versions/3.12/include/python3.12/pyport.h:317:54: note: expanded from macro 'Py_DEPRECATED'
      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))
                                                           ^
      aiohttp/_websocket.c:2741:9: warning: 'ma_version_tag' is deprecated [-Wdeprecated-declarations]
              __PYX_PY_DICT_LOOKUP_IF_MODIFIED(
              ^
      aiohttp/_websocket.c:1129:30: note: expanded from macro '__PYX_PY_DICT_LOOKUP_IF_MODIFIED'
              __pyx_dict_version = __PYX_GET_DICT_VERSION(DICT);\
                                   ^
      aiohttp/_websocket.c:1118:65: note: expanded from macro '__PYX_GET_DICT_VERSION'
      #define __PYX_GET_DICT_VERSION(dict)  (((PyDictObject*)(dict))->ma_version_tag)
                                                                      ^
      /Library/Frameworks/Python.framework/Versions/3.12/include/python3.12/cpython/dictobject.h:22:5: note: 'ma_version_tag' has been explicitly marked deprecated here
          Py_DEPRECATED(3.12) uint64_t ma_version_tag;
          ^
      /Library/Frameworks/Python.framework/Versions/3.12/include/python3.12/pyport.h:317:54: note: expanded from macro 'Py_DEPRECATED'
      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))
                                                           ^
      aiohttp/_websocket.c:3042:55: error: no member named 'ob_digit' in 'struct _longobject'
                  const digit* digits = ((PyLongObject*)x)->ob_digit;
                                        ~~~~~~~~~~~~~~~~~~  ^
      aiohttp/_websocket.c:3097:55: error: no member named 'ob_digit' in 'struct _longobject'
                  const digit* digits = ((PyLongObject*)x)->ob_digit;
                                        ~~~~~~~~~~~~~~~~~~  ^
      aiohttp/_websocket.c:3238:55: error: no member named 'ob_digit' in 'struct _longobject'
                  const digit* digits = ((PyLongObject*)x)->ob_digit;
                                        ~~~~~~~~~~~~~~~~~~  ^
      aiohttp/_websocket.c:3293:55: error: no member named 'ob_digit' in 'struct _longobject'
                  const digit* digits = ((PyLongObject*)x)->ob_digit;
                                        ~~~~~~~~~~~~~~~~~~  ^
      aiohttp/_websocket.c:3744:47: error: no member named 'ob_digit' in 'struct _longobject'
          const digit* digits = ((PyLongObject*)b)->ob_digit;
                                ~~~~~~~~~~~~~~~~~~  ^
      6 warnings and 5 errors generated.
      error: command '/usr/bin/clang' failed with exit code 1
      [end of output]

  note: This error originates from a subprocess, and is likely not a problem with pip.
  ERROR: Failed building wheel for aiohttpFailed to build aiohttpERROR: Could not build wheels for aiohttp, which is required to install pyproject.toml-based projects
OS
macOs
Python version
Python 3.12
Library version
open-python 0.28.1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/637","No module named 'openai'","2023-10-16T11:39:31Z","Closed as not planned issue","question","Describe the bug
I have uninstalled openai and reinstalled it with pip and pip3,
 I have checked that pip and openai are installed in the same folder
 and various other internet solutions but nothing has worked
 please help
I know its not a bug but i can't figure out how to log a normal issue :'(
To Reproduce
all i say is import openai
Code snippets
No response
OS
Windows
Python version
3.1.1
Library version
0.28.1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/633","aiohttp.ClientResponse.read() called and awaited twice","2023-10-18T10:13:36Z","Closed issue","bug,fixed in v1","Describe the bug
I use the aiohttp-client-cache lib to cache repeated API calls.
 This library returns a CachedResponse when the same request is made.
 The underlying implementation uses a stream to load the cached response from the cache.
 Unsurprisingly, the read() call reads through the stream and does not cache the read bytes in memory.
 Unfortunately, the api_requestor calls and awaits the read() method twice resulting in empty bytes for the second time. (Followed by a JSONDecodeError)
According to the aiohttp documentation the method automatically releases the connection upon a successful read. Combined with the streaming nature of HTTP calls I assume that this method is not designed to be called twice. (IMHO it's just an implementation detail, that the default implementation caches the response in the response object.)
This is the reason why I'm reporting this issue here. Plus there can be a slight performance benefit of not calling and awaiting a coroutine.
To Reproduce
Look at the source code, api_requestor.py line 732 and 739
Code snippets
api_requestory.py from line 731
            try:
                content_bytes = await result.read()  # saved bytes to a scoped variable
            except (aiohttp.ServerTimeoutError, asyncio.TimeoutError) as e:
                raise error.Timeout(""Request timed out"") from e
            except aiohttp.ClientError as e:
                util.log_warn(e, body=result.content)
            return (
                self._interpret_response_line(
                    content_bytes.decode(""utf-8""),  # Removed await read() and 
                    result.status,
                    result.headers,
                    stream=False,
                ),
                False,
            )
OS
Windows 10
Python version
Python v3.11.4
Library version
openai-python 0.28.1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/632","Missing HTTP/2 support because python requests only supports HTTP/1.1","2023-10-15T22:56:36Z","Closed issue","bug","Describe the bug
This official python library for OpenAI is relying on python requests which does not support HTTP2 which is much more efficient.
I recommend exploring switching to HTTPX library.
OpenAI python lib / cli:
Using custom script HTTPX (uses HTTP/2):
To Reproduce
pip install openai
Setup a proxy and intercept the traffic
$ openai -v api chat_completions.create -m gpt-3.5-turbo -g user ""Hello world""
Check it uses HTTP/1.1 as it's the only version requests supports
Code snippets
HTTPX with HTTP/2 basic example:
$ python openai-httpx.py --user ""hey, how are you doing?"" --system ""Act as robot"" --model ""gpt-4""
import httpximport osimport argparseimport asyncioimport jsonimport sys

timeout = httpx.Timeout(10.0, read=None)

async def create_chat_completion(user, system, model):
    url = ""https://api.openai.com/v1/chat/completions""

    headers = {
        ""Authorization"": f""Bearer {os.getenv('OPENAI_API_KEY')}"",
        ""Content-Type"": ""application/json"",
        ""Accept"": ""text/event-stream"",
        ""Connection"": ""keep-alive""
    }

    data = {
        ""model"": model,
        ""messages"": [{""role"": ""system"", ""content"": system},
                     {""role"": ""user"", ""content"": user}],
        ""stream"": True
    }

    async with httpx.AsyncClient(http2=True) as client:
        async with client.stream('POST', url, headers=headers, json=data, timeout=timeout) as response:
            async for line in response.aiter_lines():
                if line.startswith(""data: ""):
                    json_str = line[len(""data: ""):]
                    if json_str != '[DONE]':
                        frame = json.loads(json_str)
                        delta_content = frame.get('choices', [{}])[0].get('delta', {}).get('content', '')
                        sys.stdout.write(delta_content)
                        sys.stdout.flush()

if __name__ == ""__main__"":
    parser = argparse.ArgumentParser()
    parser.add_argument(""--user"", required=True, help=""User message to OpenAI"")
    parser.add_argument(""--system"", required=True, help=""System message to OpenAI"")
    parser.add_argument(""--model"", required=True, help=""Model to use for completion"")
    args = parser.parse_args()

    asyncio.run(create_chat_completion(args.user, args.system, args.model))
OS
macOS 14 23A344
Python version
Python 3.11.4
Library version
openai 0.27.8
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/630","Incorrect error displaying when context size is larger than organization TPM","2023-11-10T03:35:41Z","Closed as not planned issue","bug","Describe the bug
When you make a request to the OpenAI API and the context length exceeds your organizations allowed tokens per minute for the specific model, you will receive the following error:
Rate limit reached for default-gpt-4-large in organization org-xxxxxxxxxxxx on tokens per min. 

Limit: 150000 / min. Current: 0 / min. Contact us through our help center at help.openai.com if you continue to have issues.

This is confusing because it makes the issue appear to be at the organization level, when in fact a bug in your code is just causing your context to be too large. Instead, the following error message should appear:
This model's maximum context length is 4097 tokens, however you requested 150678 tokens (150000 in your prompt; 678 for the completion). Please reduce your prompt; or completion length.""

To Reproduce
Use the OpenAI python client
Use any open ai chat completions endpoint with far too large a context length
Observe error
Code snippets
No response
OS
macOS
Python version
^3.9
Library version
^0.27.8
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/628","No SSL certificate exception not handled","2023-11-10T03:43:40Z","Closed issue","bug","Describe the bug
Hey guys, first time I'm reporting a bug so I hope I'm doing it the right way :)
Yesterday, I tried to use the openai-python library in async, just as written in the documentation (see code below)
completion = await openai.ChatCompletion.acreate( model=""gpt-4"", messages=test_messages )
I was already using the sync method (and it worked) and wanted to do it in async, but it didn't work and the exception was absolutely not clear. It just said : Error communicating with OpenAI
I didn't find anything useful, neither on Google nor with chatGPT, so after like 2 hours trying to see if I missed a comma, I decided to give up and do it with aiohttp, so without the openai-python library, and doing this gave me a ClientConnectorCertificateError that told me that I actually didn't install the ssl certificates.
Therefore I installed those certificates by running this command in my terminal: /Applications/Python 3.11/Install Certificates.command
After doing so, I tried the acreate method again and it finally worked! I'm therefore suggesting that you try to improve the exceptions handling in that case, because it was really unclear if that was caused by my api key, my internet connection, a mistake in my code or something else, and I'm sure other people might have the same issue :)
To Reproduce
Make sure you don't have the ssl certificates installed in Python (not sure how to do so :/)
Just make a request to the API using the openai-python library in async (with acreate)
Code snippets
No response
OS
macOS
Python version
Python v3.11
Library version
openai-python 0.28.1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/627","Support text and image pair query?","2023-11-06T22:05:40Z","Closed issue","No label","Describe the feature or improvement you're requesting
Since the new gpt4v avaible on mobile phones, will be available on python as well?
Additional context
We really need text and image pairs query nowadays
 The text was updated successfully, but these errors were encountered: 
👍5
cesarandreslopez, alex-wenner, jcarletgo, moonylo, and CostaliyA reacted with thumbs up emoji
All reactions
👍5 reactions"
"https://github.com/openai/openai-python/issues/626","Correction needed in README.md - Replace 'a' with 'await","2023-09-27T16:29:21Z","Closed issue","bug","Describe the bug
$ git diff -U0
diff --git a/README.md b/README.md
index b2e0b1b..6cdd5cc 100644
--- a/README.md
+++ b/README.md
@@ -175 +175 @@ You can learn more in our [speech to text guide](https://platform.openai.com/doc
-Async support is available in the API by prepending `a` to a network-bound method:
+Async support is available in the API by prepending `await` to a network-bound method:

To Reproduce
Code snippets
No response
OS
Python version
Library version
latest
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/624","please add OPENAI_API_BASE to documentation","2023-11-10T03:40:33Z","Closed issue","No label","Describe the feature or improvement you're requesting
please add OPENAI_API_BASE ""https://api.openai.com/v1 to the readme
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/622","OpenAI CLI Tools for Chat Fine-Tuning","2023-09-21T21:46:22Z","Open issue","enhancement,good first issue","Describe the feature or improvement you're requesting
Hello everyone,
When using legacy fine-tuning, I find the OpenAI CLI extremely helpful due to its numerous tools.
 For instance, the Prepare Data Helper and the Create Fine-Tuning are particularly useful.
However, these tools only apply to legacy models, which consist of JSON with prompt and completion keys.
I propose the addition of operations to the existing CLI that can perform the same functions for the new chat fine-tuning.
My Proposal
For the sake of backwards compatibility, we could create a new subcommand called chat_fine_tunes. 
This subcommand would inherit all operations that fine_tunes can perform, such as assisting with data preparation, etc. We can simply replicate the existing operations with minor modifications to suit the new format.
Additional context
I am open to working on this feature if it is approved.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/621","Order","2023-11-10T03:33:50Z","Closed issue","API-feedback","Describe the bug
When the number of prompt tokens exceeds the maximum context length, an InvalidRequestError is raised. However, when the number of prompt tokens also exceeds the remaining (or total) token rate limit per minute, a RateLimitError is raised first.
This messes with potential retry logic, as upon an InvalidRequestError, every attempt to retry the request is futile, while this is not the case for RateLimitError. It only makes sense to retry a request if the request itself is valid.
To Reproduce
Send a request with the number of prompt tokens exceeding the max context length of the requested model without exceeding the token rate limit per minute.
Send a request with the number of prompt tokens exceeding both the max context length of the requested model and the token rate limit per minute.
Observe the exceptions.
Code snippets
No response
OS
Win10
Python version
Python v3.11.3
Library version
openai-python-v0.28.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/620","Make openai.proxy configurable for individual request.","2023-10-18T10:18:18Z","Closed issue","enhancement,fixed in v1","Describe the feature or improvement you're requesting
Currently openai.proxy is a global variable for every OpenAI request, but we have multiple deployments with different settings like api_type, api_base etc.
Everything is configurable now except proxy. 😢
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
👍1
Object905 reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/openai-python/issues/619","Docs describe ""suffix"" for completion models, doesn't work with gpt-3.5-turbo-instruct","2023-09-20T02:43:08Z","Closed issue","bug","Describe the bug
https://platform.openai.com/docs/api-reference/completions/create describes:
suffix, string or null, Optional
Defaults to null. The suffix that comes after a completion of inserted text
But as I found here:
curl https://api.openai.com/v1/completions \
  -H ""Content-Type: application/json"" \
  -H ""Authorization: Bearer $OPENAI_API_KEY"" \
  -d '{    ""model"": ""gpt-3.5-turbo-instruct"",    ""prompt"": ""Say this is a test"",    ""max_tokens"": 7,    ""suffix"": ""dog"",    ""temperature"": 0  }'
Produces:
{
  ""error"": {
    ""message"": ""Unrecognized request argument supplied: suffix"",
    ""type"": ""invalid_request_error"",
    ""param"": null,
    ""code"": null
  }
}
To Reproduce
curl https://api.openai.com/v1/completions \
  -H ""Content-Type: application/json"" \
  -H ""Authorization: Bearer $OPENAI_API_KEY"" \
  -d '{
    ""model"": ""gpt-3.5-turbo-instruct"",
    ""prompt"": ""Say this is a test"",
    ""max_tokens"": 7,
    ""suffix"": ""dog"",
    ""temperature"": 0
  }'

Code snippets
No response
OS
All
Python version
All
Library version
All
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/617","Announcing An OpenAI Microservice Quickstart for AWS","2023-11-10T03:41:39Z","Closed issue","No label","Describe the feature or improvement you're requesting
Just a heads up that I've released v0.1.0 of https://github.com/FullStackWithLawrence/aws-openai, a REST API implementing each of the 30 example applications from the official OpenAI API Documentation using a modularized Terraform approach. Implemented as a serverless microservice using AWS API Gateway, Lambda and the OpenAI Python Library. Leverages OpenAI's suite of AI models, including GPT-3.5, GPT-4, DALL·E, Whisper, Embeddings, and Moderation.
Additional context
This is a non-commercial community-oriented project. Contributors are welcome. Next steps: add some of the ""cookbook"" ideas.
 The text was updated successfully, but these errors were encountered: 
❤️1
regrettablemouse136 reacted with heart emoji
All reactions
❤️1 reaction"
"https://github.com/openai/openai-python/issues/616","new sessions evert 3 minutes","2023-11-03T22:29:25Z","Closed issue","bug,fixed in v1","Describe the bug
https://github.com/openai/openai-python/blob/e389823ba013a24b4c32ce38fa0bd87e6bccae94/openai/api_requestor.py#L590C2-L590C2
This MAX_SESSION_LIFETIME_SECS setting kills sessions after 3 minutes. I believe this breaks long running tasks. It would also be difficult to troubleshoot downstream issues if the client is killing sessions at an arbitrary 3 minutes. Can someone verify the PR the brought this change is 100% good to go? It seems to me like this shouldn't be there, or at least be configurable, and that it would mess with users passing in sessions.
elif (
            time.time() - getattr(_thread_context, ""session_create_time"", 0)
            >= MAX_SESSION_LIFETIME_SECS
        ):
            _thread_context.session.close()
            _thread_context.session = _make_session()
            _thread_context.session_create_time = time.time()

@jhallard
To Reproduce
Run a long running llm request, and a new session is created after 3 minutes during.
Code snippets
No response
OS
macOS
Python version
3.8
Library version
openai-python v0.28.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/615","openai.FineTuningJob.list_events doesn't support api_base","2023-10-18T10:05:59Z","Closed issue","bug","Describe the bug
when query new fine tune events with openai:
 openai.FineTuningJob.list_events(id=""ft-abc123"", limit=10,api_key=""xxxx"",api_base=""xxxx"")
the http request doesn't go to api_base domain, instead go to the offical api site.
To Reproduce
set up a nginx proxy for openai official api
query fine tune job event with openai.FineTuningJob.list_events(id=""ft-abc123"", limit=10,api_key=""xxxx"",api_base=""xxxx"")
http requests go to offical api site instead the nginx proxy.
Code snippets
openai.FineTuningJob.list_events(id=""ft-abc123"", limit=10,api_key=""xxxx"",api_base=""xxxx"")
OS
linux
Python version
python 3.10
Library version
openai-python v0.28.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/609","Azure OpenAI KeyError on 500","2023-11-10T04:14:39Z","Closed issue","bug,fixed in v1","Describe the bug
Currently, 500 errors from Azure OpenAI do not return a dictionary with key ""error"". This leads to a generic APIError/KeyError when it could attempt to return the message from the instead, which is more useful for end users
To Reproduce
Any request to Azure OpenAI that fails because of a non-specific Server Error
Example response:
{
    ""activityId"": ""123-123-123"",
    ""message"": ""Internal server error"",
    ""statusCode"": 500
}
Code snippets
No response
OS
Linux
Python version
3.11.3
Library version
v0.28.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/608","Not retrying on error","2023-10-04T19:27:25Z","Closed issue","fixed in v1","Describe the bug
I'm running a simple LLMChain with ChatOpenAI and getting 'Error [TimeoutError]: Request timed out' errors very often.
 But the main problem is that LangChain doesn't seem to be retrying to run the chain after the error.
At the very end of the error message I get:
...
  attemptNumber: 1,
  retriesLeft: 6
}

Here's the full error message:
Error in handler LangChainTracer, handleChainError: TimeoutError: The operation was aborted due to timeout
file:///Users/[redacted]/node_modules/langchain/dist/util/openai.js:6
        error = new Error(e.message);
                ^

Error [TimeoutError]: Request timed out.
    at wrapOpenAIClientError (file:///Users/[redacted]/node_modules/langchain/dist/util/openai.js:6:17)
    at file:///Users/[redacted]/node_modules/langchain/dist/chat_models/openai.js:517:31
    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)
    at async RetryOperation._fn (/Users/[redacted]/node_modules/p-retry/index.js:50:12) {
  attemptNumber: 1,
  retriesLeft: 6

To Reproduce
Make enough sub-sequential calls to ChatOpenAI using the LLMChain.
In my case I'm looping over a 2000 array, using each one of the elements to populate the prompt template and calling the chain.
Code snippets
// OPEN AI SETUPconst llm = new ChatOpenAI({
  openAIApiKey: process.env.OPENAI_API_KEY,
  modelName: ""gpt-3.5-turbo"",
  max_tokens: 1800,
  // frequency_penalty: 0.1,
  temperature: 0.1,
  timeout: 30000,
  maxRetries: 5,
});

//LLM CHAINconst chain = new LLMChain({
  llm: llm,
  prompt: promptTemplate,
});

//LOOP OVER THE DATASET AND CALL LLMfor (const [index, name] of names.entries()) {
  const result = await chain.call({
    name: name.firstName,
  });

...

 console.log(result.text);
}
OS
MacOS
Python version
Node v20.5.1
Library version
openai 3.3.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/607","Why is Azure openai api throwing an exception when attempting to handle a quota throttling error?","2023-11-10T03:11:11Z","Closed issue","bug","Describe the bug
In the example below, the Azure openai quota is exceeded and generates an expected exception. However, while handling that exception, the openai api generates another exception. It looks as though the ""error_data"" variable is a string instead of a dictionary. The ""error_data"" string contains ""Quota has been exceeded"" instead of a dictionary like {""message"": ""Quota has been exceeded""}
Running miniconda and openai 0.27.4
Ref: Why is Azure openai api throwing an exception when attempting to handle a quota throttling error?
 Can you please help me.
To Reproduce
In the example below, the Azure openai quota is exceeded and generates an expected exception. However, while handling that exception, the openai api generates another exception. It looks as though the ""error_data"" variable is a string instead of a dictionary. The ""error_data"" string contains ""Quota has been exceeded"" instead of a dictionary like {""message"": ""Quota has been exceeded""}
Running miniconda and openai 0.27.4
import os
import openai

openai.api_type = ""azure""
openai.api_base = os.getenv(""AZURE_OPENAI_ENDPOINT"")
openai.api_version = ""2023-05-15""
openai.api_key = os.getenv(""AZURE_OPENAI_KEY"")

# The intent here is to make 2 calls in rapid succession to generate
# an error that we have exceeded the quota.
response = openai.Completion.create(
engine=""gpt-35-turbo""
messages=[
{""role"": ""system"", ""content"": ""You are a helpful assistant.""},
{""role"": ""user"", ""content"": ""Tell me a story about an elephant?""}
]
)
# Somewhat brittle test that may need to be done a few times to exceed
# the quota
response = openai.Completion.create(
engine=""gpt-35-turbo""
messages=[
{""role"": ""system"", ""content"": ""You are a helpful assistant.""},
{""role"": ""user"", ""content"": ""Tell me a story about a fox?""}
]
)

AttributeError                            Traceback (most recent call last)
.
.
.
File ~\Projects\chatGPT\env_conda\lib\site-packages\openai\api_requestor.py:346, in APIRequestor.handle_error_response(self, rbody, rcode, resp, rheaders, stream_error)
       341 if ""internal_message"" in error_data:
       342     error_data[""message""] += ""\n\n"" + error_data[""internal_message""]
       344 util.log_info(
       345     ""OpenAI API error received"",
-->    346     error_code=error_data.get(""code""),
       347     error_type=error_data.get(""type""),
       348     error_message=error_data.get(""message""),
       349     error_param=error_data.get(""param""),
       350     stream_error=stream_error,
       351 )
       353 # Rate limits were previously coded as 400's with code 'rate_limit'
       354 if rcode == 429:
AttributeError: 'str' object has no attribute 'get'

Ref: Why is Azure openai api throwing an exception when attempting to handle a quota throttling error?
 Can you please help me.
Code snippets
import osimport openai

openai.api_type = ""azure""openai.api_base = os.getenv(""AZURE_OPENAI_ENDPOINT"")
openai.api_version = ""2023-05-15""openai.api_key = os.getenv(""AZURE_OPENAI_KEY"")

# The intent here is to make 2 calls in rapid succession to generate# an error that we have exceeded the quota.response = openai.Completion.create(
engine=""gpt-35-turbo""messages=[
{""role"": ""system"", ""content"": ""You are a helpful assistant.""},
{""role"": ""user"", ""content"": ""Tell me a story about an elephant?""}
]
)
# Somewhat brittle test that may need to be done a few times to exceed# the quotaresponse = openai.Completion.create(
engine=""gpt-35-turbo""messages=[
{""role"": ""system"", ""content"": ""You are a helpful assistant.""},
{""role"": ""user"", ""content"": ""Tell me a story about a fox?""}
]
)
OS
windows
Python version
Python v3.7.1
Library version
v0.27.4
 The text was updated successfully, but these errors were encountered: 
👍1
AmeroHan reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/openai-python/issues/606","Show current line of training_file for FineTuningJob event/event file","2023-11-10T04:17:24Z","Closed issue","No label","Describe the feature or improvement you're requesting
I hope that through the FineTuningJob event, we can pinpoint certain rows of data in the training set where there are abnormal losses during fine-tuning.
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/605","n=X Parameter counts as scalar for total token count towards rate limit","2023-11-10T04:18:42Z","Closed issue","bug","Describe the bug
It appears as if using n>1 works as a multiplier on the total token number of prompt and longest response when the TPM rate is calculated.
I had 5-7 requests with 1k total tokens each and n=5 and got a rate limit error saying my TPM was 76k
To Reproduce
set n = 5 and create multiple requests having larger prompts and smaller responses within short time without violating the size(prompt)+max_tokens*n limit,
 it will result in a rate limit error anyway.
 e.g. Summarize this page in a sentence
Code snippets
No response
OS
Python version
Library version
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/596","Fine-tuning documentation is incorrect","2023-09-06T03:06:42Z","Closed issue","bug","Describe the bug
In fine-tuning documentation, the openai.Model.delete method does not receive the same input as the other listed methods. While, for example, openai.FineTuningJob.retrieve receives the fine-tune job id, openai.Model.delete should receive the name of the fine-tuned model. The documentation induces us to think both methods have the same parameters.
To Reproduce
N/A
Code snippets
No response
OS
N/A
Python version
N/A
Library version
openai-python v0.27.10
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/594","Function call: return a ""content"" containing function_call instead of a ""function_call""","2023-11-10T04:27:37Z","Closed issue","Azure,bug","Describe the bug
Hello,
While using function call, we met a bug that occurs occasionally. Even in GPT4.
Instead of returning a json with key ""function_call"", which it should be, GPT returns a json with key ""content"". This ""content"" contains ""function_call"" as a string.
case 1:
case 2:
GPT_MODEL = ""gpt-4""
 API_VERSION = ""2023-07-01-preview""
 openai.api_type = ""azure""
To Reproduce
It occurs occasionally. We cannot guarantee it can be reproduced.
Code snippets
No response
OS
Linux
Python version
3.8.10
Library version
0.27.8
 The text was updated successfully, but these errors were encountered: 
👍1
schneiderfelipe reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/openai-python/issues/589","How do you set the number of epochs for FineTune.create on ChatGPT?","2023-09-06T02:57:12Z","Closed issue","bug","Describe the bug
I can fine-tune gpt-3.5-turbo just fine, but it looks like there's no way to change the number of epochs to something over than 10, even though the Guides recommend indicate that you can (""If the model becomes less diverse than expected decrease the number by 1 or 2 epochs""). It seems to be over training quite a bit. Past experience is that most LLMs just need 1-2 epochs.
There doesn't seem to be an actual argument that can be passed to FineTune.create to change the number of training epochs. I've fine-tuned davinic-003 before with the parameter n_epochs, but it doesn't seem to work with ChatGPT fine-tuning. There's no reference in help(FineTune.create) to epochs and passing epochs, n_epochs to FineTune.create yields extra fields not permitted.
Running it through openai api fine_tunes.create also doesn't permit specification of epochs. Am I missing something?
To Reproduce
Format and upload a dataset
Create a training job using the Python SDK with FineTuningJob.create passing an argument to change the number of epochs
Code snippets
No response
OS
Ubuntu 20.04
Python version
Python 3.7.5
Library version
openai-python 0.27.9
 The text was updated successfully, but these errors were encountered: 
👍2
majamil16 and Robert-Jia00129 reacted with thumbs up emoji
All reactions
👍2 reactions"
"https://github.com/openai/openai-python/issues/587","Add context length attribute to the model objects returned on the list and retrieve models endpoint.","2023-11-10T04:11:40Z","Closed issue","API-feedback","Describe the feature or improvement you're requesting
Description:
At present, the openai.Model.list() function provides a list of model objects, each characterized by four attributes, as exemplified below:
{
  ""id"": ""davinci"",
  ""object"": ""model"",
  ""created"": 1686935002,
  ""owned_by"": ""openai""
}
I would like to propose an augmentation to this functionality by introducing an additional attribute that conveys crucial information—the maximum context length or limit of each model.
Request:
I kindly request the incorporation of a new attribute to the model object that signifies the respective model's maximum context length. This enhancement would yield several benefits:
Simplified Code Implementation: By exposing the maximum context length directly through the model object, users can seamlessly adapt their code to automatically respect the context limit. This feature would empower users to fine-tune their applications to the specific capabilities of each model effortlessly, without having to manually store context limits individually for every model.
Enhanced Adaptability: The suggested attribute would enable OpenAI to dynamically adjust a model's context length, if needed, without disrupting users' code. This adaptability reflects a proactive approach towards model improvements while safeguarding the stability of user implementations.
Proposed Attribute:
I propose the inclusion of the following attribute in the model object:
{
  ""context_limit"": {
    ""type"": ""integer"",
    ""description"": ""The maximum context length that the model can effectively handle.""
  },
}
The ""context_limit"" attribute, when integrated into the model object, would be of type ""integer."" It will succinctly convey the maximum number of tokens can be processed by the model. This clear and concise addition would provide developers with essential information upfront, contributing to an improved development experience.
Additional context
There's been demand for this attribute on the OpenAI dev community as well. Here are the links for your reference:
https://community.openai.com/t/request-query-for-a-models-max-tokens/161891
https://community.openai.com/t/can-i-get-the-max-tokens-information-from-openai-api/143549
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/585","FineTuningJob does not support suffix >18 characters","2023-09-06T03:06:22Z","Closed issue","bug","Describe the bug
Hi, while updating migrating my FineTuning job from openai.FineTune => openai.FineTuningJob to support the new babbage-002 model I ran into the following issue:
InvalidRequestError                       Traceback (most recent call last)

4 frames
[/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py](https://localhost:8080/#) in _interpret_response_line(self, rbody, rcode, rheaders, stream)
    763         stream_error = stream and ""error"" in resp.data
    764         if stream_error or not 200 <= rcode < 300:
--> 765             raise self.handle_error_response(
    766                 rbody, rcode, resp.data, rheaders, stream_error=stream_error
    767             )

InvalidRequestError: invalid suffix: *******************************, ensure this value has at most 18 characters

the docs say the suffix field supports up to 40 characters. we used the same suffix with the previous endpoint with no issues.
Wondering if this is an error in the docs, or if the new API does not support a custom suffix parameter > 18 characters?
To Reproduce
invoke openai.FineTuningJob.create as follows
response = openai.FineTuningJob.create(training_file=response['id'], model=model, n_epochs=n_epochs, suffix=""suffix-that-is-31-characters"")

Code snippets
No response
OS
macOS
Python version
Python 3.10.12
Library version
0.27.9
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/581","return an async iterator when using streaming and acreate","2023-08-28T10:30:35Z","Closed issue","No label","Describe the feature or improvement you're requesting
I don't know if it is a bug or a missing feature, but I found the return value from acreate is not an async iterator and I'd like to have it.
The code is simple as the following
def sync_stream():
    # Runs totally fine
    response = openai.ChatCompletion.create(
        messages=[{
            ""role"": ""user"",
            ""content"": ""hello""
        }],
        temperature=0.2,
        stream=True,
    )

    for trunk in response:
        print(trunk)


async def async_stream():
    response = openai.ChatCompletion.acreate(
        messages=[{
            ""role"": ""user"",
            ""content"": ""hello""
        }],
        temperature=0.2,
        stream=True,
    )
    # TypeError: 'async for' requires an object with __aiter__ method, got coroutine
    async for trunk in response:
        print(trunk)
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/576","Embedding.create() fails when it receives a list of strings that contains an empty string","2023-11-10T03:48:37Z","Closed as not planned issue","API-feedback","Describe the bug
According to https://platform.openai.com/docs/api-reference/embeddings/create, you can send an input of type str or list of str. You receive a response if you send an empty string ("""") as input. But, if you send a list of str that contains an empty string like [""a"", ""b"", """", ""d""], you receive an error of type InvalidRequestError with this message '$.input' is invalid. Please check the API reference: https://platform.openai.com/docs/api-reference.
To Reproduce
res = openai.Embedding.create(input="""", engine=EMBEDDING_MODEL)
 print(len(res['data'][0]['embedding']))
 res2 = openai.Embedding.create(input=[""a"", """"], engine=EMBEDDING_MODEL)
 print(res2['data'][0]['embedding'])
Output
1536
InvalidRequestError Traceback (most recent call last)
 Cell In[74], line 3
 1 res = openai.Embedding.create(input="""", engine=EMBEDDING_MODEL)
 2 print(len(res['data'][0]['embedding']))
 ----> 3 res2 = openai.Embedding.create(input=[""a"", """"], engine=EMBEDDING_MODEL)
 4 print(res2['data'][0]['embedding'])
File ~/workspace/platzi/semantic-search/.venv/lib/python3.10/site-packages/openai/api_resources/embedding.py:33, in Embedding.create(cls, *args, **kwargs)
 31 while True:
 32 try:
 ---> 33 response = super().create(*args, **kwargs)
 35 # If a user specifies base64, we'll just return the encoded string.
 36 # This is only for the default case.
 37 if not user_provided_encoding_format:
File ~/workspace/platzi/semantic-search/.venv/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py:153, in EngineAPIResource.create(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)
 127 @classmethod
 128 def create(
 129 cls,
 (...)
 136 **params,
 137 ):
 138 (
 139 deployment_id,
 ...
 764 rbody, rcode, resp.data, rheaders, stream_error=stream_error
 765 )
 766 return resp
InvalidRequestError: '$.input' is invalid. Please check the API reference: https://platform.openai.com/docs/api-reference.tps://platform.openai.com/docs/api-reference.
Code snippets
No response
OS
Ubuntu
Python version
Python v3.10.12
Library version
openai-python v0.27.8
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/575","Incorrectly opened issue.","2023-08-17T17:23:07Z","Closed issue","bug","Sorry, incorrectly opened issue.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/574","Error: Response payload is not completed","2023-11-10T03:40:57Z","Closed issue","Azure,bug,fixed in v1","Describe the bug
When using the acreate interface and attempting to utilize streaming output, I encountered ""aiohttp.client_exceptions.ClientPayloadError: Response payload is not completed""
Time: (UTC) 2023-08-17 06:08:24,030 ~ 2023-08-17 06:13:24,180
Request ID: 9c46544d6517b7b8e9df1e9e61fd7f53
To Reproduce
using acreate interface
stream = True
Code snippets
async for chunk in completion_stream:
File ""/usr/local/lib/python3.9/site-packages/openai/api_resources/abstract/engine_api_resource.py"", line 230, in <genexpr>return (
File ""/usr/local/lib/python3.9/site-packages/openai/api_requestor.py"", line 319, in wrap_respasync for r in resp:
File ""/usr/local/lib/python3.9/site-packages/openai/api_requestor.py"", line 633, in <genexpr>return (
File ""/usr/local/lib/python3.9/site-packages/openai/api_requestor.py"", line 114, in parse_stream_asyncasync for line in rbody:
File ""/usr/local/lib/python3.9/site-packages/aiohttp/streams.py"", line 35, in __anext__rv = await self.read_func()
File ""/usr/local/lib/python3.9/site-packages/aiohttp/streams.py"", line 311, in readlinereturn await self.readuntil()
File ""/usr/local/lib/python3.9/site-packages/aiohttp/streams.py"", line 343, in readuntilawait self._wait(""readuntil"")
File ""/usr/local/lib/python3.9/site-packages/aiohttp/streams.py"", line 304, in _waitawait waiteraiohttp.client_exceptions.ClientPayloadError: Response payload is not completed
OS
Linux
Python version
Python v3.9
Library version
openai-python v0.27.2
 The text was updated successfully, but these errors were encountered: 
👍3
cwfparsonson, mpmisko, and pratyakshs reacted with thumbs up emoji
All reactions
👍3 reactions"
"https://github.com/openai/openai-python/issues/572","Suppressing token 100265 <|im_end|> using logit_bias results in InvalidRequestError","2023-11-10T03:45:28Z","Closed issue","bug","Describe the bug
When trying to ban the sampling of token 100265 using logit_bias by mapping it to -100, I get:
{'error': {'message': ""Invalid key in 'logit_bias': 100265. Maximum value is 100257."", 'type': 'invalid_request_error', 'param': 'logit_bias', 'code': None}}

As you are aware 100257 is <|endoftext|> and it can be suppressed currently to get full length (of max_tokens) completions on the Completions API.
However its equivalent on chat completions API i.e 100265 → <|im_end|> can’t be used on chat completions API.
I am curious is intentional (safety reasons) to limit tokens till 100257 OR it is inherited by reusing the old completion API spec.
To Reproduce
Simply pass a bias: {100265: -100} to the logit_bias param of chat completion request.
This results in:
Exception has occurred: InvalidRequestErrorInvalid key in 'logit_bias': 100265. Maximum value is 100257.  File ""dir/tokenext.py"", line 12, in <module>    reply = openai.ChatCompletion.create(model=""gpt-3.5-turbo"",            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^openai.error.InvalidRequestError: Invalid key in 'logit_bias': 100265. Maximum value is 100257.
Code snippets
import openaiopenai.api_key = ""API_KEY""  # supply your API key however you choose

conversation = [{
    ""role"": ""user"",
    ""content"": input(""Input:"")
}]

bias = {100265: -100}

print(""GPT: "")
reply = openai.ChatCompletion.create(model=""gpt-3.5-turbo"",
                                     messages=conversation,
                                     max_tokens=1000,
                                     logit_bias=bias)
print(reply)
OS
macOS
Python version
Python v3.11.2 64-bit
Library version
openai-python v0.27.7
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/570","Unclosed client session","2023-10-18T10:30:02Z","Closed issue","bug,fixed in v1","Describe the bug
When I use FastAPI for forwarding and set stream=True, for example: async for res in await openai.ChatCompletion.acreate(**kwargs), when this request is terminated before it completes, an exception occurs as follows: [asyncio] [ERROR]: Unclosed client session client_session: <aiohttp.client.ClientSession object at 0x7f0ca458c490>.
To Reproduce
1. Write a FastAPI request using the acreate method to return data in streaming format.
2. Use a packet sending tool to make a request to the written API, terminate the request while receiving data.
3. Wait for a moment, and the exception will be displayed in the terminal.

Code snippets
No response
OS
Linux
Python version
Python 3.11.4
Library version
openai 0.27.8
The content you are editing has changed. Please copy your edits and refresh the page.
Tasks
BetaGive feedback
No tasks being tracked yet.
Options
Convert to issue
Toggle completion
Rename
Remove
 The text was updated successfully, but these errors were encountered: 
👍6
DavidHuie, haifengwu-msft, hikilyc, hynky1999, ShantanuNair, and wanyang6-tal reacted with thumbs up emoji
All reactions
👍6 reactions"
"https://github.com/openai/openai-python/issues/569","Must provide an 'engine' or 'deployment_id' parameter","2023-11-10T04:16:56Z","Closed issue","Azure,bug","Describe the bug
I am running the github code in local using VSCode start.cmd file. All the necessary packages were installed and the website is running now. When I prompt a quesestoin, I got the below error
 Must provide an 'engine' or 'deployment_id' parameter
Error
 Must provide an 'engine' or 'deployment_id' parameter to create a <class 'openai.api_resources.chat_completion.ChatCompletion'>
To Reproduce
Clone the github report and changed few environment variables as given in the instructions.
Code snippets
No response
OS
Windows
Python version
Python v3.11.4
Library version
openai 0.27.7
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/567","Returned random texts from unknown source when using model davinci","2023-09-06T11:56:25Z","Closed issue","No label","Describe the feature or improvement you're requesting
When using 'davinci' model, regardless of the input prompt it always responds with random text/conversation/clips from the internet.
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/565","Documentation for Embedding.create() is wrong when using Azure as backend","2023-09-06T11:55:24Z","Closed issue","bug","Describe the bug
The Embedding.create() method [1] says that valid parameters are listed on https://platform.openai.com/docs/api-reference/embeddings, where available parameters are model, input and user.
However, when creating embeddings using Azure as OpenAI as backend, engine or deployment_id are required parameters and model is ignored.
I'm not sure what is a good solution. Maybe the documentation should be changed to
Creates a new embedding for the provided input and parameters.

See https://platform.openai.com/docs/api-reference/embeddings for a list
of valid parameters for calling OpenAI API

or https://learn.microsoft.com/en-gb/azure/ai-services/openai/reference#embeddings when using Azure OpenAI API.

[1] 
openai-python/openai/api_resources/embedding.py
 Line 15 in 7610c5a
	defcreate(cls, *args, **kwargs): 
To Reproduce
import openaiopenai.api_type = ""azure""openai.api_key=""ABC123...""openai.Embedding.create(model='mymodel', input='hello world')
File ~/python3.11/site-packages/openai/api_resources/abstract/engine_api_resource.py:83, in EngineAPIResource.__prepare_create_request(cls, api_key, api_base, api_type, api_version, organization, **params)
     81 if typed_api_type in (util.ApiType.AZURE, util.ApiType.AZURE_AD):
     82     if deployment_id is None and engine is None:
---> 83         raise error.InvalidRequestError(
     84             ""Must provide an 'engine' or 'deployment_id' parameter to create a %s""
     85             % cls,
     86             ""engine"",
     87         )
     88 else:
     89     if model is None and engine is None:

InvalidRequestError: Must provide an 'engine' or 'deployment_id' parameter to create a <class 'openai.api_resources.embedding.Embedding'>
Code snippets
No response
OS
Ubuntu 20.04 LTS
Python version
Python 3.11
Library version
openai-python 0.27.8
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/564","Return value of ChatGPT API","2023-09-06T11:57:52Z","Closed issue","API-feedback","Describe the feature or improvement you're requesting
I think that it would be nice for the API for any model before GPT-4, returns any type of value or a specific answer when the model is unable to answer the question (like for example; about events after 2021, and some other stuff it doesn’t know) because in this way devs could have more control about how to handle this cases, like using different resources for the wanted information.
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/563","Consider using a class constructor for configuration, not module globals","2023-11-03T22:29:12Z","Closed issue","fixed in v1","Describe the feature or improvement you're requesting
This is the current way to configure the credentials for the openai library:
    openai.api_base = f""https://{AZURE_OPENAI_SERVICE}.openai.azure.com""
    openai.api_version = ""2023-05-15""
    openai.api_type = ""azure_ad""
    openai_token = await azure_credential.get_token(
        ""https://cognitiveservices.azure.com/.default""
    )
    openai.api_key = openai_token.token

(Or simpler than that, if using keys instead of Azure credentials)
Here's how Python handles it, as far as I understand:
When a file imports openai, Python stores it in the sysmodules dict
Then when that file sets attributes of the module, Python updates the attributes of the module that's stored in the sysmodules dict
When another file later imports openai, Python finds the module in the sysmodules dict and that hopefully has the correct attributes set
This approach of configuring via module-level globals seems fairly brittle to me, since it's dependent on how Python caches modules, what order things are imported, and how things are imported. It also seems likely to be incompatible with multiprocessing or asynchronous workers in a large application.
I recommend instead something like what the NodeJS SDK uses, a class that can be instantiated with the configuration:
https://github.com/openai/openai-node
That also makes it possible to use different credentials in different parts of a codebase, by passing the credential'd object around.
I realize that would be a significantly large change, but I think it would make for much more robust code overall.
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
👍5
nam03021vnmac, mpenndev, failable, sodarfish, and fablerq reacted with thumbs up emoji👀1
aemr3 reacted with eyes emoji
All reactions
👍5 reactions
👀1 reaction"
"https://github.com/openai/openai-python/issues/561","Hope support choose if SSL verify on ChatCompletion.create","2023-09-05T14:28:36Z","Closed issue","No label","Describe the feature or improvement you're requesting
Background:
 ChatCompletion.create does not support the param ""verify"" .
When you meet some certify problem ,and you dont want to fix SSL problem at first , you maybe want to solve this accroding to extram param. Just like the ""verify"" in requests.request(xxxxx,verify=False).

Especially the underlying layer of ChatConpletion.create itself is also call requests.request method 。
But unfortunately ，ChatCompletion.create does not support this param ""verify"" ,and it has no provide any else param to solve this problem。
Undoubtedly, it is best that all requests should pass SSL validation. But in many people's environments, solving certificate issues requires a lot of extra effort. This is especially unfriendly to beginners.
Request:
 So I suggest adding the parameter 'verify' on ChatCompletion.create, then we can choose if verify SSL by ourself。
 And I‘ll make PR for this issue later.
 Hope you can also agree with my viewpoint
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
👍1
Yaakov-Belch reacted with thumbs up emoji❤️1
Yaakov-Belch reacted with heart emoji
All reactions
👍1 reaction
❤️1 reaction"
"https://github.com/openai/openai-python/issues/560","Chat Completion Hangs after error 500","2023-09-06T11:58:40Z","Closed issue","bug","Describe the bug
Hi,
Chat Completion hang at the next call after a 500 error. My code printed the error below. After that at the next Chat Completion call, it just stuck there.
The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID 90c93658c303560e33e5d459254eae54 in your email.) {
 ""error"": {
 ""message"": ""The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID 90c93658c303560e33e5d459254eae54 in your email.)"",
 ""type"": ""server_error"",
 ""param"": null,
 ""code"": null
 }
 }
To Reproduce
Call chatcompletion
Get a 500 error from openai
Call chatcompletion again
hang
Code snippets
No response
OS
Windows
Python version
Python 3.10.11
Library version
openai-0.27.2
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/558","GPT-4-0613 Finish_Reason Null Issue in Streaming Mode","2023-09-06T12:02:59Z","Closed issue","bug","Describe the bug
I am facing an issue while using the latest OpenAI GPT-4-0613 models in streaming mode. Specifically, I have noticed that for some cases, the finish_reason field in the response comes as Null instead of the expected value 'stop' for the last token. I have provided the necessary context along with my question in the prompt. Similar Question is asked in Openai Forum: https://community.openai.com/t/completion-finish-reason-is-missing-when-stream-true/90526
My understanding is that the finish_reason field should indicate 'stop' for the last token when using the streaming mode. However, in certain cases, it appears to be Null. This behavior seems inconsistent with the expected behavior of the model.
Notice: However, gpt-0314 always gives finish_reason as stop for last token
To Reproduce
Take a prompt which has Context and Question. ( Make a prompt size of 5200 approx)
Code snippets
response = openai.ChatCompletion.create(
    model='gpt-4-0613',
    messages=[
        {'role': 'user', 'content': ""Context along with question""}
    ],
    temperature=0,
    stream=True 
)

for chunk in response:
    print(chunk)
OS
macOS
Python version
Python3.9
Library version
openai from azure
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/557","Issue with populating api configs using environment variables after importing openai","2023-10-17T11:28:39Z","Closed issue","bug,fixed in v1","Describe the bug
I'm encountering an issue where I am unable to populate the api_key, api_base, api_type, and api_version using environment variables.
The openai package seems to read the api_key, api_base, api_type, and api_version from environment variables only at the moment it is imported. If these api configs are specified after the openai package is imported, they are not recognized.
The code is in here.
To Reproduce
Due to python's caching mechanism, to reproduce this issue, the openai package must be reloaded.
Code snippets
import openaiimport os

os.environ[""OPENAI_API_KEY""] = ""<api_key>""os.environ[""OPENAI_API_BASE""] = ""<api_base>""os.environ[""OPENAI_API_TYPE""] = ""<api_type>""os.environ[""OPENAI_API_VERSION""] = ""<api_version>""

openai.Completion.create(prompt=""Hello"", engine=""text-davinci-003"", max_tokens=5)
OS
Windows 11
Python version
Python v3.8.16
Library version
openai-python v0.27.8
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/555","Stream response inconsistent with OpenAi Playground response","2023-08-04T03:25:51Z","Closed issue","bug","Describe the bug
Using the streaming option with python results in a consistently different output compared to the response generated with the same model in the playground. Given the same system prompt, settings and user responses, the response in python stops the flow of conversation at the same point each time where as in playground it continues the flow by asking a question.
AI: Hello, I'm Tom from the golf course. How may I help you?
Human: hey i want to book comp
AI: Sure, I can help you with that.
AI: Are you a member of our club?
Human: yep
AI: Great!
AI: Could you please provide me with your member number?
Human: 12345
AI: Thank you for providing your member number.
AI: May I have your name, please?
Human: cam
AI: Thank you, Cam.
AI: How many players will be participating in the competition?
Human: 2
AI: Thank you for letting me know.

“””""finish_reason"": ""stop"" sent”””

Response from Playground:
I have tested this same flow over 50 times and 90%+ of the time, the response in python ends the conversation flow whereas I have never been able to reproduce this in Playground.
To Reproduce
Set streaming to True
Use the following system prompt:
""""""You are an AI-powered phone receptionist called Tom, for a Golf Club. The club has two golf courses at its location.
                Your task is to assist callers with the following:

                1. Schedule golf bookings for players based on their preferred date and time, checking for availability and ensuring a smooth booking process.
                2. Answer common questions related to golf course procedures, services, and frequently asked inquiries, providing accurate and helpful responses.
                3. If the user asks to speak with a human, do not hesitate to transfer them to the golf course receptionist.

                Initial Instructions:
                1. Determine whether the caller is wanting to play a comp or a social game.
                2. Determine if they are a member of the club.

                If they are playing social, collect the following information in this order - NOTE: only ask for the information one at a time, don't say the number:
                1. Member number (if they are a member of the club) - not compulsory
                2. Name
                3. Number of players
                4. Day they want to play and tee off time


                If they are playing comp, collect the following information in this order - NOTE: only ask for the information one at a time:
                1. If they are a member of the club, their member number otherwise you need their Golflink number.
                2. Name
                3. Number of players
                4. Day they want to play and tee off time



                The below is the list of comp days:
                Monday - Mixed Competition one course used alternating between the two each week
                Tuesday - Ladies day both courses used
                Wednesday - Men's Competition both courses used
                Thursday - Ladies and Vets Competition both courses used
                Friday - Mixed Competition one course used alternating between the two each week, this week it is the coolangatta course
                Saturday - Men's Competition both courses used
                Sunday - Mixed Competition one course used alternating between the two each week, this week it is the tweed course

                Once you have made the booking, ask them if sending a booking confirmation to their provided contact information is okay.

                To suggest available tee times, consider the caller's preferences and provide a maximum of three relevant options based on the time of day or date they have requested. Ensure that the suggested tee times are not earlier than the current time and are within the listed availability.

                Remember:
                Always move the conversation forward and ask the next question. This is very important!!!
                Only make bookings for the available tee times, don't deviate from them at all.
                You are on a phone call, so respond as if you were talking to the person with voice.

                Be polite but concise and speak casually like a receptionist would.
                Today is Sunday, 30th July 2023.
                The golf course is open from 7:00am to 6:00pm from Monday to Friday, and from 8:00am to 5:00pm on weekends.
                To end the conversation, use the following line:
                ""Thank you, {{caller name}}. Your tee time is now booked for {{date}} at {{time}}. We hope you enjoy your round. Is there anything else I can assist you with?""

                Now, proceed with assisting the caller in scheduling their tee time or answering their inquiries.""""""

Respond in the exact same way as this:
AI: Hello, I'm Tom from the golf course. How may I help you?
Human: hey i want to book comp
AI: Sure, I can help you with that.
AI: Are you a member of our club?
Human: yep
AI: Great!
AI: Could you please provide me with your member number?
Human: 12345
AI: Thank you for providing your member number.
AI: May I have your name, please?
Human: cam
AI: Thank you, Cam.
AI: How many players will be participating in the competition?
Human: 2
AI: Thank you for letting me know.

Code snippets
No response
OS
macOS & Ubuntu
Python version
3.9.6
Library version
0.27.9
 The text was updated successfully, but these errors were encountered: 
👍6
tballenger, jfurie, iteration, KShah707, tmancill, and NN1985 reacted with thumbs up emoji
All reactions
👍6 reactions"
"https://github.com/openai/openai-python/issues/554","ChatCompletion doesn't handle enum with None","2023-09-06T12:09:35Z","Closed issue","API-feedback","Describe the bug
if for example the enum for the function looks like this:
enums = ['Life cycle', 'App developer', 'Games reporting', 'User', None]
 which is later used in the functions call:
...
""parameters"": {
                ""type"": ""object"",
                ""properties"": {
                    ""parent"": {
                        ""type"": ""string"",
                        ""enum"": enums,
                    },
...

the case with None is not handled and server responds with the 500
Logs:
""outputs"": [
    {
     ""ename"": ""APIError"",
     ""evalue"": ""The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID cdee293f61742a79122d04d81a0b9465 in your email.) {\n  \""error\"": {\n    \""message\"": \""The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the **request ID cdee293f61742a79122d04d81a0b9465** in your email.)\"",\n    \""type\"": \""server_error\"",\n    \""param\"": null,\n    \""code\"": null\n  }\n}\n 500 {'error': {'message': 'The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID cdee293f61742a79122d04d81a0b9465 in your email.)',
...

request ID cdee293f61742a79122d04d81a0b9465
To Reproduce
create an enum with None as a variable
trigger a function call
Code snippets
functions = [
        {
            ""name"": ""get_relevant_action"",
            ""description"": """",
            ""parameters"": {
                ""type"": ""object"",
                ""properties"": {
                    ""parent"": {
                        ""type"": ""string"",
                        ""enum"": ['Life cycle', 'App developer', 'Games reporting', 'User', None],
                        ""description"": ""parent text of the button to click"",
                    }
                },
                ""required"": [""parent""],
            },
        }
    ]

response = openai.ChatCompletion.create(
    model=""gpt-4"",
    messages=messages,
    functions=functions,
    function_call={""name"": ""get_relevant_action""},
)
response_message = response[""choices""][0][""message""]
OS
macos
Python version
3.10
Library version
v0.27.8
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/551","Type hinting / stub files","2023-11-03T22:27:55Z","Closed issue","fixed in v1","Describe the feature or improvement you're requesting
I'm new to the openai-python library, and I'm really excited to start working on some cool projects with it! 🚀 However, I noticed that the library's type hinting could be improved to make it even better.
I'd like to kindly request some enhancements in the type hinting for the library's methods. Having more comprehensive type annotations would greatly help developers like me avoid bugs and make our code more readable and maintainable.
If possible, it would be awesome if the maintainers could consider adding these improvements. I believe it would enhance the overall developer experience and make the library even more delightful to use.
Additionally, if anyone in the community has already created some useful stub files, sharing them would be incredibly helpful! It's like having a secret guide to navigate the code with ease.
Thank you for your attention, and I really appreciate all the effort put into making this library fantastic.
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
👍18
krcm0209, reidg44, failable, nielsuit227, nicognaW, yyueniao, JoshJarabek7, nmaquet, Thalmann, kubernegit, and 8 more reacted with thumbs up emoji
All reactions
👍18 reactions"
"https://github.com/openai/openai-python/issues/549","timeout paramter is not respected in openai.ChatCompletion.create method","2023-09-06T12:13:03Z","Closed issue","No label","Describe the bug
openai.ChatCompletion.create takes timeout parameter but the call might take significantly longer without timing out.
I think it is because of this code:
timeout = kwargs.pop(""timeout"", None)

while True:
    try:
        return super().create(*args, **kwargs)
Wouldn't it be more consistent overall if you replace pop with get, like this:
timeout = kwargs.get(""timeout"", None)
To Reproduce
run
 openai.ChatCompletion.create(timeout=1, the rest of parameters...)
expected the call either complete under 1 second or fail.
actual: it can take more than 1 second.
Code snippets
No response
OS
Windows 11
Python version
Python 3.9.13
Library version
openai 0.27.8
 The text was updated successfully, but these errors were encountered: 
👍2
bioerrorlog and borisdayma reacted with thumbs up emoji
All reactions
👍2 reactions"
"https://github.com/openai/openai-python/issues/547","Custom backend URL support","2023-11-06T17:13:53Z","Closed issue","enhancement,fixed in v1","Describe the feature or improvement you're requesting
Currently, the URL for an OpenAI completion (openai.Completion.create) is predefined to support 'azure' and 'openai' backends.
Would it be possible to have a custom api_type (openai.api_type = ""custom"") where the URL would then be possible to define while creating the OpenAI completion engine?
Additional context
The feature would probably be implemented at https://github.com/openai/openai-python/blob/v0.27.8/openai/api_resources/abstract/engine_api_resource.py#L22 (method `class_url´)
 The text was updated successfully, but these errors were encountered: 
👍1
benjamin-kirkbride reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/openai-python/issues/545","Split out the core library","2023-09-06T12:14:51Z","Closed issue","No label","Describe the feature or improvement you're requesting
Would it be possible to split out the core functionality, essentially, the API wrapper, in a separate openai-core library with minimal dependencies? The advantage is precisely that - fewer dependencies and a smaller codebase for 99.9+% of the users.
If the other stuff, like the cli and the fine-tuning code, stays in openai, the users will not requires any changes.
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
👍1
Yaakov-Belch reacted with thumbs up emoji❤️1
Yaakov-Belch reacted with heart emoji
All reactions
👍1 reaction
❤️1 reaction"
"https://github.com/openai/openai-python/issues/544","Continue generating on stop_reason: length","2023-09-06T12:16:13Z","Closed issue","No label","Describe the feature or improvement you're requesting
Is there a way to continue generating without backfeeding the entire message everytime?
Additional context
Atm my code is doing this which I feel is very wasteful
import sys
import json
import openai
from dotenv import load_dotenv
import os

def recursive_chat(messages):
    chat_completion = openai.ChatCompletion.create(
        model=""gpt-3.5-turbo"",
        messages=messages,
        max_tokens=140
    )
    response_message = chat_completion['choices'][0]['message']
    print(response_message)
    messages[0]['content'] += ' ' + response_message['content']  # append the assistant's message to the user's message

    if chat_completion['choices'][0]['finish_reason'] == 'stop':
        return messages
    else:
        return recursive_chat(messages)

# load .env file
load_dotenv()
print(sys.argv)
prompt = sys.argv[1] if len(sys.argv) > 1 else ""2+2""
key = sys.argv[2] if len(sys.argv) > 2 else os.getenv('OPENAI_API_KEY')
# Load your API key from an environment variable or secret management service
openai.api_key = key

messages = [{""role"": ""user"", ""content"": ""how could openai fail?""}]

final_messages = recursive_chat(messages)
print(json.dumps(final_messages))



 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/543","API using gpt-3.5-turbo-16k-0613 even though gpt-3.5-turbo-16k specified","2023-07-17T16:33:10Z","Closed issue","bug","Describe the bug
I am attempting to use ChatCompletion on on the API with gpt-3.5-turbo-16k as the model, but all of the responses I am getting back are for gpt-3.5-turbo-16k-0613 instead.
Here is my code snippet:
def get_chat_competion_response(document):
    response = openai.ChatCompletion.create(
        model='gpt-3.5-turbo-16k',
        messages=[
            {'role': 'system', 'content': ExtractionPreamble},
            {'role': 'user', 'content': ExtractionPromptTemplate.format(text=document)},
        ],
        temperature=0,
    )
    return response

get_chat_competion_response(""test"")

Response:
<OpenAIObject chat.completion id=chatcmpl-7dJuVWETUsqnwQFB19VgkkYvnKjPB at 0x10604fe20> JSON: {
  ""id"": ""chatcmpl-7dJuVWETUsqnwQFB19VgkkYvnKjPB"",
  ""object"": ""chat.completion"",
  ""created"": 1689605683,
  ""model"": ""gpt-3.5-turbo-16k-0613"",
  ""choices"": [
    {
      ""index"": 0,
      ""message"": {
        ""role"": ""assistant"",
        ""content"": ""[]""
      },
      ""finish_reason"": ""stop""
    }
  ],
  ""usage"": {
    ""prompt_tokens"": 341,
    ""completion_tokens"": 1,
    ""total_tokens"": 342
  }
}

To Reproduce
Run the code below:
def get_chat_completion_response(document):
    response = openai.ChatCompletion.create(
        model='gpt-3.5-turbo-16k',
        messages=[
            {'role': 'system', 'content': ExtractionPreamble},
            {'role': 'user', 'content': ExtractionPromptTemplate.format(text=document)},
        ],
        temperature=0,
    )
    return response

get_chat_completion_response(""test"")

Code snippets
def get_chat_completion_response(document):
    response = openai.ChatCompletion.create(
        model='gpt-3.5-turbo-16k',
        messages=[
            {'role': 'system', 'content': ExtractionPreamble},
            {'role': 'user', 'content': ExtractionPromptTemplate.format(text=document)},
        ],
        temperature=0,
    )
    return response

get_chat_completion_response(""test"")


### OS

MacOS Monterey

### Python version

Python 3.10.12

### Library version

openai==0.27.8

 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/542","Tried querying the ""text-davinci-003"" model via an API key and got this error APIConnectionError","2023-09-06T12:16:49Z","Closed issue","bug","Describe the bug
APIConnectionError: Error communicating with OpenAI: HTTPSConnectionPool(host='api.openai.com', port=443): Max retries exceeded with url: /v1/engines/text-davinci-003/completions (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x14ea861b14e0>: Failed to establish a new connection: [Errno -2] Name or service not known'))
To Reproduce
import openai
Use the API key for authentication
openai.api_key = ""sk-XXXXXXXXXXXXXXXX""
Define the model to use
model_engine = ""text-davinci-003""
Define the prompt to use as input
prompt = ""Are all Indians into tech?""
Request a completion from the model
completions = openai.Completion.create(
 engine=model_engine,
 prompt=prompt,
 max_tokens=1024,
 n=1,
 stop=None,
 temperature=0.5,
 )
Get the first response from the completions
message = completions.choices[0].text
Print the response
print(message)
Code snippets
No response
OS
macOS
Python version
Python v3.7.1
Library version
openai-python v0.26.4
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/541","support 3rd party gateway","2023-11-10T04:12:39Z","Closed issue","enhancement","Describe the feature or improvement you're requesting
Ability to route the calls via 3rd party API gateway's such as APIGEE, MuleSoft etc.
 Client application doesn't need to store the actual API-key rather it'll be set at the API Gateway.
Right now it seem to support direct calls to OpenAI or Microsoft gateway (limited)
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/540","x","2023-07-14T08:11:34Z","Closed issue","No label","Describe the feature or improvement you're requesting
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/539","A way to determine if a response is ""not unknown""","2023-09-06T12:17:59Z","Closed issue","API-feedback","Describe the feature or improvement you're requesting
Right now, there doesn't seem to be a way of determining if response from chatgpt is vague / not known without checking the response content. There should be some response code that API devs can check to see if there's a way to escalate the situation to a human agent.
https://stackoverflow.com/questions/76685545/chatgpt-a-way-to-determine-if-a-response-is-not-unknown
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/538","ChatCompletion does not handle functions = None","2023-11-06T16:55:28Z","Closed issue","bug,fixed in v1","Describe the bug
Firstly I noticed how you have integrated the chat_completion_request function into the Python OpenAI package instead of writing it out in code. I have used prior version and the new version but am finding the newer integrated version giving me issues.
Specificaly it seems that it does not handle the case where functions = None very well. There does not appear to be a way to tell 'ChatCompletion' to not run any functions other than giving it a fake function and then setting the function_call flag to 'none'.
Example error:
 error_code=None error_message=""None is not of type 'array' - 'functions'"" error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False
It would be good if it could proces requests when functions is set to none and function_call is set to 'none'.
To Reproduce
Pass in functions=Non and function_call='none' to the code below.
Code snippets
def chat_completion_request(messages, functions=None, function_call=""auto"", model=GPT_MODEL):
    if functions is None: 
        function_call =""none""
   
    r= openai.ChatCompletion.create(
        model=GPT_MODEL,
        messages=messages,
        functions=functions,
        function_call=function_call,  # auto is default, but we'll be explicit
    )
    return r
OS
Windows 11
Python version
Latest
Library version
Latest
 The text was updated successfully, but these errors were encountered: 
👍2
schneiderfelipe and andrewnguonly reacted with thumbs up emoji
All reactions
👍2 reactions"
"https://github.com/openai/openai-python/issues/536","Tests with temporary files or subprocesses are broken on Windows","2023-11-10T04:32:10Z","Closed issue","bug","Describe the bug
Several unit tests rely inadvertently on behavior that varies by operating system, in ways that cause them not to work on Windows. These tests work on other platforms, just not Windows. This is a bug in the tests, not the code under test. I've opened #537 to fix them.
The affected tests are:
File	Test
test_file_cli.py	test_file_cli
test_long_examples_validator.py	test_long_examples_validator
test_util.py	test_openai_api_key_path
test_util.py	test_openai_api_key_path_with_malformed_key
The causes of incompatibility with Windows (some applying to multiple tests) are that, on Windows:
Attempting to open a file created by NamedTemporaryFile while it is already open raises PermissionError. (This is the incompatibility that affects the most tests.)
Giving subprocess.run a multiple-argument command to run using a shell, when done by passing a one-element list whose element is the whole command, gives the cmd.exe shell a command it cannot parse correctly. This shell fails with ""The filename, directory name, or volume label syntax is incorrect."" (Doing it in this specific way was probably unintentional, and the other ways of passing commands through subprocess.run do work.)
The system default encoding is not UTF-8. It is usually CP-1252 (Windows-1252), at least on English-language versions of Windows. This can often be addressed by passing encoding=""utf-8"" where applicable, but as an argument to subprocess.run that can cause (rather than avoid) a UnicodeDecodeError. When the child process is another Python process, its standard streams almost always use the system default encoding.
To Reproduce
Set up a Python virtual environment on Windows for any version of Python this library supports.
In it, install the project by running pip install -e '.[dev,datalib]'. (The datalib extra is needed for test_long_examples_validator, which is skipped if numpy and pandas are unavailable. For the others, it would be sufficient to run pip install -e '.[dev]'.)
Run pytest to run the tests. (To verify exactly what's going on, it may help to run pytest -vv.)
This shows the first two kinds of problems described above; the third is only observable once these problems are fixed (and a fix or workaround has been applied for #535).
Developers without access to a Windows system who want to partially verify the details of this bug can examine this CI output (from a workflow based on tuliren's).
Code snippets
As one example of the first problem, test_util.py uses NamedTemporaryFile in a fixture, api_key_file, that allows it to be used this way:
def test_openai_api_key_path(api_key_file) -> None:
    print(""sk-foo"", file=api_key_file)
    api_key_file.flush()
    assert util.default_api_key() == ""sk-foo""
However, the code under test separately attempts to open that file to read an API key. On Windows, this raises PermissionError.
OS
Windows
Python version
Python 3.7.9, 3.8.10, 3.9.13, 3.10.11, and 3.11.4.
Library version
Tested on b82a3f7 (main branch), though the problem was not introduced recently.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/535","Space start validator crashes on empty completion","2023-11-10T08:20:11Z","Closed issue","bug","Describe the bug
When running openai tools fine_tunes.prepare_data, a completion that is empty, or that becomes empty after a remediation is applied, causes openai.validators.completions_space_start_validator to raise an unhandled IndexError. @tuliren has contributed a fix in #406. I'm opening this issue to give information about the bug and its impact. Merging #406 would fix this bug.
#125 added the test that finds this bug, test_long_examples_validator. The bug is in the code under test (not in the test itself). The bug precedes that pull request.
Since #153, numpy and pandas are not installed by default, and the test is skipped in their absence. Installing with pip install -e '.[dev]' and running pytest does not reveal the bug for this reason, whereas pip install -e '.[dev,datalib]' does show the failure.
I haven't found a commit in the history of the main branch in which the test has been present but able to pass. It looks like it may have originally been run with modified data tailored to the problem in #121 (which #125 fixes), and then stopped failing for many developers once #153 was merged, due to being skipped.
When actually using openai tools fine_tunes.prepare_data, I think it is unusual to include an empty completion or one that would become empty due to a remediation, so I think the user impact of this bug is low. But it seems to me that the ongoing situation where not all tests are able to pass may be confusing to new contributors. (I personally find it confusing.) Fortunately, the fix in #406 is straightforward.
To Reproduce
On GNU/Linux or macOS, prepare an environment using Python 3.7, 3.8, 3.9, 3.10, or 3.11. (Although I've manually verified that the bug affects Windows, and test_long_examples_validator does not pass on Windows, other issues obscure this one.) I tested this locally on Ubuntu 22.04 with all those versions of Python, and on CI with ubuntu-latest and macos-latest using a workflow derived from tuliren's.
In that environment, create an editable install, including at least the datalib extra, by running:
pip install '.[dev,datalib]'
Run: pytest
 Or, for the whole traceback, run: pytest -vv
 Or, for the whole traceback and just that one test, run: pytest -vvk test_long_examples_validator
Code snippets
x[0] raises IndexError when x is empty in:
lambda x: ("""" if x[0] == "" "" else "" "") + x
This appears in the add_space_start local function, in completions_space_start_validator, in openai/validators.py.
OS
All (tested mainly in Ubuntu 22.04.2 and macOS 12.6.7).
Python version
Python 3.7.17, 3.8.17, 3.9.17, 3.10.12, and 3.11.4.
Library version
Tested on b82a3f7 (main branch), though a range of versions are affected.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/534","web app block","2023-09-06T12:27:49Z","Closed issue","bug","Describe the bug
I use the Openai package in the Flask program, but the app sometimes blocks, and nothing output in console.
After KeyboardInterrupt:
    return super().create(*args, **kwargs)
  File ""/data/python_group/py38/lib/python3.8/site-packages/openai/api_resources/abstract/engine_api_resource.py"", line 153, in create
    response, _, api_key = requestor.request(
  File ""/data/python_group/py38/lib/python3.8/site-packages/openai/api_requestor.py"", line 288, in request
    result = self.request_raw(
  File ""/data/python_group/py38/lib/python3.8/site-packages/openai/api_requestor.py"", line 596, in request_raw
    result = _thread_context.session.request(
  File ""/data/python_group/py38/lib/python3.8/site-packages/requests/sessions.py"", line 587, in request
    resp = self.send(prep, **send_kwargs)
  File ""/data/python_group/py38/lib/python3.8/site-packages/requests/sessions.py"", line 701, in send
    r = adapter.send(request, **kwargs)
  File ""/data/python_group/py38/lib/python3.8/site-packages/requests/adapters.py"", line 489, in send
    resp = conn.urlopen(
  File ""/data/python_group/py38/lib/python3.8/site-packages/urllib3/connectionpool.py"", line 703, in urlopen
    httplib_response = self._make_request(
  File ""/data/python_group/py38/lib/python3.8/site-packages/urllib3/connectionpool.py"", line 449, in _make_request
    six.raise_from(e, None)
  File ""<string>"", line 3, in raise_from
  File ""/data/python_group/py38/lib/python3.8/site-packages/urllib3/connectionpool.py"", line 444, in _make_request
    httplib_response = conn.getresponse()
  File ""/usr/local/python3/lib/python3.8/http/client.py"", line 1322, in getresponse
    response.begin()
  File ""/usr/local/python3/lib/python3.8/http/client.py"", line 303, in begin
    version, status, reason = self._read_status()
  File ""/usr/local/python3/lib/python3.8/http/client.py"", line 264, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), ""iso-8859-1"")
  File ""/usr/local/python3/lib/python3.8/socket.py"", line 669, in readinto
    return self._sock.recv_into(b)
  File ""/usr/local/python3/lib/python3.8/ssl.py"", line 1241, in recv_into
    return self.read(nbytes, buffer)
  File ""/usr/local/python3/lib/python3.8/ssl.py"", line 1099, in read
    return self._sslobj.read(len, buffer)
KeyboardInterrupt
To Reproduce
here is my flask code:
# -*- coding:utf-8 _*-

import loggingimport timefrom logging import handlers

import tiktokenfrom flask import Flask, render_template, request, jsonifyimport openaifrom gevent.pywsgi import WSGIServer

# Setup logginglogger = logging.getLogger('MainProgram')
logger.setLevel(10)
logHandler = handlers.RotatingFileHandler('flask.log', maxBytes=10000000, backupCount=10)
logger.addHandler(logHandler)
logger.info(f'{time.strftime(""%Y-%m-%d %H:%M:%S"")} Logging configuration done')
app = Flask(__name__)
openai.api_key = ''openai.api_base = ''openai.api_type = 'azure'openai.api_version = ''deployment_name = ''encoding = tiktoken.get_encoding(""cl100k_base"")
max_len = 8192

@app.route('/')def index():
    return render_template('home.html')


@app.route('/chat', methods=['POST'])def handle_message():
    data = request.get_json()
    message = data['message']
    messages = [{'role': 'system',
                 'content': f'You are ChatGPT, a large language model trained by OpenAI.\nKnowledge cutoff: 2021-09\n'
                            f'Current date:  {time.strftime(""%Y-%m-%d"", time.localtime())}'},
                *message]
    input_len = sum(len(encoding.encode(msg['content'])) for msg in messages)
    if max_len - input_len < 0:
        raise Exception('input too long')
    response = openai.ChatCompletion.create(
        engine=deployment_name,
        # model=""gpt-3.5-turbo"",
        messages=messages,
        temperature=1,
        max_tokens=max_len - input_len
    )

    result = ''
    for choice in response.choices:
        result += choice.message.content
    return jsonify({'result': result})


if __name__ == '__main__':
    http_server = WSGIServer(('0.0.0.0', 8095), app, log=logger)
    http_server.serve_forever()
Code snippets
No response
OS
centos and windows10
Python version
pythonv3.8.0
Library version
0.27.8
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/533","Add option to use client certificates.","2023-09-06T12:28:23Z","Closed issue","Azure","We want to secure our Azure OpenAI access with client certificates. To do this we need the option to add one to ""requests"".
I do not see an option to do this. If waned I can provide a PR.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/531","Invalid URL (POST /v1/engines/gpt-3.5-turbo/chat/completions)","2023-07-11T18:14:00Z","Closed issue","bug","Describe the bug
I am trying to use chat completions in python with this openai library, but keep hitting the same error.
 I am using this documentation: API Guides and API docs
I see the url is different than the one listed in the API docs, so maybe there is an issue with this library?
Thank you,
 Michael
To Reproduce
import openaiopenai.api_key = os.environ[""OPENAI_API_KEY""]
model_engine = ""gpt-3.5-turbo"" # Or gpt-4response = openai.ChatCompletion.create(
    engine=model_engine,
    messages=[
        {""role"": ""system"", ""content"":""<my setup message>""},
        {""role"": ""user"", ""content"": ""<my prompt>""}
    ],
    max_tokens=256,
    temperature=0,
)
Error:
openai.error.InvalidRequestError: Invalid URL (POST /v1/engines/gpt-3.5-turbo/chat/completions)

Code snippets
No response
OS
macOS
Python version
Python v3.8.13
Library version
openai-python v0.27.8
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/530","openai.aiohttp_session creates a new aiohttp.ClientSession despite configured one!","2023-10-18T10:11:15Z","Closed issue","bug,fixed in v1","Describe the bug
There is a way to set a custom aiohttp.ClientSession for all library's requests.
It is described here https://github.com/openai/openai-python#async-api
We need to use openai.aiosessionContextVar object and set a session for it. Quite easy isn't is?
But what happens next if you look under the hood: the internal contextmanager openai.aiohttp_session tries to get our custom set session and always gets None and creates a new aiohttp.ClientSession
To Reproduce
Configure openai.aiosession with a new ClientSession, like this https://github.com/openai/openai-python#async-api
 for example in my FastAPI application
   import openai
   from fastapi import FastAPI
   from aiohttp import ClientSession
   from contextlib import asynccontextmanager
  
   @asynccontextmanager
   async def lifespan(app: FastAPI):
       openai.aiosession.set(ClientSession())
       yield
       await openai.aiosession.get().close()

Either add prints to the openai.aiohttp_session contextmanager or set a debug breakpoint.
Make a new chatgpt Completion.acreate request (for example)
This way you can discover that openai.aiohttp_session contextmanager cannot retrive out set session (None on 646 line)

 and creates a new one for every new request.
Code snippets
No response
OS
Linux Ubuntu
Python version
3.11
Library version
0.27.4
 The text was updated successfully, but these errors were encountered: 
👍3
ted537, labdmitriy, and ShantanuNair reacted with thumbs up emoji👀1
ShantanuNair reacted with eyes emoji
All reactions
👍3 reactions
👀1 reaction"
"https://github.com/openai/openai-python/issues/529","GPT-4 Model Does Not Exist","2023-07-17T05:10:18Z","Closed issue","bug","Describe the bug
The official says that the ChatGPT4 API has been opened. I have been paying the API with 3.5 before. In order to use the ChatGPT4 model API, I upgraded the Plus, but why I still can't use the API, prompting the GPT-4 Model Does Not Exist
To Reproduce
import openai
 import os
 openai.api_key = """"
 completion = openai.ChatCompletion.create(
 model=""gpt-4"",
 messages=[
 {
 ""role"": ""user"",
 ""content"": ""Tell the world abount the ChatGPT API"",
 }
 ]
 )
 print(completion.choices[0].message.content)
openai.error.InvalidRequestError: The model: gpt-4 does not exist
Code snippets
No response
OS
windows
Python version
Python v3.11.4
Library version
openai-python v0.27.2
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/528","Audio API Resource is Missing Timeout Handling","2023-10-18T10:12:35Z","Closed issue","bug,fixed in v1","Describe the feature or improvement you're requesting
Audio transcription can be time intensive (and in my case also somewhat time critical). In certain cases the OpenAI API is taking 20+ seconds to process audio and with the currently library we have no way of timing out the call, potentially leaving a worker or thread hanging until resolved ungracefully (such as an overall network response timeout or worker timeout). As is this forces users to either reimplement the call in requests to get access to network timeouts or re-implement the timeout logic on top of this method call via threads or subprocesses.
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/525","Import time >1s when running with pandas","2023-10-18T09:58:38Z","Closed issue","bug,fixed in v1","Describe the bug
See import time when installing openai[datalib] due to pandas load time.
To Reproduce
Install openai and compare import time with and without [datalib] extension installed (pandas).
Code snippets
In local 4s


import timet1 = time.perf_counter()
from openai.datalib import pandas_helperprint(time.perf_counter() - t1, pandas_helper.pandas)

t1 = time.perf_counter()
try:
    pandas_helper.assert_has_pandas()
except pandas_helper.MissingDependencyError:
    passprint(time.perf_counter() - t1, pandas_helper.pandas)

t1 = time.perf_counter()
try:
    pandas_helper.assert_has_pandas()
except pandas_helper.MissingDependencyError:
    passprint(time.perf_counter() - t1, pandas_helper.pandas)
4.230538249947131 <module 'pandas' from '/Users/pedrovicente/.virtualenvs/openai-python-uihn/lib/python3.10/site-packages/pandas/__init__.py'>
6.669433787465096e-07 <module 'pandas' from '/Users/pedrovicente/.virtualenvs/openai-python-uihn/lib/python3.10/site-packages/pandas/__init__.py'>
2.919696271419525e-07 <module 'pandas' from '/Users/pedrovicente/.virtualenvs/openai-python-uihn/lib/python3.10/site-packages/pandas/__init__.py'>

OS
Linux
Python version
python 3.10
Library version
openai-python v0.27.2
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/522","There is an error in the README","2023-09-06T13:56:07Z","Closed issue","bug","Describe the bug

 This error is in the final output and should read print(chat_completion.choices[0].message.content) instead of print(completion.choices[0].message.content)
To Reproduce
Code snippets
No response
OS
macos
Python version
3.9
Library version
v26.4
 The text was updated successfully, but these errors were encountered: 
👍1
EliahKagan reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/openai-python/issues/519","Passing large datasets to Embedding causes an error","2023-11-10T04:05:35Z","Closed issue","bug","Describe the bug
See the bug report originally filed in chroma-core/chroma#709
To Reproduce
See the bug report originally filed in chroma-core/chroma#709
Code snippets
No response
OS
macOS
Python version
Python v3.11.4
Library version
openai==0.27.8
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/518","ClientError vs ServerError","2023-11-03T22:29:02Z","Closed issue","fixed in v1","Describe the feature or improvement you're requesting
Generally, it's a good idea to separate error responsibilities between client and server, just as HTTP status code represents with 400-499 (client error) vs 500-599 (server error).
This library has many error types, but they are all subclass of OpenAIError which makes it hard to handle exceptions at a higher level.
For instance, we'd swallow ServiceUnavailableError on the UI and let the user retry so that devs won't be bothered with the error notification on this. But we'd want to get notified upon InvalidRequestError because it's a bug in our codebase.
If we had ServerError and ClientError, we'd write some code like this:
try:
    response = openai.ChatCompletion.create(**params)
except openai.error.ClientError as e:
    # This is our bug, notify and fix it!
    raise eexcept openai.error.ServerError:
    # OpenAI seems to be busy
    # Do not raise error and prompt the user to retry later
I also see tenacity.RetryError for embeddings from time to time, so it would be great if it's handled in the same semantics.
Additional context
The semantics here is:
If we face a client error (403, 404, etc.), we want to raise the exception and report the error (often via tools like Sentry) so that devs can fix it. Do NOT retry with the same set of parameters because it will fail until you fix the bug.
If we face a server error (502, 503, etc.), the issue is usually temporary and retry would fix by itself.
Group those errors into one of the two parent classes, ServerError and ClientError.
class OpenAIError(Exception):
class APIError(OpenAIError):
class TryAgain(OpenAIError):
class Timeout(OpenAIError):
class APIConnectionError(OpenAIError):
class InvalidRequestError(OpenAIError):
class AuthenticationError(OpenAIError):
class PermissionError(OpenAIError):
class RateLimitError(OpenAIError):
class ServiceUnavailableError(OpenAIError):
class InvalidAPIType(OpenAIError):
class SignatureVerificationError(OpenAIError):
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/517","Azure API does not support functions in chat?","2023-08-08T09:14:16Z","Closed issue","bug","Describe the bug
New functions parameter in ChatCompletion is not recognized when calling MS Azure endpoints. I got this error when I pass in my functions:
  File ""call_chatgpt.py"", line 52, in chatgpt
    chat_completion = openai.ChatCompletion.create(
  File ""/root/anaconda3/lib/python3.8/site-packages/openai/api_resources/chat_completion.py"", line 25, in create
    return super().create(*args, **kwargs)
  File ""/root/anaconda3/lib/python3.8/site-packages/openai/api_resources/abstract/engine_api_resource.py"", line 153, in create
    response, _, api_key = requestor.request(
  File ""/root/anaconda3/lib/python3.8/site-packages/openai/api_requestor.py"", line 298, in request
    resp, got_stream = self._interpret_response(result, stream)
  File ""/root/anaconda3/lib/python3.8/site-packages/openai/api_requestor.py"", line 700, in _interpret_response
    self._interpret_response_line(
  File ""/root/anaconda3/lib/python3.8/site-packages/openai/api_requestor.py"", line 763, in _interpret_response_line
    raise self.handle_error_response(
openai.error.InvalidRequestError: Unrecognized request argument supplied: functions

To Reproduce
run the code snippet
get the error
Code snippets
import openai

AZURE_KEY = ""<key here>""AZURE_ENDPOINT = ""<endpoint here>""

messages = [{""role"": ""user"", ""content"": ""What's the weather like in Boston?""}]
functions = [
    {
        ""name"": ""get_current_weather"",
        ""description"": ""Get the current weather in a given location"",
        ""parameters"": {
            ""type"": ""object"",
            ""properties"": {
                ""location"": {
                    ""type"": ""string"",
                    ""description"": ""The city and state, e.g. San Francisco, CA"",
                },
                ""unit"": {""type"": ""string"", ""enum"": [""celsius"", ""fahrenheit""]},
            },
            ""required"": [""location""],
        },
    }
]
openai.api_type = ""azure""openai.api_key = AZURE_KEYopenai.api_base = AZURE_ENDPOINTopenai.api_version = ""2023-05-15""

chat_completion = openai.ChatCompletion.create(
    engine=""gpt-35-turbo"",
    messages=messages,
    functions=functions
)
OS
linux
Python version
python 3.8
Library version
v0.27.8
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/516","command line for fine tune unable to pick api type","2024-03-03T01:01:37Z","Closed as not planned issue","bug","Describe the bug
openai [-h] [-V] [-v] [-b API_BASE] [-k API_KEY] [-p PROXY [PROXY ...]] [-o ORGANIZATION] {api,tools,wandb}
we have setup the environment variables for api_key, api_base, version and api_type
 and we are trying to run fine tune model using the commands provided in open_ai_cookbook
 we are facing an error of
 Error: The API type provided in invalid. Please select one of the supported API types: 'azure', 'azure_ad', 'open_ai'
To Reproduce
%env OPENAI_API_TYPE
 %env OPENAI_API_VERSION
 %env OPENAI_API_KEY
 %env OPENAI_API_BASE
we are trying to run the following commands on cloud platform
!openai api fine_tunes.create -t ""/FileStore/train.json"" -v ""/FileStore/test.json"" --classification_metric ""accuracy"" --classification_labels ""yes,no"" --model ""text-davinci-003"" --max_iterations 100 --save_model ""test_results""
we have also tried
%sh /local_disk0/envs/pythonEnv/bin/openai --verbose --api-key <api_key> -api-base <api_base> api fine_tunes.create -t /FileStore/train.json -v /FileStore/test.json --compute_classification_metrics --classification_positive_class 
Error: The API type provided in invalid. Please select one of the supported API types: 'azure', 'azure_ad', 'open_ai'
Code snippets
No response
OS
Linux
Python version
3.9
Library version
'0.27.8'
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/515","Audio: ASR automatically infers the wrong language.","2023-09-07T14:36:18Z","Closed issue","API-feedback","Describe the bug
openai.Audio.transcribe(model= 'whisper-1',file = wavfile, options = {""language"" : ""en"",})
openai whisper automatically transcribes and translates input voice to a different language. Specifying the language parameter does nothing.
Nor does adding a prefix of [en] solve the problem. It appears that accents and environmental noise is capable of influencing the language inference model.
From the openai.Audio.transcribe() function, accepted parameters are 'model', 'file', 'apikey', 'apibase', 'apitype', 'apiversion', 'organization', and 'kwargs'. There is no parameter to set the language.
Would be nice for a parameter to force the ASR to a language if the auto-inference model is not reliable.
To Reproduce
tmp.write(audio.get_wav_data())
tmp.seek(0)  # Reset file pointer to beginning
whisp  = openai.Audio.transcribe(model= 'whisper-1',file =tmp, options = {""language"" : ""en"",})
user_input = ""[en] "" + whisp['text']

print(f'\nYou said:\n""{user_input}""')

messages.append(       {""role"": ""user"", ""content"": user_input}     )

print(""\nResponding...\n"")
      
completion = openai.ChatCompletion.create(
        model=MODEL,
        messages=messages,
        temperature=0.8
      )
created  = str( get_timestamp_from_unix( completion['created'] ) )
response = completion['choices'][0].message.content
messages.append({""role"": ""assistant"", ""content"": f'Created: {created}\n{response}'}) 
print(f""\n{created}\n{response}\n"")

You said:
""[en] Jadi, perkara ini masih tidak berfungsi. Saya telah menambahkan sufis bahasa Inggeris dan untuk sebab tertentu, ia masih memberi balik transkripsi yang diterjemahkan oleh Malaysia. Jadi, saya yakin ada masalah dengan masalah transkripsi dari 
OpenAI Whisper.""

Responding...


2023-07-03 15:31:47
Created: 2023-07-03 15:31:42
Maaf atas ketidaknyamanannya. Terkadang, ada kesalahan dalam transkripsi bahasa yang dapat menyebabkan masalah seperti yang 
Anda alami. Saya akan mencatat masalah ini dan melaporkannya kepada tim teknis untuk diperiksa. Apakah ada pertanyaan lain yang dapat saya bantu jawab?

Code snippets
No response
OS
Windows
Python version
Python 3.8.3
Library version
openai==0.27.7
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/513","Feature Request: return dollar amount for OpenAI request response","2023-11-10T04:39:50Z","Closed issue","No label","Describe the feature or improvement you're requesting
Add the dollar amount to the response, or a separate function to calculate the same.
Additional context
The openai library returns the number of tokens with each response.
 However, the pricing keeps changing. Is there a way you can add the dollar amount to that response, or a separate function for the same?
Thanks
 The text was updated successfully, but these errors were encountered: 
👍2
endolith and EliahKagan reacted with thumbs up emoji
All reactions
👍2 reactions"
"https://github.com/openai/openai-python/issues/511","Error when importing embeddings_utils","2023-11-06T16:38:21Z","Closed issue","bug,fixed in v1","Describe the bug
ImportError: cannot import name 'ARRAY_FUNCTIONS' from 'numpy.core.overrides' (/home/ygg/.cache/pypoetry/virtualenvs/stakeholderstudy-shsA1FjX-py3.10/lib/python3.10/site-packages/numpy/core/overrides.py)

openai==0.27.8                                                                                                                             
numpy==1.25.0

To Reproduce
poetry add openai
from openai import embeddings_utils
Code snippets
No response
OS
Pop!_OS 22.04 LTS x86_64
Python version
Python 3.10.6
Library version
openai==0.27.8
 The text was updated successfully, but these errors were encountered: 
👍4
doomspec, FrightenedFox, ligiamelo, and Youngtae23 reacted with thumbs up emoji
All reactions
👍4 reactions"
"https://github.com/openai/openai-python/issues/509","Audio: Error ""Resource not found"" as api_version is ignored","2023-07-06T00:50:01Z","Closed issue","bug","Describe the bug
OpenAI Audio API take api_key, api_base, api_type, api_version, organization as aparameters. Most of these parameters were ignored until the PR #369. However, this PR is incomplete as it ignored api_version, organization. As a result, we still get ""Resource not found"" error.
Fix is to ensure api_version, organization parameter are not ignored.
To Reproduce
Was using both OpenAI APIs and Auzre OpenAI APIs with in same module
Was using Chat completions from Azure OpenAI and Audio Whisper transcribe from OpenAI
The parameter overrides is supposed to allow such parallel usage of it, but ignorance of api_version, organization hinders it
Code snippets
No response
OS
Ubuntu 22.04.2 LTS (of Windows 11 WSL)
Python version
Python v3.9.14
Library version
openai-python v0.27.6
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/506","ChatML update for th gpt-3.5-turbo-0613 and gpt-4","2023-12-02T21:38:23Z","Closed issue","No label","Describe the feature or improvement you're requesting
The chatml.md document said every message is represented as
<|im_start|>{role}\n{text}<|im_end|>\n

That would add 5 more tokens beside the text itself.
This is correct for the gpt-3.5-turbo-0301 model. As the gpt-3.5-turbo-0613 and gpt-4 models, it seems that every message is represneted as the following form
<|im_start|>{role}\n{text}<|im_end|>

For eample
>>> openai.ChatCompletion.create(
... model=""gpt-3.5-turbo-0613"",
... messages=[{""role"":""user"", ""content"":""hello""}])
<OpenAIObject chat.completion id=chatcmpl-7WK2PVLogo1vxUXDAgApf9JWbXsET at 0x1da6e756210> JSON: {
  ""id"": ""chatcmpl-7WK2PVLogo1vxUXDAgApf9JWbXsET"",
  ""object"": ""chat.completion"",
  ""created"": 1687937877,
  ""model"": ""gpt-3.5-turbo-0613"",
  ""choices"": [
    {
      ""index"": 0,
      ""message"": {
        ""role"": ""assistant"",
        ""content"": ""Hello! How can I assist you today?""
      },
      ""finish_reason"": ""stop""
    }
  ],
  ""usage"": {
    ""prompt_tokens"": 8,
    ""completion_tokens"": 9,
    ""total_tokens"": 17
  }
}

>>> encoder.encode(""hello"")
[15339]
Accorsing to the response, the prompt_tokens is 8, if we follow the rule described in the chatml.md, it's represened as
<|im_start|>user\nhello<|im_end|>\n<|im_start|>assistant\n

That would be 9 tokens. My guess is that there's no need for the '\n' after the <|im_end|> token.
How can I make sure for that? Thanks.
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/505","import openai Error source code string cannot contain null bytes","2023-10-18T07:47:59Z","Closed issue","bug,fixed in v1","Describe the bug
ValueError: source code string cannot contain null bytes
To Reproduce
import openai
 Traceback (most recent call last):
 File """", line 1, in 
 File ""C:\Program Files\Python310\lib\site-packages\openai_init_.py"", line 15, in 
 import aiohttp
 File ""C:\Users\dadash\AppData\Roaming\Python\Python310\site-packages\aiohttp_init_.py"", line 6, in 
 from .client import (
 File ""C:\Users\dadash\AppData\Roaming\Python\Python310\site-packages\aiohttp\client.py"", line 67, in 
 from .connector import (
 File ""C:\Users\dadash\AppData\Roaming\Python\Python310\site-packages\aiohttp\connector.py"", line 58, in 
 from .resolver import DefaultResolver
 File ""C:\Users\dadash\AppData\Roaming\Python\Python310\site-packages\aiohttp\resolver.py"", line 11, in 
 import aiodns
 File ""C:\Users\dadash\AppData\Roaming\Python\Python310\site-packages\aiodns_init_.py"", line 4, in 
 import pycares
 File ""C:\Users\dadash\AppData\Roaming\Python\Python310\site-packages\pycares_init_.py"", line 8, in 
 from . import errno
 ValueError: source code string cannot contain null bytes
Code snippets
No response
OS
windows
Python version
3.10.10
Library version
0.27.8
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/504","openai.error.AuthenticationError: Incorrect API key provided","2023-10-18T07:51:45Z","Closed issue","bug","Describe the bug
Error occurs here:
https://github.com/grumpyp/aixplora/blob/main/backend/loaders/audio_loader.py
When coming to the Whisper part.
Key works fine on other API calls.
To Reproduce
https://github.com/grumpyp/aixplora/blob/main/backend/loaders/audio_loader.py
Code snippets
No response
OS
macOS
Python version
3.11.2
Library version
0.27.7
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/503","RuntimeError: aclose(): asynchronous generator is already running","2023-10-18T10:06:57Z","Closed issue","bug,fixed in v1","Describe the bug
I see this error after an async streaming call to the completion end-point:
an error occurred during closing of asynchronous generator <async_generator object aiohttp_session at 0x1029de110>
asyncgen: <async_generator object aiohttp_session at 0x1029de110>
RuntimeError: aclose(): asynchronous generator is already running

To Reproduce
Run the code below. The completion_text is printed and below that the error above is shown.
Code snippets
import asynciofrom types import AsyncGeneratorTypefrom dotenv import load_dotenvimport osimport openai

load_dotenv()

openai.api_key = os.getenv(""OPENAI_API_KEY"")

async def test_completion_async_stream():
    completion = await openai.Completion.acreate(
        model=""text-davinci-003"",
        prompt=""Hello!"",
        stream=True,
    )

    # Assert response body
    assert isinstance(completion, AsyncGeneratorType)

    collected_chunks = []
    completion_text = ''
    # iterate through the stream, if it breaks, the test failed
    async for chunk in completion:
        collected_chunks.append(chunk)
        finish_reason = chunk['choices'][0]['finish_reason'] # type: ignore
        if finish_reason is not None:
            break
        chunk_text = chunk['choices'][0]['text'] # type: ignore
        completion_text += chunk_text  # append the text
    print(completion_text)

if __name__ == ""__main__"":
    asyncio.run(test_completion_async_stream())
OS
macOS 13.4 (22F66)
Python version
3.11.4
Library version
0.27.8
 The text was updated successfully, but these errors were encountered: 
👍1
mvsoom reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/openai-python/issues/500","M1 issue","2023-11-10T04:00:35Z","Closed issue","bug,fixed in v1","Describe the bug
Same issue closed here, but I can't resolved in any way:
#338
To Reproduce
pip install openai[datalib]
zsh: no matches found: openai[datalib]
 michele@MacBook-Pro-di-michele model % pip install openai""[datalib]""
Defaulting to user installation because normal site-packages is not writeable
 Requirement already satisfied: openai[datalib] in /Users/michele/Library/Python/3.8/lib/python/site-packages (0.27.8)
 Requirement already satisfied: requests>=2.20 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from openai[datalib]) (2.30.0)
 Requirement already satisfied: tqdm in /Users/michele/Library/Python/3.8/lib/python/site-packages (from openai[datalib]) (4.65.0)
 Requirement already satisfied: aiohttp in /Users/michele/Library/Python/3.8/lib/python/site-packages (from openai[datalib]) (3.8.4)
 Requirement already satisfied: numpy in /Users/michele/Library/Python/3.8/lib/python/site-packages (from openai[datalib]) (1.24.3)
 Requirement already satisfied: pandas>=1.2.3 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from openai[datalib]) (2.0.2)
 Requirement already satisfied: pandas-stubs>=1.1.0.11 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from openai[datalib]) (2.0.2.230605)
 Requirement already satisfied: openpyxl>=3.0.7 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from openai[datalib]) (3.1.2)
 Requirement already satisfied: et-xmlfile in /Users/michele/Library/Python/3.8/lib/python/site-packages (from openpyxl>=3.0.7->openai[datalib]) (1.1.0)
 Requirement already satisfied: python-dateutil>=2.8.2 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from pandas>=1.2.3->openai[datalib]) (2.8.2)
 Requirement already satisfied: pytz>=2020.1 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from pandas>=1.2.3->openai[datalib]) (2023.3)
 Requirement already satisfied: tzdata>=2022.1 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from pandas>=1.2.3->openai[datalib]) (2023.3)
 Requirement already satisfied: types-pytz>=2022.1.1 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from pandas-stubs>=1.1.0.11->openai[datalib]) (2023.3.0.0)
 Requirement already satisfied: charset-normalizer<4,>=2 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from requests>=2.20->openai[datalib]) (3.1.0)
 Requirement already satisfied: idna<4,>=2.5 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from requests>=2.20->openai[datalib]) (3.4)
 Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from requests>=2.20->openai[datalib]) (2.0.2)
 Requirement already satisfied: certifi>=2017.4.17 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from requests>=2.20->openai[datalib]) (2023.5.7)
 Requirement already satisfied: attrs>=17.3.0 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from aiohttp->openai[datalib]) (23.1.0)
 Requirement already satisfied: multidict<7.0,>=4.5 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from aiohttp->openai[datalib]) (6.0.4)
 Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from aiohttp->openai[datalib]) (4.0.2)
 Requirement already satisfied: yarl<2.0,>=1.0 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from aiohttp->openai[datalib]) (1.9.2)
 Requirement already satisfied: frozenlist>=1.1.1 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from aiohttp->openai[datalib]) (1.3.3)
 Requirement already satisfied: aiosignal>=1.1.2 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from aiohttp->openai[datalib]) (1.3.1)
 Requirement already satisfied: six>=1.5 in /Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas>=1.2.3->openai[datalib]) (1.15.0)
 michele@MacBook-Pro-di-michele model % pip3 install pandas
Defaulting to user installation because normal site-packages is not writeable
 Requirement already satisfied: pandas in /Users/michele/Library/Python/3.8/lib/python/site-packages (2.0.2)
 Requirement already satisfied: python-dateutil>=2.8.2 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from pandas) (2.8.2)
 Requirement already satisfied: pytz>=2020.1 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from pandas) (2023.3)
 Requirement already satisfied: tzdata>=2022.1 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from pandas) (2023.3)
 Requirement already satisfied: numpy>=1.20.3 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from pandas) (1.24.3)
 Requirement already satisfied: six>=1.5 in /Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas) (1.15.0)
 michele@MacBook-Pro-di-michele model % openai tools fine_tunes.prepare_data -f training_data.jsonl
 Analyzing...
 Traceback (most recent call last):
 File ""/opt/homebrew/bin/openai"", line 8, in 
 sys.exit(main())
 File ""/opt/homebrew/lib/python3.9/site-packages/openai/_openai_scripts.py"", line 78, in main
 args.func(args)
 File ""/opt/homebrew/lib/python3.9/site-packages/openai/cli.py"", line 594, in prepare_data
 df, remediation = read_any_format(fname)
 File ""/opt/homebrew/lib/python3.9/site-packages/openai/validators.py"", line 481, in read_any_format
 assert_has_pandas()
 File ""/opt/homebrew/lib/python3.9/site-packages/openai/datalib/pandas_helper.py"", line 15, in assert_has_pandas
 raise MissingDependencyError(PANDAS_INSTRUCTIONS)
 openai.datalib.common.MissingDependencyError:
OpenAI error:
missing `pandas`

This feature requires additional dependencies:
$ pip install openai[datalib]

michele@MacBook-Pro-di-michele model %
michele@MacBook-Pro-di-michele model % pip3 install openai [datalib]
 Defaulting to user installation because normal site-packages is not writeable
 ERROR: Invalid requirement: '[datalib]'
michele@MacBook-Pro-di-michele model % pip3 install openai [datalib]
 michele@MacBook-Pro-di-michele model %
 michele@MacBook-Pro-di-michele model % pip install openai[datalib]
 Defaulting to user installation because normal site-packages is not writeable
 Requirement already satisfied: openai[datalib] in /Users/michele/Library/Python/3.8/lib/python/site-packages (0.27.8)
 Requirement already satisfied: requests>=2.20 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from openai[datalib]) (2.30.0)
 Requirement already satisfied: tqdm in /Users/michele/Library/Python/3.8/lib/python/site-packages (from openai[datalib]) (4.65.0)
 Requirement already satisfied: aiohttp in /Users/michele/Library/Python/3.8/lib/python/site-packages (from openai[datalib]) (3.8.4)
 Requirement already satisfied: numpy in /Users/michele/Library/Python/3.8/lib/python/site-packages (from openai[datalib]) (1.24.3)
 Requirement already satisfied: pandas>=1.2.3 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from openai[datalib]) (2.0.2)
 Requirement already satisfied: pandas-stubs>=1.1.0.11 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from openai[datalib]) (2.0.2.230605)
 Requirement already satisfied: openpyxl>=3.0.7 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from openai[datalib]) (3.1.2)
 Requirement already satisfied: et-xmlfile in /Users/michele/Library/Python/3.8/lib/python/site-packages (from openpyxl>=3.0.7->openai[datalib]) (1.1.0)
 Requirement already satisfied: python-dateutil>=2.8.2 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from pandas>=1.2.3->openai[datalib]) (2.8.2)
 Requirement already satisfied: pytz>=2020.1 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from pandas>=1.2.3->openai[datalib]) (2023.3)
 Requirement already satisfied: tzdata>=2022.1 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from pandas>=1.2.3->openai[datalib]) (2023.3)
 Requirement already satisfied: types-pytz>=2022.1.1 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from pandas-stubs>=1.1.0.11->openai[datalib]) (2023.3.0.0)
 Requirement already satisfied: charset-normalizer<4,>=2 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from requests>=2.20->openai[datalib]) (3.1.0)
 Requirement already satisfied: idna<4,>=2.5 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from requests>=2.20->openai[datalib]) (3.4)
 Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from requests>=2.20->openai[datalib]) (2.0.2)
 Requirement already satisfied: certifi>=2017.4.17 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from requests>=2.20->openai[datalib]) (2023.5.7)
 Requirement already satisfied: attrs>=17.3.0 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from aiohttp->openai[datalib]) (23.1.0)
 Requirement already satisfied: multidict<7.0,>=4.5 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from aiohttp->openai[datalib]) (6.0.4)
 Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from aiohttp->openai[datalib]) (4.0.2)
 Requirement already satisfied: yarl<2.0,>=1.0 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from aiohttp->openai[datalib]) (1.9.2)
 Requirement already satisfied: frozenlist>=1.1.1 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from aiohttp->openai[datalib]) (1.3.3)
 Requirement already satisfied: aiosignal>=1.1.2 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from aiohttp->openai[datalib]) (1.3.1)
 Requirement already satisfied: six>=1.5 in /Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas>=1.2.3->openai[datalib]) (1.15.0)
 michele@MacBook-Pro-di-michele model % pip3.8 install openai[datalib]
 Defaulting to user installation because normal site-packages is not writeable
 Requirement already satisfied: openai[datalib] in /Users/michele/Library/Python/3.8/lib/python/site-packages (0.27.8)
 Requirement already satisfied: requests>=2.20 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from openai[datalib]) (2.30.0)
 Requirement already satisfied: tqdm in /Users/michele/Library/Python/3.8/lib/python/site-packages (from openai[datalib]) (4.65.0)
 Requirement already satisfied: aiohttp in /Users/michele/Library/Python/3.8/lib/python/site-packages (from openai[datalib]) (3.8.4)
 Requirement already satisfied: numpy in /Users/michele/Library/Python/3.8/lib/python/site-packages (from openai[datalib]) (1.24.3)
 Requirement already satisfied: pandas>=1.2.3 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from openai[datalib]) (2.0.2)
 Requirement already satisfied: pandas-stubs>=1.1.0.11 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from openai[datalib]) (2.0.2.230605)
 Requirement already satisfied: openpyxl>=3.0.7 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from openai[datalib]) (3.1.2)
 Requirement already satisfied: et-xmlfile in /Users/michele/Library/Python/3.8/lib/python/site-packages (from openpyxl>=3.0.7->openai[datalib]) (1.1.0)
 Requirement already satisfied: python-dateutil>=2.8.2 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from pandas>=1.2.3->openai[datalib]) (2.8.2)
 Requirement already satisfied: pytz>=2020.1 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from pandas>=1.2.3->openai[datalib]) (2023.3)
 Requirement already satisfied: tzdata>=2022.1 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from pandas>=1.2.3->openai[datalib]) (2023.3)
 Requirement already satisfied: types-pytz>=2022.1.1 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from pandas-stubs>=1.1.0.11->openai[datalib]) (2023.3.0.0)
 Requirement already satisfied: charset-normalizer<4,>=2 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from requests>=2.20->openai[datalib]) (3.1.0)
 Requirement already satisfied: idna<4,>=2.5 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from requests>=2.20->openai[datalib]) (3.4)
 Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from requests>=2.20->openai[datalib]) (2.0.2)
 Requirement already satisfied: certifi>=2017.4.17 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from requests>=2.20->openai[datalib]) (2023.5.7)
 Requirement already satisfied: attrs>=17.3.0 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from aiohttp->openai[datalib]) (23.1.0)
 Requirement already satisfied: multidict<7.0,>=4.5 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from aiohttp->openai[datalib]) (6.0.4)
 Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from aiohttp->openai[datalib]) (4.0.2)
 Requirement already satisfied: yarl<2.0,>=1.0 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from aiohttp->openai[datalib]) (1.9.2)
 Requirement already satisfied: frozenlist>=1.1.1 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from aiohttp->openai[datalib]) (1.3.3)
 Requirement already satisfied: aiosignal>=1.1.2 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from aiohttp->openai[datalib]) (1.3.1)
 Requirement already satisfied: six>=1.5 in /Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas>=1.2.3->openai[datalib]) (1.15.0)
 michele@MacBook-Pro-di-michele model % openai tools fine_tunes.prepare_data -f training_data.jsonl
 Analyzing...
 Traceback (most recent call last):
 File ""/opt/homebrew/bin/openai"", line 8, in 
 sys.exit(main())
 File ""/opt/homebrew/lib/python3.9/site-packages/openai/_openai_scripts.py"", line 78, in main
 args.func(args)
 File ""/opt/homebrew/lib/python3.9/site-packages/openai/cli.py"", line 594, in prepare_data
 df, remediation = read_any_format(fname)
 File ""/opt/homebrew/lib/python3.9/site-packages/openai/validators.py"", line 481, in read_any_format
 assert_has_pandas()
 File ""/opt/homebrew/lib/python3.9/site-packages/openai/datalib/pandas_helper.py"", line 15, in assert_has_pandas
 raise MissingDependencyError(PANDAS_INSTRUCTIONS)
 openai.datalib.common.MissingDependencyError:
OpenAI error:
missing `pandas`

This feature requires additional dependencies:
$ pip install openai[datalib]

michele@MacBook-Pro-di-michele model %
Code snippets
pip install openai[datalib]

zsh: no matches found: openai[datalib]
michele@MacBook-Pro-di-michele model % pip install openai""[datalib]""

Defaulting to user installation because normal site-packages is not writeableRequirement already satisfied: openai[datalib] in /Users/michele/Library/Python/3.8/lib/python/site-packages (0.27.8)
Requirement already satisfied: requests>=2.20 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from openai[datalib]) (2.30.0)
Requirement already satisfied: tqdm in /Users/michele/Library/Python/3.8/lib/python/site-packages (from openai[datalib]) (4.65.0)
Requirement already satisfied: aiohttp in /Users/michele/Library/Python/3.8/lib/python/site-packages (from openai[datalib]) (3.8.4)
Requirement already satisfied: numpy in /Users/michele/Library/Python/3.8/lib/python/site-packages (from openai[datalib]) (1.24.3)
Requirement already satisfied: pandas>=1.2.3 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from openai[datalib]) (2.0.2)
Requirement already satisfied: pandas-stubs>=1.1.0.11 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from openai[datalib]) (2.0.2.230605)
Requirement already satisfied: openpyxl>=3.0.7 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from openai[datalib]) (3.1.2)
Requirement already satisfied: et-xmlfile in /Users/michele/Library/Python/3.8/lib/python/site-packages (from openpyxl>=3.0.7->openai[datalib]) (1.1.0)
Requirement already satisfied: python-dateutil>=2.8.2 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from pandas>=1.2.3->openai[datalib]) (2.8.2)
Requirement already satisfied: pytz>=2020.1 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from pandas>=1.2.3->openai[datalib]) (2023.3)
Requirement already satisfied: tzdata>=2022.1 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from pandas>=1.2.3->openai[datalib]) (2023.3)
Requirement already satisfied: types-pytz>=2022.1.1 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from pandas-stubs>=1.1.0.11->openai[datalib]) (2023.3.0.0)
Requirement already satisfied: charset-normalizer<4,>=2 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from requests>=2.20->openai[datalib]) (3.1.0)
Requirement already satisfied: idna<4,>=2.5 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from requests>=2.20->openai[datalib]) (3.4)
Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from requests>=2.20->openai[datalib]) (2.0.2)
Requirement already satisfied: certifi>=2017.4.17 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from requests>=2.20->openai[datalib]) (2023.5.7)
Requirement already satisfied: attrs>=17.3.0 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from aiohttp->openai[datalib]) (23.1.0)
Requirement already satisfied: multidict<7.0,>=4.5 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from aiohttp->openai[datalib]) (6.0.4)
Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from aiohttp->openai[datalib]) (4.0.2)
Requirement already satisfied: yarl<2.0,>=1.0 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from aiohttp->openai[datalib]) (1.9.2)
Requirement already satisfied: frozenlist>=1.1.1 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from aiohttp->openai[datalib]) (1.3.3)
Requirement already satisfied: aiosignal>=1.1.2 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from aiohttp->openai[datalib]) (1.3.1)
Requirement already satisfied: six>=1.5 in /Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas>=1.2.3->openai[datalib]) (1.15.0)
michele@MacBook-Pro-di-michele model % pip3 install pandas

Defaulting to user installation because normal site-packages is not writeableRequirement already satisfied: pandas in /Users/michele/Library/Python/3.8/lib/python/site-packages (2.0.2)
Requirement already satisfied: python-dateutil>=2.8.2 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from pandas) (2.8.2)
Requirement already satisfied: pytz>=2020.1 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from pandas) (2023.3)
Requirement already satisfied: tzdata>=2022.1 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from pandas) (2023.3)
Requirement already satisfied: numpy>=1.20.3 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from pandas) (1.24.3)
Requirement already satisfied: six>=1.5 in /Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas) (1.15.0)
michele@MacBook-Pro-di-michele model % openai tools fine_tunes.prepare_data -f training_data.jsonlAnalyzing...
Traceback (most recent call last):
  File ""/opt/homebrew/bin/openai"", line 8, in <module>
    sys.exit(main())
  File ""/opt/homebrew/lib/python3.9/site-packages/openai/_openai_scripts.py"", line 78, in main
    args.func(args)
  File ""/opt/homebrew/lib/python3.9/site-packages/openai/cli.py"", line 594, in prepare_data
    df, remediation = read_any_format(fname)
  File ""/opt/homebrew/lib/python3.9/site-packages/openai/validators.py"", line 481, in read_any_format
    assert_has_pandas()
  File ""/opt/homebrew/lib/python3.9/site-packages/openai/datalib/pandas_helper.py"", line 15, in assert_has_pandas
    raise MissingDependencyError(PANDAS_INSTRUCTIONS)
openai.datalib.common.MissingDependencyError: 

OpenAI error:

    missing `pandas`

This feature requires additional dependencies:

    $ pip install openai[datalib]


michele@MacBook-Pro-di-michele model % 

michele@MacBook-Pro-di-michele model % pip3 install openai \[datalib\]
Defaulting to user installation because normal site-packages is not writeableERROR: Invalid requirement: '[datalib]'


michele@MacBook-Pro-di-michele model % pip3 install openai \[datalib\]
michele@MacBook-Pro-di-michele model % 
michele@MacBook-Pro-di-michele model % pip install openai\[datalib\]
Defaulting to user installation because normal site-packages is not writeableRequirement already satisfied: openai[datalib] in /Users/michele/Library/Python/3.8/lib/python/site-packages (0.27.8)
Requirement already satisfied: requests>=2.20 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from openai[datalib]) (2.30.0)
Requirement already satisfied: tqdm in /Users/michele/Library/Python/3.8/lib/python/site-packages (from openai[datalib]) (4.65.0)
Requirement already satisfied: aiohttp in /Users/michele/Library/Python/3.8/lib/python/site-packages (from openai[datalib]) (3.8.4)
Requirement already satisfied: numpy in /Users/michele/Library/Python/3.8/lib/python/site-packages (from openai[datalib]) (1.24.3)
Requirement already satisfied: pandas>=1.2.3 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from openai[datalib]) (2.0.2)
Requirement already satisfied: pandas-stubs>=1.1.0.11 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from openai[datalib]) (2.0.2.230605)
Requirement already satisfied: openpyxl>=3.0.7 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from openai[datalib]) (3.1.2)
Requirement already satisfied: et-xmlfile in /Users/michele/Library/Python/3.8/lib/python/site-packages (from openpyxl>=3.0.7->openai[datalib]) (1.1.0)
Requirement already satisfied: python-dateutil>=2.8.2 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from pandas>=1.2.3->openai[datalib]) (2.8.2)
Requirement already satisfied: pytz>=2020.1 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from pandas>=1.2.3->openai[datalib]) (2023.3)
Requirement already satisfied: tzdata>=2022.1 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from pandas>=1.2.3->openai[datalib]) (2023.3)
Requirement already satisfied: types-pytz>=2022.1.1 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from pandas-stubs>=1.1.0.11->openai[datalib]) (2023.3.0.0)
Requirement already satisfied: charset-normalizer<4,>=2 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from requests>=2.20->openai[datalib]) (3.1.0)
Requirement already satisfied: idna<4,>=2.5 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from requests>=2.20->openai[datalib]) (3.4)
Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from requests>=2.20->openai[datalib]) (2.0.2)
Requirement already satisfied: certifi>=2017.4.17 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from requests>=2.20->openai[datalib]) (2023.5.7)
Requirement already satisfied: attrs>=17.3.0 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from aiohttp->openai[datalib]) (23.1.0)
Requirement already satisfied: multidict<7.0,>=4.5 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from aiohttp->openai[datalib]) (6.0.4)
Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from aiohttp->openai[datalib]) (4.0.2)
Requirement already satisfied: yarl<2.0,>=1.0 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from aiohttp->openai[datalib]) (1.9.2)
Requirement already satisfied: frozenlist>=1.1.1 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from aiohttp->openai[datalib]) (1.3.3)
Requirement already satisfied: aiosignal>=1.1.2 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from aiohttp->openai[datalib]) (1.3.1)
Requirement already satisfied: six>=1.5 in /Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas>=1.2.3->openai[datalib]) (1.15.0)
michele@MacBook-Pro-di-michele model % pip3.8 install openai\[datalib\]
Defaulting to user installation because normal site-packages is not writeableRequirement already satisfied: openai[datalib] in /Users/michele/Library/Python/3.8/lib/python/site-packages (0.27.8)
Requirement already satisfied: requests>=2.20 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from openai[datalib]) (2.30.0)
Requirement already satisfied: tqdm in /Users/michele/Library/Python/3.8/lib/python/site-packages (from openai[datalib]) (4.65.0)
Requirement already satisfied: aiohttp in /Users/michele/Library/Python/3.8/lib/python/site-packages (from openai[datalib]) (3.8.4)
Requirement already satisfied: numpy in /Users/michele/Library/Python/3.8/lib/python/site-packages (from openai[datalib]) (1.24.3)
Requirement already satisfied: pandas>=1.2.3 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from openai[datalib]) (2.0.2)
Requirement already satisfied: pandas-stubs>=1.1.0.11 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from openai[datalib]) (2.0.2.230605)
Requirement already satisfied: openpyxl>=3.0.7 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from openai[datalib]) (3.1.2)
Requirement already satisfied: et-xmlfile in /Users/michele/Library/Python/3.8/lib/python/site-packages (from openpyxl>=3.0.7->openai[datalib]) (1.1.0)
Requirement already satisfied: python-dateutil>=2.8.2 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from pandas>=1.2.3->openai[datalib]) (2.8.2)
Requirement already satisfied: pytz>=2020.1 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from pandas>=1.2.3->openai[datalib]) (2023.3)
Requirement already satisfied: tzdata>=2022.1 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from pandas>=1.2.3->openai[datalib]) (2023.3)
Requirement already satisfied: types-pytz>=2022.1.1 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from pandas-stubs>=1.1.0.11->openai[datalib]) (2023.3.0.0)
Requirement already satisfied: charset-normalizer<4,>=2 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from requests>=2.20->openai[datalib]) (3.1.0)
Requirement already satisfied: idna<4,>=2.5 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from requests>=2.20->openai[datalib]) (3.4)
Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from requests>=2.20->openai[datalib]) (2.0.2)
Requirement already satisfied: certifi>=2017.4.17 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from requests>=2.20->openai[datalib]) (2023.5.7)
Requirement already satisfied: attrs>=17.3.0 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from aiohttp->openai[datalib]) (23.1.0)
Requirement already satisfied: multidict<7.0,>=4.5 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from aiohttp->openai[datalib]) (6.0.4)
Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from aiohttp->openai[datalib]) (4.0.2)
Requirement already satisfied: yarl<2.0,>=1.0 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from aiohttp->openai[datalib]) (1.9.2)
Requirement already satisfied: frozenlist>=1.1.1 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from aiohttp->openai[datalib]) (1.3.3)
Requirement already satisfied: aiosignal>=1.1.2 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from aiohttp->openai[datalib]) (1.3.1)
Requirement already satisfied: six>=1.5 in /Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas>=1.2.3->openai[datalib]) (1.15.0)
michele@MacBook-Pro-di-michele model % openai tools fine_tunes.prepare_data -f training_data.jsonlAnalyzing...
Traceback (most recent call last):
  File ""/opt/homebrew/bin/openai"", line 8, in <module>
    sys.exit(main())
  File ""/opt/homebrew/lib/python3.9/site-packages/openai/_openai_scripts.py"", line 78, in main
    args.func(args)
  File ""/opt/homebrew/lib/python3.9/site-packages/openai/cli.py"", line 594, in prepare_data
    df, remediation = read_any_format(fname)
  File ""/opt/homebrew/lib/python3.9/site-packages/openai/validators.py"", line 481, in read_any_format
    assert_has_pandas()
  File ""/opt/homebrew/lib/python3.9/site-packages/openai/datalib/pandas_helper.py"", line 15, in assert_has_pandas
    raise MissingDependencyError(PANDAS_INSTRUCTIONS)
openai.datalib.common.MissingDependencyError: 

OpenAI error:

    missing `pandas`

This feature requires additional dependencies:

    $ pip install openai[datalib]


michele@MacBook-Pro-di-michele model %
OS
MacOS Big Sur 11.7.7 (20G1345)
Python version
Python 3.8
Library version
openai 0.27.2
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/498","SSE Stream parser expects additional space after colon ""data:""","2023-09-25T00:12:25Z","Closed issue","bug","Describe the bug
In openai-python the SSE parser always check the additional space after ""data:""

openai-python/openai/api_requestor.py
 Line 106 in 041bf5a
	ifline.startswith(b""data: ""): 
But according to the SSE spec ""data: "" is identical to ""data:""
https://html.spec.whatwg.org/multipage/server-sent-events.html#server-sent-events
 `The following stream fires two identical events:
 data:test
data: test
 This is because the space after the colon is ignored if present.`
The use case is some organizations might proxy the response for additional control but not all library put a whitespace after the colon ex. springframework https://github.com/spring-projects/spring-framework/blob/f06cf21341a35f863b327e0bfe2305111f69c468/spring-web/src/main/java/org/springframework/http/codec/ServerSentEventHttpMessageWriter.java#L146
if (data != null) { sb.append(""data:""); }
To Reproduce
Proxy OpenAI API with spring framework
Code snippets
No response
OS
All
Python version
All
Library version
All
 The text was updated successfully, but these errors were encountered: 
👍1
michaelfeil reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/openai-python/issues/494","openai.error.AuthenticationError: Incorrect API key provided: cmdftp@e******.com if .netrc file is present","2023-11-10T04:01:20Z","Closed issue","bug,fixed in v1","Describe the bug
I am trying the example code from https://github.com/openai/openai-python
openai.api_key = os.getenv(""OPENAI_API_KEY"")
        # list models
        models = openai.Model.list()
        
        # print the first model's id
        print(models.data[0].id)
        
        # create a chat completion
        chat_completion = openai.ChatCompletion.create(model=""gpt-3.5-turbo"", messages=[{""role"": ""user"", ""content"": ""Hello world""}])
        
        # print the chat completion
        print(chat_completion.choices[0].message.content)
I have checked my API key with:
curl https://api.openai.com/v1/chat/completions \
    -H ""Content-Type: application/json"" \
    -H ""Authorization: Bearer $OPENAI_API_KEY"" \
    -d '{       ""model"": ""gpt-3.5-turbo"",       ""messages"": [{""role"": ""user"", ""content"": ""Say this is a test!""}],       ""temperature"": 0.7     }'
which returns:
{
  ""id"": ""chatcmpl-7TsXvCYItiGrF46Ljo26yS9BgsZWt"",
  ""object"": ""chat.completion"",
  ""created"": 1687355543,
  ""model"": ""gpt-3.5-turbo-0301"",
  ""choices"": [
    {
      ""index"": 0,
      ""message"": {
        ""role"": ""assistant"",
        ""content"": ""This is a test!""
      },
      ""finish_reason"": ""stop""
    }
  ],
  ""usage"": {
    ""prompt_tokens"": 14,
    ""completion_tokens"": 5,
    ""total_tokens"": 19
  }
}
To Reproduce
macOS Big Sur 11.6.2
Darwin Kernel Version 20.6.0
python --version
Python 3.10.8
create a .netrc file
default
login anonymous
password cmdftp@example.com
save the codesnippet below as testopenai and run
./testopenai
which gives the error message:
openai.error.AuthenticationError: Incorrect API key provided: cmdftp@e******.com. You can find your API key at https://platform.openai.com/account/api-keys.
The payment plan has been setup, the key has been created after setting the plan and the curl command works - so why doesn't the python one? See also https://stackoverflow.com/a/76523513/1497139
Code snippets
#!/bin/bash# WF 2023-06-21if [ ""$OPENAI_API_KEY"" == """" ]
then
  echo ""OPENAI_API_KEY env variable needs to be set to a key see https://platform.openai.com/account/api-keys""
  echo ""export OPENAI_API_KEY=""
  exit 1fi#if [ ""$OPENAI_API_ORG"" == """" ]# then#  echo ""OPENAI_API_ORG env variable needs to be set  see https://platform.openai.com/account/org-settings""#  echo ""export OPENAI_API_ORG=""#  exit 1#fi

## test via curl#viacurl() {
  curl https://api.openai.com/v1/chat/completions \
    -H ""Content-Type: application/json"" \
    -H ""Authorization: Bearer $OPENAI_API_KEY"" \
    -d '{       ""model"": ""gpt-3.5-turbo"",       ""messages"": [{""role"": ""user"", ""content"": ""Say this is a test!""}],
       ""temperature"": 0.7
     }'
}

## test via python#viapython() {
  code=""/tmp/testopenai.py""cat << EOF > $codeimport openaiimport osopenai.api_key = os.getenv('OPENAI_API_KEY')

# list modelsmodels = openai.Model.list()

# print the first model's idprint(models.data[0].id)

# create a chat completionchat_completion = openai.ChatCompletion.create(model=""gpt-3.5-turbo"", messages=[{""role"": ""user"", ""content"": ""Hello world""}])

# print the chat completionprint(chat_completion.choices[0].message.content)
EOFpython $code
}

viacurlviapython
OS
macOS
Python version
Python 3.10
Library version
openai-python 0.27.8
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/493","TimeOut should be better documented in Docs and in API call","2023-09-06T14:49:35Z","Closed issue","No label","Describe the feature or improvement you're requesting
The API call has the timeout argument. It is not documented and not mentioned in the docstring.
    def create(cls, *args, **kwargs):
        """"""        Creates a new chat completion for the provided messages and parameters.        See https://platform.openai.com/docs/api-reference/chat-completions/create        for a list of valid parameters.        """"""
        start = time.time()
        timeout = kwargs.pop(""timeout"", None)
It should be added in the docstring and in the Docs on the API reference.
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/492","Make Decorators a built-in part of the function call functionality","2024-03-03T01:02:49Z","Closed issue","enhancement","Describe the feature or improvement you're requesting
It would be really nice if we could have built-in decorators that abstract away a lot of the unnecessary function-calling boilerplate that causes the code to be more confusing and difficult to use than necessary.
I felt like it took much more time to wrap my head around the value and utility of function calls in the recent API update than was necessary in part because of the boilerplate. Additionally, having a bunch of text describing functions that will eventually no longer be needed or might need to be changed syntactically or for other reasons in future updates seems like it would take a lot of time and effort to refactor in the long term.
So I'd propose that built-in decorators for function calling are at least considered as a future option when granular control over the function call isn't immediately required.
Additional context
I think this project does a great job of creating the functionality that I'm referring to here.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/490","ChatML Update","2023-09-06T13:58:39Z","Closed issue","No label","Describe the feature or improvement you're requesting
I was wondering if we could get an update to the ChatML document about the new function capabilities. I've been writing some code to parse ChatML into messages, and I'd love to be able to add the assistant's function calls and the new function role.
I've been doing some prompting and it seems that function calls are done through something like:
<|im_start|>assistant to=functions.[the function name]<|im_sep|>[the JSON parameters]<|im_end|>

But I'm not sure if this is hallucination on the model's part...
And is the function role simply <|im_start|>function name=[the function name]?
Thanks for the help! Would really appreciate hearing from the team.
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/488","Functions: Arguments are absent when called via ChatCompletion.acreate","2023-06-17T13:39:54Z","Closed issue","bug","Describe the bug
Hi! If you create a chat using acreate, provide functions, up-to-date model, the arguments field is always empty regardless, reproduces every single time. Altho, if you are using simple synchronous create on the same setup, it works perfectly.
To Reproduce
ChatCompletion.acreate
gen = await openai.ChatCompletion.acreate(
    model='gpt-3.5-turbo-0613',
    messages=[{""role"": ""user"", ""content"": ""What's the weather like in Boston?""}],
    stream=True,
    functions=[
        {
            ""name"": ""get_current_weather"",
            ""description"": ""Get the current weather in a given location"",
            ""parameters"": {
                ""type"": ""object"",
                ""properties"": {
                    ""location"": {
                        ""type"": ""string"",
                        ""description"": ""The city and state, e.g. San Francisco, CA"",
                    },
                    ""unit"": {""type"": ""string"", ""enum"": [""celsius"", ""fahrenheit""]},
                },
                ""required"": [""location""],
            },
        }
    ],
    function_call='auto'
)

  resp = gen.__anext__()
  delta = resp.choices[0].delta
  print(f'DELTA:\n{delta}')

OUTPUT (ARGUMENTS ARE ABSENT)
DELTA:
{
  ""role"": ""assistant"",
  ""content"": null,
  ""function_call"": {
    ""name"": ""get_current_weather"",
    ""arguments"": """"
  }
}

ChatCompletion.create
response = openai.ChatCompletion.create(
    model=""gpt-3.5-turbo-0613"",
    messages=[{""role"": ""user"", ""content"": ""What's the weather like in Boston?""}],
    functions=[
        {
            ""name"": ""get_current_weather"",
            ""description"": ""Get the current weather in a given location"",
            ""parameters"": {
                ""type"": ""object"",
                ""properties"": {
                    ""location"": {
                        ""type"": ""string"",
                        ""description"": ""The city and state, e.g. San Francisco, CA"",
                    },
                    ""unit"": {""type"": ""string"", ""enum"": [""celsius"", ""fahrenheit""]},
                },
                ""required"": [""location""],
            },
        }
    ],
    function_call=""auto"",
)

message = response[""choices""][0][""message""]
print(message)

OUTPUT (ARGUMENTS ARE PRESENT)
{
  ""role"": ""assistant"",
  ""content"": null,
  ""function_call"": {
    ""name"": ""get_current_weather"",
    ""arguments"": ""{\n  \""location\"": \""Boston\""\n}""
  }
}

Code snippets
No response
OS
Ubuntu 20.04.5 LTS
Python version
Python 3.9.15
Library version
openai-python v0.27.8
 The text was updated successfully, but these errors were encountered: 
👍2
nc and valaises reacted with thumbs up emoji
All reactions
👍2 reactions"
"https://github.com/openai/openai-python/issues/487","typo: two ""into""","2023-06-25T06:59:12Z","Closed issue","bug","Describe the bug
https://platform.openai.com/docs/api-reference/audio/create?lang=python
Translates audio into into English.
To Reproduce
/
Code snippets
No response
OS
/
Python version
/
Library version
/
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/486","Support for functions","2024-03-03T01:03:42Z","Closed issue","enhancement","Describe the feature or improvement you're requesting
Pythonic support for functions as described in the feature announcement.
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
👍9
lucemia, Ja-sonYun, harupy, gitrc, andreaazzini, bonelli, crizCraig, guyschlider, and schneiderfelipe reacted with thumbs up emoji
All reactions
👍9 reactions"
"https://github.com/openai/openai-python/issues/485","Different token limit error than OpenAI while using Azure OpenAI Service with gpt-3.5-turbo, api version 2023-03-15-preview","2023-09-06T14:50:33Z","Closed issue","Azure","Describe the bug
When I send a request for gpt-3.5-turbo chat completion which is 8218(8138 in messages and 80 in completion) tokens long, I get the error message as InvalidRequestError: This model's maximum context length is 8192 tokens. However, you requested 8218 tokens (8138 in the messages, 80 in the completion). Please reduce the length of the messages or completion. Which is incorrect as the gpt-3.5-turbo model's max token limit is 4096 and the error that used to be present while using OpenAI platform. Also when I try to send a 8191 token long chat completion message I get Azure Content Filtering error.
To Reproduce
Create an Azure Deployment.
Write the openai base code to send a request to Azure OpenAI with the deployment name.
Send a 8218 tokens long chat completion request.
Code snippets
No response
OS
macOS
Python version
Python v3.10
Library version
openai-python-0.27.7
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/484","Different accounts at same time?","2023-09-06T14:51:47Z","Closed issue","No label","Describe the feature or improvement you're requesting
How might I use different accounts/api keys at the same time? For example if I have a company account that I want to use budget for embedding model, and personal account that I want to use budget for chat model?
Is there a way to create two client objects and pass one to each model class?
 The text was updated successfully, but these errors were encountered: 
👍1
Barqawiz reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/openai-python/issues/483","ChatCompletion ignoring model parameter?","2023-09-06T15:05:35Z","Closed issue","bug","Describe the bug
Hello! I am attempting to use this library to query the gpt-3.5-turbo model but when examining the query response, it is returning gpt-3.5-turbo-0301. I am using the command from the API, which defines gpt-3.5-turbo as the model. Is there a bug here, or is there some reason that it is using the -0301 model that I am unaware of? Thank you!
To Reproduce
I made the following request, copied from the README for this project.
chat_completion = openai.ChatCompletion.create(model=""gpt-3.5-turbo"", messages=[{""role"": ""user"", ""content"": ""Hello world""}])
print(chat_completion)

And received the following response:
{
  ""id"": ""chatcmpl-7PUqCAWmRjabhgZDrbtgyOOcA8ZNq"",
  ""object"": ""chat.completion"",
  ""created"": 1686311108,
  ""model"": ""gpt-3.5-turbo-0301"",
  ""usage"": {
    ""prompt_tokens"": 10,
    ""completion_tokens"": 68,
    ""total_tokens"": 78
  },
  ""choices"": [...]
   }

Code snippets
No response
OS
Docker - python:3 base image
Python version
3.11.4
Library version
openai 0.27.8
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/482","How can I create a whl file to be able to use this library directly in snowflake?","2023-09-06T15:06:15Z","Closed issue","No label","Describe the feature or improvement you're requesting
I tried creating a whl file out of the base folder extracted from the zip but I am unable to upload to snowflake as it says it's not a valid binary. Should I make the whl from a specific folder?
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/479","Is there a way to prevent the API from returning the same answers in different outputs of the same HTTP response?","2023-09-06T15:08:32Z","Closed issue","API-feedback","Describe the feature or improvement you're requesting
When I make a request to the OpenAI API (specifically, the chat endpoint) with this package, I expect that different outputs, in the same response, are different. There's no reason for expecting different outputs to be the same - you could just copy them. This problem happens so often and it's so annoying. Even if we increase the temperature, this issue still occurs, i.e. the model may always return the same answer in different outputs. We should never pay for getting the same output in the same HTTP response multiple times. It would be like asking someone (a human) to come up with 5 different descriptions of a product, but 3 of them are repetitions.
So, is there a way to prevent the API from returning the same answer in different outputs? I think the answer is no.
If not, could this be implemented in some way in the backend? It would also be great if this feature allowed us to make the comparison by ignoring cases (some standardized form) or maybe using some kind of similarity metric (but this is more advanced).
Clearly, we need this to avoid making unnecessary calls and spending unnecessary credits.
Prompt engineering doesn't really work in many cases. It's trial-and-error combined with headaches. What we need is the API or the backend to enforce the different outputs to contain different outputs/answers, otherwise, we'd better just remove the possibility to return multiple outputs because, at least, that's less misleading.
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/476","custom prompt answer from model is different than when same question passed directly to model. how?","2023-09-06T15:09:04Z","Closed issue","bug","Describe the bug
custom prompt answer from model is different than when same question passed directly to model. how?
To Reproduce
..
Code snippets
No response
OS
windows
Python version
3.9
Library version
0.27.7
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/474","AuthenticationError: <empty message>","2023-09-06T15:10:38Z","Closed issue","bug","Describe the bug
I am using ChromaDB, I have a collection with the embeddings obtained with the text-embedding-ada-002 model and I want to query it. I already changed the OpenAI Key several times and it does not work.
This is my code (see the image)
To Reproduce
Code snippets
No response
OS
Ubuntu
Python version
Python v3.10
Library version
openai-python v0.27.7
 The text was updated successfully, but these errors were encountered: 
👍1
jinfar reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/openai-python/issues/473","Issue with OpenAI API: Error when including the Particular word “Ideation”","2023-09-06T15:12:36Z","Closed issue","bug","Describe the bug
I am using OpenAI api for our application. I have noticed that when I include the word “Ideation” (with a capital letter ‘I’) in my input text, the API fails to generate a response. However, when the word is in lowercase (‘ideation’), the API giving response as expected.
Input Text: When I send a prompt containing the word “Ïdeation” (capital ‘I’) to the API, the response generation process fails, and I receive no output. It appears that the API encounters an error or gets stuck.
 Lowercase Text: On the other hand, if I use the lowercase form of the word (‘ideation’), the API works as expected, producing the desired response.
 We are unable to find exact cause of this issue. Can you please help us for the same?
 We are using TextDavinci003 and Gpt3.5-Turbo models for generating response.
To Reproduce
I am expecting summary when input text contain word Ideation''
Code snippets
def gen_summary(input_text, model, prompt):
    configure_openai(model)
    text_size = len(input_text.split())
    if text_size <=0:
       input_text = prompt
       
       try: 
            if model != ""Turbo35-0301"":

                response = openai.Completion.create(
                    engine=model,
                    prompt=f"" {prompt} {input_text}"",
                    temperature=0,
                    max_tokens=2000,
                    top_p=1,
                    frequency_penalty=0,
                    presence_penalty=0,
                   
                )
                summary = response[""choices""][0][""text""]
                chargable_tokens= response[""usage""][""prompt_tokens""]
                return summary.replace(""\n"","""").strip(),chargable_tokens
            else:
    
                response = openai.ChatCompletion.create(
                    engine=model,
                    messages=[{""role"": ""user"", ""content"": f""{input_text}\n\n {prompt} ""}],
                    temperature=0,
                    max_tokens=2000,
                    top_p=1,
                    frequency_penalty=0,
                    presence_penalty=0,
                  
                )
                summary = response[""choices""][0][""message""][""content""]
                chargable_tokens= response[""usage""][""prompt_tokens""]
                print(summary)
                return summary,chargable_tokens
       except Exception as e:
            return e 
    else:   
     try: 
            if model != ""Turbo35-0301"":
   
                response = openai.Completion.create(
                    engine=model,
                    prompt=f""{input_text} {prompt} "",
                    temperature=0,
                    max_tokens=2000,
                    top_p=1,
                    frequency_penalty=0,
                    presence_penalty=0,
                  
                )
                summary = response[""choices""][0][""text""]
                chargable_tokens= response[""usage""][""prompt_tokens""]
                return summary.replace(""\n"","""").strip(),chargable_tokens 
           
            else:
                  
                response = openai.ChatCompletion.create(
                    engine=model,
                    messages=[{""role"": ""user"", ""content"": f"" {input_text}\n\n {prompt}""}],
                    temperature=0,
                    max_tokens=2000,
                    top_p=1,
                    frequency_penalty=0,
                    presence_penalty=0,
                   
                )
                summary = response[""choices""][0][""message""][""content""]
                print(summary)
                chargable_tokens= response[""usage""][""prompt_tokens""]
                return summary,chargable_tokens

     except Exception as e:
            return e
OS
Windows10
Python version
Python 3.9.11
Library version
Opena ai==0.27.2
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/472","Issue with OpenAI API: Incompatibility with Specific Word","2023-09-06T15:13:05Z","Closed issue","bug","Describe the bug
We are using OpenAI api for our application. It’s not giving response (which is a summary of particular text) when text includes “Ideation” word, if we remove Ideation word or makes it lower case (ideation) its giving us response. We are unable to find exact cause of this issue. Can you please help us for the same?
 We are using TextDavinci003 and Gpt3.5-Turbo models for generating response.
To Reproduce
We are expecting summary with ""Ideation"" word included in that
Code snippets
def gen_summary(input_text, model, prompt):
    configure_openai(model)
    text_size = len(input_text.split())
    if text_size <=0:
       input_text = prompt
       
       try: 
            if model != ""Turbo35-0301"":

                response = openai.Completion.create(
                    engine=model,
                    prompt=f"" {prompt} {input_text}"",
                    temperature=0,
                    max_tokens=2000,
                    top_p=1,
                    frequency_penalty=0,
                    presence_penalty=0,
                   
                )
                summary = response[""choices""][0][""text""]
                chargable_tokens= response[""usage""][""prompt_tokens""]
                return summary.replace(""\n"","""").strip(),chargable_tokens
            else:
    
                response = openai.ChatCompletion.create(
                    engine=model,
                    messages=[{""role"": ""user"", ""content"": f""{input_text}\n\n {prompt} ""}],
                    temperature=0,
                    max_tokens=2000,
                    top_p=1,
                    frequency_penalty=0,
                    presence_penalty=0,
                  
                )
                summary = response[""choices""][0][""message""][""content""]
                chargable_tokens= response[""usage""][""prompt_tokens""]
                print(summary)
                return summary,chargable_tokens
       except Exception as e:
            return e 
    else:   
     try: 
            if model != ""Turbo35-0301"":
   
                response = openai.Completion.create(
                    engine=model,
                    prompt=f""{input_text} {prompt} "",
                    temperature=0,
                    max_tokens=2000,
                    top_p=1,
                    frequency_penalty=0,
                    presence_penalty=0,
                  
                )
                summary = response[""choices""][0][""text""]
                chargable_tokens= response[""usage""][""prompt_tokens""]
                return summary.replace(""\n"","""").strip(),chargable_tokens 
           
            else:
                  
                response = openai.ChatCompletion.create(
                    engine=model,
                    messages=[{""role"": ""user"", ""content"": f"" {input_text}\n\n {prompt}""}],
                    temperature=0,
                    max_tokens=2000,
                    top_p=1,
                    frequency_penalty=0,
                    presence_penalty=0,
                   
                )
                summary = response[""choices""][0][""message""][""content""]
                print(summary)
                chargable_tokens= response[""usage""][""prompt_tokens""]
                return summary,chargable_tokens

     except Exception as e:
            return e
OS
Windows
Python version
Python 3.10
Library version
Open ai 0.27.2
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/471","Using openai.proxy on Anaconda Jupyter Notebook","2023-09-06T15:14:49Z","Closed issue","bug","Describe the bug
import openai
 openai.api_key = ""XXXX""
 proxies = {
 'http': ""http://xxxx:xxxx/"",
 'https': ""https://xxxx:xxxx/""}
 openai.proxy = proxies
I see this error :
 APIConnectionError: Error communicating with OpenAI: HTTPSConnectionPool(host='api.openai.com', port=443): Max retries exceeded with url: /v1/engines (Caused by ProtocolError('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None)))
To Reproduce
proxies = {
 'http': ""http://xxxx:xxxx/"",
 'https': ""https://xxxx:xxxx/""}
 openai.proxy = proxies
Code snippets
No response
OS
Windows
Python version
Python 3.10.9
Library version
openai 0.27.7
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/470","Unsupported error message in using SDK nad playground","2023-06-12T08:37:08Z","Closed issue","bug","Describe the bug
Code example using readme:
import openaiopenai.api_key = ""my key""

# list modelsmodels = openai.Model.list()

# print the first model's idprint(models.data[0].id)

# create a chat completionchat_completion = openai.ChatCompletion.create(model=""gpt-3.5-turbo"", messages=[{""role"": ""user"", ""content"": ""Hello world""}])

# print the chat completionprint(chat_completion.choices[0].message.content)
The following is the error message
whisper-1
Traceback (most recent call last):
  File ""c:\Users\admin\Desktop\test-code\test.py"", line 25, in <module>
    chat_completion = openai.ChatCompletion.create(model=""gpt-3.5-turbo"", messages=[{""role"": ""user"", ""content"": ""Hello world""}])
  File ""C:\Python310\lib\site-packages\openai\api_resources\chat_completion.py"", line 25, in create
    return super().create(*args, **kwargs)
  File ""C:\Python310\lib\site-packages\openai\api_resources\abstract\engine_api_resource.py"", line 153, in create
    response, _, api_key = requestor.request(
  File ""C:\Python310\lib\site-packages\openai\api_requestor.py"", line 230, in request
    resp, got_stream = self._interpret_response(result, stream)
  File ""C:\Python310\lib\site-packages\openai\api_requestor.py"", line 624, in _interpret_response
    self._interpret_response_line(
  File ""C:\Python310\lib\site-packages\openai\api_requestor.py"", line 687, in _interpret_response_line
    raise self.handle_error_response(
openai.error.InvalidRequestError: This is not a chat model and thus not supported in the v1/chat/completions endpoint. Did you mean to use v1/completions?

To Reproduce
I thought it was a problem with my SDK, so I tried playgroundand it was also an error.
Code snippets
No response
OS
win
Python version
3.10.4
Library version
0.27.7
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/469","Retrial Mode","2023-10-04T19:26:10Z","Closed issue","enhancement,fixed in v1","Describe the feature or improvement you're requesting
Hello, I am currently maintaining AutoGPT and we have a lot of errors that we handle manually on the client side.
Did you consider providing a retrial option to abstract all this away from the user ? Pretty much everyone is building an exponential backoff and adds retrials to catch the dozens of errors OpenAI issues when you query them too much.
 Less time building that = more time spending openai tokens :)
It could be an argument called ""retrials"", you can add it incrementally to the most important methods first and you can also include the exponential backoff in there to avoid abuse.
There is a ton of errors that OpenAI issues and that are not ""real errors"" and should lead to the client retrying.
Examples above.
Additional context
here is one example, this is a 500, but it should definitely be retried:
openai.error.APIError: The server had an error while processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 1aa6b47fe412ec21d7cbeff95babf6d4 in your message.) {
  ""error"": {
    ""message"": ""The server had an error while processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 1aa6b47fe412ec21d7cbeff95babf6d4 in your message.)"",
    ""type"": ""server_error"",
    ""param"": null,
    ""code"": null
  }
}

and there are some 408 as well.
 The text was updated successfully, but these errors were encountered: 
❤️1
logankilpatrick reacted with heart emoji
All reactions
❤️1 reaction"
"https://github.com/openai/openai-python/issues/468","Get error when using async request","2024-03-03T01:03:56Z","Closed as not planned issue","bug","Describe the bug
I got the following error when using openai arequest_raw.
Traceback (most recent call last):
 File ""/home/admin/gpt/curious-cub-server/test_openai.py"", line 31, in arequest_raw
 result = await session.request(**request_kwargs)
 File ""/home/admin/.local/lib/python3.10/site-packages/aiohttp/client.py"", line 536, in _request
 conn = await self._connector.connect(
 File ""/home/admin/.local/lib/python3.10/site-packages/aiohttp/connector.py"", line 540, in connect
 proto = await self._create_connection(req, traces, timeout)
 File ""/home/admin/.local/lib/python3.10/site-packages/aiohttp/connector.py"", line 901, in _create_connection
 _, proto = await self._create_direct_connection(req, traces, timeout)
 File ""/home/admin/.local/lib/python3.10/site-packages/aiohttp/connector.py"", line 1206, in _create_direct_connection
 raise last_exc
 File ""/home/admin/.local/lib/python3.10/site-packages/aiohttp/connector.py"", line 1175, in _create_direct_connection
 transp, proto = await self._wrap_create_connection(
 File ""/home/admin/.local/lib/python3.10/site-packages/aiohttp/connector.py"", line 982, in _wrap_create_connection
 raise ClientConnectorCertificateError(req.connection_key, exc) from exc
 aiohttp.client_exceptions.ClientConnectorCertificateError: Cannot connect to host www.baidu.com:443 ssl:True [SSLCertVerificationError: (1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)')]
To Reproduce
Even i added the following line to fix my issue, i just wanna know if it's due to ssl lib issue.
    ssl_ctx = ssl.create_default_context(cafile=certifi.where())  //add ssl ctx

    try:
        result = await session.request(**request_kwargs, ssl=ssl_ctx) //add ssl=ssl_ctx
        util.log_info(
            ""OpenAI API response"",
            path=abs_url,
            response_code=result.status,
            processing_ms=result.headers.get(""OpenAI-Processing-Ms""),
            request_id=result.headers.get(""X-Request-Id""),
        )
        # Don't read the whole stream for debug logging unless necessary.
        if openai.log == ""debug"":
            util.log_debug(
                ""API response body"", body=result.content, headers=result.headers
            )
        return result
    except (aiohttp.ServerTimeoutError, asyncio.TimeoutError) as e:
        raise error.Timeout(""Request timed out"") from e
    except aiohttp.ClientError as e:
        raise error.APIConnectionError(""Error communicating with OpenAI"") from e

aiohttp version: 3.8.4
Code snippets
No response
OS
CentOS7.9
Python version
Python3.10
Library version
openai-python0.27.7
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/467","Exact Match template","2023-05-26T01:32:33Z","Closed as not planned issue","No label","Describe the feature or improvement you're requesting
The documentation in eval-templates.md describes basic/match.py as Match: any([b.startswith(a) for b in B]) ""[f]or a model completion a and a reference list of correct answers B. This is a poor fit for arithmetic and other algorithmic tasks, where we want the model response to exactly match some ideal answer, i.e., any([b == a for b in B])
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/466","Azure content management exception triggered by charset encoding issues","2024-03-03T01:04:22Z","Closed as not planned issue","bug","Describe the bug
I'm getting exceptions triggered by content management filter at the Azure endpoint, i.e. an InvalidRequestError with fields
error_code: content_filter
error_message: The response was filtered due to the prompt triggering Azure OpenAI?s content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766
The prompt sent is trivial, the only issue is that it's in Spanish and has non-ASCII characters. I'm using gpt-35-turbo and the ChatCompletion method. The prompt is just:
  [{'role': 'user', 'content': 'quiero un móvil con una buena cámara'}]

(meaning I want a mobile with a good camera). This triggers the content filter, which does not make much sense, given that the message is harmless. Furthermore, if I remove the accented character in cámara, i.e.
  [{'role': 'user', 'content': 'quiero un móvil con una buena camara'}]

... then the filter is not triggered, and I get a proper text response. Which makes less sense.
I've dug a bit in the code. The prompt seems to be serialized here:
            if params and not files:
                data = json.dumps(params).encode()
                headers[""Content-Type""] = ""application/json""
Interestingly, if I change the serialization to
            if params and not files:
                data = json.dumps(params, ensure_ascii=False).encode()
                headers[""Content-Type""] = ""application/json""
..then the content management filter is not triggered, even with an accented character, and I get a proper response. Which suggests some kind of unusual behavior at the endpoint, given that it behaves differently with escaped unicode characters than with regular UTF-8 text.
To Reproduce
Peepare a prompt `[{'role': 'user', 'content': 'quiero un móvil con una buena cámara'}]
Send to the ChatCompletion API, with a gpt-35-turbo model
Content Management filter is triggered
Code snippets
No response
OS
Ubuntu
Python version
Python 3.8.10
Library version
openai-python v0.27.7
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/465","Numba error when inference in thread (QThread, pyside6)","2024-03-03T01:04:55Z","Closed as not planned issue","bug","Describe the bug
Doing a chat inference with ""text-davinci-003"" into a QThread hangs the function ""openai.Completion.create"" ""ad infinitum"".
Only one error is reported in console: Numba reports that it cannot be in a thread because the data is not copied from the parent thread.
Finally, when I cancel the request, an exception is fired.
If I change the code to use -only- async, and it works. But I need use threads, because I want avoid locking the main thread.
To Reproduce
class QUERY(QThread):
        def __init__(self, tutor, prompt):
            super().__init__(tutor);
            self.tutor=tutor;
            self.prompt=prompt;

        def query(self, prompt):
            MODEL=""gpt-3.5-turbo"";
            print(""OpenAI: enviando texto a openAI"", flush=True);
            r=openai.ChatCompletion.create(model=MODEL, messages=prompt, stream=False);
            print(""OpenAI: esperando por la respuesta"", flush=True);
            try:
               if ""choices"" in r:
                  if len(r[""choices""])>0 and ""message"" in r[""choices""][0]:
                     if ""content"" in r[""choices""][0][""message""]:
                        response=r[""choices""][0][""message""][""content""];
                        self.tutor.history.append(response);
                        self.tutor.onResponse.emit(response);
                     else:
                        self.tutor.onError.emit(""Ha fallado en: content"",r);
                  else:
                     self.tutor.onError.emit(""Ha fallado en: len choices o message"",r);
               else:
                  self.tutor.onError.emit(""Ha fallado en: choices"",r);

            except:
               print(traceback.format_exc(), flush=True);
               self.tutor.onError.emit(""OpenAI ha fallado!"");

            finally:
               print(""OpenAI: respuesta finalizada"", flush=True);

Code snippets
No response
OS
Linux Ubuntu Jammy
Python version
Python 3.9.16
Library version
openai-python 0.27.0
 numba 0.57
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/464","openai.error.AuthenticationError: <empty message>","2024-03-03T01:05:22Z","Closed issue","bug","Describe the bug
Since yesterday 24.05.2023 around your outage I am facing this errors:
..
  File ""/opt/homebrew/lib/python3.10/site-packages/openai/api_resources/embedding.py"", line 33, in create
    response = super().create(*args, **kwargs)
  File ""/opt/homebrew/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py"", line 153, in create
    response, _, api_key = requestor.request(
  File ""/opt/homebrew/lib/python3.10/site-packages/openai/api_requestor.py"", line 230, in request
    resp, got_stream = self._interpret_response(result, stream)
  File ""/opt/homebrew/lib/python3.10/site-packages/openai/api_requestor.py"", line 624, in _interpret_response
    self._interpret_response_line(
  File ""/opt/homebrew/lib/python3.10/site-packages/openai/api_requestor.py"", line 687, in _interpret_response_line
    raise self.handle_error_response(
openai.error.AuthenticationError: <empty message>

I tried several approaches, thought LangChain would be the problem, thought a vectorstore is it, but now I think it must be something with the API as I rebuilt with several options and it all leads to the same error.
To Reproduce
openai.api_key = OPENAI_KEY or os.getenv(""OPENAI_API_KEY"")

# where texts can be a list of strings like 
texts = [""test"", ""foo""]
for i in texts:
    response = openai.Embedding.create(
        input=i,
        model=""text-embedding-ada-002""
    )
    embeddings = response['data'][0]['embedding']

Code snippets
No response
OS
macOS, m1-chip
Python version
Python 3.10.9
Library version
openai==0.27.7
 The text was updated successfully, but these errors were encountered: 
👍14
NikeNano, tor0405, abdihashii, thisiseshan, mehmetkoca, freddiebarrsmith, hylarucoder, nbro10, khemonb, zhlren, and 4 more reacted with thumbs up emoji
All reactions
👍14 reactions"
"https://github.com/openai/openai-python/issues/463","Questions about using openai libraries and network proxies together","2023-11-06T17:25:33Z","Closed as not planned issue","question","Describe the bug
I have successfully installed the openai-python library. When I copy the following code from README.md, I get an error when trying to run the program.
Code content：
import openaiwith open('.\config\config.json', 'r', encoding='utf-8') as f:
    config = json.load(f)
openai.api_key = config['api-key']
chat_completion = openai.ChatCompletion.create(model=""gpt-3.5-turbo"", messages=[{""role"": ""user"", ""content"": ""Hello world""}])
print(chat_completion)
Note: Due to relevant confidentiality requirements, I save my api-key in my local json file, and after testing, my code can normally read the api-key stored in the json file
Error content：
ERROR: APIConnectionError
Error communicating with OpenAI: HTTPSConnectionPool(host='api.openai.com', port=443): Max retries exceeded with url: /v1/chat/completions (Caused by ProxyError('Unable to connect to proxy', SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:1129)'))))
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='api.openai.com', port=443): Max retries exceeded with url: /v1/chat/completions (Caused by ProxyError('Unable to connect to proxy', SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:1129)'))))

During handling of the above exception, another exception occurred:

requests.exceptions.ProxyError: HTTPSConnectionPool(host='api.openai.com', port=443): Max retries exceeded with url: /v1/chat/completions (Caused by ProxyError('Unable to connect to proxy', SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:1129)'))))

The above exception was the direct cause of the following exception:

  File ""C:\Users\xupei\Desktop\ChatBot(GPT3.5turbo)\main.py"", line 32, in <module>
    chat_completion = openai.ChatCompletion.create(model=""gpt-3.5-turbo"", messages=[{""role"": ""user"", ""content"": ""Hello world""}])
openai.error.APIConnectionError: Error communicating with OpenAI: HTTPSConnectionPool(host='api.openai.com', port=443): Max retries exceeded with url: /v1/chat/completions (Caused by ProxyError('Unable to connect to proxy', SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:1129)'))))

After repeated attempts, I found that it was because I was using a network proxy. So I tried shutting down the network agent and rerun the code. But after a long waiting period, the error was still reported.
Error content：
Error communicating with OpenAI: HTTPSConnectionPool(host='api.openai.com', port=443): Max retries exceeded with url: /v1/chat/completions (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000002ABD10FB130>: Failed to establish a new connection: [WinError 10060] 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。'))
TimeoutError: [WinError 10060] 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。

The above exception was the direct cause of the following exception:

urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x000002ABD10FB130>: Failed to establish a new connection: [WinError 10060] 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。

The above exception was the direct cause of the following exception:

urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='api.openai.com', port=443): Max retries exceeded with url: /v1/chat/completions (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000002ABD10FB130>: Failed to establish a new connection: [WinError 10060] 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。'))

During handling of the above exception, another exception occurred:

requests.exceptions.ConnectionError: HTTPSConnectionPool(host='api.openai.com', port=443): Max retries exceeded with url: /v1/chat/completions (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000002ABD10FB130>: Failed to establish a new connection: [WinError 10060] 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。'))

The above exception was the direct cause of the following exception:

  File ""C:\Users\xupei\Desktop\ChatBot(GPT3.5turbo)\main.py"", line 32, in <module>
    chat_completion = openai.ChatCompletion.create(model=""gpt-3.5-turbo"", messages=[{""role"": ""user"", ""content"": ""Hello world""}])
openai.error.APIConnectionError: Error communicating with OpenAI: HTTPSConnectionPool(host='api.openai.com', port=443): Max retries exceeded with url: /v1/chat/completions (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000002ABD10FB130>: Failed to establish a new connection: [WinError 10060] 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。'))

The connection to the host timed out or failed to connect. This is due to the fact that my country and region cannot connect to
 openai's server without network proxy.
I really hope this problem can be solved and I am looking forward to your reply.
To Reproduce
1.edit code
 code content:
import openaiwith open('.\config\config.json', 'r', encoding='utf-8') as f:
    config = json.load(f)
openai.api_key = config['api-key']
chat_completion = openai.ChatCompletion.create(model=""gpt-3.5-turbo"", messages=[{""role"": ""user"", ""content"": ""Hello world""}])
print(chat_completion)
Note: Due to relevant confidentiality requirements, I save my api-key in my local json file, and after testing, my code can normally read the api-key stored in the json file
 2.run
 3.see error
Code snippets
No response
OS
Windows11
Python version
Python 3.9.10
Library version
openai-python v0.27.6
 The text was updated successfully, but these errors were encountered: 
👍1
MiniSuperDev reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/openai-python/issues/462","openssl dependency conflicts with default macos","2023-10-18T07:45:16Z","Closed issue","bug,fixed in v1","Describe the bug
macOS comes with LibreSSL 2.8.3 by default and urllib3 v2.0, as an openai dependency, only supports OpenSSL 1.1.1+
 Would it be easier to switch to nix for package management or is there a way to reassign the command alias openssl to somehow link to the desired version?
To Reproduce
Obtain system with MacOS 13.0.1
 Install openai per the instructions on the website. https://platform.openai.com/docs/guides/fine-tuning
 As it instructs, run pip install --upgrade openai
 Generate some fine tuning training json file, myfile.json
 Then attempt to run openai tools fine_tunes.prepare_data -f myfile.json
Code snippets
openai tools fine_tunes.prepare_data -f training.json 
Traceback (most recent call last):
  File ""/Users/timothy/Library/Python/3.9/bin/openai"", line 5, in <module>
    from openai._openai_scripts import main
  File ""/Users/timothy/Library/Python/3.9/lib/python/site-packages/openai/__init__.py"", line 19, in <module>
    from openai.api_resources import (
  File ""/Users/timothy/Library/Python/3.9/lib/python/site-packages/openai/api_resources/__init__.py"", line 1, in <module>
    from openai.api_resources.audio import Audio  # noqa: F401
  File ""/Users/timothy/Library/Python/3.9/lib/python/site-packages/openai/api_resources/audio.py"", line 4, in <module>
    from openai import api_requestor, util
  File ""/Users/timothy/Library/Python/3.9/lib/python/site-packages/openai/api_requestor.py"", line 22, in <module>
    import requests
  File ""/Users/timothy/Library/Python/3.9/lib/python/site-packages/requests/__init__.py"", line 43, in <module>
    import urllib3
  File ""/Users/timothy/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py"", line 38, in <module>
    raise ImportError(
ImportError: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with LibreSSL 2.8.3. See: https://github.com/urllib3/urllib3/issues/2168
openssl version
LibreSSL 3.3.6



### OS

13.0.1

### Python version

3.9

### Library version

openai version 0.27.7

 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/461","Finetuning keeps failing over and over after a minute","2023-09-07T15:29:18Z","Closed issue","bug","No description provided.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/460","When changing the version specification, the Azure deployment name cannot be retrieved.","2024-03-03T01:05:40Z","Closed issue","bug","Describe the bug
Until now, I had been referring to the following post to retrieve the Azure deployment name.
import openai

openai.api_type = ""azure""
openai.api_base = ""https://<your-resource>.openai.azure.com/""
openai.api_key = ""<your-key>""
openai.api_version = ""2023-03-15-preview""

deployments = openai.Deployment.list()
print(deployments)

When I set the openai.api_version to the latest 2023-05-15, it stopped functioning properly. Since there are no other changes, I believe it's due to the version. Is there any way to address this issue?
To Reproduce
Here is the code that works without any issues:
import openai

openai.api_type = ""azure""
openai.api_base = ""https://<your-resource>.openai.azure.com/""
openai.api_key = ""<your-key>""
openai.api_version = ""2023-03-15-preview""

deployments = openai.Deployment.list()
print(deployments)

This returns a response where id is your deployment name and model is the model name.
When executing the code by simply switching to openai.api_version = ""2023-05-15"", the following stack trace is returned:
---------------------------------------------------------------------------
InvalidRequestError                       Traceback (most recent call last)
[<ipython-input-20-6ce049a89318>](https://localhost:8080/#) in <cell line: 8>()
      6 openai.api_version = ""2023-05-15""
      7 
----> 8 deployments = openai.Deployment.list()
      9 print(deployments)

4 frames
[/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py](https://localhost:8080/#) in _interpret_response_line(self, rbody, rcode, rheaders, stream)
    685         stream_error = stream and ""error"" in resp.data
    686         if stream_error or not 200 <= rcode < 300:
--> 687             raise self.handle_error_response(
    688                 rbody, rcode, resp.data, rheaders, stream_error=stream_error
    689             )

InvalidRequestError: Resource not found

Code snippets
No response
OS
Google Colab
Python version
3.10.11
Library version
openai-python v0.27.7
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/457","openai.Deployment.Create - deployment name setting not working","2023-11-10T04:30:53Z","Closed issue","bug","Deployment name not setting with python api
I need to create a model deployment in my Azure Open AI workspace, specifically using openai.Deployment.create() python API. There is no clear spec on what needs to be passed in here, so I have not been able to get deployment name to set correctly.
I've tried setting the following params for setting it: name, deployment_name, deployment_id, openai_id, deployment, id
None seem to work - all set a generic default - I'm not sure if this functionality is broken or there is another param name.
openai.Deployment.create(model=completion_params[""model_name""], name=completion_params[""deployment_id""], scale_settings=scale_settings)
Currently, its been setting a random default Id:
Please help here with guidance.
To Reproduce
Run:
scale_settings = {
                ""scale_type"": ""standard"",
            }
openai.Deployment.create(model=""text-chat-davinci-002"", name=""test-deployment-name"", scale_settings=scale_settings)

Check AOAI workspace to see deployment name , and its not set to what I want
Code snippets
No response
OS
Windows
Python version
3.8.16
Library version
openai 0.27.6
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/456","stream completions did not work on async api","2023-05-19T10:54:01Z","Closed issue","bug","Describe the bug
According to How_to_stream_completions
response = openai.ChatCompletion.create(
    model='gpt-3.5-turbo',
    messages=[
        {'role': 'user', 'content': ""What's 1+1? Answer in one word.""}
    ],
    temperature=0,
    stream=True  
)

for chunk in response:
    print(chunk)

we can stream a chat completion via ChatCompletion.create
 but when I use ChatCompletion.acreate , it failed.
TypeError: 'async_generator' object is not iterable

To Reproduce
response = openai.ChatCompletion.acreate(
    model='gpt-3.5-turbo',
    messages=[
        {'role': 'user', 'content': ""What's 1+1? Answer in one word.""}
    ],
    temperature=0,
    stream=True  
)

for chunk in response:
    print(chunk)

Code snippets
response = openai.ChatCompletion.acreate(
    model='gpt-3.5-turbo',
    messages=[
        {'role': 'user', 'content': ""What's 1+1? Answer in one word.""}
    ],
    temperature=0,
    stream=True  
)

for chunk in response:
    print(chunk)


### OS

macOS

### Python version

Python 3.11

### Library version

0.27.2

 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/455","Module 'openai' has no attribute 'ChatCompletion'","2023-10-11T11:05:50Z","Closed as not planned issue","question","Describe the bug
The module doesn't work :(
Error:
 Traceback (most recent call last):
 File ""openai.py"", line 1, in 
 import openai
 File ""C:\Users\SonicandTailsCD\desktop\openai.py"", line 13, in 
 response = openai.ChatCompletion.create(
 AttributeError: module 'openai' has no attribute 'ChatCompletion'
To Reproduce
1.- Create a basic OpenAI script
 2.- See the error!
Code snippets
import openai

openai.organization = *censored*openai.api_key = *censored*

messages = [
    {
        ""role"": ""system"",
        ""content"": ""Hi, ChatGPT! So just to test something, say \""This is a test!\"" please :)""
    }
]

response = openai.ChatCompletion.create(
    engine=""gpt-3.5-turbo"",
    messages=messages,
    max_tokens=2000,
    temperature=0.7,
    top_p=1,
    n=1
)

print(response)
OS
Windows 7 x64 (Multiboot)
Python version
Python v3.8.10
Library version
version 0.27.6
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/454","FineTune.list_events() breaks when stream=True","2023-10-18T10:33:27Z","Closed issue","bug,fixed in v1","Describe the bug
When I tried to list the events for my fine tune job through the Python API with streaming I get a correct response without error. However when I specify stream=True, this results in an exception being raised and crashing my application, even though the response was clearly received. Looking at the traceback, I can see that the response was received but failed to be parsed into JSON by openai.
Here's a traceback for the exception that I see:
Traceback (most recent call last):
  File ""venv_py3105/lib/python3.10/site-packages/openai/api_requestor.py"", line 677, in _interpret_response_line
    data = json.loads(rbody)
  File ""/Users/yun.kim/.pyenv/versions/3.10.5/lib/python3.10/json/__init__.py"", line 346, in loads
    return _default_decoder.decode(s)
  File ""/Users/yun.kim/.pyenv/versions/3.10.5/lib/python3.10/json/decoder.py"", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File ""/Users/yun.kim/.pyenv/versions/3.10.5/lib/python3.10/json/decoder.py"", line 355, in raw_decode
    raise JSONDecodeError(""Expecting value"", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

The above exception was the direct cause of the following exception:

File ""openai_demo.py"", line 216, in fine_tune_list_events
    resp = openai.FineTune.list_events(
  File ""venv_py3105/lib/python3.10/site-packages/openai/api_resources/abstract/nested_resource_class_methods.py"", line 123, in list_nested_resources
    return getattr(cls, resource_request_method)(""get"", url, **params)
  File ""venv_py3105/lib/python3.10/site-packages/openai/api_resources/abstract/nested_resource_class_methods.py"", line 44, in nested_resource_request
    response, _, api_key = requestor.request(
  File ""venv_py3105/lib/python3.10/site-packages/openai/api_requestor.py"", line 230, in request
    resp, got_stream = self._interpret_response(result, stream)
  File ""venv_py3105/lib/python3.10/site-packages/openai/api_requestor.py"", line 624, in _interpret_response
    self._interpret_response_line(
  File ""venv_py3105/lib/python3.10/site-packages/openai/api_requestor.py"", line 680, in _interpret_response_line
    raise error.APIError(
openai.error.APIError: HTTP code 200 from API (data: {""object"": ""fine-tune-event"", ""level"": ""info"", ""message"": ""Created fine-tune: ft-XXXXXXX"", ""created_at"": 1684155377}

data: {""object"": ""fine-tune-event"", ""level"": ""info"", ""message"": ""Fine-tune costs $X.XX"", ""created_at"": 1684155451}

data: {""object"": ""fine-tune-event"", ""level"": ""info"", ""message"": ""Fine-tune enqueued. Queue number: 1"", ""created_at"": 1684155452}

data: {""object"": ""fine-tune-event"", ""level"": ""info"", ""message"": ""Fine-tune is in the queue. Queue number: 0"", ""created_at"": 1684155452}

data: {""object"": ""fine-tune-event"", ""level"": ""info"", ""message"": ""Fine-tune started"", ""created_at"": 1684155503}

data: {""object"": ""fine-tune-event"", ""level"": ""info"", ""message"": ""Completed epoch 1/4"", ""created_at"": 1684155598}

data: {""object"": ""fine-tune-event"", ""level"": ""info"", ""message"": ""Completed epoch 2/4"", ""created_at"": 1684155603}

data: {""object"": ""fine-tune-event"", ""level"": ""info"", ""message"": ""Completed epoch 3/4"", ""created_at"": 1684155607}

data: {""object"": ""fine-tune-event"", ""level"": ""info"", ""message"": ""Completed epoch 4/4"", ""created_at"": 1684155615}

data: {""object"": ""fine-tune-event"", ""level"": ""info"", ""message"": ""Uploaded model: davinci:ft-XXXXXXX-2023-05-15-13-00-56"", ""created_at"": 1684155656}

data: {""object"": ""fine-tune-event"", ""level"": ""info"", ""message"": ""Uploaded result file: file-XXXXXXX"", ""created_at"": 1684155658}

data: {""object"": ""fine-tune-event"", ""level"": ""info"", ""message"": ""Fine-tune succeeded"", ""created_at"": 1684155658}

data: [DONE]

)

To Reproduce
I am using Python 3.10.5 with openai==0.27.6.
Here is my application code:
resp = openai.FineTune.list_events(
  ""ft-XXXXX"",  # fine tune job ID
  api_key=os.getenv(""OPENAI_API_KEY""),
  stream=True
)
Code snippets
No response
OS
macOS
Python version
Python 3.10.5
Library version
v0.27.6
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/453","Regarding continue generation using API","2023-09-07T15:31:27Z","Closed issue","API-feedback","Describe the bug
I have been using OpenAI API key for long time. When I give some question to the model, call the API and it'll return the answer, sometimes it won't return the complete answer due to token issues.
Recently, I saw that ChatGPT has come up with new feature, continue generation, where if i click on that, it is continuing the generation in the same block. Below is the picture,
Is there any way to do that for the API code as well?
To Reproduce
Completion of the answer
Limitation of tokens
Code snippets
import openai

messages = [
    {""role"": ""system"", ""content"": ""You are a kind helpful assistant.""},
]

while True:
    message = input(""User : "")
    if message:
        messages.append(
            {""role"": ""user"", ""content"": message},
        )
        chat = openai.ChatCompletion.create(
            model=""gpt-3.5-turbo"", messages=messages
        )
    
    reply = chat.choices[0].message.content
    print(f""ChatGPT: {reply}"")
    messages.append({""role"": ""assistant"", ""content"": reply})
OS
WindowsOS
Python version
Python 3.8.5
Library version
latest version
 The text was updated successfully, but these errors were encountered: 
👍3
bhaktatejas922, satoooh, and EliahKagan reacted with thumbs up emoji
All reactions
👍3 reactions"
"https://github.com/openai/openai-python/issues/451","The parameter logit_bias is invalid in the ChatCompletion model","2023-11-22T17:51:27Z","Closed issue","needs validation","Describe the bug
The parameter logit_bias is invalid in the ChatCompletion model
To Reproduce

""""""
The parameter logit_bias is invalid in the ChatCompletion model

https://platform.openai.com/tokenizer
Red: 7738
red: 445
 Red: 2297
 red: 2266
""""""

import openai

openai.api_key = ""sk-xxx""

temperature = 0
logit_bias = {7738: -100, 445: -100, 2297: -100, 2266: -100}


# 1. Complete Mode
resp = openai.Completion.create(
 engine=""text-davinci-003"",
 temperature=temperature,
 prompt=""What color is an apple? Answer with one word"",
 logit_bias=logit_bias,
)

# The result is: Green.
print(resp['choices'][0][""text""])


# 2. Chat Mode
resp = openai.ChatCompletion.create(
 model=""gpt-3.5-turbo"",
 messages=[{""role"": ""user"", ""content"": ""What color is an apple? Answer with one word""}],
 temperature=temperature,
 logit_bias=logit_bias
)

# The result is: Red.
print(resp['choices'][0]['message']['content'])

Code snippets
No response
OS
Windows
Python version
3.8.9
Library version
0.27.2
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/448","Retrieve token limit and cost of a model from API","2023-09-07T15:34:33Z","Closed issue","API-feedback","Describe the feature or improvement you're requesting
It should be possible to retrieve the token limit and cost per 1k tokens programmatically from the API instead of needing to look them up on the website and hardcode them into programs.
openai/tiktoken#109
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
👍7
EliahKagan, Yaakov-Belch, logankilpatrick, Hannibal046, dannycunningham-8451, daehuikim, and lewismazzei reacted with thumbs up emoji❤️2
Yaakov-Belch and logankilpatrick reacted with heart emoji
All reactions
👍7 reactions
❤️2 reactions"
"https://github.com/openai/openai-python/issues/447","How to extract model name from Azure deployment name ?","2023-05-11T22:41:16Z","Closed issue","No label","Describe the feature or improvement you're requesting
I'm using Azure OpenAI and I have more than one deployed model of the same type. I'd like to know how can I map a deployed model's name to the corresponding model's name, e.g. 'TestDavinci' ->'text-davinci-003'
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/445","How to set the context in jsonl file for fine tuning","2023-09-07T15:36:58Z","Closed issue","No label","Describe the feature or improvement you're requesting
I need to fine-tune an ada model.
 I have the instruction set which describes the task to perform. The instruction set contains 8-10 commands which are the rules to be followed in the inference time.
 Along with these commands, I have some examples to feed the system as a few shots.
So while finetuning I can put the few shot examples in the JSONL file in the following format
{""prompt"":<example 1>"", ""completion"":<example1 response>}
{""prompt"":<example 2>"", ""completion"":<example2 response>}

But where should i keep the instruction set/rules to follow/command set/prompt while finetuning?
 Is it possible to keep this also in the JSONL file?
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/444","Moderation endpoint is missing a timeout/request_timeout parameter","2023-11-06T17:42:53Z","Closed issue","bug,fixed in v1","Describe the feature or improvement you're requesting
Just like any other endpoint, a timeout is very much needed for the moderation endpoint.
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/443","import openai failure under specific folder","2023-05-09T09:33:33Z","Closed issue","bug","Describe the bug
Well, I am trying to use GPT this afternoon and all of a sudden, I can't import openai and gives the following error.
I tried reinstall but didn't work. Along with the error, some previous feeded text (by previous I mean, when this library works, I used it to conduct text completion and feed some text to the api) are also automatically printed again.
To Reproduce
import openai
Code snippets
Traceback (most recent call last):
  File ""test.py"", line 1, in <module>
    import openai
  File ""/opt/miniconda3/envs/transformers/lib/python3.8/site-packages/openai/__init__.py"", line 15, in <module>
    import aiohttp
  File ""/opt/miniconda3/envs/transformers/lib/python3.8/site-packages/aiohttp/__init__.py"", line 6, in <module>
    from .client import (
  File ""/opt/miniconda3/envs/transformers/lib/python3.8/site-packages/aiohttp/client.py"", line 3, in <module>
    import asyncio
  File ""/opt/miniconda3/envs/transformers/lib/python3.8/asyncio/__init__.py"", line 8, in <module>
    from .base_events import *
  File ""/opt/miniconda3/envs/transformers/lib/python3.8/asyncio/base_events.py"", line 23, in <module>
    import socket
  File ""/opt/miniconda3/envs/transformers/lib/python3.8/socket.py"", line 52, in <module>
    import os, sys, io, selectors
  File ""/opt/miniconda3/envs/transformers/lib/python3.8/selectors.py"", line 290, in <module>
    class SelectSelector(_BaseSelectorImpl):
  File ""/opt/miniconda3/envs/transformers/lib/python3.8/selectors.py"", line 317, in SelectSelector
    _select = select.selectAttributeError: module 'select' has no attribute 'select'
OS
macOS
Python version
python 3.8
Library version
openai-python-0.27.6
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/442","Cannot install openai package due to SSL error","2023-05-17T01:00:24Z","Closed issue","bug","Describe the bug
Description
I'm trying to install the openai package using pip, but I'm encountering an SSL error.
To Reproduce
Steps to reproduce
Run pip install openai
Expected behavior
The openai package should be installed without any issues.
Actual behavior
The following SSL error occurs:
 Title: Cannot install openai package due to SSL error
Description
I'm trying to install the openai package using pip, but I'm encountering an SSL error.
Steps to reproduce
Run pip install openai
Expected behavior
The openai package should be installed without any issues.
Actual behavior
The following SSL error occurs:
WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:997)'))': /simple/openai/
 WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:997)'))': /simple/openai/
 WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:997)'))': /simple/openai/
 WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:997)'))': /simple/openai/
 WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:997)'))': /simple/openai/
 Could not fetch URL https://pypi.org/simple/openai/: There was a problem confirming the ssl certificate: HTTPSConnectionPool(host='pypi.org', port=443): Max retries exceeded with url: /simple/openai/ (Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:997)'))) - skipping
 ERROR: Could not find a version that satisfies the requirement openai (from versions: none)
 ERROR: No matching distribution found for openai
 Could not fetch URL https://pypi.org/simple/pip/: There was a problem confirming the ssl certificate: HTTPSConnectionPool(host='pypi.org', port=443): Max retries exceeded with url: /simple/pip/ (Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:997)'))) - skipping
Code snippets
No response
OS
windows
Python version
python3.10
Library version
openai
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/436","What is the default value of max_token argument in ChatCompletion API","2023-10-15T01:06:51Z","Closed issue","bug","Describe the bug
What is the default value of max_token argument in ChatCompletion API
To Reproduce
What is the default value of max_token argument in ChatCompletion API
Code snippets
No response
OS
Ubuntu
Python version
Python3.9
Library version
openai==0.27.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/435","Pandas UDF Failure","2023-11-10T03:37:31Z","Closed issue","bug","Describe the bug
When trying to create a PANDAS UDF to run the latest GPT3.5Turbo API via Azure openAI completion function it gives an error regarding missing key.
To Reproduce
Code snippets
#!pip install openai

from pyspark.sql import SparkSession
import openai
from pyspark.sql.functions import pandas_udf  
from pyspark.sql.types import StringType
import pandas as pd
import os

# Bootstrap Spark Session
spark = SparkSession.builder.getOrCreate()

service_name = ""oai-eus-ww-int-hqfin""
deployment_name = ""GPT35Model""

os.environ[""OPENAI_API_KEY""] = """"

# Setup for OpenAI API for GPT3.5Turbo
openai.api_type = ""azure""
openai.api_base = """"
openai.api_version = ""2023-03-15-preview""
openai.api_key = os.environ.get(""OPENAI_API_KEY"")

# Create a Spark DataFrame with a prompt column
data = [  
    (""Python code for Hello World""),  
    (""What is GZCL?""),
    (""Who is Sandman?"")  
]  
df = spark.createDataFrame(pd.DataFrame(data, columns=[""prompt""]))

# Define the pandas UDF function  
@pandas_udf(returnType=StringType())
def generate_text(input: pd.Series) -> pd.Series:  
    results = []
    prompt = """"
    for n in input:  
        message = prompt + n  
          
        responseGPT = openai.ChatCompletion.create(  
            engine=""GPT35Model"",  
            messages=[{  
                ""role"": ""user"",  
                ""content"": message  
            }],  
            temperature=0.1,  
            max_tokens=800,  
            top_p=0.95,  
            frequency_penalty=0,  
            presence_penalty=0,  
            stop=None  
        )  
          
        results.append(responseGPT[""choices""][0][""message""][""content""])  
      
    return pd.Series(results)

df = df.withColumn(""results"", generate_text(""prompt"")).cache()
display(df)

OS
linux
Python version
Python v3.10
Library version
openai-python v0.27.6
 The text was updated successfully, but these errors were encountered: 
👍2
promisinganuj and sreedhar-guda reacted with thumbs up emoji
All reactions
👍2 reactions"
"https://github.com/openai/openai-python/issues/434","Return finish_reason early in streaming multi choice ChatCompletion","2024-03-03T00:31:20Z","Closed issue","No label","Describe the feature or improvement you're requesting
Return finish_reason early in streaming multi choice(n > 1) ChatCompletion.
Additional context
Messages containing finish_reason for different choices are sent at the end of streaming request. However, different choices can be completed at different times, and it would be very useful to have finish_reason to determine if a choice is completed early.
This may not be related to Python library, and possibly must be fixed in the API implementation.
 The text was updated successfully, but these errors were encountered: 
👍1
JackWH reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/openai-python/issues/433","openai.error.InvalidRequestError: Unrecognized request argument supplied: logprobs","2024-03-03T00:44:34Z","Closed issue","bug","Describe the bug
I need the logprobs value from the API.
The code I tried
response = openai.ChatCompletion.create(
            model='gpt-3.5-turbo',
            messages=[{""role"": ""user"", ""content"": ""Who won the world series in 2020?""}],
            max_tokens=193,
            temperature=0,
            logprobs=1
        )

but it shows the error
openai.error.InvalidRequestError: Unrecognized request argument supplied: logprobs
To Reproduce
I need the logprobs value from the API.
The code I tried
response = openai.ChatCompletion.create(
            model='gpt-3.5-turbo',
            messages=[{""role"": ""user"", ""content"": ""Who won the world series in 2020?""}],
            max_tokens=193,
            temperature=0,
            logprobs=1
        )

but it shows the error
openai.error.InvalidRequestError: Unrecognized request argument supplied: logprobs
Code snippets
No response
OS
Ubuntu
Python version
Python3.9
Library version
openai-0.27.6
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/432","Internal error when calling completion with incorrect name","2024-03-03T00:45:55Z","Closed issue","bug","Describe the bug
I got a funky HTTP 500 internal error when I accidentally provided an incorrect model name for the completion API.
To Reproduce
response = openai.Completion.create(model='	text-davinci-003', prompt=context)

(Accidental whitespace at the front of the model name)
Code snippets
File "".../server.py"", line 62, in query
    response = openai.Completion.create(model='	text-davinci-003', prompt=context)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

...

  File ""...venv/lib/python3.11/site-packages/openai/api_requestor.py"", line 687, in _interpret_response_line
    raise self.handle_error_response(
openai.error.APIError: internal error {
    ""error"": {
        ""message"": ""internal error"",
        ""type"": ""invalid_request_error"",
        ""param"": null,
        ""code"": null
    }
}


### OS

Linux

### Python version

3.11.3+chromium.29

### Library version

0.27.6

 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/426","Error when uploading JSONL file for training","2023-09-07T15:38:07Z","Closed issue","bug","Describe the bug
Title: Error when uploading JSONL file for training
Description: I am trying to upload a JSONL file for training on OpenAI, but I'm encountering an error. The error message says ""Expected file to have JSONL format, where every line is a JSON dictionary. Line 1 is not a dictionary (HINT: line starts with: ""{..."").""
I created the JSONL file by converting a CSV file to JSONL using Python's csv and json libraries. I followed the instructions on OpenAI's documentation for formatting the file: each line is a JSON dictionary with the keys ""prompt"" and ""completion"".
Here is an example of the first two lines of the file:
 {""prompt"": ""date: 3/11/2015, open: 1.070939064, high: 1.071455359, low: 1.056140184, close: 1.070870161, adj close: 1.070870161, sma: 1.121346676, ema: 1.114991641, rsi: 19.71425449, %K: 17.71232347, %D: 12.06409742, sma_50: 1.143293602, ema_20: 1.114991641, stoch_k: 17.71232347, stoch_d: 12.06409742"", ""completion"": ""Price Decreased""}
 {""prompt"": ""date: 3/12/2015, open: 1.054941297, high: 1.068150163, low: 1.050155401, close: 1.054874539, adj close: 1.054874539, sma: 1.117530596, ema: 1.109266202, rsi: 16.57194042, %K: 5.293653385, %D: 13.38069099, sma_50: 1.140193837, ema_20: 1.109266202, stoch_k: 5.293653385, stoch_d: 13.38069099"", ""completion"": ""Price Decreased""}
To Reproduce
Run the code below and try to upload to fine tune.
Traceback (most recent call last):
 File ""ChatGPTPlaygrounf\train_py.py"", line 155, in 
 upload_training_data(training_data)
 File ""ChatGPTPlaygrounf\train_py.py"", line 132, in upload_training_data
 response = openai.File.create(
 File ""chatgpt\lib\site-packages\openai\api_resources\file.py"", line 59, in create
 response, _, api_key = requestor.request(""post"", url, files=files)
 File ""chatgpt\lib\site-packages\openai\api_requestor.py"", line 181, in request
 resp, got_stream = self._interpret_response(result, stream)
 File ""chatgpt\lib\site-packages\openai\api_requestor.py"", line 396, in _interpret_response
 self._interpret_response_line(
 File ""chatgpt\lib\site-packages\openai\api_requestor.py"", line 429, in _interpret_response_line
 raise self.handle_error_response(
 openai.error.InvalidRequestError: Expected file to have JSONL format, where every line is a JSON dictionary. Line 1 is not a dictionary (HINT: line starts with: ""[{""p..."").
Code snippets
`def create_json():
    csv_file = ""csv/eurusd_historical_data_with_indicators.csv""
    jsonl_file = 'data.jsonl'

    with open(csv_file, 'r') as f_csv, open(jsonl_file, 'w') as f_jsonl:
        reader = csv.DictReader(f_csv, delimiter=',')
        for row in reader:
            prompt = {
                ""prompt"": f""date: {row['date']}, open: {row['open']}, high: {row['high']}, low: {row['low']}, close: {row['close']}, adj close: {row['adj close']}, sma: {row['sma']}, ema: {row['ema']}, rsi: {row['rsi']}, %K: {row['%K']}, %D: {row['%D']}, sma_50: {row['sma_50']}, ema_20: {row['ema_20']}, stoch_k: {row['stoch_k']}, stoch_d: {row['stoch_d']}""
            }
            completion = {""completion"": row['price movement']}
            data = {**prompt, **completion}
            f_jsonl.write(json.dumps(data) + '\n')
`
OS
Windows 11
Python version
v3.9
Library version
v0.27
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/425","AttributeError: module 'openai' has no attribute 'ModelFineTuning'","2023-05-01T13:34:36Z","Closed issue","bug","Describe the bug
Traceback (most recent call last):
 File ""ChatGPTPlayground\train_py.py"", line 119, in train
 response = openai.ModelFineTuning.create(model_id=model_id, **params)
 AttributeError: module 'openai' has no attribute 'ModelFineTuning'
To Reproduce
Open AI Version 0.27
 Python 3.9
 Open AI installed through pip, running a fresh conda environment.
Code snippets
Code snippit 
    with open(""data.jsonl"", ""r"") as file:
        chunk_size = 100
        while True:
            chunk = list(file.readlines(chunk_size))
            if not chunk:
                break
            data = [json.loads(line) for line in chunk]
            text_data = ""\n"".join([d[""completion""] for d in data])
            params[""training_data""] = [{""text"": text_data}]
            response = openai.ModelFineTuning.create(model_id=model_id, **params)
            print(f""Fine-tuned model on chunk of {len(data)} examples."")
OS
Windows 11
Python version
v3.9
Library version
v0.27
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/424","Why does the OpenAI key expire so fast?","2023-05-11T08:28:02Z","Closed issue","No label","Describe the feature or improvement you're requesting
It seems the OpenAI key will expire on 1st May UTC. Any way around that?
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/423","AttributeError: module 'openai' has no attribute 'ChatCompletion/Completion'","2023-11-10T03:57:09Z","Closed issue","bug","Describe the bug
The feature is not working at all for me, so I created a new venv and it still does not work
To Reproduce
import openai

openai.api_key = ""key""

completion = openai.Completion.create(
         model=""gpt-3.5-turbo"", 
         messages=[{""role"": ""user"", ""content"": ""Hello world!""}]
)

Code snippets
Additionally, I tried to run this cURL command in my terminal given by the openai documentation:

curl https://api.openai.com/v1/completions \
  -H ""Authorization: Bearer $OPENAI_API_KEY"" \
  -H ""Content-Type: application/json"" \
  -d '{""prompt"": YOUR_PROMPT, ""model"": FINE_TUNED_MODEL}'

and replaced it with my api key which in return printed:
    ""error"": {
        ""message"": ""Incorrect API key provided: -*****************************************. You can find your API key at https://platform.openai.com/account/api-keys."",
        ""type"": ""invalid_request_error"",
        ""param"": null,
        ""code"": ""invalid_api_key""
    }

I've used 3 api keys at this point, I am not sure if that's what is causing the issue since both my python and openai are up to date.
OS
macOS
Python version
Python v3.11.3
Library version
openai-python v0.27.5
 The text was updated successfully, but these errors were encountered: 
👍2
florian-hoenicke and deuscapturus reacted with thumbs up emoji
All reactions
👍2 reactions"
"https://github.com/openai/openai-python/issues/422","get_embedding does not update.","2023-11-06T17:33:53Z","Closed as not planned issue","question","Describe the bug
I've noticed that using get_embedding repeatedly does not actually embeds the tokens but returns the previous result. Side by side comparison of from openai.embeddings_utils import get_embedding and tiktoken with just a length check:
Generating Text.
1536 1320
1536 1707
1536 1986
1536 1901
1536 2239
1536 2429
1536 2406
1536 2364
Final Length: 1328 (which is normal)

To Reproduce
It should be simple enough to count tokens for different files with both get_embedding and tiktoken.
Code snippets
No response
OS
macOs
Python version
Python 3
Library version
openai-python
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/421","Wrong Data issue with API","2023-09-07T15:38:57Z","Closed issue","No label","Describe the feature or improvement you're requesting
When I tried to use the API of Openai, I got an issue that the API is providing wrong data. And also providing very limited character answers to any question.
How can I fix all these issues?
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/420","Is the order of batched embeddings data indeterministic?","2023-05-03T15:24:39Z","Closed issue","bug","Describe the bug
Looking over the code related to embeddings, I've noticed:

openai-python/openai/embeddings_utils.py
 Lines 48 to 49 in c556584
	data=openai.Embedding.create(input=list_of_text, engine=engine, **kwargs).data
	data=sorted(data, key=lambdax: x[""index""]) # maintain the same order as input.
Which I have not seen in the API documentation (or other examples), tho I may have missed it. Should the user expect that the order of returned batched embeddings from the API may not be deterministic and thus not be in the same order as input data?
To Reproduce
get_embeddings([""foo bar"", ""foo baz""])
Code snippets
No response
OS
macOS
Python version
python 3.10
Library version
0.27.4
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/418","Why does newline negatively impact embedding performance?","2023-11-06T17:18:43Z","Closed as not planned issue","question","Describe the bug
While reading the code of the embeddings_utils I have stumbled upon this:
openai-python/openai/embeddings_utils.py
 Lines 20 to 21 in db3f352
	# replace newlines, which can negatively affect performance.
	text=text.replace(""\n"", "" "") 
Could you please provide more context on:
replace newlines, which can negatively affect performance.
Are there any references/papers/numbers behind that negative impact?
To Reproduce
get_embedding(""foo bar\nbaz"")
Code snippets
No response
OS
macOS
Python version
3.10
Library version
0.27.4
 The text was updated successfully, but these errors were encountered: 
👍2
EliahKagan and hossein-amirkhani reacted with thumbs up emoji
All reactions
👍2 reactions"
"https://github.com/openai/openai-python/issues/416","Expose X-Ratelimit-* headers on ChatCompletion API requests","2023-11-06T16:43:37Z","Closed issue","enhancement,fixed in v1","Describe the feature or improvement you're requesting
If you make a call to POST /v1/chat/completions manually, you get a few very helpful headers that estimate time to rate-limiting
x-ratelimit-limit-requests: 3500
x-ratelimit-remaining-requests: 3499
x-ratelimit-reset-requests: 17ms

However, if you call openai.ChatCompletion.create(...) or openai.ChatCompletion.acreate(...), these helpful rate limit values are not returned by the function.
It would be nice if we could get access to these headers, or have them returned in a helpful format along with the response
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
👍32
jzumwalt, cmishra, GarryOne, NQDM-Jody, binardo, pors, RR-28023, laurids-reichardt, gaziqbal, Gabriel-Macias, and 22 more reacted with thumbs up emoji
All reactions
👍32 reactions"
"https://github.com/openai/openai-python/issues/415","fine tune for json does not work","2024-03-03T00:46:18Z","Closed issue","bug","Describe the bug
I am parsing a json file with the command openai tools fine_tunes.prepare_data -f javi.json, and i always have this error message:
Analyzing...
ERROR in read_any_format validator: Your file javi.json does not appear to be in valid JSON format. Please ensure your file is formatted as a valid JSON file.
Aborting...%
It does not matter whats the content of the file, even with something as simple as { ""name"": ""javi""} it throws this error
To Reproduce
Create a json file with any content
Run openai tools fine_tunes.prepare_data -f javi.json
It errors
Code snippets
No response
OS
Monterrey
Python version
Python 3.9.6
Library version
openai 0.27.4
 The text was updated successfully, but these errors were encountered: 
👍1
zzj0402 reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/openai-python/issues/414","[QUESTION] Sometime it said the model is overload","2023-05-08T01:47:02Z","Closed issue","bug","Describe the bug
Sometime i use Open AI API, it have this message:
 model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID fac8b65ed1c009c456a00410c4f6c376 in your message.)
Does anybody know why?
To Reproduce
.
Code snippets
No response
OS
Ubuntu 22.04
Python version
Python 3.9
Library version
0.26.4
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/413","error_data is 'Not Found'","2023-11-10T04:31:48Z","Closed issue","bug","Describe the bug
when I run this script,it make mistake, AttributeError: 'str' object has no attribute 'get'
 then I debug this script and find that error_data is 'Not Found'
To Reproduce
update 'api_requestor.py' file,add lines in request_raw function:
proxy = {
            'http': '127.0.0.1:7890',
            'https': '127.0.0.1:7890'
        }
# 7890 is my proxy port
update 'proxies=_thread_context.session.proxies' to proxies=proxy
run it in main program:
import openai

openai.api_key = ""my openai key""openai.api_base = ""https://api.openai-proxy.com""

# list modelsmodels = openai.Model.list()

# print the first model's idprint(models.data[0].id)

# create a completioncompletion = openai.Completion.create(model=""ada"", prompt=""Hello world"")

# print the completionprint(completion.choices[0].text)
Code snippets
No response
OS
windows
Python version
Python v3.10.0
Library version
openai-poython v0.27.4
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/412","Function to calculate number of tokens","2023-04-23T09:57:59Z","Open issue","enhancement","Describe the feature or improvement you're requesting
It would be useful if the module provided a function to calculate number of token in a given prompt for a given model, without having to use another 3rd party modules to do so.
One examle when it would be useful is to trim fed history if the entire prompt (+max_tokens) is above a given model limit, before sending the query.
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
👍3
AdventLee, nbro10, and kovashikawa reacted with thumbs up emoji
All reactions
👍3 reactions"
"https://github.com/openai/openai-python/issues/411","Using Azure and OpenAI at the same time","2023-10-16T13:17:59Z","Closed issue","bug","Describe the bug
Currently it seems there is no strightforward way to be able to send requests to both OpenAI and Azure from same python program. Ideally this should be possible:
openai = OpenAI(key=key)
 azureai = OpenAI(key=key, api_base=xxx, ....)
While currently given key, api_base, etc. are global variables to the module, the above won't work.
To Reproduce
Code snippets
No response
OS
Linux
Python version
Python 3
Library version
latest
 The text was updated successfully, but these errors were encountered: 
👍5
artdent, sondt2709, fabswt, atalnarayan, and vickyliin reacted with thumbs up emoji
All reactions
👍5 reactions"
"https://github.com/openai/openai-python/issues/407","api_key arg for openai.Audio.transcribe doesn't work","2023-04-28T11:24:08Z","Closed issue","bug","Describe the bug
    def create_transcription(self, audio_file : str):
            if not os.path.exists(audio_file):
                return None
            file = open(audio_file, 'rb')
            resp = openai.Audio.transcribe(
                api_key=self._API_KEY,
                file=file,
                model='whisper-1',
                response_format='text',
                language='it'
            )
            return resp
Using the api_key parameter of openai.Audio.transcribe raises a openai.error.AuthenticationError.
 I know for sure that the api key provided in self._API_KEY is correct because I have already used it in another function.
Looking in the audio.py module, the function:
    def transcribe(
        cls,
        model,
        file,
        api_key=None,
        api_base=None,
        api_type=None,
        api_version=None,
        organization=None,
        **params,
    ):
        requestor, files, data = cls._prepare_request(file, file.name, model, **params)
        url = cls._get_url(""transcriptions"")
        response, _, api_key = requestor.request(""post"", url, files=files, params=data)
        return util.convert_to_openai_object(
            response, api_key, api_version, organization
        )
does not use the api_key that I am passing it and that it inserts a default key, which is therefore not good.
To Reproduce
Try to use the function openai.Audio.transcribe() with the api_key parameter filled with a valid key
Code snippets
No response
OS
linux
Python version
Python 3.10.6
Library version
openai-python-0.27.2
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/403","ModuleNotFoundError: No module named 'openai.datalib' on main branch","2023-04-17T17:59:17Z","Closed issue","bug","Describe the bug
Building the following Dockerfile with docker build .:
FROM python:3.8

RUN pip install git+https://github.com/openai/openai-python
RUN python -c 'from openai.api_resources.embedding import Embedding'
gives:
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""/usr/local/lib/python3.8/site-packages/openai/__init__.py"", line 19, in <module>
    from openai.api_resources import (
  File ""/usr/local/lib/python3.8/site-packages/openai/api_resources/__init__.py"", line 7, in <module>
    from openai.api_resources.embedding import Embedding  # noqa: F401
  File ""/usr/local/lib/python3.8/site-packages/openai/api_resources/embedding.py"", line 6, in <module>
    from openai.datalib.numpy_helper import assert_has_numpy
ModuleNotFoundError: No module named 'openai.datalib'

To Reproduce
See above
Code snippets
See above
OS
Linux
Python version
3.8
Library version
main
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/402","openai cli --stream error","2023-11-06T17:37:59Z","Closed issue","bug,fixed in v1","Describe the bug
When adding the --stream parameter to the OpenAI command, an error occurred.
To Reproduce
When adding the --stream parameter to the OpenAI command, an error occurred.
> openai api chat_completions.create -m gpt-3.5-turbo -g user ""Hello"" --stream
Traceback (most recent call last):
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python310\lib\runpy.py"", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python310\lib\runpy.py"", line 86, in _run_code
    exec(code, run_globals)
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python310\Scripts\openai.exe\__main__.py"", line 7, in <module>
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_openai_scripts.py"", line 70, in main
    args.func(args)
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\cli.py"", line 144, in create
    sys.stdout.write(c[""message""][""content""])
KeyError: 'message'

Code snippets
No response
OS
Windows
Python version
Python v3.10.6
Library version
openai v0.27.4
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/401","Immediately cease any form of development of OpenAI","2023-04-15T13:40:54Z","Closed as not planned issue","No label","Describe the feature or improvement you're requesting
To Reproduce
golang
OS
Stop stalking me :)
Node version
No response
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/399","Stream error after 5 minutes","2023-11-10T04:16:22Z","Closed issue","bug,fixed in v1","Describe the bug
Streaming output from gpt4 consistently fails with Connection broken: InvalidChunkLength(got length b'', 0 bytes read) for prompts that trigger long-form responses. I've been able to reproduce this by prompting gpt to rewrite a long document.
A few observations
This failure occurs exactly at the 5 minute mark every time, which leads me to believe there's a timeout somewhere.
The python implementation of stream reader assumes that an empty string will never be returned by the server.
I sent the same request using a non-streaming client with a timeout at 10min. The timeout wasn't triggered until the 10min mark,so this problem seems isolated to streaming
Code snippet from urllib3/response.py:
def _update_chunk_length(self):
        # First, we'll figure out length of a chunk and then
        # we'll try to read it from socket.
        if self.chunk_left is not None:
            return
        line = self._fp.fp.readline()  # self._fp.fp.readline() returns b''
        line = line.split(b"";"", 1)[0]
        try:
            self.chunk_left = int(line, 16)  # this raises a value error
        except ValueError:
            # Invalid chunked protocol response, abort.
            self.close()
            raise InvalidChunkLength(self, line)

It seems like a problem on the server side but Im not confident
To Reproduce
Create gpt4 streaming client
Prompt it to rewrite a very long text. For example, verses 1:1 -> 4:10 in the book of genesis
Code snippets
No response
OS
macOs
Python version
Python 3.10.9
Library version
v.0.26.4
 The text was updated successfully, but these errors were encountered: 
👍12
karim-attia, shunwen, givery-mo, elh, t4ng, elonzh, Dador, caoergou, lucianfelix, pratyakshs, and 2 more reacted with thumbs up emoji
All reactions
👍12 reactions"
"https://github.com/openai/openai-python/issues/398","mock supports for OpenAIObjects","2023-11-03T22:27:37Z","Closed as not planned issue","fixed in v1","Describe the feature or improvement you're requesting
I'm writing tests where I mock calls like this:
    response = openai.ChatCompletion.create(
        model=""gpt-4"",
        messages=[
            {""role"": ""system"", ""content"": system_message},
            {""role"": ""user"", ""content"": message},
        ],
        n=1,
        max_tokens=150,
    )
But I notice that the response is not a plain dict, but a set of nested OpenAIObjects. In order to mock the response I find myself creating a helper method like so:
def create_openai_object(payload):
   obj = OpenAIObject()
   message = OpenAIObject()
   content = OpenAIObject()
   content.content = payload
   message.message = content
   obj.choices = [message]
   return obj
which is a little clunky, and I'm wondering if there's a cleaner way to create a simple mock of the response - I've had a look through the code and the tests, and not really seeing anything similar ... am I missing something, or perhaps just over-testing?
I mean I'm just keen to be able to test my code on a realistic response without the tests incurring the cost of actually hitting the api ...
Additional context
here's my test:
def test_send_message_integration():
    message = ""Hello, Bot!""
    expected_messages = [
        {""role"": ""system"", ""content"": ""You're a friendly fellow""},
        {""role"": ""user"", ""content"": message},
    ]

    with patch(""main.openai.ChatCompletion.create"") as mock_create:
        mock_create.return_value = create_openai_object(""Hello, user!"")

        response = client.post(""/send_message"", json={""message"": message})

        assert response.status_code == 200
        mock_create.assert_called_once_with(
            model=""gpt-4"", messages=expected_messages, n=1, max_tokens=150
        )
 The text was updated successfully, but these errors were encountered: 
👍18
chriscarrollsmith, jsquirrelz, harley, Ai-Yukino, vlade11115, carloe, ahmedmustahid, keitakn, anenoglyadov, wong-codaio, and 8 more reacted with thumbs up emoji
All reactions
👍18 reactions"
"https://github.com/openai/openai-python/issues/397","Embeddings ""/v1/embeddings""endpoint support.","2023-11-06T17:36:17Z","Closed issue","No label","Describe the feature or improvement you're requesting
Embeddings use api url as ""/engines/text-embedding-ada-002/embeddings"", can we support the ""/v1/embeddings"" with model in Request body?
 I 'm new to openai api and only found new entrypoint described in https://platform.openai.com/docs/api-reference/embeddings/create
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
👀2
imldy and uicosp reacted with eyes emoji
All reactions
👀2 reactions"
"https://github.com/openai/openai-python/issues/393","Onboarding link not working","2023-04-19T07:48:14Z","Closed issue","bug","Describe the bug
I expected the link in 
openai-python/openai/util.py
 Line 187 in d6fa3bf
	""No API key provided. You can set your API key in code using 'openai.api_key = <API-KEY>', or you can set the environment variable OPENAI_API_KEY=<API-KEY>). If your API key is stored in a file, you can point the openai module at it with 'openai.api_key_path = <PATH>'. You can generate API keys in the OpenAI web interface. See https://onboard.openai.com for details, or email support@openai.com if you have any questions.""
 aka https://onboard.openai.com to work.
 It didn't
To Reproduce
https://onboard.openai.com
Code snippets
No response
OS
Ubuntu 20.04
Python version
Not applicable
Library version
Latest
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/392","no response from embedding api for 600s","2024-03-03T00:47:06Z","Closed as not planned issue","bug","Describe the bug
This maybe true for other openai python api's as well. We are using openai api's via the python library in a multi threaded application which is always ON, i.e. we do not frequently start/restart the application.
Following is the code:
from embeddings_utils import get_embedding
input = get_embedding(""this is a test"", engine=""text-embedding-ada-002"")

When there is a long period of inactivity, intermittently, we get responses for above after 600s. This problem some time resolves itself after some requests/time. But during the time it gives issue, response is received after exact 600-601 seconds. It looks like request goes and there is no response for 600s and then it retries and that works immediately.
Any idea about this issue?
To Reproduce
No particular steps, please see description above.
Code snippets
No response
OS
Linux/Ubuntu
Python version
Python v3.7.5
Library version
open-ai 0.27.2
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/391","InvalidRequestError: Invalid URL (POST /v1/engines/gpt-3.5-turbo/chat/completions)","2023-04-12T14:23:47Z","Closed as not planned issue","bug","Describe the bug
command :
def classify_tokens_gpt3_5_turbo_multiple_parts(email_text: str) -> list:
    # Define the prompt
    prompt = (
        f""Given the request for quotation email below, identify and classify the following information for each part: ""
        f""email_Subject_Phrase, RFQ_number, Manufacturer_Part_Number, Qty_Required, Manufacturer_name, ""
        f""Customer_Part_Number, Product_Description, Target_Price, Lead_Time_days, Date_Code, Packaging_Type, ""
        f""Dispatch_Date, Comments, Currency, min_ord_qty, STD_PACK_QTY, SENDER_NAME, SENDER_POSITION, SENDER_COMPANY, SENDER_MOBILE, ""
        f""SENDER_EMAIL, SENDER_ADDRESS, SENDER_COUNTRY, SENDER_PINCODE, SENDER_CITY, SENDER_STATE.\n\n""
        '''for example response. : {'email_Subject_Phrase':'send me your quote to following parts',        'RFQ_number':'123455ASDJKH',        'Manufacturer_Part_Number':['aas1d32f1','df4gs45'],        'Qty_Required':['32','5510'],        'Manufacturer_name':['TI','MAXIUM'],        'Customer_Part_Number':['12UIOY3','KJHG2134'],        'Product_Description':['PN DIODE','3W LED'],        'Target_Price':['120','5'],        'Lead_Time_days':['5','10'],        'Date_Code':['12-FEB-2023','10-JAN-2023'],        'Packaging_Type':['SMD','THOURHG HOLE'],        'Dispatch_Date':['11-FEB-2023','10-JAN-2023'],        'Comments':['SEND WITH MINIMUM PLASIC PACKAGING'],        'Currency':'USD',        'min_ord_qty':['10K','5K'],        'STD_PACK_QTY':['2K','1K'],        'SENDER_NAME':'HEMANG JOSHI',        'SENDER_POSITION':'DESIGN ENGINEER',        'SENDER_COMPANY':'HJLABS.IN',        'SENDER_MOBILE':'+917016525813',        'SENDER_EMAIL':'INFO@HJLABS.IN',        'SENDER_ADDRESS':'SHED#:180, MAHAVIR INDIUSTRIAL PARK-2, KATHVADA GIDC, AHMEDABAD',        'SENDER_COUNTRY':'INDIA',        'SENDER_PINCODE':'365610',        'SENDER_CITY':'AHMEDABAD',        'SENDER_STATE':'GUJARAT'        }'''
        f""Email:\n{email_text}\n\n""
    )

    # Call the GPT-3.5-turbo API
    response = openai.ChatCompletion.create(
#         engine=""text-davinci-002"",
        engine=""gpt-3.5-turbo"",
#         engine=""gpt-4"",
        prompt=prompt,
        max_tokens=1024,
        n=1,
        stop=None,
        temperature=0.1,
    )

    # Extract the generated answer from the API response
    answer = response.choices[0].text.strip()
    print(f'{answer=}')

    # Split the answer into a list of parts
    parts = re.split(r'\n\s*\n', answer)

    # Parse the individual parts into a list of dictionaries
    classified_tokens_list = []
    for part in parts:
        classified_tokens = {}
        for line in part.split('\n'):
            if ':' in line:
                key, value = line.split(':', 1)
                classified_tokens[key.strip()] = value.strip()
        classified_tokens_list.append(classified_tokens)

    return classified_tokens_list

mlemail = '''Dear HEMANG,Please send me your quote to following parts:Maxim S21DF3G 210pc.Taiyo Yuden TMK325B7226MM-TR 1k / 7kTaiyo Yuden TMK325B7226KM-PR 1k / 7kTI TM4C129XNCZAD 1415pc.Maxim MAX823SEUK+T 100 / 1550pc.Linear / Analog LT8708EUHG#PBF 2.300Texas Instruments LP38691SD-ADJ/NOPB 2.400Texas Instruments LP38690SD-ADJ 2.400Texas Instruments LP38690SDX-ADJ 2.400Texas Instruments LMP8481MM-T/NOPB 2300UCC EMZL350ARA561MJA0G 6.900Omron G5V-1-DC9 2.300Thank you very much for your efforts.Best regards,SATYA NADELLA--SATYA NADELLAVerkauf / SalesMICROSOFT'''

classify_tokens_gpt3_5_turbo_multiple_parts(mlemail)
ERROR :
---------------------------------------------------------------------------InvalidRequestError                       Traceback (most recent call last)
Cell In [20], line 1----> 1 classify_tokens_gpt3_5_turbo_multiple_parts(mlemail)

Cell In [18], line 40, in classify_tokens_gpt3_5_turbo_multiple_parts(email_text)
      3     prompt = (
      4         f""Given the request for quotation email below, identify and classify the following information for each part: ""
      5         f""email_Subject_Phrase, RFQ_number, Manufacturer_Part_Number, Qty_Required, Manufacturer_name, ""
   (...)
     36         f""Email:\n{email_text}\n\n""
     37     )
     39     # Call the GPT-3.5-turbo API---> 40     response = openai.ChatCompletion.create(
     41 #         engine=""text-davinci-002"",
     42         engine=""gpt-3.5-turbo"",
     43 #         engine=""gpt-4"",
     44         prompt=prompt,
     45         max_tokens=1024,
     46         n=1,
     47         stop=None,
     48         temperature=0.1,
     49     )
     51     # Extract the generated answer from the API response
     52     answer = response.choices[0].text.strip()

File ~/.local/lib/python3.11/site-packages/openai/api_resources/chat_completion.py:25, in ChatCompletion.create(cls, *args, **kwargs)
     23 while True:
     24     try:
---> 25         return super().create(*args, **kwargs)
     26     except TryAgain as e:
     27         if timeout is not None and time.time() > start + timeout:

File ~/.local/lib/python3.11/site-packages/openai/api_resources/abstract/engine_api_resource.py:153, in EngineAPIResource.create(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)
    127 @classmethod
    128 def create(
    129     cls,
   (...)
    136     **params,
    137 ):
    138     (
    139         deployment_id,
    140         engine,
   (...)
    150         api_key, api_base, api_type, api_version, organization, **params
    151     )
--> 153     response, _, api_key = requestor.request(
    154         ""post"",
    155         url,
    156         params=params,
    157         headers=headers,
    158         stream=stream,
    159         request_id=request_id,
    160         request_timeout=request_timeout,
    161     )
    163     if stream:
    164         # must be an iterator
    165         assert not isinstance(response, OpenAIResponse)

File ~/.local/lib/python3.11/site-packages/openai/api_requestor.py:226, in APIRequestor.request(self, method, url, params, headers, files, stream, request_id, request_timeout)
    205 def request(
    206     self,
    207     method,
   (...)
    214     request_timeout: Optional[Union[float, Tuple[float, float]]] = None,
    215 ) -> Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], bool, str]:
    216     result = self.request_raw(
    217         method.lower(),
    218         url,
   (...)
    224         request_timeout=request_timeout,
    225     )
--> 226     resp, got_stream = self._interpret_response(result, stream)
    227     return resp, got_stream, self.api_key

File ~/.local/lib/python3.11/site-packages/openai/api_requestor.py:620, in APIRequestor._interpret_response(self, result, stream)
    612     return (
    613         self._interpret_response_line(
    614             line, result.status_code, result.headers, stream=True
    615         )
    616         for line in parse_stream(result.iter_lines())
    617     ), True
    618 else:
    619     return (
--> 620         self._interpret_response_line(
    621             result.content.decode(""utf-8""),
    622             result.status_code,
    623             result.headers,
    624             stream=False,
    625         ),
    626         False,
    627     )

File ~/.local/lib/python3.11/site-packages/openai/api_requestor.py:683, in APIRequestor._interpret_response_line(self, rbody, rcode, rheaders, stream)
    681 stream_error = stream and ""error"" in resp.data
    682 if stream_error or not 200 <= rcode < 300:
--> 683     raise self.handle_error_response(
    684         rbody, rcode, resp.data, rheaders, stream_error=stream_error
    685     )
    686 return resp

InvalidRequestError: Invalid URL (POST /v1/engines/gpt-3.5-turbo/chat/completions)
To Reproduce
SEE THE DESCRIBE PART
Code snippets
SEE THE DESCRIBE PART
OS
LINUX-UBUNTU-LATEST
Python version
PYTHON3.11
Library version
LATEST PYPI VERSION
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/385","ChatGPT is able to access private GitHub repositories!","2023-04-08T00:16:32Z","Closed issue","bug","Describe the bug
I tried to ask ChatGPT for information about a private GitHub repository of mine. To my amazement, ChatGPT was able to tell me about the repository perfectly well, with all the details. What is concerning is that it is able to access a private GitHub repository, which shouldn't happen, as it might be looked at as a 'breach of privacy' by others.
To Reproduce
Enter the prompt: ""Tell me about the GitHub repo: repoName""
Then on the prompted to enter username: ""Username of Repository owner: userName""
ChatGpt is able to access the Github repository, and provide information, even though the repository is private.
Code snippets
No response
OS
Windows 11
Python version
Python v3.7.1
Library version
openai-python v0.26.4
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/383","fine_tunes.prepare_data ERROR in read_any_format validator: Your file train.txt does not appear to be in valid TXT format","2023-04-08T15:12:09Z","Closed issue","bug","Describe the bug
hi,
 everyone
 When i use fine_tunes to import my train date, there is an error about it.
 Please give me some support about it. thanks.
To Reproduce
step1 ： pip3 install --upgrade openai
 step2：set OPENAI_API_KEY=""sk-XXXXXXXXX""
 step3: openai tools fine_tunes.prepare_data -f train.txt
Code snippets
(base) PS D:\Personal> openai tools fine_tunes.prepare_data -f train.txtAnalyzing...


ERROR in read_any_format validator: Your file `train.txt` does not appear to be in valid TXT format. Please ensure your file is formatted as a valid TXT file.

Aborting...
(base) PS D:\Personal>
OS
windows
Python version
Python 3.9.13
Library version
openai-0.27.4
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/380","import openai seems to take a long time","2023-04-08T15:12:23Z","Closed issue","No label","Describe the feature or improvement you're requesting
Doing a fresh import openai seems to take about 4-5 seconds
(reproducible with start_time = time.time(); import openai; print(time.time() - start_time)). This startup time seems like given the straightforward nature of the repo.
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/379","Missing rate limit details for chatGPT turbo API","2023-09-07T15:42:42Z","Closed issue","bug","Describe the bug
There is no details on the TPM unit for chatgpt turbo api since the website only provided https://platform.openai.com/docs/guides/rate-limits/overview
TYPE 1 TPM EQUALS
 davinci 1 token per minute
 curie 25 tokens per minute
 babbage 100 tokens per minute
 ada 200 tokens per minute
To Reproduce
Read https://platform.openai.com/docs/guides/rate-limits/overview
Realize TPM is not really tokens per minute, but depends on model type
Realize there is no details on ChatGPT API's ""1 TPM equals ...""
Wonder why not show number of raw tokens per minute, and instead create ""TPM"" as a misleading abstraction?
Code snippets
No response
OS
Any
Python version
Any
Library version
Any
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/378","ImportError: cannot import name 'OpenAI' from 'openai'","2023-04-05T18:55:02Z","Closed issue","bug","Describe the bug
Please help me solve the problem...
 suggested, char JPT, query connection option in Django - doesn't work:
 //------------------------------------------------ ------------------------------------
 from openai import OpenAI
 import requests
Get the API key
api_key=""""
Initialize OpenAI
openai = OpenAI(api_key)
Function to get response from ChatGPT
def get_response(query):
 # Form a request to the API
 response = openai.completion.create(
 engine=""davinci"",
 prompt=query
 max_tokens=100
 )
 # Get response
 response_json = response.json()
 reply = response_json[""choices""][0][""text""]

 return reply

Function for processing the dialog form
def process_form(query):
 # Get response from ChatGPT
 response = get_response(query)
 # Send response to user
 return response

//------------------------------------------------ ------------------------------------
 I get an error - it is not possible to load OpenAI!
 the package is installed, for example, this version of the error does not cause an error at startup:
//------------------------------------------------ ------------------------------------
api_key = ""here is my key""
 openai.api_key = api_key
Function to get response from ChatGPT
def get_response(query):
 # Form a request to the API
 response = openai.completion.create(
 engine=""davinci"",
 prompt=query
 max_tokens=200
 )
 # Get response
 response_json = response.json()
 reply = response_json[""choices""][0][""text""]
 return reply

//------------------------------------------------ ------------------------------------
but a runtime error occurs when calling: response.json()
 some argument error
To Reproduce
//------------------------------------------------ ------------------------------------
 from openai import OpenAI
 import requests
Get the API key
api_key=""""
Initialize OpenAI
openai = OpenAI(api_key)
Function to get response from ChatGPT
def get_response(query):
 # Form a request to the API
 response = openai.completion.create(
 engine=""davinci"",
 prompt=query
 max_tokens=100
 )
 # Get response
 response_json = response.json()
 reply = response_json[""choices""][0][""text""]

 return reply

Function for processing the dialog form
def process_form(query):
 # Get response from ChatGPT
 response = get_response(query)
 # Send response to user
 return response

//------------------------------------------------ ------------------------------------
 I get an error - it is not possible to load OpenAI!
 ImportError: cannot import name 'OpenAI' from 'openai'
the package is installed, for example, this version of code the error does not cause an error at startup:
//------------------------------------------------ ------------------------------------
api_key = ""here is my key""
 openai.api_key = api_key
Function to get response from ChatGPT
def get_response(query):
 # Form a request to the API
 response = openai.completion.create(
 engine=""davinci"",
 prompt=query
 max_tokens=200
 )
 # Get response
 response_json = response.json()
 reply = response_json[""choices""][0][""text""]
 return reply

//------------------------------------------------ ------------------------------------
but a runtime error occurs when calling: response.json()
 some argument error
Exception Type: | AttributeError
 json
 /home/murd/buf/ai_chat/.ai_env/lib/python3.8/site-packages/openai/openai_object.py, line 61, in getattr
response_json = response.json()
 raise AttributeError(*err.args)
k | 'json'
 self | {'choices': [{'finish_reason': 'length', 'index': 0, 'logprobs': None, 'text': '\n' '\n'
Code snippets
No response
OS
Ubuntu
Python version
Python 3.8.10
Library version
Name: openai Version: 0.27.3 Summary: Python client library for the OpenAI API Home-page: https://github.com/openai/openai-python Author: OpenAI Author-email: support@openai.com License: None Location: /home/murd/buf/ai_chat/.ai_env/lib/python3.8/site-packages Requires: aiohttp, requests, tqdm
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/377","Curl works but Python fails for the same API key and same request","2023-04-04T16:27:30Z","Closed issue","bug","Describe the bug
I'm trying to use the ChatCompletion API by following the documentation examples.
 I'm getting: openai.error.InvalidRequestError: Invalid URL (POST /v1/engines/gpt-3.5-turbo/chat/completions)
I did check that I'm using ChatCompletion and not Completion.
I generated another API key to solve the problem.
This is strictly a bug from the Python lib since using curl, the POST request works.
⚠️ I feel like I should mention I was recently accepted in the GPT-4 beta (according to an email I've received), so it may be due to this ?
Thanks in advance for your help.
To Reproduce
Just use the ChatCompletion python example from the docs. This worked fine for me until 2 weeks ago.
Code snippets
import openaiopenai.api_key=""myAPIkey""openai.organization = ""myorg"" # I've tried without this as wellopenai.ChatCompletion.create(engine=""gpt-3.5-turbo"",messages=[{""role"": ""user"", ""content"":""yo""}])
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/tristan/miniforge3/lib/python3.10/site-packages/openai/api_resources/chat_completion.py"", line 25, in create
    return super().create(*args, **kwargs)
  File ""/Users/tristan/miniforge3/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py"", line 153, in create
    response, _, api_key = requestor.request(
  File ""/Users/tristan/miniforge3/lib/python3.10/site-packages/openai/api_requestor.py"", line 226, in request
    resp, got_stream = self._interpret_response(result, stream)
  File ""/Users/tristan/miniforge3/lib/python3.10/site-packages/openai/api_requestor.py"", line 619, in _interpret_response
    self._interpret_response_line(
  File ""/Users/tristan/miniforge3/lib/python3.10/site-packages/openai/api_requestor.py"", line 679, in _interpret_response_line
    raise self.handle_error_response(
openai.error.InvalidRequestError: Invalid URL (POST /v1/engines/gpt-3.5-turbo/chat/completions)
The above code fails, but this works!
curl https://api.openai.com/v1/chat/completions \                         
  -H ""Content-Type: application/json"" \
  -H ""Authorization: Bearer $OPENAI_API_KEY"" \
  -d '{     ""model"": ""gpt-3.5-turbo"",     ""messages"": [{""role"": ""user"", ""content"": ""Say this is a test!""}],     ""temperature"": 0.7   }'
{""id"":""...."",""object"":""chat.completion"",""created"":1680624274,""model"":""gpt-3.5-turbo-0301"",""usage"":{""prompt_tokens"":14,""completion_tokens"":5,""total_tokens"":19},""choices"":[{""message"":{""role"":""assistant"",""content"":""This is a test!""},""finish_reason"":""stop"",""index"":0}]}
OS
macOS Monterey 12.6
Python version
Python 3.10.6, but fails also in python:3.10 docker image
Library version
openai-python v0.27.0, also v0.27.3
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/376","Pricing in the api","2023-09-07T15:43:45Z","Closed issue","API-feedback","Describe the feature or improvement you're requesting
Might not be a critical request, but we have several users using our service (which uses openai api) and we'd like to be able to be able to do further cost analysis on their usage, so we want to calculate the query cost for each query and store it.
I know I can use the tokens count and multiply by the price in the pricing page, but this pricing has changed in the past and likely will change in the future so it would be awesome not to have this hardcoded on our side and rather use it from openai source.
I think the simplest approach would be to have this returned directly in the OpenAIObject returned from Completion/ChatCompletion, etc calls.
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
👍7
alifanov, jucor, kxtran, endolith, dannycunningham-8451, archfear, and WillBoan reacted with thumbs up emoji
All reactions
👍7 reactions"
"https://github.com/openai/openai-python/issues/375","Use python to report an error","2023-04-04T14:44:16Z","Closed issue","bug","Describe the bug
To Reproduce
Code snippets
No response
OS
macOS 12.6
Python version
Python2.7
Library version
openai-0.6.4-py2.7.egg-info
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/372","查询余额时出现错误","2023-04-04T18:49:49Z","Closed issue","bug","Describe the bug
我原本期望程序能够查询到我的余额并将其返回，但是似乎出现了一些问题。请问 OpenAI 的开发人员能够提供一些帮助和支持吗？我很想知道我应该如何修改程序才能够正确地查询我的余额。
如上图，报错：
 Traceback (most recent call last):
 File ""C:\Program Files (x86)\Python311\Lib\runpy.py"", line 198, in _run_module_as_main
 return _run_code(code, main_globals, None,
 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
 File ""C:\Program Files (x86)\Python311\Lib\runpy.py"", line 88, in run_code
 exec(code, run_globals)
 File ""c:\Users\zskj.vscode\extensions\ms-python.python-2023.6.0\pythonFiles\lib\python\debugpy\adapter/../..\debugpy\launcher/../..\debugpy_main.py"", line 39, in 
 cli.main()
 File ""c:\Users\zskj.vscode\extensions\ms-python.python-2023.6.0\pythonFiles\lib\python\debugpy\adapter/../..\debugpy\launcher/../..\debugpy/..\debugpy\server\cli.py"", line 430, in main
 run()
 File ""c:\Users\zskj.vscode\extensions\ms-python.python-2023.6.0\pythonFiles\lib\python\debugpy\adapter/../..\debugpy\launcher/../..\debugpy/..\debugpy\server\cli.py"", line 284, in run_file
 runpy.run_path(target, run_name=""main"")
 File ""c:\Users\zskj.vscode\extensions\ms-python.python-2023.6.0\pythonFiles\lib\python\debugpy_vendored\pydevd_pydevd_bundle\pydevd_runpy.py"", line 321, in run_path
 return _run_module_code(code, init_globals, run_name,
 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
 File ""c:\Users\zskj.vscode\extensions\ms-python.python-2023.6.0\pythonFiles\lib\python\debugpy_vendored\pydevd_pydevd_bundle\pydevd_runpy.py"", line 135, in _run_module_code
 _run_code(code, mod_globals, init_globals,
 File ""c:\Users\zskj.vscode\extensions\ms-python.python-2023.6.0\pythonFiles\lib\python\debugpy_vendored\pydevd_pydevd_bundle\pydevd_runpy.py"", line 124, in _run_code
 exec(code, run_globals)
 File ""D:\编程\Python\开源仓库\我的仓库\chatgpt-py\使用组织的API密钥.py"", line 21, in 
 secrets=manage.get_secret(""openai"")
 ^^^^^^^^^^^^^^^^^^^^^^^^^^^
 File ""C:\Users\zskj\AppData\Roaming\Python\Python311\site-packages\openai_secret_manager.py"", line 6, in get_secret
 return openai_secret_manager.get_secret(key_name)
 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
 File ""C:\Users\zskj\AppData\Roaming\Python\Python311\site-packages\openai_secret_manager.py"", line 6, in get_secret
 return openai_secret_manager.get_secret(key_name)
 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
 File ""C:\Users\zskj\AppData\Roaming\Python\Python311\site-packages\openai_secret_manager.py"", line 6, in get_secret
 return openai_secret_manager.get_secret(key_name)
 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
 [Previous line repeated 984 more times]
 File ""C:\Users\zskj\AppData\Roaming\Python\Python311\site-packages\openai_secret_manager.py"", line 5, in get_secret
 def get_secret(key_name):
File ""C:\Users\zskj\AppData\Roaming\Python\Python311\site-packages\openai_secret_manager.py"", line 5, in get_secret
 def get_secret(key_name):
RecursionError: maximum recursion depth exceeded
To Reproduce
如下面的代码片段所示
 我原本期望程序能够查询到我的余额并将其返回，但是似乎出现了一些问题。
Code snippets
import openai_secret_manager as manageimport openaiimport syssys.setrecursionlimit(1000) # 设置递归深度限制为10000

# 获取API KEY和模型IDsecrets=manage.get_secret(""openai"")

# 连接API# openai.api_key=secrets[""api_key""]openai.api_key=""sk-FMxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx""

# 查询余额balance=openai.Organization.balance()

# 输出余额print(""余额："",balance)
OS
Windows
Python version
Python v3.11
Library version
openai库，openai_secret_manager库
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/371","Connection reset from long-running or stale API connections","2023-11-03T22:28:34Z","Closed issue","bug,fixed in v1","Describe the bug
As we've used the openai.ChatCompletion.create (with gpt-3.5-turbo), we've had intermittent
requests.exceptions.ConnectionError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))

without a clear reproduction. At first I thought it was #91 and due to too many open connections to the OpenAI servers. Now I think it looks more like #368 instead, but I have some hypotheses about it. I'm opening a new issue separate from #368 in case they're different. If this is a duplicate, we can feel free to tack on my details there.
My hypothesis is that if you have a long running process (like a web server), and it calls out to OpenAI, that periods of inactivity cause the server side to terminate the connection and it takes a long time for the client to reestablish the connection. I dug into related issues on the requests side (like this one, psf/requests#4937) that hinted at the root cause. Essentially, what I think is happening is that,
First connection is made to OpenAI, returns a result, requests maintains a connection under the hood with default keep-alive
some time passes, in my experience, around 10 minutes should do
New connection is made to OpenAI, but the client throws a ConnectionResetError 
A new call after this succeeds
I believe that the OpenAI servers are terminating the connection after a brief time (perhaps minutes) but the client still tries to keep it alive.
The reason why I think this is a bug worth reporting is that I think you could modify the client code so it responds more gracefully to these server-side settings. Changing some of the keep-alive settings from the default ones would help out several folks using this.
To Reproduce
Write a long-running program. In our case, we have a Python web server running FastAPI
As part of a route for the server, call OpenAI to do some work. In our case, we're calling openai.ChatCompletion.create with gpt-3.5-turbo to manipulate some input language and respond back with it
Run the server and call the endpoint once
Wait 10 minutes
Call the endpoint again
You'll likely get a Connection reset by peer issue on the second call
Code snippets
No response
OS
Linux
Python version
Python v3.8
Library version
openai-python 0.27.2
 The text was updated successfully, but these errors were encountered: 
👍22
jackadair, hc20k, milovan68, zwhitchcox, peternagy-seon, indrasvat, Leo-Mooney, AlexandreDeRiemaecker, hansvdam, sergej-tretjakov, and 12 more reacted with thumbs up emoji
All reactions
👍22 reactions"
"https://github.com/openai/openai-python/issues/368","Intermittent Connection Aborted Error on Moderation Endpoint","2024-03-03T00:47:31Z","Closed as not planned issue","bug","Describe the bug
I'll leave this specific to my use case but as others have reported on the forums there is a intermittent error:
Error communicating with OpenAI: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))
In our case we are encountering this on the Moderation endpoint. Only started happening within the last week or so.
To Reproduce
Make Request to Moderation endpoint using openai.Moderation.create()
It may work it may not, our users have found that this most often happens at the beginning of a conversation thread, the fact that it is so intermittent is what is making me believe this is a server error on OpenAI's part.
Code snippets
try:
        input = body[""messages""][-1][""content""]
        moderation = openai.Moderation.create(input=input, model='text-moderation-latest')
        moderation_flagged = moderation[""results""][0][""flagged""]
    except Exception as e:
        print(e) #error is received here
        return EventSourceResponse(error_generate(message=""Error: Unable to moderate prompt"", status_code=500))
OS
macOS/Windows
Python version
Python v3.9.16
Library version
0.27.2
 The text was updated successfully, but these errors were encountered: 
👍1
kaikaiyao reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/openai-python/issues/367","Getting a fix response data structure","2023-04-04T00:15:32Z","Closed issue","bug","Describe the bug
I sent a prompt and ask to return the result with a fixed structure by adding ""please return these ideas as a JSON Object with the structure { 'Idea':['Title':'string','Description':'string']} and using a maximum of 567 tokens per idea."" to the end of prompt. but I have different result every time I try to get data.
To Reproduce
        var queryModel = new openAIModel
        {
            model = 'text-davinci-002',
            prompt = '',
            temperature = artificialIntelligencePreference.Temperature,
            max_tokens = maxCompletionLength
        };

string json = JsonConvert.SerializeObject(queryModel);
request.AddJsonBody(json);
var responseContent = templateApi.Execute(request); ---> response = openai.ChatCompletion.create(model=request_data[""model""],
 max_tokens=request_data[""max_tokens""],
 temperature=request_data[""temperature""],
 messages = request_data[""message""])
 return responseContent.Content;
Code snippets
this is the result which I get :

1- {""choices"":[{""finish_reason"":""stop"",""index"":0,""logprobs"":null,""text"":""\n\n1. Join or attend an event for senior .NET developers\n2. Get a mentor to help guide your development journey\n3. Use online resources to improve your understanding of the platform\n4. Use social media to connect with other developers and learn from them\n5. Attend a .NET development conference\n6. Read books and blog posts about .NET development\n7. Watch online video tutorials about .NET development\n8. Take an online course about .NET development\n9. Use Visual Studio to its fullest potential\n10. Get involved in the open source .NET community""}],""created"":1680191774,""id"":""cmpl-6zovGa10EKPEVJBpnB1ZkiRmkHcmg"",""model"":""text-davinci-002"",""object"":""text_completion"",""usage"":{""completion_tokens"":119,""prompt_tokens"":38,""total_tokens"":157}}

2-{""choices"":[{""finish_reason"":""stop"",""index"":0,""logprobs"":null,""text"":""\n\n1. ASP.NET Core MVC\n\nDescription: ASP.NET Core MVC is a framework for building web applications using the Model-View-Controller design pattern. It is a powerful tool for creating rich and dynamic web applications.\n\n2. C#\n\nDescription: C# is a powerful object-oriented programming language that enables developers to create robust and scalable applications.\n\n3. .NET Framework\n\nDescription: The .NET Framework is a comprehensive and integrated development platform that provides a wide range of services and libraries for building applications.\n\n4. SQL Server\n\nDescription: SQL Server is a relational database management system that enables developers to create and manage databases.\n\n5. Visual Studio\n\nDescription: Visual Studio is an integrated development environment that provides a complete set of tools for developing applications.""}],""created"":1680192829,""id"":""cmpl-6zpCHmuiUqbaWy8L1mP2ptcz2kuFh"",""model"":""text-davinci-002"",""object"":""text_completion"",""usage"":{""completion_tokens"":170,""prompt_tokens"":64,""total_tokens"":234}}
OS
windows10
Python version
python-dotenv==0.21.1
Library version
openai-python v0.26.04
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/366","Whisper Timestamp support.","2023-11-10T03:17:41Z","Closed issue","No label","Describe the feature or improvement you're requesting
output returned by Whisper should contain sentence-level ad word-level timestamps.
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
👍1
GriesserP reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/openai-python/issues/365","Suspect issues on python 3.11.2 with error: AttributeError: module 'subprocess' has no attribute 'PIPE' due to white space in file path","2023-04-04T02:27:55Z","Closed as not planned issue","bug","Describe the bug
I'm facing issues with openai on Python 3.11.2. When I run the demo code, I'm getting the following issues. I suspect there's an issue due to a space in the Python file path. Please let me know of any workaround or fixes:
Please note that the Python script is in a folder: Z. Temp This folder has a space.
/Users/mithun % /usr/local/bin/python3 ""/Users/mithun/Documents/Z. Temp/Python/GPT_Test2.py""
 Traceback (most recent call last):
 File ""/Users/mithun/Documents/Z. Temp/Python/GPT_Test2.py"", line 1, in 
 import openai
 File ""/usr/local/lib/python3.11/site-packages/openai/init.py"", line 9, in 
 from openai.api_resources import (
 File ""/usr/local/lib/python3.11/site-packages/openai/api_resources/init.py"", line 1, in 
 from openai.api_resources.audio import Audio # noqa: F401
 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
 File ""/usr/local/lib/python3.11/site-packages/openai/api_resources/audio.py"", line 4, in 
 from openai import api_requestor, util
 File ""/usr/local/lib/python3.11/site-packages/openai/api_requestor.py"", line 1, in 
 import asyncio
 File ""/usr/local/Cellar/python@3.11/3.11.2_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/init.py"", line 8, in 
 from .base_events import *
 File ""/usr/local/Cellar/python@3.11/3.11.2_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py"", line 40, in 
 from . import events
 File ""/usr/local/Cellar/python@3.11/3.11.2_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/events.py"", line 203, in 
 class AbstractEventLoop:
 File ""/usr/local/Cellar/python@3.11/3.11.2_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/events.py"", line 511, in AbstractEventLoop
 stdin=subprocess.PIPE,
 ^^^^^^^^^^^^^^^
 AttributeError: module 'subprocess' has no attribute 'PIPE'
However, when I run the sample code from MacOS Desktop, I'm no longer getting the subprocess error, but rather the expected API key error. Note that the file path does not have any spaces.
/Users/mithun % /usr/local/bin/python3 /Users/mithun/Desktop/GPT_Test2.py
 Traceback (most recent call last):
 File ""/Users/mithun/Desktop/GPT_Test2.py"", line 5, in 
 models = openai.Model.list()
 ^^^^^^^^^^^^^^^^^^^
 File ""/usr/local/lib/python3.11/site-packages/openai/api_resources/abstract/listable_api_resource.py"", line 60, in list
 response, _, api_key = requestor.request(
 ^^^^^^^^^^^^^^^^^^
 File ""/usr/local/lib/python3.11/site-packages/openai/api_requestor.py"", line 226, in request
 resp, got_stream = self._interpret_response(result, stream)
 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
 File ""/usr/local/lib/python3.11/site-packages/openai/api_requestor.py"", line 619, in _interpret_response
 self._interpret_response_line(
 File ""/usr/local/lib/python3.11/site-packages/openai/api_requestor.py"", line 682, in _interpret_response_line
 raise self.handle_error_response(
 openai.error.AuthenticationError: Incorrect API key provided: sk-dvmDm***************************************po9e. You can find your API key at https://platform.openai.com/account/api-keys.
To Reproduce
Execute the sample code on MacOS Desktop and another folder with a space in the file path
Code snippets
import openaiopenai.api_key = ""sk-xxxx""

# list modelsmodels = openai.Model.list()

# print the first model's idprint(models.data[0].id)

# create a completioncompletion = openai.Completion.create(model=""ada"", prompt=""Hello world"")

# print the completionprint(completion.choices[0].text)
OS
macOS
Python version
Python v3.11.2
Library version
openai-python v0.27.2
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/363","Using 'logit_bias' to prevent chatgpt or gpt4 from stop generating","2024-03-03T00:48:19Z","Closed as not planned issue","bug","Describe the bug
I have reported here, but probably I should also report here.
Hi,
 as the example in the official documentation, we can pass logit_bias={""50256"": -100} to the Completion API to prevent the <|endoftext|> token from being generated.
I'm trying to do the same for ChatCompletion but it seems doesn't work.
To Reproduce
Here is an example:
import os
import openai
openai.api_key = os.getenv(""OPENAI_API_KEY"")

completion = openai.ChatCompletion.create(
    model=""gpt-3.5-turbo"",
    messages=[
        {""role"": ""user"", ""content"": ""Hello!""}
    ],
    logit_bias={'100257': -100}
)

where I get <|endoftext|> token id for gpt-3.5-turbo by:
import tiktoken
print(tiktoken.encoding_for_model('gpt-3.5-turbo').eot_token)

#results: 100257

I still getting ""finish_reason"": ""stop"" in the returned response.
I also found a positive value works. If set logit_bias={'100257': 100}, the generation will stop immediately and return an empty string message. Seems only negative value doesn't work. Is this a bug or on purpose?
Code snippets
No response
OS
macOS
Python version
Python3.8
Library version
v0.27.2
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/362","error_message='Resource not found' for gpt-35-turbo model in Azure OpenAI Service","2024-03-03T00:45:26Z","Closed issue","Azure,bug","Describe the bug
It was successfully called via python openai library. But from a few days ago. It returns error_code=404 error_message='Resource not found' error_param=None error_type=None message='OpenAI API error received' stream_error=False.
I am pretty sure that the model deployment is existed since I can still call the endpoint on Postman and get the response.
 And I also tried to delete and deploy it again in Azure OpenAI service. But still not work on python side.
Is this issue from Microsoft side? or anyone can help me figure out?
To Reproduce
response = openai.ChatCompletion.create(
        engine=""NAME of Model Deployment"",
        messages=msg,
        temperature=temperature,
        max_tokens=max_tokens,
        top_p=top_p,
        frequency_penalty=frequency_penalty,
        presence_penalty=presence_penalty,
        stop=stop)

Code snippets
No response
OS
macOS
Python version
Python v3.10
Library version
openai-python 0.27.2
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/360","Invalid URL (POST /v1/completions)","2023-03-30T20:23:33Z","Closed issue","bug","Describe the bug
CHATGPT 4 will not allow new requests even though user haven't hit their request limitation and have paid for the plus subscription. They are also noticing their payment information has been removed from their account. They have found paying for an additional subscription has fixed the issue, even though they already have an active subscription.
To Reproduce
open chatgpt4 and enter any request
 The error pops up
Code snippets
Invalid URL (POST /v1/completions)
OS
macOS
Python version
PYTHON V.7.1
Library version
OPENAI-PYTHON V0.26.4
 The text was updated successfully, but these errors were encountered: 
👍5
morgenthum, YuraLi88, Krol22, Benjoyo, and Alevale reacted with thumbs up emoji
All reactions
👍5 reactions"
"https://github.com/openai/openai-python/issues/358","request_id is not work","2023-03-31T03:57:13Z","Closed issue","bug","Describe the bug
I want to write a simple chatpgt interaction program that can record the history of conversations each time chatgpt talks to the user. According to the suggestion on the web, when chatgpt returns the result, I need to get the request_id of it and pass it to the next conversation. But when I do this, it doesn't work
Here is the result of the run:

To Reproduce
just run the following code 👇
Code snippets
this is my demo：

import openaiopenai.api_key = ""sk-......""

contents = [
    ""hello, my name is todo"",
    ""do you know my name in this session""
]
id = Nonefor content in contents:
    chat = openai.ChatCompletion.create(
            model=""gpt-3.5-turbo"",
            messages=[
                {""role"": ""user"", ""content"": content}
            ],
            user = ""user0001"",
            request_id = id
        )
    message = chat.choices[0].message.content
    id = chat.id
    print(""message: "", message)


### OS

Linux team317 5.15.90.1-microsoft-standard-WSL2

### Python version

python v3.10

### Library version

openai-python 0.27.2

 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/357","Can I request my account balance in Python?","2023-03-30T18:36:03Z","Closed issue","No label","Describe the feature or improvement you're requesting
Can I request my account balance in Python, rather than viewing https://platform.openai.com/account/usage.
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/355","pip install openai==0.27.2 is not working","2023-03-29T21:30:29Z","Closed issue","bug","Describe the bug
I'm getting this error when trying to install latest version of openai python pip package.
ERROR: Ignored the following versions that require a different python version: 0.11.0 Requires-Python >=3.7.1; 0.11.1 Requires-Python >=3.7.1; 0.11.2 Requires-Python >=3.7.1; 0.11.3 Requires-Python >=3.7.1; 0.11.4 Requires-Python >=3.7.1; 0.11.5 Requires-Python >=3.7.1; 0.11.6 Requires-Python >=3.7.1; 0.12.0 Requires-Python >=3.7.1; 0.13.0 Requires-Python >=3.7.1; 0.14.0 Requires-Python >=3.7.1; 0.15.0 Requires-Python >=3.7.1; 0.16.0 Requires-Python >=3.7.1; 0.18.0 Requires-Python >=3.7.1; 0.18.1 Requires-Python >=3.7.1; 0.19.0 Requires-Python >=3.7.1; 0.20.0 Requires-Python >=3.7.1; 0.22.0 Requires-Python >=3.7.1; 0.22.1 Requires-Python >=3.7.1; 0.23.0 Requires-Python >=3.7.1; 0.23.1 Requires-Python >=3.7.1; 0.24.0 Requires-Python >=3.7.1; 0.25.0 Requires-Python >=3.7.1; 0.26.0 Requires-Python >=3.7.1; 0.26.1 Requires-Python >=3.7.1; 0.26.2 Requires-Python >=3.7.1; 0.26.3 Requires-Python >=3.7.1; 0.26.4 Requires-Python >=3.7.1; 0.26.5 Requires-Python >=3.7.1; 0.27.0 Requires-Python >=3.7.1; 0.27.1 Requires-Python >=3.7.1; 0.27.2 Requires-Python >=3.7.1
ERROR: Could not find a version that satisfies the requirement openai==0.27.2 (from versions: 0.0.2, 0.1.0, 0.1.1, 0.1.2, 0.1.3, 0.2.0, 0.2.1, 0.2.3, 0.2.4, 0.2.5, 0.2.6, 0.3.0, 0.4.0, 0.6.0, 0.6.1, 0.6.2, 0.6.3, 0.6.4, 0.7.0, 0.8.0, 0.9.0, 0.9.1, 0.9.2, 0.9.3, 0.9.4, 0.10.0, 0.10.1, 0.10.2, 0.10.3, 0.10.4, 0.10.5)
ERROR: No matching distribution found for openai==0.27.2

I'm on conda environment with python 3.10.10.
pip install openai seems to install older version of 0.8.0 on my system and not the latest version, my pip is updated.
Collecting openai
  Using cached openai-0.10.5.tar.gz (157 kB)
  Preparing metadata (setup.py) ... done
Requirement already satisfied: requests>=2.20 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from openai) (2.28.1)
Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from openai) (4.64.1)
  Using cached openai-0.10.4.tar.gz (157 kB)
  Preparing metadata (setup.py) ... done
  Using cached openai-0.10.3.tar.gz (157 kB)
  Preparing metadata (setup.py) ... done
  Using cached openai-0.10.2.tar.gz (156 kB)
  Preparing metadata (setup.py) ... done
  Using cached openai-0.10.1.tar.gz (155 kB)
  Preparing metadata (setup.py) ... done
  Using cached openai-0.10.0.tar.gz (155 kB)
  Preparing metadata (setup.py) ... done
  Using cached openai-0.9.4.tar.gz (156 kB)
  Preparing metadata (setup.py) ... done
  Using cached openai-0.9.3.tar.gz (155 kB)
  Preparing metadata (setup.py) ... done
  Using cached openai-0.9.2.tar.gz (155 kB)
  Preparing metadata (setup.py) ... done
  Using cached openai-0.9.1.tar.gz (156 kB)
  Preparing metadata (setup.py) ... done
  Using cached openai-0.9.0.tar.gz (155 kB)
  Preparing metadata (setup.py) ... done
  Using cached openai-0.8.0-py3-none-any.whl
Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from requests>=2.20->openai) (1.26.12)
Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from requests>=2.20->openai) (2022.9.24)
Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from requests>=2.20->openai) (3.4)
Requirement already satisfied: charset-normalizer<3,>=2 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from requests>=2.20->openai) (2.1.1)
Installing collected packages: openai
Successfully installed openai-0.8.0

To Reproduce
pip install openai==0.27.2 on M1 Macbook
Code snippets
No response
OS
macOS
Python version
3.10.10
Library version
0.8.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/354","The actual result and expected results are not matching after fine tuning.","2023-03-30T04:00:04Z","Closed issue","bug","Describe the bug
After fine tuning the model with custom data, there is a difference in the actual result and expected result.
To Reproduce
Example:
 {""prompt"":""Who is the highest scorer in the NBA?"",""completion"":"" The highest scorer in the NBA is currently Stephen Curry, with an average of 30.1 points per game.""}
Prompt: Who is the highest scorer in the NBA?
 Actual Result: The highest scorer in the NBA is currently Stephen Curry, with an average of 30.1 points per game.
 This player has an average field goal percentage of 58.5%. This player has an average 3-point field goal percentage of 56.3%. This player has an average free throw percentage of 64.9%. This player has an average field goal percentage of 54.7%. This player has an average 3-point field goal percentage of 54.7%. This player has an average free throw percentage of 51.1%. This player has an average field goal percentage of 49.8%. This player has an average 3-point field goal percentage of 55.2%. This player has an average free throw percentage of 55.2%. This player has an average 3-point field goal percentage of 56.3%. This player has an average free throw percentage of 54.7%. This player has an average 3-point field goal percentage of 54.7%. This player has an average free throw percentage of 51.1%. This player has an average field goal percentage of 51.1%. This player has an average 3-point field goal percentage of 56.3%. This player has an average free throw percentage of 54.7%. This player
Expected Result: The highest scorer in the NBA is currently Stephen Curry, with an average of 30.1 points per game.
Code snippets
    # Generate a response from the fine-tuned model
    response = openai.Completion.create(
        engine=model_id,
        prompt=prompt,
        max_tokens=1024,
        n=1,
        stop=None,
        temperature=0.7
    width=80, height=20)
OS
macOS
Python version
3.8
Library version
openai 0.27.2
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/351","async openai.audio.atranscribe doesn't want to take aiofiles (async file) object","2024-01-02T18:18:11Z","Closed issue","bug","Describe the bug
Hello. I tried to pass an aiofiles object directly to async atranscribe but got an error about expected file like object, not generator.
async def some_function():
	async with aiofiles.open(fwav,""rb"") as f:
		r=await openai.Audio.atranscribe(""whisper-1"", f, response_format=""verbose_json"")

Isn't it bad to pass file like objects to an async functions? regular file objects are blocking and synchronous and they will block the concurrent task.
 If i'm not right, comment out and close the issue. Thank you.
To Reproduce
pip install aiofiles.
enter the code that I wrote earlier, but as the first param to open, pass the filename
run it. and you will get a beautifull error and the windows xp crytical error sound will come from your computer. no jokin'.
Code snippets
No response
OS
windows 10 but is it really necessary?
Python version
python 3.11
Library version
the last. 27.2 i think.
 The text was updated successfully, but these errors were encountered: 
👍4
SantosXP, kaaloo, ssilim, and aitmrza reacted with thumbs up emoji
All reactions
👍4 reactions"
"https://github.com/openai/openai-python/issues/349","Exception is thrown during parsing of response to a request which triggered Azure's content management","2023-11-10T03:27:01Z","Closed issue","Azure,bug,fixed in v1","Describe the bug
When the Azure content management system flags a request, the library fails to handle the response, causing an exception to be thrown.
Example stack trace:
Traceback (most recent call last):
  File ""/Users/WSkinner/.pyenv/versions/3.10.7/envs/ml-generative/lib/python3.10/site-packages/openai/api_requestor.py"", line 331, in handle_error_response
    error_data = resp[""error""]
TypeError: string indices must be integers

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/Users/WSkinner/work/ripcord/ml-generative/bug.py"", line 9, in <module>
    response = openai.Completion.create(
  File ""/Users/WSkinner/.pyenv/versions/3.10.7/envs/ml-generative/lib/python3.10/site-packages/openai/api_resources/completion.py"", line 25, in create
    return super().create(*args, **kwargs)
  File ""/Users/WSkinner/.pyenv/versions/3.10.7/envs/ml-generative/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py"", line 153, in create
    response, _, api_key = requestor.request(
  File ""/Users/WSkinner/.pyenv/versions/3.10.7/envs/ml-generative/lib/python3.10/site-packages/openai/api_requestor.py"", line 226, in request
    resp, got_stream = self._interpret_response(result, stream)
  File ""/Users/WSkinner/.pyenv/versions/3.10.7/envs/ml-generative/lib/python3.10/site-packages/openai/api_requestor.py"", line 619, in _interpret_response
    self._interpret_response_line(
  File ""/Users/WSkinner/.pyenv/versions/3.10.7/envs/ml-generative/lib/python3.10/site-packages/openai/api_requestor.py"", line 682, in _interpret_response_line
    raise self.handle_error_response(
  File ""/Users/WSkinner/.pyenv/versions/3.10.7/envs/ml-generative/lib/python3.10/site-packages/openai/api_requestor.py"", line 333, in handle_error_response
    raise error.APIError(
  File ""/Users/WSkinner/.pyenv/versions/3.10.7/envs/ml-generative/lib/python3.10/site-packages/openai/error.py"", line 32, in __init__
    self.error = self.construct_error_object()
  File ""/Users/WSkinner/.pyenv/versions/3.10.7/envs/ml-generative/lib/python3.10/site-packages/openai/error.py"", line 62, in construct_error_object
    or not isinstance(self.json_body[""error""], dict)
TypeError: string indices must be integers

There is a related issue, from which I have copied the prompt which triggers the content management policy. However, that issue appears to be treating the rejection of a request due to content management, resulting in a 400 status code, as a bug. This new issue is specifically regarding the openai-python library's treatment of that response, not the fact that the response was returned.
To Reproduce
To reproduce the issue, run the following code.
import osimport openai

openai.api_type = ""azure""openai.api_version = ""2023-03-15-preview""openai.api_base = ""https://rc-ai.openai.azure.com/""openai.api_key = os.getenv(""AZURE_OPENAI_API_KEY"")

response = openai.Completion.create(
    engine=""gpt-35-turbo"",
    prompt=""SUBREDDIT: r/AskReddit TITLE: Cock blocked by a friend (Who's a girl). POST: So for the past week there's ""
        ""been this girl in one of my classes I've been talking to, she's pretty cute (dyed red hair, fair skin, ""
        ""a few freckles, not ginger), she loves star wars and I suspect she's a redditor. I was going to ask her for ""
        ""her number today, but a girl i met about a year ago came and sat right where the red head had been sitting, ""
        ""effectively cock-blocking me and driving the girl I was interested in away. Now it seems like the red head ""
        ""thinks I'm uninterested in her and has since found some other guy to talk to. Has anybody been in a similar ""
        ""scenario? Advice? \nTL;DR: Got cock blocked by a friend who's a girl.""
)
Code snippets
No response
OS
macOS
Python version
3.10.7
Library version
0.27.2
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/348","Asynchronous openai.Completion.acreate method fails for ChatGPT.","2023-03-27T17:19:27Z","Closed issue","bug","Describe the bug
The following minimal code
messages=[
  {""role"": ""system"", ""content"": ""be concise""},
  {""role"": ""user"", ""content"": ""async test""}
]

async for segment in await openai.Completion.acreate(
    model=""gpt-3.5-turbo"",
    temperature=0.2,
    n=1,
    prompt=messages,
    stream=True,
):
    print(segment)

will yield the error
openai.error.InvalidRequestError: [{'role': 'system', 'content': 'be concise'}, {'role': 'user', 'content': 'async test'}] is valid under each of {'type': 'array', 'minItems': 1, 'items': {'oneOf': [{'type': 'integer'}, {'type': 'object', 'properties': {'buffer': {'type': 'string', 'description': 'A serialized numpy buffer'}, 'shape': {'type': 'array', 'items': {'type': 'integer'}, 'description': 'Array shape'}, 'dtype': {'type': 'string', 'description': 'Stringified dtype'}, 'token': {'type': 'string'}}}]}, 'example': '[1, 1313, 451, {""buffer"": ""abcdefgh"", ""shape"": [1024], ""dtype"": ""float16""}]'}, {'type': 'array', 'minItems': 1, 'maxItems': 2048, 'items': {'oneOf': [{'type': 'string'}, {'type': 'object', 'properties': {'buffer': {'type': 'string', 'description': 'A serialized numpy buffer'}, 'shape': {'type': 'array', 'items': {'type': 'integer'}, 'description': 'Array shape'}, 'dtype': {'type': 'string', 'description': 'Stringified dtype'}, 'token': {'type': 'string'}}}], 'default': '', 'example': 'This is a test.', 'nullable': False}} - 'prompt'

and setting prompt=messages, to prompt=json.loads(messages),
yields
openai.error.InvalidRequestError: This is a chat model and not supported in the v1/completions endpoint. Did you mean to use v1/chat/completions?

It is also notable that the synchronous version openai.Completion.create(...) will require the use of messages=messages instead of prompt=messages. The parameter messages does not exist as an acreate function parameter.
The synchronous version works as expected, but due to its implementation it will block the entire async loop, which openai.Completion.acreate(...) is supposed to fix.
To Reproduce
Instead of using
messages=[
  {""role"": ""system"", ""content"": ""be concise""},
  {""role"": ""user"", ""content"": ""async test""}
]

completion_stream = openai.ChatCompletion.create(
  model=""gpt-3.5-turbo"",
  temperature=0.2,
  n=1,
  messages=messages,
  stream=True,
)
for segment in completion_stream:
  print(segment)

use
messages=[
  {""role"": ""system"", ""content"": ""be concise""},
  {""role"": ""user"", ""content"": ""async test""}
]

async for segment in await openai.Completion.acreate(
    model=""gpt-3.5-turbo"",
    temperature=0.2,
    n=1,
    prompt=messages,
    stream=True,
):
    print(segment)

Code snippets
No response
OS
Ubuntu 20.04
Python version
Python v3.8.10
Library version
openai-python v0.27.2
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/347","Does the text-davinci-003 model support 4000, 4096 or 4097 tokens?","2023-04-04T02:24:40Z","Closed issue","No label","Describe the feature or improvement you're requesting
I'm just trying to make sure that the documentation is really correct, because, often, the documentation doesn't seem to contain all info necessary to understand something.
The docs say that the context length for text-davinci-003 is 4000 (now they say 4097). There would be no reason to believe that this is not correct, if most models didn't have a context length of 2048 and 4096 = 2048*2
So, can an official OpenAI employee/developer confirm that the context length for text-davinci-003 is really 4000 (4097) and not 4096?
Can you please also tell me the actual context length of models like
text-curie-001
text-babbage-001
text-ada-001
The docs now say 2049, but previously they used to say 2048.
Additional context
I already asked this question here https://community.openai.com/t/does-the-text-davinci-003-model-support-4000-or-4096-tokens/89507, but I got an answer from a bot (apparently) or a user that posts useless answers, which I don't know if I can trust. I have also tried to ask for help in the help center, but the chat doesn't even open. See this. That's why I am asking here, with the hope that I get an official answer from an official OpenAI qualified employee.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/346","one shot whisper to chat completion","2023-04-01T02:51:01Z","Closed issue","enhancement","Describe the feature or improvement you're requesting
Probably not the right place, but it would improve the api tremendously if instead of a text prompt of a chat completion request to add a feature where an audio file can be transcribed and interpreted at once.
Additional context
Right now it is necessary to do two calls.
 first: to whisper
 second: to gpt completion with the response of whisper.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/343","Issue with updating the openai.proxy field","2023-04-01T20:26:20Z","Closed as not planned issue","bug","Describe the bug
I'm experiencing an issue with updating the openai.proxy . When I make a request to OpenAI using this library, I set the openai.proxy field to a proxy value. However, if the request fails and I change the openai.proxy field to a different value, subsequent requests continue to use the old proxy value instead of the updated one.
To Reproduce
Create an array of two elements, where the first element is an invalid proxy and the second element is a valid one.
Run a loop over this array, updating the value of openai.proxy each time, and attempt to make requests to the OpenAI API.
Requests will fail on all proxies, as the value of openai.proxy does not seem to change. Additionally, if a valid proxy is placed first, all requests will succeed, as if all proxies were valid.
Code snippets
No response
OS
ubuntu
Python version
python:3.8.3
Library version
openai==0.27.2
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/342","Add compression support","2024-03-03T00:43:59Z","Closed issue","enhancement","Describe the feature or improvement you're requesting
HTTP compression is a capability that can be built into web servers and web clients to improve transfer speed and bandwidth utilization.
Additional context
eg.
br – Brotli, a compression algorithm specifically designed for HTTP content encoding, defined in RFC 7932 and implemented in all modern major browsers.
 compress – UNIX ""compress"" program method (historic; deprecated in most applications and replaced by gzip or deflate)
 deflate – compression based on the deflate algorithm (described in RFC 1951), a combination of the LZ77 algorithm and Huffman coding, wrapped inside the zlib data format (RFC 1950);
 exi – W3C Efficient XML Interchange
 gzip – GNU zip format (described in RFC 1952). Uses the deflate algorithm for compression, but the data format and the checksum algorithm differ from the ""deflate"" content-encoding. This method is the most broadly supported as of March 2011.[5]
 identity – No transformation is used. This is the default value for content coding.
 pack200-gzip – Network Transfer Format for Java Archives[6]
zstd – Zstandard compression, defined in RFC 8478
 In addition to these, a number of unofficial or non-standardized tokens are used in the wild by either servers or clients:
bzip2 – compression based on the free bzip2 format, supported by lighttpd[7]
 lzma – compression based on (raw) LZMA is available in Opera 20, and in elinks via a compile-time option[8]
 peerdist[9] – Microsoft Peer Content Caching and Retrieval
 rsync[10] - delta encoding in HTTP, implemented by a pair of rproxy proxies.
 xpress - Microsoft compression protocol used by Windows 8 and later for Windows Store application updates. LZ77-based compression optionally using a Huffman encoding.[11]
 xz - LZMA2-based content compression, supported by a non-official Firefox patch;[12] and fully implemented in mget since 2013-12-31.[13]
 The text was updated successfully, but these errors were encountered: 
🚀1
irgolic reacted with rocket emoji
All reactions
🚀1 reaction"
"https://github.com/openai/openai-python/issues/341","Error regarding knowledge cutoff","2023-03-29T23:11:19Z","Closed issue","bug","Describe the bug
In the system message to GPT-3.5-turbo, I have set the knowledge cutoff to the 27/3/23 (so it knows all the events that have happened today). But, when I ask it who the current PM is, it says Boris Johnson instead of Rishi Sunak. I have put images in the code snippets below. Am I doing anything wrong?
To Reproduce
Use GPT-3.5-turbo
Set the knowledge cutoff date to 27/3/23
Ask it who the British PM is.
Code snippets
Code (user is a variable):
messages=[
        {""role"": ""system"", ""content"": ""You are ChatGPT, a large language model trained by OpenAI. Answer as concisely as possible. Knowledge cutoff: {27/3/23} Current date: {26/3/23}""},
        {""role"": ""user"", ""content"": user},
        {""role"": ""assistant"", ""content"": ""Hi user, your question delights me the answer is:""},
    ]
Output: 
As of my knowledge cutoff date of March 27, 2023, Boris Johnson is still serving as Prime Minister of the United Kingdom, so there hasn't been a Prime Minister after him yet.
OS
Windows 11
Python version
Python v3.10.4
Library version
openai-python v0.27.2
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/338","openai tools fine_tunes.prepare_data have error missing pandas","2023-03-29T23:12:28Z","Closed as not planned issue","bug,wontfix","Describe the bug
pip install --upgrade openai
and
when ""openai tools fine_tunes.prepare_data"" have error
openai.datalib.MissingDependencyError: 

OpenAI error: 

    missing `pandas` 

To Reproduce
step 1:
 执行：
openai tools fine_tunes.prepare_data --file dataSet-10latS7FRxjUjk9FgbMwhZ.jsonl --quiet
 报错：
Analyzing...
Traceback (most recent call last):
  File ""/opt/homebrew/bin/openai"", line 8, in <module>
    sys.exit(main())
  File ""/opt/homebrew/lib/python3.8/site-packages/openai/_openai_scripts.py"", line 63, in main
    args.func(args)
  File ""/opt/homebrew/lib/python3.8/site-packages/openai/cli.py"", line 586, in prepare_data
    df, remediation = read_any_format(fname)
  File ""/opt/homebrew/lib/python3.8/site-packages/openai/validators.py"", line 477, in read_any_format
    assert_has_pandas()
  File ""/opt/homebrew/lib/python3.8/site-packages/openai/datalib.py"", line 56, in assert_has_pandas
    raise MissingDependencyError(PANDAS_INSTRUCTIONS)
openai.datalib.MissingDependencyError: 

OpenAI error: 

    missing `pandas` 

This feature requires additional dependencies:

    $ pip install openai[datalib]

step2:
 执行：
pip install openai[datalib]
 报错：
zsh: no matches found: openai[datalib]
step3:
 执行：
 pip install openai""[datalib]""
 结果：安装成功
step4:
 与step1内容一致
step5:
 执行：
pip3 install pandas
 结果：
Requirement already satisfied: pandas in /Users/jetsommax/opt/anaconda3/envs/openai/lib/python3.10/site-packages (1.5.3)
Requirement already satisfied: numpy>=1.21.0 in /Users/jetsommax/opt/anaconda3/envs/openai/lib/python3.10/site-packages (from pandas) (1.24.2)
Requirement already satisfied: pytz>=2020.1 in /Users/jetsommax/opt/anaconda3/envs/openai/lib/python3.10/site-packages (from pandas) (2022.7.1)
Requirement already satisfied: python-dateutil>=2.8.1 in /Users/jetsommax/opt/anaconda3/envs/openai/lib/python3.10/site-packages (from pandas) (2.8.2)
Requirement already satisfied: six>=1.5 in /Users/jetsommax/opt/anaconda3/envs/openai/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)

Code snippets
No response
OS
Apple M1 Max; MacOS ventura 13.2;
Python version
Python 3.10.10
Library version
openai 0.27.2
 The text was updated successfully, but these errors were encountered: 
👍10
mabebrahimi, schmik, LucilleH, whateverneveranywhere, Stefan3Zz, anas-jumpace, battie0509, zzj0402, dinal24, and ahmad-moussawi reacted with thumbs up emoji
All reactions
👍10 reactions"
"https://github.com/openai/openai-python/issues/336","Additional ClientTimeout parameter support on async API calls - sock_read, sock_connect","2023-11-03T22:29:19Z","Closed issue","fixed in v1","Describe the feature or improvement you're requesting
Want a logic to retry API if sock_read on stream is not working in <2 seconds. Sometimes this happens on a web-server due to various issues with the requesting library (requests.request / aiohttp). Currently only
            timeout = aiohttp.ClientTimeout(
                connect=request_timeout[0],
                total=request_timeout[1],
            )

is supported. Perhaps accepting additional lengths on request_timeout, or adding a new kwarg for timeout object, or accepting a dictionary instead a tuple on request_timeout would be helpful.
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/333","[Whisper] Audio format errors on valid file","2023-11-10T03:15:32Z","Closed issue","bug","Describe the bug
Hello
I am trying to integrate the whisper API into my Flask app. However I get the following error when I input the received file from the flask endpoint, I get the following error:
openai.error.InvalidRequestError: Invalid file format. Supported formats: ['m4a', 'mp3', 'webm', 'mp4', 'mpga', 'wav', 'mpeg']

However, loading the file in the interactive console works fine.
In [16]: r = openai.Audio.transcribe('whisper-1',open('../Downloads/sample.mp3','rb'))

In [17]: r
Out[17]:
<OpenAIObject at 0x192993c6750> JSON: {
  ""text"": ""This episode is actually a co-production with another podcast called Digital Folklore, which is hosted by Mason Amadeus and Perry Carpenter. We've been doing a lot of our research together and our brainstorming sessions have been so thought-provoking, I wanted to bring them on so we could discuss the genre of analog horror together. So, why don't you guys introduce yourselves so we know who's who? Yeah, this is Perry Carpenter and I'm one of the hosts of Digital Folklore. And I'm Mason Amadeus and I'm the other host of Digital Folklore. And tell me, what is Digital Folklore? Yeah, so Digital Folklore is the evolution of folklore, you know, the way that we typically think about it. And folklore really is the product of basically anything that humans create that doesn't have a centralized canon. But when we talk about digital folklore, we're talking about...""
}

To Reproduce
Create a Flask App.
Add an end point that receives an valid audio file.
pass the bytes data of the file to openai.Audio.transcribe method through 'request.files[fileName].stream.read()`.
Code snippets
The end point code:


with tempfile.TemporaryFile() as temp_file:
    temp_file.write(audio_file)
    transcript_read = openai.Audio.transcribe(""whisper-1"", temp_file)
return transcript_read
the FFprobe info of the file:
ffprobe version 4.4.1-full_build-www.gyan.dev Copyright (c) 2007-2021 the FFmpeg developers
  built with gcc 11.2.0 (Rev1, Built by MSYS2 project)
  configuration: --enable-gpl --enable-version3 --enable-static --disable-w32threads --disable-autodetect --enable-fontconfig --enable-iconv --enable-gnutls --enable-libxml2 --enable-gmp --enable-lzma --enable-libsnappy --enable-zlib --enable-librist --enable-libsrt --enable-libssh --enable-libzmq --enable-avisynth --enable-libbluray --enable-libcaca --enable-sdl2 --enable-libdav1d --enable-libzvbi --enable-librav1e --enable-libsvtav1 --enable-libwebp --enable-libx264 --enable-libx265 --enable-libxvid --enable-libaom --enable-libopenjpeg --enable-libvpx --enable-libass --enable-frei0r --enable-libfreetype --enable-libfribidi --enable-libvidstab --enable-libvmaf --enable-libzimg --enable-amf --enable-cuda-llvm --enable-cuvid --enable-ffnvcodec --enable-nvdec --enable-nvenc --enable-d3d11va --enable-dxva2 --enable-libmfx --enable-libglslang --enable-vulkan --enable-opencl --enable-libcdio --enable-libgme --enable-libmodplug --enable-libopenmpt --enable-libopencore-amrwb --enable-libmp3lame --enable-libshine --enable-libtheora --enable-libtwolame --enable-libvo-amrwbenc --enable-libilbc --enable-libgsm --enable-libopencore-amrnb --enable-libopus --enable-libspeex --enable-libvorbis --enable-ladspa --enable-libbs2b --enable-libflite --enable-libmysofa --enable-librubberband --enable-libsoxr --enable-chromaprint
  libavutil      56. 70.100 / 56. 70.100
  libavcodec     58.134.100 / 58.134.100
  libavformat    58. 76.100 / 58. 76.100
  libavdevice    58. 13.100 / 58. 13.100
  libavfilter     7.110.100 /  7.110.100
  libswscale      5.  9.100 /  5.  9.100
  libswresample   3.  9.100 /  3.  9.100
  libpostproc    55.  9.100 / 55.  9.100
Input #0, mp3, from '.\Downloads\sample.mp3':
  Metadata:
    title           : Monsters in the Static
    comment         : We look at the subgenre of analog horror, where something sinister might be lurking in the horizontal lines and vertical holds of those old VHS tapes.
    lyrics-ENG      : <p>In the subgenre of analog horror, there’s something sinister or supernatural lurking in the horizontal lines and vertical holds in those old VHS tapes. Filmmaker <a href=""https://wnuf.bigcartel.com/"">Chris LaMartina</a> explains why he wanted his mov
    album           : Imaginary Worlds
    genre           : Podcast
    date            : 2020
    encoder         : Lavf58.76.100
  Duration: 00:00:50.05, start: 0.025057, bitrate: 128 kb/s
  Stream #0:0: Audio: mp3, 44100 Hz, stereo, fltp, 128 kb/s
    Metadata:
      encoder         : Lavc58.13

OS
Windows 11
Python version
Python v10.5
Library version
0.27.2
 The text was updated successfully, but these errors were encountered: 
👍6
Tafkas, easy-Coder, glebmachine, zieen, jawwad-btrt, and AvasDream reacted with thumbs up emoji
All reactions
👍6 reactions"
"https://github.com/openai/openai-python/issues/332","Chat completions API doesnt retuns the completion text in json format.","2023-03-29T23:58:15Z","Closed issue","bug","Describe the bug
I asked the to return the response in JSON type in the chatGPT prompt. But it returns always in a raw_text manner inside {""message"":{""role"":""assistant"",""content"":
 How can I ask/make the actual response inside {""message"":{""role"":""assistant"",""content"": as JSON object instead of simple strings
To Reproduce
curl --location --insecure --request POST 'https://api.openai.com/v1/chat/completions' --header 'Authorization: Bearer token' --header 'Content-Type: application/json' --data-raw '{
 ""model"": ""gpt-3.5-turbo"",
 ""messages"": [{""role"": ""user"", ""content"": ""What is the OpenAI mission?Put the result in JSON format""}]
}'

It returns output as
{""id"":""chatcmpl-6wpbfhG1c0k0D4d74mhdwBWf66APk"",""object"":""chat.completion"",""created"":1679479419,""model"":""gpt-3.5-turbo-0301"",""usage"":{""prompt_tokens"":19,""completion_tokens"":131,""total_tokens"":150},""choices"":[{""message"":{""role"":""assistant"",""content"":""\n\n{\n  \""mission\"": \""The mission of OpenAI is to ensure that artificial intelligence (AI) benefits humanity as a whole, and to create and advance AI in a way that is safe and beneficial for everyone.\"",\n  \""focus_areas\"": [\n    \""Developing and advancing cutting-edge AI technologies\"",\n    \""Conducting research in AI safety and ethics\"",\n    \""Promoting responsible AI development and deployment\"",\n    \""Advocating for policy changes that support the safe and ethical development of AI\""\n  ],\n  \""values\"": [\n    \""Collaboration\"",\n    \""Transparency\"",\n    \""Responsibility\"",\n    \""Impact\""\n  ]\n}""},""finish_reason"":""stop"",""index"":0}]}


The content is still in string format not json
Code snippets
No response
OS
Ubuntu
Python version
Python3.9
Library version
openai==0.27.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/331","Error when trigger Azure OpenAI’s content management policy","2023-11-10T03:26:07Z","Closed as not planned issue","Azure,bug","Describe the bug
Azure OpenAI API will return in some cases
{""error"":{""message"":""The response was filtered due to the prompt triggering Azure OpenAI’s content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766"",""type"":null,""param"":""prompt"",""code"":""content_filter"",""status"":400}}

it will failed when using like this:
response = openai.Completion.create(
    engine=""text-davinci-003"",
    prompt=""something here"",
    temperature=0.3,
    max_tokens=250,
    top_p=1,
    frequency_penalty=0,
    presence_penalty=0,
    best_of=1,
    stop=None)

To Reproduce
run the code snippets
Code snippets
response = openai.Completion.create(
    engine=""text-davinci-003"",
    prompt=""SUBREDDIT: r/AskReddit TITLE: Cock blocked by a friend (Who's a girl). POST: So for the past week there's ""
         ""been this girl in one of my classes I've been talking to, she's pretty cute (dyed red hair, fair skin, ""
         ""a few freckles, not ginger), she loves star wars and I suspect she's a redditor. I was going to ask her for ""
         ""her number today, but a girl i met about a year ago came and sat right where the red head had been sitting, ""
         ""effectively cock-blocking me and driving the girl I was interested in away. Now it seems like the red head ""
         ""thinks I'm uninterested in her and has since found some other guy to talk to. Has anybody been in a similar ""
         ""scenario? Advice? \nTL;DR: Got cock blocked by a friend who's a girl."",
    temperature=0.3,
    max_tokens=250,
    top_p=1,
    frequency_penalty=0,
    presence_penalty=0,
    best_of=1,
    stop=None)
OS
Windows
Python version
Python v3.11
Library version
openai v0.27.2
 The text was updated successfully, but these errors were encountered: 
👀4
JensMadsen, AbadBnz20, axiangcoding, and rafiktelli reacted with eyes emoji
All reactions
👀4 reactions"
"https://github.com/openai/openai-python/issues/328","openai.error.APIConnectionError: Error communicating with OpenAI: HTTPSConnectionPool","2023-03-22T02:56:31Z","Closed issue","bug","Describe the bug
import openai

# Set your API keyopenai.api_key = 'sk...'  # I have filled in my own keys for this# Example OpenAI Python library requestMODEL = ""gpt-3.5-turbo""response = openai.ChatCompletion.create(
    model=MODEL,
    messages=[
        {""role"": ""system"", ""content"": ""你是一个乐于助人的助手.""},
        {""role"": ""user"", ""content"": ""咚咚.""},
        {""role"": ""assistant"", ""content"": ""是谁？""},
        {""role"": ""user"", ""content"": ""橙.""},
    ],
    temperature=0,
)

print(response)
I'm sure
My server is from the United States, there are no regional restrictions
I have tested several servers, but the result is the same
1. The above exception was the direct cause of the following exception:
Traceback (most recent call last):
  File ""/work/code/github/image_ai/main.py"", line 34, in <module>
    response = openai.ChatCompletion.create(
  File ""/home/V01/extittivns03/.pyenv/versions/3.10.10/lib/python3.10/site-packages/openai/api_resources/chat_completion.py"", line 25, in create
    return super().create(*args, **kwargs)
  File ""/home/V01/extittivns03/.pyenv/versions/3.10.10/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py"", line 153, in create
    response, _, api_key = requestor.request(
  File ""/home/V01/extittivns03/.pyenv/versions/3.10.10/lib/python3.10/site-packages/openai/api_requestor.py"", line 216, in request
    result = self.request_raw(
  File ""/home/V01/extittivns03/.pyenv/versions/3.10.10/lib/python3.10/site-packages/openai/api_requestor.py"", line 528, in request_raw
    raise error.APIConnectionError(
openai.error.APIConnectionError: Error communicating with OpenAI: HTTPSConnectionPool(host='api.openai.com', port=443): Max retries exceeded with url: /v1/chat/completions (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f4c743e5de0>: Failed to establish a new connection: [Errno 101] Network is unreachable')

2. The above exception was the direct cause of the following exception:
Traceback (most recent call last):
  File ""test.py"", line 7, in <module>
    response = openai.ChatCompletion.create(
  File ""/home/ubuntu/.local/lib/python3.8/site-packages/openai/api_resources/chat_completion.py"", line 25, in create
    return super().create(*args, **kwargs)
  File ""/home/ubuntu/.local/lib/python3.8/site-packages/openai/api_resources/abstract/engine_api_resource.py"", line 153, in create
    response, _, api_key = requestor.request(
  File ""/home/ubuntu/.local/lib/python3.8/site-packages/openai/api_requestor.py"", line 226, in request
    resp, got_stream = self._interpret_response(result, stream)
  File ""/home/ubuntu/.local/lib/python3.8/site-packages/openai/api_requestor.py"", line 619, in _interpret_response
    self._interpret_response_line(
  File ""/home/ubuntu/.local/lib/python3.8/site-packages/openai/api_requestor.py"", line 674, in _interpret_response_line
    raise error.APIError(
openai.error.APIError: HTTP code 502 from API (<html>
<head><title>502 Bad Gateway</title></head>
<body>
<center><h1>502 Bad Gateway</h1></center>
<hr><center>nginx</center>
</body>
</html>

To Reproduce
run code
Code snippets
No response
OS
ubuntu 20.04 tls
Python version
python 3.8 3.10
Library version
openai-python V0.27.2
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/327","InvalidRequestError: 'answers' is not one of ['fine-tune'] - 'purpose'","2023-03-22T02:53:49Z","Closed issue","bug","Describe the bug
Following the steps used to create an answers service at: https://platform.openai.com/docs/guides/answers
While trying the following command:
 openai.File.create(file=open(""data.json""), purpose='answers')
Getting the following error:
InvalidRequestError Traceback (most recent call last)
 Cell In[32], line 1
 ----> 1 openai.File.create(file=open(""data.json""), purpose='answers')
File c:\Users\akash.sharma\Miniconda3\envs\gptenv\lib\site-packages\openai\api_resources\file.py:84, in File.create(cls, file, purpose, model, api_key, api_base, api_type, api_version, organization, user_provided_filename)
 60 @classmethod
 61 def create(
 62 cls,
 (...)
 71 user_provided_filename=None,
 72 ):
 73 requestor, url, files = cls.__prepare_file_create(
 74 file,
 75 purpose,
 (...)
 82 user_provided_filename,
 83 )
 ---> 84 response, _, api_key = requestor.request(""post"", url, files=files)
 85 return util.convert_to_openai_object(
 86 response, api_key, api_version, organization
 87 )
File c:\Users\akash.sharma\Miniconda3\envs\gptenv\lib\site-packages\openai\api_requestor.py:226, in APIRequestor.request(self, method, url, params, headers, files, stream, request_id, request_timeout)
 205 def request(
 ...
 683 rbody, rcode, resp.data, rheaders, stream_error=stream_error
 684 )
 685 return resp
InvalidRequestError: 'answers' is not one of ['fine-tune'] - 'purpose'
To Reproduce
Just following steps on the OpeAI Answers Service Page.
Error:
InvalidRequestError Traceback (most recent call last)
 Cell In[32], line 1
 ----> 1 openai.File.create(file=open(""data.json""), purpose='answers')
File c:\Users\akash.sharma\Miniconda3\envs\gptenv\lib\site-packages\openai\api_resources\file.py:84, in File.create(cls, file, purpose, model, api_key, api_base, api_type, api_version, organization, user_provided_filename)
 60 @classmethod
 61 def create(
 62 cls,
 (...)
 71 user_provided_filename=None,
 72 ):
 73 requestor, url, files = cls.__prepare_file_create(
 74 file,
 75 purpose,
 (...)
 82 user_provided_filename,
 83 )
 ---> 84 response, _, api_key = requestor.request(""post"", url, files=files)
 85 return util.convert_to_openai_object(
 86 response, api_key, api_version, organization
 87 )
File c:\Users\akash.sharma\Miniconda3\envs\gptenv\lib\site-packages\openai\api_requestor.py:226, in APIRequestor.request(self, method, url, params, headers, files, stream, request_id, request_timeout)
 205 def request(
 ...
 683 rbody, rcode, resp.data, rheaders, stream_error=stream_error
 684 )
 685 return resp
InvalidRequestError: 'answers' is not one of ['fine-tune'] - 'purpose'
Code snippets
openai.File.create(file=open(""data.json""), purpose='answers')
OS
Windows 10
Python version
Python v 3.9.16
Library version
v0.26.4
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/326","OpenAI APR error code","2023-03-30T03:53:15Z","Closed as not planned issue","bug","Describe the bug
API Error on OpenAI when calling in the visual studio code. I don't know how to fix this.
raise error.APIConnectionError(
 openai.error.APIConnectionError: Error communicating with OpenAI: HTTPSConnectionPool(host='api.openai.com', port=443): Max retries exceeded with url: /v1/chat/completions (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:992)')))
To Reproduce
install visual studio code
enter:
 import openai
openai.api_key = ""API Key""
completion = openai.ChatCompletion.create(model=""gpt-3.5-turbo"", messages=[{""role"": ""user"", ""content"": ""Write an essay about penguins""}])
 print(completion.choices[0].message.content)
 3. Run: issue occurred
Code snippets
No response
OS
window
Python version
Python v2022.10.0
Library version
n/a
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/325","Delete model","2023-04-06T14:25:15Z","Closed issue","bug","Describe the bug
I delete model but don't disappear from list.
To Reproduce
List
Code snippets
{
      ""created_at"": 1679331655,
      ""fine_tuned_model"": ""curie:ft-personal-2023-03-20-17-13-10"",
      ""hyperparams"": {
        ""batch_size"": 1,
        ""learning_rate_multiplier"": 0.1,
        ""n_epochs"": 4,
        ""prompt_loss_weight"": 0.01
      },
      ""id"": """",
      ""model"": ""curie"",
      ""object"": ""fine-tune"",
      ""organization_id"": """",
      ""result_files"": [
        {
          ""bytes"": 1273,
          ""created_at"": 1679332390,
          ""filename"": ""compiled_results.csv"",
          ""id"": ""file-TPbWVieAJoXsL1mNaX2Du3Bz"",
          ""object"": ""file"",
          ""purpose"": ""fine-tune-results"",
          ""status"": ""processed"",
          ""status_details"": null
        }
      ],
      ""status"": ""succeeded"",
      ""training_files"": [
        {
          ""bytes"": 492,
          ""created_at"": 1679331654,
          ""filename"": ""file"",
          ""id"": ""file-C8uOZi48g7mOz39Eml01KOf7"",
          ""object"": ""file"",
          ""purpose"": ""fine-tune"",
          ""status"": ""processed"",
          ""status_details"": null
        }
      ],
      ""updated_at"": 1679332391,
      ""validation_files"": []
    },
OS
Linux
Python version
Python 3.10.6
Library version
0.27.2
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/324","Stop using connection params as global variable in the package","2023-10-18T07:36:26Z","Closed issue","fixed in v1","Describe the feature or improvement you're requesting
Currently, in order to create openai connection, we need to do the following:
import openaiopenai.organization = ""ORG""openai.api_key = os.getenv(""KEY"")

openai.Completion.create(.....)
In my use case I have Flask server with 2 endpoint, each on them uses different OpenAI credentials.
 With the current implementation, when the credentials are global param in the package, I cannot use the 2 endpoints simultaneously
Ideally, I want something like this:
import openai

with openai.create_connection(org, key, ...) as conn:
    conn.Completion.create(.....)
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
👍7
ewpatton, Mattie, TSPereira, vdobrovolskii, skvark, JoanBonnin, and Elijas reacted with thumbs up emoji
All reactions
👍7 reactions"
"https://github.com/openai/openai-python/issues/322","Chat.create removes ""timeout"" from args","2023-10-18T07:33:58Z","Closed issue","bug,fixed in v1","Describe the bug
When making a call to the ChatCompletion create method, e.g.:
response = openai.ChatCompletion.create(
        model=self.model_id,
        messages=self.get_formatted_messages(),
        timeout=10
    )

The timeout is popd off of the kwargs by this line in the completion class (link):
timeout = kwargs.pop(""timeout"", None)
The pop method removes the arg from kwargs before it's passed to super, which means that the timeout doesn't have the intended effect (the local method there does do some stuff with it, but that would only allow you to increase the timeout).
To Reproduce
Run an API call with a shorter timeout:
response = openai.ChatCompletion.create(
        model=self.model_id,
        messages=self.get_formatted_messages(),
        timeout=10
    )

And observe that said timeout is not respected. The timeout remains 600 seconds.
Code snippets
No response
OS
macOS
Python version
3.10.9
Library version
openai==0.27.2
 The text was updated successfully, but these errors were encountered: 
👍7
Daedra22, paramendula, xcnkx, zzh1996, nbro10, huntzhan, and SidJain1412 reacted with thumbs up emoji
All reactions
👍7 reactions"
"https://github.com/openai/openai-python/issues/321","No rate limit error when using asynchronous calls for the chatgpt api","2023-12-31T00:08:03Z","Closed issue","bug","Describe the bug
When using the ChatGPT api with asynchronous calls, I do not receive rate error limits, instead the code seems stuck.
To Reproduce
Here are 2 example codes that use asynchronous calls.
Note that I learned about asynchronous calls for this purpose and so have little experience.
1 Without the Python wrapper
import asyncio
from aiohttp import ClientSession

openai_api_key = ""key_here""

headers = {
    ""Content-Type"": ""application/json"",
    ""Authorization"": ""Bearer "" + openai_api_key,
}
url = ""https://api.openai.com/v1/chat/completions""

async def ChatBatch(prompts,temperature=0,model= ""gpt-3.5-turbo"") :

    async def Chat_Result(session, prompt, url=url, headers=headers):
        data = {""model"": model,
            ""messages"": [{""role"": ""user"", ""content"": prompt}],
            ""temperature"": temperature
        }
        async with session.post(url, headers=headers, json=data) as result:
            return await result.json()

    async def get_response():
        results = []
        async with ClientSession() as session:
            for prompt in prompts:
                result = Chat_Result(session, prompt)
                results.append(result)
            return await asyncio.gather(*results)


    # return await get_response()
    return [response['choices'][0]['message']['content'] for response in await get_response()]


test :
await ChatBatch([""Hello how are you ?""]*30)
2 With the Python Wrapper
import openai
import asyncio
from aiohttp import ClientSession

openai.api_key = ""key_here""


openai.aiosession.set(ClientSession())

async def create_completion(prompt=""How are you ?""):
    return await openai.ChatCompletion.acreate(messages=[{""role"": ""user"", ""content"": prompt}], model=""gpt-3.5-turbo"")

async def main():

    return await asyncio.gather(*[create_completion() for _ in range(14)])

answers=await main()

await openai.aiosession.get().close()

answers

Code snippets
No response
OS
Linux
Python version
Python 3.9.12
Library version
openai v0.27
 The text was updated successfully, but these errors were encountered: 
👍1
phoneee reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/openai-python/issues/318","Must provide an 'engine' when using ChatCompletion api","2023-12-31T00:07:42Z","Closed issue","Azure,bug","Describe the bug
I followed the instructions from https://github.com/openai/openai-cookbook/blob/main/examples/How_to_format_inputs_to_ChatGPT_models.ipynb
 but got an error:
 openai.error.InvalidRequestError: Must provide an 'engine' or 'deployment_id' parameter to create a <class 'openai.api_resources.chat_completion.ChatCompletion'>
To Reproduce
using a Azure openai endpoint
 execute the following code snippets with your own api_key
import openai
openai.api_type = ""azure""
# Example OpenAI Python library request
MODEL = ""gpt-3.5-turbo""
response = openai.ChatCompletion.create(
    model=MODEL,
    messages=[
        {""role"": ""system"", ""content"": ""You are a helpful assistant.""},
        {""role"": ""user"", ""content"": ""Knock knock.""},
        {""role"": ""assistant"", ""content"": ""Who's there?""},
        {""role"": ""user"", ""content"": ""Orange.""},
    ],
    temperature=0,
)

response

Code snippets
import openaiopenai.api_type = ""azure""# Example OpenAI Python library requestMODEL = ""gpt-3.5-turbo""response = openai.ChatCompletion.create(
    model=MODEL,
    messages=[
        {""role"": ""system"", ""content"": ""You are a helpful assistant.""},
        {""role"": ""user"", ""content"": ""Knock knock.""},
        {""role"": ""assistant"", ""content"": ""Who's there?""},
        {""role"": ""user"", ""content"": ""Orange.""},
    ],
    temperature=0,
)

response
OS
macOS
Python version
Python 3.9.13
Library version
openai-python-0.27.2
 The text was updated successfully, but these errors were encountered: 
👍1
jonathanbell reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/openai-python/issues/315","Support openai api --json completions.create ...","2023-03-17T13:50:02Z","Open issue","CLI,enhancement","Describe the feature or improvement you're requesting
I have this tool I made which I'm using day to day now: https://github.com/backus/ai.sh
It is super useful and I actually like that it is written in pure bash because it is very portable and it is also a testament to just how easy it is to add some AI magic to your application. That said, one thing I'd like to add to the tool is the ability to see why the completion finished generating (did it hit a stop token or did it hit the token limit?).
I could do this to switching to a pure cURL, but I'd love if I could just tell the client to give me JSON instead of just the .choices[0].text. Would be really easy then to use jq to manipulate the response inside of scripts.
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/314","Transcription function cannot handle response format specifications of ""text"" or ""vtt""","2023-03-20T23:52:42Z","Closed issue","bug","Describe the bug
Transcription function cannot handle response format specifications of ""text"" or ""vtt"". When those are specified, an exception is raised.
To Reproduce
Using Python, obtain a transcript for an audio file: transcript = openai.Audio.transcribe(""whisper-1"", audio_file, response_format=response_format).
Note that response_format=""json"" and response_format=""verbose_json"" work without issue.
Replace response format with response_format=""vtt"" or response_format=""text""
Note that the code now raises an exception json.decoder.JSONDecodeError
Code snippets
No response
OS
Linux
Python version
Python 3.10.6
Library version
openai 0.27.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/312","Async openai.Embedding.acreate forcibly prints INFO, whereas openai.Embedding.create prints DEBUG","2023-10-18T07:27:23Z","Closed issue","bug,fixed in v1","Describe the bug
When openai.Embedding.acreate gets called many times, it prints a lot to STDERR, like so:
INFO:openai:message='OpenAI API response' path=https://api.openai.com/v1/engines/text-embedding-ada-002/embeddings processing_ms=350 request_id=681e326f4d3241ef9427e95c9eede485 response_code=200
INFO:openai:message='OpenAI API response' path=https://api.openai.com/v1/engines/text-embedding-ada-002/embeddings processing_ms=350 request_id=681e326f4d3241ef9427e95c9eede485 response_code=200
...

and there's no way to stop it to clutter the console. That is because util.log_info is used in APIRequestor.arequest_raw whereas util.log_debug is used in APIRequestor.request_raw, and it actually prints directly to STDERR.
https://github.com/openai/openai-python/blob/main/openai/util.py#L63-L67
def log_info(message, **params):
    msg = logfmt(dict(message=message, **params))
    if _console_log_level() in [""debug"", ""info""]:
        print(msg, file=sys.stderr)
    logger.info(msg)
It may be intentional, and avoidable as follows, but it also suppress other logging. The different behaviors between sync/async seems like it's a bug.
logger = logging.getLogger(""openai"")
logger.setLevel(logging.WARN)
... API calls ...
logger.setLevel(logging.INFO)
To Reproduce
Call openai.Embedding.acreate
Watch the console output for STDERR
Code snippets
No response
OS
macOS
Python version
Python v3.10.10
Library version
openai-python v0.26.2
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/309","ChatML: name property should support '@' character","2023-11-10T04:08:21Z","Closed issue","bug","Describe the bug
Lots of existing usernames contain @ as namespace and prefixing. Complete understand why this is defined narrowly, but you are pushing a bunch of work on to developers and this surprised me.
File ""/Users/dmp/Library/Python/3.9/lib/python/site-packages/openai/api_requestor.py"", line 679, in interpret_response_line
 raise self.handle_error_response(
 openai.error.InvalidRequestError: '@username' does not match '^[a-zA-Z0-9-]{1,64}$' - 'messages.6.name'
To Reproduce
`#add the user + message in the third argument
 whoiam = getpass.getuser()
 whoiam = ""@"" + whoiam
 prompt = {""role"": ""user"", ""content"": sys.argv[3], ""name"": whoiam}
#talk to chatGPT
 completion = openai.ChatCompletion.create(
 model = ""gpt-3.5-turbo"",
 messages = chatml + [prompt]
 )`
Code snippets
#add the user + message in the third argumentwhoiam = getpass.getuser()
whoiam = ""@"" + whoiamprompt = {""role"": ""user"", ""content"": sys.argv[3], ""name"": whoiam}

#talk to chatGPTcompletion = openai.ChatCompletion.create(
  model = ""gpt-3.5-turbo"", 
  messages = chatml + [prompt]
)
OS
macOS
Python version
Python
Library version
openai-python
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/307","HTTP 500 Internal Server Error & HTTP 429 Too Many Requests","2023-12-31T00:06:14Z","Closed as not planned issue","bug","Describe the bug
Today is March 14, 2023, I read logs and find some ""HTTP 500 Internal Server Error"" errors, here are two request id:
65f570730b40007dde214d6c94758fce
 e5ad2d7adee633e0c72ba78c54f63e44
Please help analysis what happened.
At the same time, I meet the error ""HTTP 429 Too Many Requests"", honestly say my request is not up the limitation according to Api document, but who knows what happened.
Thank you.
To Reproduce
I submit request id above
Code snippets
No response
OS
Ubuntu
Python version
Python 3.7.1
Library version
latest
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/306","TypeError upon API response","2023-04-04T15:10:14Z","Closed issue","bug","Describe the bug
When executing the API call specified in the code snippet the following error occurs:
error.txt
Based on printouts, in some cases the API return with an ill-formed Python dictionary as below.
API response with null part of the dictionary:
 {""error"":{""message"":""The response was filtered due to the prompt triggering Azure OpenAI’s content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766"",
 ""type"":null,""param"":""prompt"",""code"":""content_filter"",""status"":400}}
To Reproduce
Run the code snippet attached on the input 'doc.txt'.
Code snippets
def prompt_openai(doc, max_tokens):

    prompt = f""""""    Create a summary of the text below.    '{doc}'    """"""
    try:
        response = openai.Completion.create(
        engine = openai.deployment,
        prompt = prompt,
        temperature = 1,
        max_tokens = max_tokens,
        top_p = 0.95,
        frequency_penalty = 0.5,
        presence_penalty = 0.5,
        stop = None#""<|im_end|>""
        )

    return response['choices'][0]['text'].replace('\n', '').replace(' .', '.').strip()
OS
Windows 10
Python version
Python 3.10.9
Library version
openai 0.27.2
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/305","Make two discuss with each other","2023-03-29T23:14:43Z","Closed issue","No label","Please move to discussions, I mistakenly opened an issue.
How about a concept where it's not us asking the questions to the AI, but reversing the process of it, making a one that asks questions, either randomized, or by providing them with specific subject to the other one?
It could be very interesting and some of the results on high enough computing power, could actually lead to answers in many fields that are unanswered. The bottleneck in this technology is how slow people think compared to computers, and provide data input at such a slow pace, it's inadequate to the potential of this technology.
Let's imagine a situation where I have a list of 100 options to choose from a book, and want to ask ChatGPT which either of them means, does and which is the best for a given subject to get the best result That's time consuming. I'd probably use optical test recognition and feed it to a file, making corrections if necessary, and that's the main problem - it's too slow. If we had two of them discussing that subject and not limited to only to only 100 options, maybe even letting them do the optical recognitions themselves, I bet the results could be very interesting, potentially leading to discoveries of new solutions to problems.
There's a lot of data and knowledge that can be combined to create new ideas, and having this kind of a tool to work with it, it would be a shame not to try.
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/304","Tiktoken says ChatGPT's API, gpt-3.5-turbo, uses the cl100k_base encoder, but it appears to use p50k_base in openai","2023-03-30T17:04:36Z","Closed issue","bug","Describe the bug
Tiktoken (https://github.com/openai/tiktoken/blob/3e8620030c68d2fd6d4ec6d38426e7a1983661f5/tiktoken/model.py#L14) shows ChatGPT's API, gpt-3.5-turbo, tiktoken encoder to be cl100k_base; however, when using the openai package if I use the cl100k_base encoder to truncate my prompt, I get the following error, but if I use p50k_base, I don't get the error. So, it appears that either the correct tokenizer is p50k_base or the wrong tokenizer may be set in openai.
openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, your messages resulted in 4104 tokens. Please reduce the length of the messages.

To Reproduce
It will fail if you run the code as is with full_text longer than 4061 tokens. However, if you change
tokenizer = tiktoken.get_encoding(""cl100k_base"" if model_name == ""gpt-3.5-turbo"" else ""p50k_base"")

to
tokenizer = tiktoken.get_encoding(""p50k_base"")

everything works as expected.
Code snippets
import tiktokenfrom langchain import OpenAI, PromptTemplate

full_text = ""The content of this article, https://nymag.com/news/features/mark-zuckerberg-2012-5/?mid=nymag_press""model_name = ""gpt-3.5-turbo""num_keyphrases = 5

# Define the prompt templatetemplate = """"""Suggest the top {num_keyphrases} keywords that best describe the most important topics or themes in following text:###TEXT: {full_text}###Top {num_keyphrases} Keywords:""""""

prompt_template = PromptTemplate(
    input_variables=[""num_keyphrases"", ""full_text""], template=template
)

# Get the top keyphrases from the article

# Load the modelllm = OpenAI(model_name=model_name, temperature=0)
# Get the maximum length of the texttokenizer = tiktoken.get_encoding(""cl100k_base"" if model_name == ""gpt-3.5-turbo"" else ""p50k_base"")
model_context_size = (
    4097 if model_name == ""gpt-3.5-turbo"" else llm.modelname_to_contextsize(model_name)
)
text_max_length = model_context_size - len(
    tokenizer.encode(
        prompt_template.format(num_keyphrases=num_keyphrases, full_text="""")
    )
)

# Truncate the text if it is too longfull_text = tokenizer.decode(tokenizer.encode(full_text)[:text_max_length])

# Get the top keyphrases from the articleresponse = llm(
    prompt_template.format(num_keyphrases=num_keyphrases, full_text=full_text)
)

print(response)
OS
macOS
Python version
Python v3.10.9
Library version
openai v0.27.2
 The text was updated successfully, but these errors were encountered: 
👀1
felix98765 reacted with eyes emoji
All reactions
👀1 reaction"
"https://github.com/openai/openai-python/issues/303","openai.Audio.transcribe's api_key does not work; openai.error.AuthenticationError is raised","2023-04-03T21:26:29Z","Closed issue","bug","Describe the bug
When passing the API Key to the api_key parameter of the openai.Audio.transcribe() method, an openai.error.AuthenticationError is thrown.
Traceback (most recent call last):
  File ""<stdin>"", line 2, in <module>
  File ""/.../venv/lib/python3.10/site-packages/openai/api_resources/audio.py"", line 55, in transcribe
    requestor, files, data = cls._prepare_request(file, file.name, model, **params)
  File ""/.../venv/lib/python3.10/site-packages/openai/api_resources/audio.py"", line 28, in _prepare_request
    requestor = api_requestor.APIRequestor(
  File ""/.../venv/lib/python3.10/site-packages/openai/api_requestor.py"", line 130, in __init__
    self.api_key = key or util.default_api_key()
  File ""/.../venv/lib/python3.10/site-packages/openai/util.py"", line 186, in default_api_key
    raise openai.error.AuthenticationError(
openai.error.AuthenticationError: No API key provided. You can set your API key in code using 'openai.api_key = <API-KEY>', or you can set the environment variable OPENAI_API_KEY=<API-KEY>). If your API key is stored in a file, you can point the openai module at it with 'openai.api_key_path = <PATH>'. You can generate API keys in the OpenAI web interface. See https://onboard.openai.com for details, or email support@openai.com if you have any questions.

Upon inspecting the trace back, it seems that the cause is that the api_key is not passed to the _prepare_request method.
openai-python/openai/api_resources/audio.py
 Line 55 in 6c23b7f
	requestor, files, data=cls._prepare_request(file, file.name, model, **params) 
When making the following modification, the AuthenticationError is not thrown and the transcription result is obtained.
-        requestor, files, data = cls._prepare_request(file, file.name, model, **params)+        requestor, files, data = cls._prepare_request(file, file.name, model, api_key=api_key, **params)
To Reproduce
pip install openai
Create sample.wav (example: say 親譲りの無鉄砲で子供の時から損ばかりしている -o sample.wav --data-format=LEF32@16000)
Run the following snippets
Code snippets
>>> OPENAI_API_KEY = ""sk-***"">>> import openai>>> with open(""sample.wav"", ""rb"") as audio_file:
...   transcript = openai.Audio.transcribe(""whisper-1"", audio_file, api_key=OPENAI_API_KEY)
OS
macOS
Python version
Python 3.10.9
Library version
openai-python 0.27.2
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/302","How can ChatGPT API support multiple conversations with an account?","2023-03-13T15:56:41Z","Closed issue","No label","Describe the feature or improvement you're requesting
How can ChatGPT API support multiple conversations with an account?
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/300","Prompt only works for the first audio","2023-12-31T00:05:57Z","Closed as not planned issue","bug","Describe the bug
Somehow, the prompt only works for my first prompt in python.
`for audio_file in sound_track:
 print('prompt_for_whisperer: ' + prompt_for_whisperer)
 transcript = openai.Audio.transcribe(
 ""whisper-1"",
 audio_file,
 response_format = 'srt',
 prompt = prompt_for_whisperer)
transcripts_ary.append(transcript)`
To Reproduce
I include the whole script below to reproduce it.
 I told whisperer to limit words within 20, it works for the first audio only.
Code snippets
#@title 安裝相關套件 (yt-dlp, openAI API, Pydub)
! pip install --upgrade pip
! pip install yt-dlp
! pip install openai
! pip install pydub#@title 下載 youtube 影片 import yt_dlp

 #@markdown ### Youtube 連結:url = 'https://youtu.be/VZD5iLl0E_E' #@param {type:""string""} 

# 抓取影片標題with yt_dlp.YoutubeDL() as ydl:
  info_dict = ydl.extract_info(url, download=False)
  video_title = info_dict.get('title', None)

filename = video_title

prompt_for_whisperer = ""This is about %s."" \
""Consider its title in your response: %s."" \
""Some Keywords you should expect: %s."" \
""Additional info: %s."" \
""Additional info: %s."" \
% \
('AWS Elastic Load Balancer' \
, filename \
, 'AZ, ASG, EC2, ELB, Target Group.' \
# the below one does not work for all
, 'Add one space before and after every English words, such as AZ, ALB, Target Group'\
, 'for every transcrtipt, limit it within 20 words, but do not cut sentence in the middel.'
)

# 設定選項ydl_opts = {
    'format': 'bestaudio/best',
    'outtmpl': filename , 
    'postprocessors': [{
        'key': 'FFmpegExtractAudio',
        'preferredcodec': 'mp3',
        'preferredquality': '192',
    }],
}


# 建立 yt_dlp 下載器物件with yt_dlp.YoutubeDL(ydl_opts) as ydl:
    ydl.download([url])
#@title 分割 youtube 影片 from pydub import AudioSegment

#@markdown ### 分割檔案的長度（單位：毫秒）:segment_length_ms = 60000 #@param {type:""integer""} segment_length_s = segment_length_ms/1000

# 載入 MP3 音檔sound = AudioSegment.from_file(f'{filename}.mp3', format='mp3')

sound_track = []
# 將音檔分割成多個檔案for i, chunk in enumerate(sound[::segment_length_ms]):
    # 設定分割檔案的檔名
    chunk.export(f'output_{i}.mp3', format='mp3')
    audio_file = open(f'output_{i}.mp3', ""rb"")
    sound_track.append(audio_file)
    

import openai#@markdown ### 填入 OpenAI API Secret Key:openai.api_key = '' #@param {type:""string""}openai.api_key = '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'

transcripts_ary = []
for audio_file in sound_track:
  print('prompt_for_whisperer: ' + prompt_for_whisperer)
  transcript = openai.Audio.transcribe(
      ""whisper-1"", 
      audio_file, 
      response_format = 'srt',
      prompt = prompt_for_whisperer)

  transcripts_ary.append(transcript)
# debugfor transcript in transcripts_ary:
  print('transcript: ' , transcript)
! pip install pysrtimport pysrt

# 轉成 subtitle 物件 subtitles = []
for transcript in transcripts_ary:
  subtitle = pysrt.from_string(transcript)
  subtitles.append(subtitle)

# 處理最後時間超過問題 for subtitle in subtitles:
  max_time = pysrt.SubRipTime(seconds = segment_length_s)
  for sub in subtitle:
    sub.start = sub.start if sub.start < max_time else max_time
    sub.end = sub.end if sub.end < max_time else max_time

# 處理字幕時間銜接問題shift_time_s = 0for subtitle in subtitles:
  shift_time = pysrt.SubRipTime(seconds = shift_time_s)
  print(""shift_time: "", shift_time)
  for sub in subtitle:
    sub.start = sub.start + shift_time
    sub.end = sub.end + shift_time    
  shift_time_s = shift_time_s + segment_length_s

# 全部合體subtitle_merged = pysrt.from_string('')
for subtitle in subtitles:
  subtitle_merged.extend(subtitle)

# 存成檔案subtitle_merged.save(filename + '.srt')
OS
maxOS
Python version
Python v3.7
Library version
openai-python v0.26.4
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/299","You exceeded your current quota, please check your plan and billing details","2023-03-13T16:07:18Z","Closed issue","bug","Describe the bug
python code:
`import openai
 openai.api_key = ""sk-m0DqPqTSAWoZ2XXXXXXXXXBZ4C"" # api_key hidden
completion = openai.ChatCompletion.create(model=""gpt-3.5-turbo"", messages=[{""role"": ""user"", ""content"": ""Hello world!""}])
 print(completion.choices[0].message.content)`
openai.error.RateLimitError: You exceeded your current quota, please check your plan and billing details.
personal accout, google about this problem. Seems like i need to bind a credit card first before i can use ths API?
To Reproduce
pip install openai
 run the code above
Code snippets
import openaiopenai.api_key = ""sk-m0DqPqTSAWoZ2XXXXXXXXXBZ4C""  # api_key hidden

completion = openai.ChatCompletion.create(model=""gpt-3.5-turbo"", messages=[{""role"": ""user"", ""content"": ""Hello world!""}])
print(completion.choices[0].message.content)
OS
Windows 10 Home Version 22H2
Python version
Python 3.10.2 (tags/v3.10.2:a58ebcc, Jan 17 2022, 14:12:15) [MSC v.1929 64 bit (AMD64)] on win32
Library version
openai.version.VERSION '0.27.1'
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/296","ERROR in ready_any_format validator: file does not appear to be in valid JSON Format","2023-04-10T15:57:17Z","Closed issue","bug","Describe the bug
When I try to use the prepare data tool for fine tuning I always get the following error that i'm not using a valid JSON file. I have even used a valid JSON validator which comes back as valid. I have a feeling this has to do with my environment more than anything.
from my understanding this tool accepts any data format, even JSON to convert to JSONL
I'm new to python so please excuse my lack of experience with the language.
I even installed it with just pip install openai and still had no luck. along with this I tried using python with pyenv but still had no luck.
And again i'm new to programming so this is one of my first ever big reports, sorry if it's not in the right format if you will
To Reproduce
Latest version of python3 was installed using home brew with
brew install python
pip3 install openai
export OPENAI_API_KEY=""<OPENAI_API_KEY>""
openai tools fine_tunes.prepare_data -f <LOCAL_FILE>
pip v23.0.1
Code snippets
user@user-MBP training_models % openai tools fine_tunes.prepare_data -f prompt_completion_pairs_manual.jsonAnalyzing...


ERROR in read_any_format validator: Your file `prompt_completion_pairs_manual.json` does not appear to be in valid JSON format. Please ensure your file is formatted as a valid JSON file.

Aborting...%
OS
macOS Ventura v13.0.1
Python version
Python 3.11.2
Library version
openai-0.27.1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/295","Version argument missing","2023-03-30T04:00:44Z","Closed issue","No label","Describe the feature or improvement you're requesting
CLI is missing a version argument.
openai --version would be a nice improvement
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
👍1
arunsathiya reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/openai-python/issues/294","pip install openai==0.27.1 unable to install","2023-03-11T03:54:10Z","Closed issue","bug","Describe the bug
Unable to install openai using pip for latest version. Works fine with pip3 install openai==0.27.0
To Reproduce
pip3 install openai
 Output:
 Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/
 Collecting openai
 Using cached openai-0.27.1.tar.gz (57 kB)
 Installing build dependencies ... done
 error: subprocess-exited-with-error
× Getting requirements to build wheel did not run successfully.
 │ exit code: 1
 ╰─> See above for output.
note: This error originates from a subprocess, and is likely not a problem with pip.
 Getting requirements to build wheel ... error
 error: subprocess-exited-with-error
× Getting requirements to build wheel did not run successfully.
 │ exit code: 1
 ╰─> See above for output.
note: This error originates from a subprocess, and is likely not a problem with pip.
 Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/
 Requirement already satisfied: python-dotenv in /usr/local/lib/python3.9/dist-packages (1.0.0)
pip3 install openai==0.27.0
 Succesful
Code snippets
No response
OS
Linux
Python version
python 3.9.16
Library version
openai-python v0.27.1
 The text was updated successfully, but these errors were encountered: 
👍8
johnny-duo, ar-mccabe, hinaloe, mchequers-convictional, claytoncohn, EvanWiederspan, panozzaj, and SaliCalgarvi reacted with thumbs up emoji❤️1
HansamalDharmananda reacted with heart emoji
All reactions
👍8 reactions
❤️1 reaction"
"https://github.com/openai/openai-python/issues/293","Client Disconnected during Fine Tune","2023-12-31T00:04:51Z","Closed issue","bug","Describe the bug
When running openai api fine_tunes.create ... the streaming cli response is continually interrupted. A bit of investigation revealed that the underlying exception is Invalid chunk encoding ""Connection broken: InvalidChunkLength(got length b'', 0 bytes read)"".
As we iterate over the events on the stream, there is a ProtocolError because a response is coming back with no bytes in it.
The full call stack:
  File ""\lib\site-packages\openai\cli.py"", line 537, in _stream_events
    for event in events:
  File ""\lib\site-packages\openai\api_resources\fine_tune.py"", line 158, in <genexpr>
    return (
  File ""\lib\site-packages\openai\api_requestor.py"", line 611, in <genexpr>
    return (
  File ""\lib\site-packages\openai\api_requestor.py"", line 107, in parse_stream
    for line in rbody:
  File ""\lib\site-packages\requests\models.py"", line 865, in iter_lines
    for chunk in self.iter_content(
  File ""\lib\site-packages\requests\models.py"", line 818, in generate
    raise ChunkedEncodingError(e)
requests.exceptions.ChunkedEncodingError: (""Connection broken: InvalidChunkLength(got length b'', 0 bytes read)"", InvalidChunkLength(got length b'', 0 bytes read))

To Reproduce
Update the cli.py to not discard all exceptions while streaming events
Start a fine tune and wait for error
Code snippets
No response
OS
Windows
Python version
3.10
Library version
0.27.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/292","Support Whisper from CLI","2023-03-20T23:55:40Z","Closed issue","enhancement","Describe the feature or improvement you're requesting
It would be nice to upload audio files with the CLI tool and get the API response.
 Thank you for considering.
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/290","newline char(\n) in stop param leads to 500 error in gpt-3.5-turbo","2023-03-29T23:54:59Z","Closed issue","bug","Describe the bug
unlike text-davinci-003 model, gpt-3.5-turbo model fails when using stop='\n'.
 maybe it would be backend problem but it would be good to have front-end translates '\n' into some form that backend server can deal with.
To Reproduce
with openai(0.27.1) package
 running this line of code would result in 500 error.
Code snippets
completion = openai.ChatCompletion.create(
  model=""gpt-3.5-turbo"",
  messages=[{""role"": ""user"", ""content"":""hello""}], 
  stop=['\n']
)


### OS

Ubuntu 18.04

### Python version

Python3.7.15

### Library version

v0.27.1

 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/288","Why do we get a openai.error.InvalidRequestError if we pass None to logit_bias even if it defaults to null?","2023-11-10T03:20:35Z","Closed issue","API-feedback,bug","Describe the bug
According to the docs, logit_bias defaults to null, but if we pass None to this parameter we get openai.error.InvalidRequestError
openai.error.InvalidRequestError: None is not of type 'object' - 'logit_bias'
So, I suppose this is a bug in this package, as None should correspond to what the docs refer to as null, or the documentation is wrong.
To Reproduce
Use the script below
Code snippets
import openai

completions = openai.Completion.create(model=""text-davinci-003"",
                                       prompt=""hello"",
                                       logit_bias=None)
print(completions)
OS
macos monterey (12.5.1)
Python version
Python 3.8.13
Library version
0.27.0
 The text was updated successfully, but these errors were encountered: 
👍1
KyleKing reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/openai-python/issues/287","Why doesn't the OpenAI API throw an error when we pass a value to frequency_penalty outside its valid range?","2023-12-31T00:04:05Z","Closed issue","bug","Describe the bug
The documentation says
Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.
However, I can pass values like 2.3 to this parameter and the API doesn't raise anything. So, I suppose this is a bug with the API or the documentation is wrong. Note: this doesn't happen with the presence_penalty parameter, which, if set to e.g. 2.2, makes the API returns an error openai.error.InvalidRequestError: 2.2 is greater than the maximum of 2 - 'presence_penalty'. Even if I set say frequency_penalty=10000, the api doesn't err. Similarly, the API doesn't err if we set this parameter to negative values like -1000. So, maybe this parameter is supposed to take any floating-point number and the documentation is wrong? Or maybe the API is buggy?
To Reproduce
You can make any call to the completions endpoint with any valid value for the other parameters and you will not get an error you're supposed to get
Code snippets
import openai
# Set your api key

completions = openai.Completion.create(model=""text-davinci-003"", 
                                       prompt=""hello"", 
                                       frequency_penalty=1000)
print(completions)

OS
macos monterey (12.5.1)
Python version
Python 3.8.13
Library version
0.27.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/286","Why doesn't this package provide a way to calculate the number of tokens in the prompt, suffix and messages parameters?","2023-12-31T00:03:14Z","Closed as not planned issue","enhancement","Describe the feature or improvement you're requesting
People often need to calculate the number of tokens in the prompt and suffix parameters of the completions endpoint or in the messages parameter of the new chat endpoint. We can use a package like tokenizers (from huggingface) or tiktoken, but it's really not clear which pre-trained model we should use to create the tokenizer or how exactly the number of tokens from these different parameters should be summed and compared against the model's context length.
So, why doesn't this package simply provide the functionality of counting the number of tokens in the prompts, suffixes and messages or the functionality to check that the sum of these tokens doesn't exceed the model's context length?
Additional context
I would be happy to work on this in my free time, if you tell me exactly how this is done under the hood for each model.
 The text was updated successfully, but these errors were encountered: 
👍1
hdragos reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/openai-python/issues/285","openai.api_base -Error communicating with OpenAI: Failed to parse:","2023-03-17T13:38:39Z","Closed issue","bug","Describe the bug
I use nginx proxy api_base “https://api.openai.com” on my server : “http://myserverip:port”
 and get a error msg:
My server is can connect https://api.openai.com
and i use this way apiBaseUrl on https://github.com/transitive-bullshit/chatgpt-api is no error
To Reproduce
[WARNING][2023-03-08 18:35:58][log.py:39] - Error communicating with OpenAI: Failed to parse: http://myserverip:port/v1/chat/completion
Code snippets
nginx conf:
server {
    listen       9999;
    server_name  127.0.0.1;
    error_log /var/log/nginx/chatgpt.error.log error;
    access_log  /var/log/nginx/chatgpt.access.log combined;
    location / {
       proxy_pass https://api.openai.com/;
    }
}

python code:

class ChatGPTModel(Model):
    def __init__(self):
        openai.api_key = model_conf(const.OPEN_AI).get('api_key')
        # https://api.openai.com/v1/chat/completions
        openai.api_base = model_conf(const.OPEN_AI).get('api_url')

python conf:
    ""openai"": {
      ""api_url"": ""http://myserverip:port/v1""
OS
Debian10.0
Python version
3.9.2
Library version
0.27.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/284","Connection failed.","2023-04-08T15:27:02Z","Closed issue","bug","Describe the bug
Request timed out: HTTPSConnectionPool(host='api.openai.com', port=443): Max retries exceeded with url: /v1/completions (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000002CE434DAC50>, 'Connection to api.openai.com timed out. (connect timeout=600)'))
To Reproduce
Request timed out: HTTPSConnectionPool(host='api.openai.com', port=443): Max retries exceeded with url: /v1/completions (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000002CE434DAC50>, 'Connection to api.openai.com timed out. (connect timeout=600)'))
Code snippets
No response
OS
win
Python version
3.11
Library version
last
 The text was updated successfully, but these errors were encountered: 
👍4
nbro10, work-peeagar, jennyckaplan, and kjonh2 reacted with thumbs up emoji
All reactions
👍4 reactions"
"https://github.com/openai/openai-python/issues/283","use openai cli ""chat_completions.create"" failed to generate a response of sufficient length, and setting ""--max-tokens"" did not work.","2023-03-29T22:47:30Z","Closed issue","bug","Describe the bug
use openai cli “chat_completions.create"" failed to generate a response of sufficient length, and setting ""--max-tokens"" did not work.
To Reproduce
By default, the number of tokens the model can return will be (4096 - prompt tokens). but not in cli mode
Code snippets
openai api chat_completions.create  -m gpt-3.5-turbo -g user '请给出10个使用chatGPT的示例'

OS
Ubuntu
Python version
Python v3.10.6
Library version
openai v0.27.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/281","openai.error.InvalidRequestError: Invalid URL (POST /v1/chat/completions) when calling the chat completions endpoints multiple times","2023-03-17T20:33:48Z","Closed issue","bug","Describe the bug
when calling
openai.ChatCompletion.create( model=""gpt-3.5-turbo-0301"", messages=[ { ""role"": ""user"", ""content"" : prompt } ], temperature=0.7, max_tokens=MAX_OUTPUT_BASE, n=1) 
multiple times in a loop I noticed that the first few attempts run perfectly fine I believe the first 3-5, however after that it fails with the error code openai.error.InvalidRequestError: Invalid URL (POST /v1/chat/completions)
which is very odd as the prior ones are running smoothly within the same exact function call...
I can verify that I'm running on openai==0.27.0 with python 3.9
To Reproduce
create an array of texts
 2 run the following for text in texts:
 MAX_TOKENS = 3500
 # used for testing to see how the token limit progrsses over time


 response = openai.ChatCompletion.create(
   model=""gpt-3.5-turbo-0301"",
   messages=[
     {
         ""role"": ""user"",
         ""content"" : ""text""
     }
   ],            
   temperature=0.7, 
   max_tokens=MAX_TOKENS, n=1)

Code snippets
No response
OS
macos
Python version
python 3.9
Library version
openai-python v0.27.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/279","Moderation bypasses proxy settings","2023-12-31T00:02:18Z","Closed issue","bug","Describe the bug
In an environment without direct access to the Internet, If you pass proxy settings to the openai object and try to perform input validation, the openai.Moderation.create() call hangs on or times out.
Those same proxies do work for openai.ChatCompletion.create() and openai.ChatCompletion.create() calls.
To Reproduce
Prepare a no-internet environment that includes http&https proxy
Import openai library, set api key and proxy
Try to moderate a message
openai.Moderation.create() hangs on forever or until timeout
Code snippets
openai.api_key = ""OPENAI_API_KEY""openai.proxy = {""http"": ""<PROXY>"", ""https"": ""<PROXY>""}

message = ""I will kill you""moderation_resp = openai.Moderation.create(input=message) # Times out
OS
Linux
Python version
Python v3.9
Library version
v0.27.0
 The text was updated successfully, but these errors were encountered: 
👍2
YangYangGirl and ddzzhen reacted with thumbs up emoji
All reactions
👍2 reactions"
"https://github.com/openai/openai-python/issues/278","acreate method fail when using async","2023-11-03T22:31:05Z","Closed issue","bug,fixed in v1","Describe the bug
when using async acreate , the API timeout
To Reproduce
async def chat(query, retry_count=0):
 response = await openai.ChatCompletion.acreate(
 model=""gpt-3.5-turbo"",
 messages=query,
 temperature=0.7,
 max_tokens=1200,
 top_p=1,
 frequency_penalty=0.0,
 presence_penalty=0.0,
 )
 out = response.choices[0]['message']['content']

@gpt_app.post(""/gpt_go"")
 async def gpt_go(request: Request):
 json_data = await request.form()
role = json_data[""role""]
content = json_data[""content""]
output = await chat([{""role"": ""system"", ""content"":""hello""}])
return output

Code snippets
No response
OS
linux
Python version
Python 3.7
Library version
openai 0.27.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/276","Incomprehensible Response Using Davinci","2023-03-06T17:02:55Z","Closed issue","bug","Describe the bug
I used the OpenAI API to call the Davinci model, but the results are incomprehensible and different from what is returned in the playground.
Here is what it responds:
 { ""id"": ""cmpl-6qyqT0nO2doVmbzd4zdVoOvbtWM00"", ""object"": ""text_completion"", ""created"": 1678084965, ""model"": ""gpt-3.5-turbo-0301"", ""choices"": [ { ""text"": ""们以前的关于春天万物竞发，勃勃生机，优势在我们以前的资源浮奏游戏。\n\n1. 软件开发错误。\n\n2. 微软离席。\n\n3. 我不想对大部分小偷一点说话，但是总要加快进一步的开发人员应当迁移所有行为之间关系。如果大家负责开发人员或者已经选用了一些理由来重复使用不合理性，那么刚才凭着中文字幕来汇集数据，随时上传到Twitter、Facebook、Google+各方面的微信公众图片账户。而这一时候如果能在微信上带有明显的内容时遇到一个微博广告，就相当关注。你不能标准化交流，不能原始冲突..."", ""index"": 0, ""logprobs"": null, ""finish_reason"": ""stop"" } ], ""usage"": { ""prompt_tokens"": 68, ""completion_tokens"": 420, ""total_tokens"": 488 } }
To Reproduce
just using the api introduced in https://platform.openai.com/docs/guides/completion
Code snippets
response = openai.Completion.create(
  model=""gpt-3.5-turbo"",
  prompt=""按照以下要求生成一篇中文文章，关于春天万物竞发，勃勃生机，优势在我"",
  temperature=0.7,
  max_tokens=1695,
  top_p=1,
  frequency_penalty=0.59,
  presence_penalty=0.77
)
OS
macOS
Python version
Python 3.7.6
Library version
v.0.27.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/275","Add proxy functionality for request (sync)","2023-04-08T15:31:48Z","Closed issue","No label","Describe the feature or improvement you're requesting
\openai\api_resources\abstract\engine_api_resource.py
result = _thread_context.session.request(
                method,
                abs_url,
                headers=headers,
                data=data,
                files=files,
                stream=stream,
                timeout=request_timeout if request_timeout else TIMEOUT_SECS,
            )
add param
proxies=openai.proxy
I see asynchronous request support, synchronous not. Am I using it incorrectly?
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
👍3
pigletfly, su27, and ymybxx reacted with thumbs up emoji
All reactions
👍3 reactions"
"https://github.com/openai/openai-python/issues/272","Django text not near and no space","2023-03-06T17:06:02Z","Closed issue","No label","Describe the feature or improvement you're requesting
Django bot chatgpt text not near, why?
chatgpt text near and space
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/271","import openai openai.error.InvalidRequestError: Invalid URL (POST /v1/chat/completions)","2023-03-17T20:27:55Z","Closed issue","bug","Describe the bug
[ERROR][2023-03-04 18:25:20][chat_gpt_bot.py:68] - Invalid URL (POST /v1/chat/completions)
 Traceback (most recent call last):
 File ""/opt/chatgpt-on-wechat/bot/chatgpt/chat_gpt_bot.py"", line 44, in reply_text
 response = openai.ChatCompletion.create(
 File ""/usr/local/lib/python3.9/site-packages/openai/api_resources/chat_completion.py"", line 25, in create
 return super().create(*args, **kwargs)
 File ""/usr/local/lib/python3.9/site-packages/openai/api_resources/abstract/engine_api_resource.py"", line 153, in create
 response, _, api_key = requestor.request(
 File ""/usr/local/lib/python3.9/site-packages/openai/api_requestor.py"", line 226, in request
 resp, got_stream = self._interpret_response(result, stream)
 File ""/usr/local/lib/python3.9/site-packages/openai/api_requestor.py"", line 619, in _interpret_response
 self._interpret_response_line(
 File ""/usr/local/lib/python3.9/site-packages/openai/api_requestor.py"", line 679, in _interpret_response_line
 raise self.handle_error_response(
 openai.error.InvalidRequestError: Invalid URL (POST /v1/chat/completions)
To Reproduce
response = openai.ChatCompletion.create(
openai.error.InvalidRequestError: Invalid URL (POST /v1/chat/completions)
Code snippets
response = openai.ChatCompletion.create(

openai.error.InvalidRequestError: Invalid URL (POST /v1/chat/completions)


[root@vps87388579 chatgpt-on-wechat]# pip3 show openaiName: openaiVersion: 0.27.0Summary: Python client library for the OpenAI APIHome-page: https://github.com/openai/openai-pythonAuthor: OpenAIAuthor-email: support@openai.comLicense: NoneLocation: /usr/local/lib/python3.9/site-packagesRequires: requests, aiohttp, tqdmRequired-by:
OS
centos 8.6
Python version
3.9
Library version
0.27
 The text was updated successfully, but these errors were encountered: 
👍1
IvoryF reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/openai-python/issues/270","Linting issues with Pyright","2023-11-03T22:28:12Z","Closed issue","bug,fixed in v1","Describe the bug
Given reply = response[""choices""][0][""message""][""content""], Pyright throws linting errors:
 chatbot.py    49  27 error    reportGeneralTypeIssues ""__getitem__"" method not defined on type ""Generator[Unknown | list[Unknown] | dict[Unknown, Unknown], None, None]"" (lsp)
 chatbot.py    49  27 error    reportGeneralTypeIssues Argument of type ""Literal['usage']"" cannot be assigned to parameter ""__s"" of type ""slice"" in function ""__getitem__""
   ""Literal['usage']"" is incompatible with ""slice"" (lsp)

The code works but something wonky appears to be afoot.
To Reproduce
Use Pyright LSP (likely also affects Pylance)
View errors when using any ChatGPT API example where a response is fetched
Code snippets
import openai

messages = []
system_msg = input(""What type of chatbot would you like to create? "")
messages.append({""role"": ""system"", ""content"": system_msg})

print(""Say hello to your new assistant!"")
while input != ""quit()"": 
    message = input()
    messages.append({""role"": ""user"", ""content"": message})
    response = openai.ChatCompletion.create(
        model=""gpt-3.5-turbo"",
        messages=messages)
    reply = response[""choices""][0][""message""][""content""]
    messages.append({""role"": ""assistant"", ""content"": reply})
    print(""\n"" + reply + ""\n"")
OS
Linux
Python version
Python 3.11.1
Library version
openai 0.27.0
 The text was updated successfully, but these errors were encountered: 
👍12
guicbrito, ymoslem, soleilcot, CrustyCode, DigitumDei, hueldera, XY-King, martingeorgiu, kumpulak, collinarnett, and 2 more reacted with thumbs up emoji
All reactions
👍12 reactions"
"https://github.com/openai/openai-python/issues/269","Consider using hatch as build-backend?","2023-10-18T07:20:35Z","Closed issue","fixed in v1","Describe the feature or improvement you're requesting
setup.py + setup.cfg is an outdated way of building Python projects. The configuration can be moved to pyproject.toml for consistency.
hatch is a modern python project management tool. hatch is used by the likes of fastAPI, black, pydantic and others to manage projects.
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/268","ChatGPT API","2023-03-06T17:06:48Z","Closed issue","No label","Describe the feature or improvement you're requesting
I would like to be able to manage multiple users in the ChatGPT. The idea is for the API to understand who is speaking as now only one user, one system and one assistant. It would be cool to have possibility to define multiple users and even multiple assistants (specialization?)
 So the inputs would be sonething similar as below:
 {role: “user1”, ”content”: ”I think open source is great way to improve LLMs..”}
 {role: “user12, ”content”: ”I think open source LLMs can be risky in hands of bad actors.”}
Additional context
See above
 The text was updated successfully, but these errors were encountered: 
👍3
juliantcchu, logankilpatrick, and harshitsaini reacted with thumbs up emoji
All reactions
👍3 reactions"
"https://github.com/openai/openai-python/issues/267","Magic sauce is missing. Why is the API not equivalent to backend-api/conversation?","2023-03-06T17:09:34Z","Closed issue","question","Describe the feature or improvement you're requesting
Thank you for your great work in providing an API.
 What seems strangely confusing is that the main use of a conversation_id is not part of the API. Not providing the main use of a chat context as a reference point leads to pointless data stored in memory, repeated requests within a chat, limitations regarding the no of tokens and unnecessary costs.
Since it is definitely technically feasible, as there is an API for the chat window, I wonder what the intention behind this is? Is it simply temporary? Before any workarounds are developed, and if so, when, is a full deployment planned? It also seems that other parties e.g. the companies of the demos have that ability?
Appreciate your guys work and thank you very much for your help!
 The text was updated successfully, but these errors were encountered: 
👍1
sebslomski reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/openai-python/issues/265","openai.error.APIConnectionError: Unexpected error communicating with OpenAI. If this problem persists, let us know at support@openai.com.","2023-03-03T14:11:36Z","Closed issue","bug","Describe the bug
openai.error.APIConnectionError: Unexpected error communicating with OpenAI. If this problem persists,
 let us know at support@openai.com.
To Reproduce
openai.error.APIConnectionError: Unexpected error communicating with OpenAI. If this problem persists,
 let us know at support@openai.com.
Code snippets
response = openai.Completion.create
OS
win
Python version
3.7
Library version
openai==0.8.0 openai==0.27.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/264","Possibility of token limit increase","2023-03-03T14:04:08Z","Closed issue","bug","Describe the bug
Hi, currently we are facing problems with token limitation which is 4097. Is there any possibility to increase existing token limit to 7000 tokens.
 Thanks in advance
To Reproduce
NA
Code snippets
No response
OS
Linux
Python version
Python v3.8
Library version
openai-python v0.26.5
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/263","openai.error.InvalidRequestError: Invalid URL (POST /v1/chat/completions)","2023-03-03T14:20:30Z","Closed issue","bug","Describe the bug
Traceback (most recent call last):
 File """", line 1, in 
 File ""/Users/ruili/miniforge3/envs/gpt/lib/python3.9/site-packages/openai/api_resources/chat_completion.py"", line 25, in create
 return super().create(*args, **kwargs)
 File ""/Users/ruili/miniforge3/envs/gpt/lib/python3.9/site-packages/openai/api_resources/abstract/engine_api_resource.py"", line 153, in create
 response, _, api_key = requestor.request(
 File ""/Users/ruili/miniforge3/envs/gpt/lib/python3.9/site-packages/openai/api_requestor.py"", line 226, in request
 resp, got_stream = self._interpret_response(result, stream)
 File ""/Users/ruili/miniforge3/envs/gpt/lib/python3.9/site-packages/openai/api_requestor.py"", line 619, in _interpret_response
 self._interpret_response_line(
 File ""/Users/ruili/miniforge3/envs/gpt/lib/python3.9/site-packages/openai/api_requestor.py"", line 679, in _interpret_response_line
 raise self.handle_error_response(
 openai.error.InvalidRequestError: Invalid URL (POST /v1/chat/completions)
To Reproduce
response = openai.ChatCompletion.create(model=""gpt-3.5-turbo-0301"",messages=[{""role"": ""user"", ""content"": ""q""}])
Code snippets
No response
OS
macOS Monterey
Python version
Python 3.9.13
Library version
openai 0.27.0
 The text was updated successfully, but these errors were encountered: 
👍1
andrewchou reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/openai-python/issues/262","How to use proxy when sync request ?","2023-03-03T02:22:33Z","Closed issue","bug","Describe the bug
I notice that in asynchronous requests, the asynchronous code in line 583 uses proxy, but in synchronous requests in line 516, this parameter is not used.
How do I use a proxy when making a synchronous request? If this is a bug I can help fix it
To Reproduce
None
Code snippets
No response
OS
MacOS
Python version
3.8.16
Library version
openai 0.27.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/261","[Audio.transcribe] Prompt parameter appears to have no effect","2023-03-03T18:59:24Z","Closed issue","question","Describe the bug
I am attempting to transcribe a file with an accompanying prompt, however it appears to have no effect at all on the output.
This is a sample of the code:
prompt = ""This is a recording of a conversation between 4 people, please present it in the following format: 'Person 1: Thanks for coming today Person 2: No problem', etc.""
audio_file = open(""/path/to/file.WAV"", ""rb"")
transcript = openai.Audio.transcribe(""whisper-1"", audio_file, prompt=prompt)
print(transcript)
The returned text is exactly the same with or without the prompt parameter.
To Reproduce
Send a transcription request with a prompt parameter
Code snippets
No response
OS
macOs
Python version
Python v3.7.1
Library version
OpenAI Python v0.27.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/257","We could not parse the JSON body of your request.","2023-03-02T17:06:17Z","Closed issue","bug","Describe the bug
wrong report
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/256","How to get the parameters in gpt 3.5 turbo?","2023-03-03T14:07:31Z","Closed issue","No label","Describe the feature or improvement you're requesting
With davinci I used these:
                    max_tokens=40,
                    n=1,
                    stop=None,
                    temperature=0.5

How can I get the same with gpt 3.5 turbo in python?
Additional context
NEVERMIND: I just found out that it works the same way. I had an error when I tried it initially.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/254","Ability to batch prompts in Chat Completion like for the Completion endpoint.","2023-03-03T16:29:02Z","Closed issue","enhancement","Describe the feature or improvement you're requesting
In the Completion endpoint we can batch input prompts in one API request. It would be nice to have the same feature for the Chat Completion.
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
👍11
tueda, makretch, jjshop, 551E5, kevinhu, PorkShoulderHolder, usholanb, ebigelow, yaviddang20, heyalexchoi, and ahmedoumar reacted with thumbs up emoji
All reactions
👍11 reactions"
"https://github.com/openai/openai-python/issues/253","AttributeError: module 'openai' has no attribute 'ChatCompletion'","2023-03-03T14:15:35Z","Closed issue","bug","Describe the bug
Hello.
I cant access gpt-3.5-turbo with python because I seem to be getting the error "" AttributeError: module ‘openai’ has no attribute ‘ChatCompletion’""
I have updated to openai v0.27.0, as well as tried using new API keys just incase that was causing the issue. Ive double checked that I actually have updated to the new version of openai 0.27.0 by running “pip list”. Ive also tried "" pip install --upgrade openai "" and “pip install openai-0.27.0-py3-none-any.whl”
Im not sure what else I can try, does anyone have any ideas?
 Thanks.
To Reproduce
import os
 import openai
 openai.api_key = os.getenv(""key"")
completion = openai.ChatCompletion.create(
 model=""gpt-3.5-turbo"",
 messages=[
 {""role"": ""user"", ""content"": ""Hello!""}
 ]
 )
print(completion.choices[0].message)
Code snippets
No response
OS
macOS
Python version
Python v3.11.2
Library version
openai 0.27.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/250","InvalidRequestError:Invalid URL when using chatcompletion","2023-03-03T14:19:42Z","Closed issue","bug","Describe the bug
when I upgrade openai to v0.27.0 and reproduce your sample:
 import openai
openai.ChatCompletion.create(
 model=""gpt-3.5-turbo"",
 messages=[
 {""role"": ""system"", ""content"": ""You are a helpful assistant.""},
 {""role"": ""user"", ""content"": ""Who won the world series in 2020?""},
 {""role"": ""assistant"", ""content"": ""The Los Angeles Dodgers won the World Series in 2020.""},
 {""role"": ""user"", ""content"": ""Where was it played?""}
 ]
 )
I got:
Traceback (most recent call last):
File ""C:\Users\Administrator\AppData\Local\Temp\ipykernel_4560\2473871474.py"", line 1, in 
 openai.ChatCompletion.create(
File ""D:\Anaconda\lib\site-packages\openai\api_resources\chat_completion.py"", line 25, in create
 return super().create(*args, **kwargs)
File ""D:\Anaconda\lib\site-packages\openai\api_resources\abstract\engine_api_resource.py"", line 153, in create
 response, _, api_key = requestor.request(
File ""D:\Anaconda\lib\site-packages\openai\api_requestor.py"", line 226, in request
 resp, got_stream = self._interpret_response(result, stream)
File ""D:\Anaconda\lib\site-packages\openai\api_requestor.py"", line 619, in _interpret_response
 self._interpret_response_line(
File ""D:\Anaconda\lib\site-packages\openai\api_requestor.py"", line 679, in _interpret_response_line
 raise self.handle_error_response(
InvalidRequestError: Invalid URL (POST /v1/chat/completions)
To Reproduce
import openai
openai.ChatCompletion.create(
 model=""gpt-3.5-turbo"",
 messages=[
 {""role"": ""system"", ""content"": ""You are a helpful assistant.""},
 {""role"": ""user"", ""content"": ""Who won the world series in 2020?""},
 {""role"": ""assistant"", ""content"": ""The Los Angeles Dodgers won the World Series in 2020.""},
 {""role"": ""user"", ""content"": ""Where was it played?""}
 ]
 )
Code snippets
No response
OS
Windows
Python version
3.9.13
Library version
0.27.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/248","logprobs exposed in chat completion endpoint","2023-03-03T16:29:11Z","Closed issue","enhancement","Describe the feature or improvement you're requesting
We found it useful to have logprobs for completions to evaluate confidence. We would like to have access to this attribute in the new chat completion endpoint. Thank you!
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
👍72
primeviking, logankilpatrick, tmgthb, vlyubin, nata108, whitead, tiangolo, FergusFettes, jonathan-laurent, inbararan, and 62 more reacted with thumbs up emoji👀1
WillBoan reacted with eyes emoji
All reactions
👍72 reactions
👀1 reaction"
"https://github.com/openai/openai-python/issues/247","can not work with jupyterlab-desktop","2023-03-03T16:22:24Z","Closed issue","bug","Describe the bug
When I use openai package with jupyterlab-desktop, the following error occurred:
environment:
MacOS 13.2.1(M1)
openai == 0.27.0
jupyterlab-desktop == 3.6.1-2 
python3.8
To Reproduce
open jupyterlab-desktop
install openai: !pip install openai
import openai
Code snippets
import openai
OS
MacOS 13.2.1(M1)
Python version
python3.8
Library version
openai == 0.27.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/246","AttributeError: module 'openai' has no attribute 'ChatCompletion'","2023-03-03T16:25:14Z","Closed issue","bug","Describe the bug
The feature was working few hours ago, now it's giving error.
To Reproduce
import openai

openai.api_key = 'key'

openai.ChatCompletion.create(
  model=""gpt-3.5-turbo"",
  messages=[
        {""role"": ""system"", ""content"": ""You are a helpful assistant.""},
        {""role"": ""user"", ""content"": ""Who won the world series in 2020?""},
        {""role"": ""assistant"", ""content"": ""The Los Angeles Dodgers won the World Series in 2020.""},
        {""role"": ""user"", ""content"": ""Where was it played?""}
    ]
)

Code snippets
No response
OS
macOS
Python version
Python v3.7.1
Library version
openai v0.26.5
no function as ChatCompletion the defined methods
import openai
print(openai.__dir__())

['name', 'doc', 'package', 'loader', 'spec', 'path', 'file', 'cached', 'builtins', 'annotations', 'os', 'Optional', 'error', 'util', 'version', 'openai_response', 'api_requestor', 'openai_object', 'api_resources', 'Answer', 'Classification', 'Completion', 'Customer', 'Edit', 'Deployment', 'Embedding', 'Engine', 'ErrorObject', 'File', 'FineTune', 'Image', 'Model', 'Moderation', 'Search', 'APIError', 'InvalidRequestError', 'OpenAIError', 'api_key', 'api_key_path', 'organization', 'api_base', 'api_type', 'api_version', 'verify_ssl_certs', 'proxy', 'app_info', 'enable_telemetry', 'ca_bundle_path', 'debug', 'log', 'all']
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/245","[Audio.transcribe] Logprobs for each token in verbose_json","2023-03-03T16:28:49Z","Closed issue","enhancement","Describe the feature or improvement you're requesting
Currently, Whisper exposes avg_logprob for an entire segment. The request is to expose logprobs for each token.
{
  ""duration"": 4.01,
  ""language"": ""english"",
  ""segments"": [
    {
      ""avg_logprob"": -0.40153955010806813,
      ""compression_ratio"": 1.0526315789473684,
      ""end"": 4.0,
      ""id"": 0,
      ""no_speech_prob"": 0.1633709967136383,
      ""seek"": 0,
      ""start"": 0.0,
      ""temperature"": 0.0,
      ""text"": "" Testing, testing, this is going to be a new audio recording."",
      ""tokens"": [
        50364,
        45517,
        11,
        4997,
        11,
        341,
        307,
        516,
        281,
        312,
        257,
        777,
        6278,
        6613,
        13,
        50564
      ],
      ""transient"": false
    }
  ],
  ""task"": ""transcribe"",
  ""text"": ""Testing, testing, this is going to be a new audio recording.""
}
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/243","[Audio.transcribe] JsonDecodeError when printing vtt from m4a","2023-03-08T20:50:41Z","Closed issue","bug","Describe the bug
This section of the codebase expects json even when the response_format is not json:
https://github.com/openai/openai-python/blob/75c90a71e88e4194ce22c71edeb3d2dee7f6ac93/openai/api_requestor.py#L668C7-L673
I think I can contribute a quick bug fix PR today!
To Reproduce
Open an m4a file in a jupyter notebook (python 3.10.10)
Transcribe with whisper-1
Print transcript
Stack:
 JSONDecodeError Traceback (most recent call last)
 File ~\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\openai\api_requestor.py:669, in APIRequestor._interpret_response_line(self, rbody, rcode, rheaders, stream)
 668 try:
 --> 669 data = json.loads(rbody)
 670 except (JSONDecodeError, UnicodeDecodeError) as e:
File C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.10_3.10.2800.0_x64__qbz5n2kfra8p0\lib\json\__init__.py:346, in loads(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)
    343 if (cls is None and object_hook is None and
    344         parse_int is None and parse_float is None and
    345         parse_constant is None and object_pairs_hook is None and not kw):
--> 346     return _default_decoder.decode(s)
    347 if cls is None:

File C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.10_3.10.2800.0_x64__qbz5n2kfra8p0\lib\json\decoder.py:337, in JSONDecoder.decode(self, s, _w)
    333 """"""Return the Python representation of ``s`` (a ``str`` instance
    334 containing a JSON document).
    335 
    336 """"""
--> 337 obj, end = self.raw_decode(s, idx=_w(s, 0).end())
    338 end = _w(s, end).end()

File C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.10_3.10.2800.0_x64__qbz5n2kfra8p0\lib\json\decoder.py:355, in JSONDecoder.raw_decode(self, s, idx)
    354 except StopIteration as err:
--> 355     raise JSONDecodeError(""Expecting value"", s, err.value) from None

Code snippets
f = open(""testing.m4a"", ""rb"")
transcript = openai.Audio.transcribe(""whisper-1"", f,response_format=""vtt"")
print(transcript)


https://github.com/openai/openai-python/blob/75c90a71e88e4194ce22c71edeb3d2dee7f6ac93/openai/api_requestor.py#L668C7-L673`
OS
Windows 11
Python version
Python v3.10.10
Library version
openai-python 0.27.0
 The text was updated successfully, but these errors were encountered: 
👍4
d6y, MattFisher, GunheeYi, and AfterAllDev reacted with thumbs up emoji
All reactions
👍4 reactions"
"https://github.com/openai/openai-python/issues/242","Can't access gpt-3.5-turbo - AttributeError: module 'openai' has no attribute 'ChatCompletion'","2023-03-02T00:10:21Z","Closed issue","bug","Describe the bug
Can't access gpt-3.5-turbo with python. getting an error when i try to use openai.ChatCompletion.create
To Reproduce
pip install openai
import openai + key
enter code snippet into jupyter notebook
get error
Code snippets
# Note: you need to be using OpenAI Python v0.27.0 for the code below to workimport openai

openai.ChatCompletion.create(
  model=""gpt-3.5-turbo"",
  messages=[
        {""role"": ""system"", ""content"": ""You are a helpful assistant.""},
        {""role"": ""user"", ""content"": ""Who won the world series in 2020?""},
        {""role"": ""assistant"", ""content"": ""The Los Angeles Dodgers won the World Series in 2020.""},
        {""role"": ""user"", ""content"": ""Where was it played?""}
    ]
)
OS
windows11
Python version
python 3.9.7
Library version
openai-python v0.27.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/241","There is no metadata, please fix it","2023-03-01T23:30:21Z","Closed issue","bug","Describe the bug
There is no metadata in the response for individual words like what's the language of a word, start time and end time.
To Reproduce
Run the code snippet below.
Code snippets
osimport openai

openai.organization = os.environ.get(""OPENAIORGANIZATIONID"")
openai.api_key = os.environ.get(""OPENAIAPIKEY"")

file = open(""/Users/olehkopyl/Downloads/test.m4a"", ""rb"")
transcription = openai.Audio.transcribe(""whisper-1"", file)

print(transcription)
OS
macOS
Python version
3.8.13
Library version
0.27.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/240","AttributeError: module 'openai' has no attribute 'openai_response'","2023-03-03T16:26:51Z","Closed issue","bug","Describe the bug
Just ran the documentation code on Jupyter notebook and got
 in python3.9/site-packages/openai/util.py:112
 if isinstance(resp, openai.openai_response.OpenAIResponse):
 113 organization = resp.organization
 114 response_ms = resp.response_ms
AttributeError: module 'openai' has no attribute 'openai_response'
To Reproduce
pip install -U openai
 in jupyter notebook
Code snippets
import openai

openai.ChatCompletion.create(
  model=""gpt-3.5-turbo"",
  messages=[
        {""role"": ""system"", ""content"": ""You are a helpful assistant.""},
        {""role"": ""user"", ""content"": ""Who won the world series in 2020?""},
        {""role"": ""assistant"", ""content"": ""The Los Angeles Dodgers won the World Series in 2020.""},
        {""role"": ""user"", ""content"": ""Where was it played?""}
    ]
)
OS
macOS m1
Python version
Python v3.9.12
Library version
openai-python v0.27.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/239","AttributeError: module 'openai' has no attribute 'ChatCompletion'. Did you mean: 'Completion'?","2023-03-01T20:07:31Z","Closed issue","bug","Describe the bug
Trying to run gpt-3.5-turbo model but it's throwing next error: AttributeError: module 'openai' has no attribute 'ChatCompletion'. Did you mean: 'Completion'?
 Updated to latest version, not working.
To Reproduce
Install latest openai
Copy code from official documentation
Run
Code snippets
import openaiopenai.api_key = 'sk-***'

completion = openai.ChatCompletion.create(
  model=""gpt-3.5-turbo"", 
  messages=[{""role"": ""user"", ""content"": ""Tell the world about the ChatGPT API in the style of a pirate.""}]
)

print(completion)
OS
Arch Linux - 6.1.4-arch1-1
Python version
Python v3.10.9
Library version
openai-python v0.27.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/235","JSON parsing issues with embedding create with large batch size.","2023-12-31T00:01:56Z","Closed issue","bug","Describe the bug
When I call Embedding.create with a large number of text chunks (batch) I get JSON decoding errors in the reponse. If I keep the batch size small (say 50) it works fine, but for large batch sizes (say 12k) it shows this problem. Looks very similar to the problems seen in #184. I initially saw this when using langchain, but reproduced the openai alone. Oddly, I sometimes get an InvalidRequestError instead.
To Reproduce
Run the following code:
texts = [""AI""*100 for i in range(4000)]
e = openai.Embedding.create(input=texts, model=""text-embedding-ada-002"")
Code snippets
No response
OS
macOS
Python version
Python 3.11
Library version
0.26.5
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/234","OpenAI Install Error when trying to install via a proxy server","2023-03-01T20:13:00Z","Closed issue","bug","Describe the bug
pip install --proxy http://userid:passwd@56.0.142.24:8080 openai
Results in following error
 Collecting openai
 Using cached openai-0.26.5.tar.gz (55 kB)
 Installing build dependencies ... error
 error: subprocess-exited-with-error
× pip subprocess to install build dependencies did not run successfully.
 │ exit code: 1
 ╰─> [7 lines of output]
 WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProxyError('Your proxy appears to only use HTTP and not HTTPS, try changing your proxy URL to be HTTP. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#https-proxy-error-http-proxy', SSLError(SSLError(1, '[SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:992)')))': /simple/setuptools/
 WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProxyError('Your proxy appears to only use HTTP and not HTTPS, try changing your proxy URL to be HTTP. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#https-proxy-error-http-proxy', SSLError(SSLError(1, '[SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:992)')))': /simple/setuptools/
 WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProxyError('Your proxy appears to only use HTTP and not HTTPS, try changing your proxy URL to be HTTP. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#https-proxy-error-http-proxy', SSLError(SSLError(1, '[SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:992)')))': /simple/setuptools/
 WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProxyError('Your proxy appears to only use HTTP and not HTTPS, try changing your proxy URL to be HTTP. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#https-proxy-error-http-proxy', SSLError(SSLError(1, '[SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:992)')))': /simple/setuptools/
 WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProxyError('Your proxy appears to only use HTTP and not HTTPS, try changing your proxy URL to be HTTP. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#https-proxy-error-http-proxy', SSLError(SSLError(1, '[SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:992)')))': /simple/setuptools/
 ERROR: Could not find a version that satisfies the requirement setuptools (from versions: none)
 ERROR: No matching distribution found for setuptools
 [end of output]
note: This error originates from a subprocess, and is likely not a problem with pip.
 error: subprocess-exited-with-error
× pip subprocess to install build dependencies did not run successfully.
 │ exit code: 1
 ╰─> See above for output.
To Reproduce
run with appropriate values
 pip install --proxy http://userid:passwd@proxyserver:port openai
Code snippets
No response
OS
Windows 10 Enterprise
Python version
Python 3.11.2
Library version
openai-0.26.5
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/233","Proper encapsulation of the API keys","2023-11-03T22:28:26Z","Closed issue","fixed in v1","Describe the feature or improvement you're requesting
In the Node.js API we have the API accepting keys upon creation of the OpenAIApi object:
import { Configuration, OpenAIApi } from ""openai"";const configuration = new Configuration({
    organization: ""org-1qRDHAPS0UN4vVhAii7VYY7y"",
    apiKey: process.env.OPENAI_API_KEY,});const openai = new OpenAIApi(configuration);
In Python, however, the key is set globally, which poses a problem when dealing with multiple keys/organizations in requests. It would be nice to have proper encapsulation of the key into a Python class that is responsible for the API like it is usually done for many Python APIs, instead of using global objects for keys.
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
👍18
logankilpatrick, Normalizex, MattFisher, johnnygreco, vishalwadhwa13, KellyRousselHoomano, CameronSima, liffiton, xbasset, gbrlmtrz, and 8 more reacted with thumbs up emoji
All reactions
👍18 reactions"
"https://github.com/openai/openai-python/issues/232","AttributeError: module 'openai' has no attribute 'search'","2023-02-28T15:52:41Z","Closed issue","bug","Describe the bug
the main question when i use example
openai.error.InvalidRequestError: 'search' is not one of ['fine-tune'] - 'purpose'
 AttributeError: module 'openai' has no attribute 'search'
To Reproduce
i can't use guide from this website
https://platform.openai.com/docs/guides/search
Code snippets
openai.File.create(
  file=open(""test.jsonl"", ""rb""),
  purpose='search'
)

openai.Search.create(
    model=""ada"",
    query=""happy"",
    max_rerank=5,
    file=""file-Lwjuy0q2ezi00jdpfCbl28CO""
)
OS
windows
Python version
python 3.7
Library version
openai-python 0.26.5
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/231","openai.Completion.retrieve()","2023-03-07T03:14:50Z","Closed issue","bug","Describe the bug
EngineAPIResource.init() takes from 1 to 2 positional arguments but 3 were given
To Reproduce
does this only happen to me when using 'retrieve' in openai.
 i got this error : ""EngineAPIResource.init() takes from 1 to 2 positional arguments but 3 are given""
Code snippets
key = ""my_api_key""openai.api_key=keyid_chat = ""my_completion_id""try:
    result = openai.Completion.retrieve(id_chat)
    print(result['choices'][0]['text'])
except Exception as a:
    print(a)
OS
arch linux
Python version
Python 3.10.9
Library version
openai-python v0.26.5
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/230","How can I correctly count the usage of each API Key?","2023-03-03T16:31:22Z","Closed issue","No label","Describe the feature or improvement you're requesting
I hope to count the usage consumption of several API Keys under my OpenAI account.
 On the help page, I saw this introduction.

 I tried to enter this command in python:
import openai
openai.api_key = ""sk-vaU0ES******fhvr""
usages = openai.usage.list() 
for usage in usages.data: 
 print(f""Key: {usage.key}"")
 print(f""Total requests: {usage.total_requests}"")
 print(f""Total cost: {usage.total_cost}"")
But I got the prompt
AttributeError: module 'openai' has no attribute 'usage'
How can I correctly count the usage of each API Key?
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/229","Why not open to China","2023-03-03T16:31:55Z","Closed issue","No label","Describe the feature or improvement you're requesting
I hope that the OpenAI API will be opened to China.
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
👍1
clear-sea reacted with thumbs up emoji👀3
abodacs, silence-tang, and lphcreat reacted with eyes emoji
All reactions
👍1 reaction
👀3 reactions"
"https://github.com/openai/openai-python/issues/226","pip3 install openai does not install required dependencies","2023-02-22T19:38:16Z","Closed issue","bug","Describe the bug
When creating a new environment and running pip3 install openai the following modules are required to run some functions within the openai.embedding_utils:
matplotlib
plotly
To Reproduce
create a new python environment ex: python3 -m venv venv
activate env, ex: source venv/bin/activate
import the embedding_utils function get_embedding
run file
expected output:
ModuleNotFoundError: No module named 'matplotlib'
 and after installing matplotlib
ModuleNotFoundError: No module named 'plotly' 
Code snippets
import openaifrom openai.embeddings_utils import get_embedding""""""will receive the ModuleNotFoundErrors mentioned above. Expected result would be: install the above dependencies when installing openai""""""
OS
macOS
Python version
Python v3.10.5
Library version
openai-python v0.26.5
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/223","No clear information or method to retrieve Usage.","2023-02-18T21:22:00Z","Closed issue","bug","Describe the bug
Hi Team,
It seems that there is no way to get usage / quota information when using the api. I am not sure if this feature exists.
 Was this available in earlier versions ? I am using openai==0.26.5
To Reproduce
Try to fetch the usage of an organization.
Code snippets
No response
OS
ubuntu
Python version
Python 3.8.10
Library version
openai==0.26.5
 The text was updated successfully, but these errors were encountered: 
👍1
tingofurro reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/openai-python/issues/222","pip install failure deprecation on >=v0.26.4","2023-03-11T03:55:10Z","Closed issue","bug","Describe the bug
Versions >= v0.26.4 fail to install on fresh Ubuntu 22.04 container w/ Kernel Linux [redacted]-2017 5.10.16.3-microsoft-standard-WSL2 #1 SMP Fri Apr 2 22:23:49 UTC 2021 x86_64 x86_64 x86_64 GNU/Linux
To Reproduce
With the following Dockerfile:
FROM ubuntu:22.04

RUN \
    DEBIAN_FRONTEND=noninteractive apt-get update && \
    DEBIAN_FRONTEND=noninteractive apt-get install -y \
    curl \
    git \
    python3 python3-pip

RUN echo ""openai==0.26.4"" > /requirements.txt
RUN pip install -r /requirements.txt

Note the installation failure:
 > [4/4] RUN pip install -r /requirements.txt:                                                                                                                                                                                                                                           
#10 1.029 Collecting openai==0.26.4                                                                                                                                                                                                                                                      
#10 1.265   Downloading openai-0.26.4.tar.gz (55 kB)                                                                                                                                                                                                                                     
#10 1.298      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 55.6/55.6 KB 1.6 MB/s eta 0:00:00                                                                                                                                                                                                
#10 1.463   Installing build dependencies: started                                                                                                                                                                                                                                       
#10 4.490   Installing build dependencies: finished with status 'done'
#10 4.493   Getting requirements to build wheel: started
#10 4.689   Getting requirements to build wheel: finished with status 'error'
#10 4.698   error: subprocess-exited-with-error
#10 4.698   
#10 4.698   × Getting requirements to build wheel did not run successfully.
#10 4.698   │ exit code: 1
#10 4.698   ╰─> [81 lines of output]
#10 4.698       Traceback (most recent call last):
#10 4.698         File ""/usr/lib/python3/dist-packages/setuptools/config.py"", line 419, in _parse_attr
#10 4.698           return getattr(StaticModule(module_name), attr_name)
#10 4.698         File ""/usr/lib/python3/dist-packages/setuptools/config.py"", line 26, in __init__
#10 4.698           spec = importlib.util.find_spec(name)
#10 4.698         File ""/usr/lib/python3.10/importlib/util.py"", line 94, in find_spec
#10 4.698           parent = __import__(parent_name, fromlist=['__path__'])
#10 4.698         File ""/tmp/pip-install-7lrzz_ii/openai_2922e99e37a54fe8816c6c965a07945b/openai/__init__.py"", line 9, in <module>
#10 4.698           from openai.api_resources import (
#10 4.698         File ""/tmp/pip-install-7lrzz_ii/openai_2922e99e37a54fe8816c6c965a07945b/openai/api_resources/__init__.py"", line 1, in <module>
#10 4.698           from openai.api_resources.completion import Completion  # noqa: F401
#10 4.698         File ""/tmp/pip-install-7lrzz_ii/openai_2922e99e37a54fe8816c6c965a07945b/openai/api_resources/completion.py"", line 4, in <module>
#10 4.698           from openai.api_resources.abstract import DeletableAPIResource, ListableAPIResource
#10 4.698         File ""/tmp/pip-install-7lrzz_ii/openai_2922e99e37a54fe8816c6c965a07945b/openai/api_resources/abstract/__init__.py"", line 3, in <module>
#10 4.698           from openai.api_resources.abstract.api_resource import APIResource
#10 4.698         File ""/tmp/pip-install-7lrzz_ii/openai_2922e99e37a54fe8816c6c965a07945b/openai/api_resources/abstract/api_resource.py"", line 4, in <module>
#10 4.698           from openai import api_requestor, error, util
#10 4.698         File ""/tmp/pip-install-7lrzz_ii/openai_2922e99e37a54fe8816c6c965a07945b/openai/api_requestor.py"", line 21, in <module>
#10 4.698           import aiohttp
#10 4.698       ModuleNotFoundError: No module named 'aiohttp'
#10 4.698       
#10 4.698       During handling of the above exception, another exception occurred:
#10 4.698       
#10 4.698       Traceback (most recent call last):
#10 4.698         File ""/usr/lib/python3/dist-packages/pip/_vendor/pep517/in_process/_in_process.py"", line 363, in <module>
#10 4.698           main()
#10 4.698         File ""/usr/lib/python3/dist-packages/pip/_vendor/pep517/in_process/_in_process.py"", line 345, in main
#10 4.698           json_out['return_val'] = hook(**hook_input['kwargs'])
#10 4.698         File ""/usr/lib/python3/dist-packages/pip/_vendor/pep517/in_process/_in_process.py"", line 130, in get_requires_for_build_wheel
#10 4.698           return hook(config_settings)
#10 4.698         File ""/usr/lib/python3/dist-packages/setuptools/build_meta.py"", line 162, in get_requires_for_build_wheel
#10 4.698           return self._get_build_requires(
#10 4.698         File ""/usr/lib/python3/dist-packages/setuptools/build_meta.py"", line 143, in _get_build_requires
#10 4.698           self.run_setup()
#10 4.698         File ""/usr/lib/python3/dist-packages/setuptools/build_meta.py"", line 158, in run_setup
#10 4.698           exec(compile(code, __file__, 'exec'), locals())
#10 4.698         File ""setup.py"", line 3, in <module>
#10 4.698           setup()
#10 4.698         File ""/usr/lib/python3/dist-packages/setuptools/__init__.py"", line 153, in setup
#10 4.698           return distutils.core.setup(**attrs)
#10 4.698         File ""/usr/lib/python3/dist-packages/setuptools/_distutils/core.py"", line 122, in setup
#10 4.698           dist.parse_config_files()
#10 4.698         File ""/usr/lib/python3/dist-packages/setuptools/dist.py"", line 804, in parse_config_files
#10 4.698           parse_configuration(
#10 4.698         File ""/usr/lib/python3/dist-packages/setuptools/config.py"", line 158, in parse_configuration
#10 4.698           meta.parse()
#10 4.698         File ""/usr/lib/python3/dist-packages/setuptools/config.py"", line 498, in parse
#10 4.698           section_parser_method(section_options)
#10 4.698         File ""/usr/lib/python3/dist-packages/setuptools/config.py"", line 469, in parse_section
#10 4.698           self[name] = value
#10 4.698         File ""/usr/lib/python3/dist-packages/setuptools/config.py"", line 222, in __setitem__
#10 4.698           value = parser(value)
#10 4.698         File ""/usr/lib/python3/dist-packages/setuptools/config.py"", line 599, in _parse_version
#10 4.698           version = self._parse_attr(value, self.package_dir)
#10 4.698         File ""/usr/lib/python3/dist-packages/setuptools/config.py"", line 422, in _parse_attr
#10 4.698           module = importlib.import_module(module_name)
#10 4.698         File ""/usr/lib/python3.10/importlib/__init__.py"", line 126, in import_module
#10 4.698           return _bootstrap._gcd_import(name[level:], package, level)
#10 4.698         File ""<frozen importlib._bootstrap>"", line 1050, in _gcd_import
#10 4.698         File ""<frozen importlib._bootstrap>"", line 1027, in _find_and_load
#10 4.698         File ""<frozen importlib._bootstrap>"", line 992, in _find_and_load_unlocked
#10 4.698         File ""<frozen importlib._bootstrap>"", line 241, in _call_with_frames_removed
#10 4.698         File ""<frozen importlib._bootstrap>"", line 1050, in _gcd_import
#10 4.698         File ""<frozen importlib._bootstrap>"", line 1027, in _find_and_load
#10 4.698         File ""<frozen importlib._bootstrap>"", line 1006, in _find_and_load_unlocked
#10 4.698         File ""<frozen importlib._bootstrap>"", line 688, in _load_unlocked
#10 4.698         File ""<frozen importlib._bootstrap_external>"", line 883, in exec_module
#10 4.698         File ""<frozen importlib._bootstrap>"", line 241, in _call_with_frames_removed
#10 4.698         File ""/tmp/pip-install-7lrzz_ii/openai_2922e99e37a54fe8816c6c965a07945b/openai/__init__.py"", line 9, in <module>
#10 4.698           from openai.api_resources import (
#10 4.698         File ""/tmp/pip-install-7lrzz_ii/openai_2922e99e37a54fe8816c6c965a07945b/openai/api_resources/__init__.py"", line 1, in <module>
#10 4.698           from openai.api_resources.completion import Completion  # noqa: F401
#10 4.698         File ""/tmp/pip-install-7lrzz_ii/openai_2922e99e37a54fe8816c6c965a07945b/openai/api_resources/completion.py"", line 4, in <module>
#10 4.698           from openai.api_resources.abstract import DeletableAPIResource, ListableAPIResource
#10 4.698         File ""/tmp/pip-install-7lrzz_ii/openai_2922e99e37a54fe8816c6c965a07945b/openai/api_resources/abstract/__init__.py"", line 3, in <module>
#10 4.698           from openai.api_resources.abstract.api_resource import APIResource
#10 4.698         File ""/tmp/pip-install-7lrzz_ii/openai_2922e99e37a54fe8816c6c965a07945b/openai/api_resources/abstract/api_resource.py"", line 4, in <module>
#10 4.698           from openai import api_requestor, error, util
#10 4.698         File ""/tmp/pip-install-7lrzz_ii/openai_2922e99e37a54fe8816c6c965a07945b/openai/api_requestor.py"", line 21, in <module>
#10 4.698           import aiohttp
#10 4.698       ModuleNotFoundError: No module named 'aiohttp'
#10 4.698       [end of output]
#10 4.698   
#10 4.698   note: This error originates from a subprocess, and is likely not a problem with pip.
#10 4.702 error: subprocess-exited-with-error
#10 4.702 
#10 4.702 × Getting requirements to build wheel did not run successfully.
#10 4.702 │ exit code: 1
#10 4.702 ╰─> See above for output.
#10 4.702 
#10 4.702 note: This error originates from a subprocess, and is likely not a problem with pip.

Installs cleanly with:
FROM ubuntu:22.04

RUN \
    DEBIAN_FRONTEND=noninteractive apt-get update && \
    DEBIAN_FRONTEND=noninteractive apt-get install -y \
    curl \
    git \
    python3 python3-pip

RUN echo ""openai==0.26.3"" > /requirements.txt
RUN pip install -r /requirements.txt

Code snippets
No response
OS
Windows 10/WSL2
Python version
Python 3.10.6
Library version
openai-python >=v0.26.4
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/221","Image.create() fails with request_timeout param","2023-10-18T07:40:30Z","Closed issue","bug,fixed in v1","Describe the bug
When I use request_timeout on a text completion, it works fine:
openai.Completion.create(prompt=""1,2,3,4,"", request_timeout=10.0)
But when I try to do it with an image creation, it fails:
openai.Image.create(prompt=""beautiful sunset"", request_timeout=10.0)
 ==> openai.error.InvalidRequestError: Additional properties are not allowed ('request_timeout' was unexpected)
I'm not familiar enough with the code base to know how this ought to work, but I can get it to work with an ugly hack like (starting at line 36 of openai/api_resouces/image.py):
response, _, api_key = requestor.request(
           ""post"", cls._get_url(""generations""), params
)

becomes:
request_timeout = params.pop(""request_timeout"", None) 
response, _, api_key = requestor.request(
            ""post"", cls._get_url(""generations""), params, request_timeout=request_timeout
 )

To Reproduce
import openai
openai.api_key = ...
openai.Image.create(prompt=""beautiful sunset"", request_timeout=10.0)

Code snippets
No response
OS
macOS
Python version
Python 3.11.1
Library version
openai-python v0.26.5 (and others)
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/220","What are the depencies? Why are these not specified in setup.py","2023-02-14T16:49:14Z","Closed issue","bug","Describe the bug
Installing openai on a clean repo doesn't work because dependencies are missing such as numpy and matplotlib.
To Reproduce
Code snippets
No response
OS
linux
Python version
3.8
Library version
latest
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/219","Can't install openai because aiohttp is supposedly missing?","2023-03-03T16:34:12Z","Closed issue","bug","Describe the bug
I'm trying to install openai on my ubuntu machine, but the installation wheel crashes because a module called ""aiohttp"" is missing. I installed aiohttp, but the error doesn't dissapear. I have run the following command ""python3 -m pip install openai"" in 2 different ubuntu 22.04 computers several times and after several reboots, and the result has been the same. I simply can't install openai. I'm using python 3.10
To Reproduce
Run ""python3 -m pip install openai""
The installation fails
Code snippets
Collecting openai
  Using cached openai-0.26.5.tar.gz (55 kB)
  Installing build dependencies ... done
  Getting requirements to build wheel ... error
  error: subprocess-exited-with-error

  × Getting requirements to build wheel did not run successfully.
  │ exit code: 1
  ╰─> [81 lines of output]
      Traceback (most recent call last):
        File ""/usr/lib/python3/dist-packages/setuptools/config.py"", line 419, in _parse_attr
          return getattr(StaticModule(module_name), attr_name)
        File ""/usr/lib/python3/dist-packages/setuptools/config.py"", line 26, in __init__
          spec = importlib.util.find_spec(name)
        File ""/usr/lib/python3.10/importlib/util.py"", line 94, in find_spec
          parent = __import__(parent_name, fromlist=['__path__'])
        File ""/tmp/pip-install-syczp33h/openai_d2bb5765cb594c07a4e3c96cae3c38b0/openai/__init__.py"", line 9, in <module>
          from openai.api_resources import (
        File ""/tmp/pip-install-syczp33h/openai_d2bb5765cb594c07a4e3c96cae3c38b0/openai/api_resources/__init__.py"", line 1, in <module>
          from openai.api_resources.completion import Completion  # noqa: F401
        File ""/tmp/pip-install-syczp33h/openai_d2bb5765cb594c07a4e3c96cae3c38b0/openai/api_resources/completion.py"", line 4, in <module>
          from openai.api_resources.abstract import DeletableAPIResource, ListableAPIResource
        File ""/tmp/pip-install-syczp33h/openai_d2bb5765cb594c07a4e3c96cae3c38b0/openai/api_resources/abstract/__init__.py"", line 3, in <module>
          from openai.api_resources.abstract.api_resource import APIResource
        File ""/tmp/pip-install-syczp33h/openai_d2bb5765cb594c07a4e3c96cae3c38b0/openai/api_resources/abstract/api_resource.py"", line 4, in <module>
          from openai import api_requestor, error, util
        File ""/tmp/pip-install-syczp33h/openai_d2bb5765cb594c07a4e3c96cae3c38b0/openai/api_requestor.py"", line 21, in <module>
          import aiohttp
      ModuleNotFoundError: No module named 'aiohttp'

      During handling of the above exception, another exception occurred:

      Traceback (most recent call last):
        File ""/usr/lib/python3/dist-packages/pip/_vendor/pep517/in_process/_in_process.py"", line 363, in <module>
          main()
        File ""/usr/lib/python3/dist-packages/pip/_vendor/pep517/in_process/_in_process.py"", line 345, in main
          json_out['return_val'] = hook(**hook_input['kwargs'])
        File ""/usr/lib/python3/dist-packages/pip/_vendor/pep517/in_process/_in_process.py"", line 130, in get_requires_for_build_wheel
          return hook(config_settings)
        File ""/usr/lib/python3/dist-packages/setuptools/build_meta.py"", line 162, in get_requires_for_build_wheel
          return self._get_build_requires(
        File ""/usr/lib/python3/dist-packages/setuptools/build_meta.py"", line 143, in _get_build_requires
          self.run_setup()
        File ""/usr/lib/python3/dist-packages/setuptools/build_meta.py"", line 158, in run_setup
          exec(compile(code, __file__, 'exec'), locals())
        File ""setup.py"", line 3, in <module>
          setup()
        File ""/usr/lib/python3/dist-packages/setuptools/__init__.py"", line 153, in setup
          return distutils.core.setup(**attrs)
        File ""/usr/lib/python3/dist-packages/setuptools/_distutils/core.py"", line 122, in setup
          dist.parse_config_files()
        File ""/usr/lib/python3/dist-packages/setuptools/dist.py"", line 804, in parse_config_files
          parse_configuration(
        File ""/usr/lib/python3/dist-packages/setuptools/config.py"", line 158, in parse_configuration
          meta.parse()
        File ""/usr/lib/python3/dist-packages/setuptools/config.py"", line 498, in parse
          section_parser_method(section_options)
        File ""/usr/lib/python3/dist-packages/setuptools/config.py"", line 469, in parse_section
          self[name] = value
        File ""/usr/lib/python3/dist-packages/setuptools/config.py"", line 222, in __setitem__
          value = parser(value)
        File ""/usr/lib/python3/dist-packages/setuptools/config.py"", line 599, in _parse_version
          version = self._parse_attr(value, self.package_dir)
        File ""/usr/lib/python3/dist-packages/setuptools/config.py"", line 422, in _parse_attr
          module = importlib.import_module(module_name)
        File ""/usr/lib/python3.10/importlib/__init__.py"", line 126, in import_module
          return _bootstrap._gcd_import(name[level:], package, level)
        File ""<frozen importlib._bootstrap>"", line 1050, in _gcd_import
        File ""<frozen importlib._bootstrap>"", line 1027, in _find_and_load
        File ""<frozen importlib._bootstrap>"", line 992, in _find_and_load_unlocked
        File ""<frozen importlib._bootstrap>"", line 241, in _call_with_frames_removed
        File ""<frozen importlib._bootstrap>"", line 1050, in _gcd_import
        File ""<frozen importlib._bootstrap>"", line 1027, in _find_and_load
        File ""<frozen importlib._bootstrap>"", line 1006, in _find_and_load_unlocked
        File ""<frozen importlib._bootstrap>"", line 688, in _load_unlocked
        File ""<frozen importlib._bootstrap_external>"", line 883, in exec_module
        File ""<frozen importlib._bootstrap>"", line 241, in _call_with_frames_removed
        File ""/tmp/pip-install-syczp33h/openai_d2bb5765cb594c07a4e3c96cae3c38b0/openai/__init__.py"", line 9, in <module>
          from openai.api_resources import (
        File ""/tmp/pip-install-syczp33h/openai_d2bb5765cb594c07a4e3c96cae3c38b0/openai/api_resources/__init__.py"", line 1, in <module>
          from openai.api_resources.completion import Completion  # noqa: F401
        File ""/tmp/pip-install-syczp33h/openai_d2bb5765cb594c07a4e3c96cae3c38b0/openai/api_resources/completion.py"", line 4, in <module>
          from openai.api_resources.abstract import DeletableAPIResource, ListableAPIResource
        File ""/tmp/pip-install-syczp33h/openai_d2bb5765cb594c07a4e3c96cae3c38b0/openai/api_resources/abstract/__init__.py"", line 3, in <module>
          from openai.api_resources.abstract.api_resource import APIResource
        File ""/tmp/pip-install-syczp33h/openai_d2bb5765cb594c07a4e3c96cae3c38b0/openai/api_resources/abstract/api_resource.py"", line 4, in <module>
          from openai import api_requestor, error, util
        File ""/tmp/pip-install-syczp33h/openai_d2bb5765cb594c07a4e3c96cae3c38b0/openai/api_requestor.py"", line 21, in <module>
          import aiohttp
      ModuleNotFoundError: No module named 'aiohttp'
      [end of output]

  note: This error originates from a subprocess, and is likely not a problem with pip.
error: subprocess-exited-with-error

× Getting requirements to build wheel did not run successfully.
│ exit code: 1
╰─> See above for output.

note: This error originates from a subprocess, and is likely not a problem with pip.
OS
Ubuntu 22.04
Python version
Python 3.10.6
Library version
openai-python-0.26.5
 The text was updated successfully, but these errors were encountered: 
👍2
EthanBlackburn and RogerYang01 reacted with thumbs up emoji
All reactions
👍2 reactions"
"https://github.com/openai/openai-python/issues/218","Getting requests.exceptions.ChunkedEncodingError when using TaskCompletion in stream mode or","2023-12-31T00:01:06Z","Closed issue","bug","Describe the bug
Hi,
I'm using the openaI python module and I'm experiencing some issues lately. I'm making a streaming completion task. Randomly I'm getting the following exception ""requests.exceptions.ChunkedEncodingError : (""Connection broken: InvalidChunkLength(got length b'', 0 bytes read)"".
def send_completion_task_stream(self):
 prompt = """"""my prompt""""""
 response = self.completion_task.create(prompt=prompt, model=self.modal_name, stop=self.stop,
 temperature=self.temperature,
 top_p=self.top_p, frequency_penalty=self.frequency_penalty,
 presence_penalty=self.presence_penalty,
 max_tokens=3000,
 stream=True
 ) return response `
 do you know what can cause this error ?
Thank you
Best regards
To Reproduce
1- Send a streaming completion task and put max-token to 3000
 2 - The errors appears (but randomly)
Code snippets
No response
OS
macOs
Python version
Python 3.10
Library version
openai-python-0.26.1
 The text was updated successfully, but these errors were encountered: 
👍1
mneira10 reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/openai-python/issues/213","openai.error.RateLimitError: The server had an error while processing your request. Sorry about that!","2023-02-06T18:50:16Z","Closed issue","No label","I encountered many RateLimitError errors when I accessed the Completion interface today, but I actually did not call frequently, only about a few times a minute.
error message:
openai.error.RateLimitError: The server had an error while processing your request. Sorry about that!
my request:
response = openai.Completion.create(
      model=""text-davinci-003"",  
      prompt=query,
      temperature=0.9,  
      max_tokens=1200,  
      top_p=1,
      frequency_penalty=0.0,  
      presence_penalty=0.0,  
      stop=[""#""]
  )
my env:
Python 3.7.5
ubuntu 16.04
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/212","RateLimitError (You exceeded your current quota, please check your plan and billing details.)","2023-02-06T18:51:01Z","Closed issue","No label","I started using openai, created an account, got API Key.
 When I try to use it, I get the error message: RateLimitError. You exceeded your current quota, please check your plan and billing details.
    openai.api_key = ""...""
    openai.Model.list()
    response = openai.Completion.create(
        model='text-davinci-003',
        ........................
    )

 The text was updated successfully, but these errors were encountered: 
👀14
zhayujie, TheBigEye, VachhaniDeep, venkatesan-ramasamy, sumantgigacars, jorxgod, MorningStar2021, sabinedawaliby, harsimran-04, mr-tuhin, and 4 more reacted with eyes emoji
All reactions
👀14 reactions"
"https://github.com/openai/openai-python/issues/210","sklearn requirement breaks openai[embeddings] install","2023-03-01T17:33:54Z","Closed issue","No label","when installing the openai[embeddings] requirement, both sklearn and scikit-learn are listed as requirements source.
However, sklearn is deprecated, and is causing pip install openai[embeddings] to fail. See the following output from my CI build:
Collecting sklearn
  Downloading sklearn-0.0.post1.tar.gz (3.6 kB)
  Preparing metadata (setup.py): started
  Preparing metadata (setup.py): finished with status 'error'
  error: subprocess-exited-with-error
  
  × python setup.py egg_info did not run successfully.
  │ exit code: 1
  ╰─> [18 lines of output]
      The 'sklearn' PyPI package is deprecated, use 'scikit-learn'
      rather than 'sklearn' for pip commands.
      
      Here is how to fix this error in the main use cases:
      - use 'pip install scikit-learn' rather than 'pip install sklearn'
      - replace 'sklearn' by 'scikit-learn' in your pip requirements files
        (requirements.txt, setup.py, setup.cfg, Pipfile, etc ...)
      - if the 'sklearn' package is used by one of your dependencies,
        it would be great if you take some time to track which package uses
        'sklearn' instead of 'scikit-learn' and report it to their issue tracker
      - as a last resort, set the environment variable
        SKLEARN_ALLOW_DEPRECATED_SKLEARN_PACKAGE_INSTALL=True to avoid this error
      
      More information is available at
      https://github.com/scikit-learn/sklearn-pypi-package
      
      If the previous advice does not cover your use case, feel free to report it at
      https://github.com/scikit-learn/sklearn-pypi-package/issues/new
      [end of output]
  
  note: This error originates from a subprocess, and is likely not a problem with pip.
error: metadata-generation-failed

× Encountered error while generating package metadata.
╰─> See above for output.

A simple fix would be to simply remove sklearn from the list of dependencies of openai[embeddings].
I can open a PR for this if necessary.
 The text was updated successfully, but these errors were encountered: 
👍5
mmeehan-utilant, ashrielbrian, alvitawa, clear-sea, and amenegola reacted with thumbs up emoji
All reactions
👍5 reactions"
"https://github.com/openai/openai-python/issues/209","Override base url and headers with gateway data","2023-03-30T04:04:00Z","Closed issue","No label","So in my company we use an api gateway (Gravitee) to provide a central point for all apis (internal and external).
I read in the docs that the base url can be changed, but it looked to me that this is only intended for OpenAI on Azure.
 I also didn't find a way to easily override the headers of all requests. As we use our own api key internally and let the gateway transform the headers.
Question 1: How can we change the base url to our gateway?
 Question 2: How can we use different headers for all requests?
I have not that much experience with python, so very likely that I overlooked something.
 All help is greatly appreciated :)
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/208","Unable to access ChatGPT from this openai package","2023-02-05T16:34:03Z","Closed issue","No label","I am trying to get ChatGPT model from list of OpenAI models using following code. But I can not find the ChatGPT in the list. Do OpenAI provide dedicated API to communicate with ChatGPT?
import openai
openai.api_key = ""MY SECRET KEY""

# list engines
engines = openai.Engine.list()

# print the first engine's id
print(engines)

This results in the following response
{                                                                                  
  ""data"": [                                                                        
    {                                                                              
      ""created"": null,                                                             
      ""id"": ""babbage"",                                                             
      ""object"": ""engine"",                                                          
      ""owner"": ""openai"",                                                           
      ""permissions"": null,                                                         
      ""ready"": true                                                                
    },                                                                             
    {                                                                              
      ""created"": null,                                                             
      ""id"": ""ada"",                                                                 
      ""object"": ""engine"",                                                          
      ""owner"": ""openai"",                                                           
      ""permissions"": null,                                                         
      ""ready"": true                                                                
    },                                                                             
    {                                                                              
      ""created"": null,                                                             
      ""id"": ""davinci"",                                                             
      ""object"": ""engine"",                                                          
      ""owner"": ""openai"",                                                           
      ""permissions"": null,                                                         
      ""ready"": true                                                                
    },                                                                             
    {                                                                              
      ""created"": null,                                                             
      ""id"": ""text-embedding-ada-002"",                                              
      ""object"": ""engine"",                                                          
      ""owner"": ""openai-internal"",                                                  
      ""permissions"": null,                                                         
      ""ready"": true                                                                
    },                                                                             
    {                                                                              
      ""created"": null,                                                             
      ""id"": ""babbage-code-search-code"",                                            
      ""object"": ""engine"",                                                          
      ""owner"": ""openai-dev"",                                                       
      ""permissions"": null,                                                         
      ""ready"": true                                                                
    },                                                                             
    {                                                                              
      ""created"": null,                                                             
      ""id"": ""text-similarity-babbage-001"",                                         
      ""object"": ""engine"",                                                          
      ""owner"": ""openai-dev"",                                                       
      ""permissions"": null,                                                         
      ""ready"": true                                                                
    },                                                                             
    {                                                                              
      ""created"": null,                                                             
      ""id"": ""text-davinci-003"",                                                    
      ""object"": ""engine"",                                                          
      ""owner"": ""openai-internal"",                                                  
      ""permissions"": null,                                                         
      ""ready"": true                                                                
    },                                                                             
    {                                                                              
      ""created"": null,                                                             
      ""id"": ""text-davinci-001"",                                                    
      ""object"": ""engine"",                                                          
      ""owner"": ""openai"",                                                           
      ""permissions"": null,                                                         
      ""ready"": true                                                                
    },                                                                             
    {                                                                              
      ""created"": null,                                                             
      ""id"": ""curie-instruct-beta"",                                                 
      ""object"": ""engine"",                                                          
      ""owner"": ""openai"",                                                           
      ""permissions"": null,                                                         
      ""ready"": true                                                                
    },                                                                             
    {                                                                              
      ""created"": null,                                                             
      ""id"": ""babbage-code-search-text"",                                            
      ""object"": ""engine"",                                                          
      ""owner"": ""openai-dev"",                                                       
      ""permissions"": null,                                                         
      ""ready"": true                                                                
    },                                                                             
    {                                                                              
      ""created"": null,                                                             
      ""id"": ""babbage-similarity"",                                                  
      ""object"": ""engine"",                                                          
      ""owner"": ""openai-dev"",                                                       
      ""permissions"": null,                                                         
      ""ready"": true                                                                
    },                                                                             
    {                                                                              
      ""created"": null,                                                             
      ""id"": ""curie-search-query"",                                                  
      ""object"": ""engine"",                                                          
      ""owner"": ""openai-dev"",                                                       
      ""permissions"": null,                                                         
      ""ready"": true                                                                
    },                                                                             
    {                                                                              
      ""created"": null,                                                             
      ""id"": ""code-search-babbage-text-001"",                                        
      ""object"": ""engine"",                                                          
      ""owner"": ""openai-dev"",                                                       
      ""permissions"": null,                                                         
      ""ready"": true                                                                
    },                                                                             
    {                                                                              
      ""created"": null,                                                             
      ""id"": ""code-cushman-001"",                                                    
      ""object"": ""engine"",                                                          
      ""owner"": ""openai"",                                                           
      ""permissions"": null,                                                         
      ""ready"": true                                                                
    },                                                                             
    {                                                                              
      ""created"": null,                                                             
      ""id"": ""code-search-babbage-code-001"",                                        
      ""object"": ""engine"",                                                          
      ""owner"": ""openai-dev"",                                                       
      ""permissions"": null,                                                         
      ""ready"": true                                                                
    },                                                                             
    {                                                                              
      ""created"": null,                                                             
      ""id"": ""audio-transcribe-deprecated"",                                         
      ""object"": ""engine"",                                                          
      ""owner"": ""openai-internal"",                                                  
      ""permissions"": null,                                                         
      ""ready"": true                                                                
    },                                                                             
    {                                                                              
      ""created"": null,                                                             
      ""id"": ""code-davinci-002"",                                                    
      ""object"": ""engine"",                                                          
      ""owner"": ""openai"",                                                           
      ""permissions"": null,                                                         
      ""ready"": true                                                                
    },                                                                             
    {                                                                              
      ""created"": null,                                                             
      ""id"": ""text-ada-001"",                                                        
      ""object"": ""engine"",                                                          
      ""owner"": ""openai"",                                                           
      ""permissions"": null,                                                         
      ""ready"": true                                                                
    },                                                                             
    {                                                                              
      ""created"": null,                                                             
      ""id"": ""text-similarity-ada-001"",                                             
      ""object"": ""engine"",                                                          
      ""owner"": ""openai-dev"",                                                       
      ""permissions"": null,                                                         
      ""ready"": true                                                                
    },                                                                             
    {                                                                              
      ""created"": null,                                                             
      ""id"": ""text-davinci-insert-002"",                                             
      ""object"": ""engine"",                                                          
      ""owner"": ""openai"",                                                           
      ""permissions"": null,                                                         
      ""ready"": true                                                                
    },                                                                             
    {                                                                              
      ""created"": null,                                                             
      ""id"": ""ada-code-search-code"",                                                
      ""object"": ""engine"",                                                          
      ""owner"": ""openai-dev"",                                                       
      ""permissions"": null,                                                         
      ""ready"": true                                                                
    },                                                                             
    {                                                                              
      ""created"": null,                                                             
      ""id"": ""text-davinci-002"",                                                    
      ""object"": ""engine"",                                                          
      ""owner"": ""openai"",                                                           
      ""permissions"": null,                                                         
      ""ready"": true                                                                
    },                                                                             
    {                                                                              
      ""created"": null,                                                             
      ""id"": ""ada-similarity"",                                                      
      ""object"": ""engine"",                                                          
      ""owner"": ""openai-dev"",                                                       
      ""permissions"": null,                                                         
      ""ready"": true                                                                
    },                                                                             
    {                                                                              
      ""created"": null,                                                             
      ""id"": ""code-search-ada-text-001"",                                            
      ""object"": ""engine"",                                                          
      ""owner"": ""openai-dev"",                                                       
      ""permissions"": null,                                                         
      ""ready"": true                                                                
    },                                                                             
    {                                                                              
      ""created"": null,                                                             
      ""id"": ""text-search-ada-query-001"",                                           
      ""object"": ""engine"",                                                          
      ""owner"": ""openai-dev"",                                                       
      ""permissions"": null,                                                         
      ""ready"": true                                                                
    },                                                                             
    {                                                                              
      ""created"": null,                                                             
      ""id"": ""text-curie-001"",                                                      
      ""object"": ""engine"",                                                          
      ""owner"": ""openai"",                                                           
      ""permissions"": null,                                                         
      ""ready"": true                                                                
    },                                                                             
    {                                                                              
      ""created"": null,                                                             
      ""id"": ""text-davinci-edit-001"",                                               
      ""object"": ""engine"",                                                          
      ""owner"": ""openai"",                                                           
      ""permissions"": null,                                                         
      ""ready"": true                                                                
    },                                                                             
    {                                                                              
      ""created"": null,                                                             
      ""id"": ""davinci-search-document"",                                             
      ""object"": ""engine"",                                                          
      ""owner"": ""openai-dev"",                                                       
      ""permissions"": null,                                                         
      ""ready"": true                                                                
    },                                                                             
    {                                                                              
      ""created"": null,                                                             
      ""id"": ""ada-code-search-text"",                                                
      ""object"": ""engine"",                                                          
      ""owner"": ""openai-dev"",                                                       
      ""permissions"": null,                                                         
      ""ready"": true                                                                
    },                                                                             
    {                                                                              
      ""created"": null,                                                             
      ""id"": ""text-search-ada-doc-001"",                                             
      ""object"": ""engine"",                                                          
      ""owner"": ""openai-dev"",                                                       
      ""permissions"": null,                                                         
      ""ready"": true                                                                
    },                                                                             
    {                                                                              
      ""created"": null,                                                             
      ""id"": ""code-davinci-edit-001"",                                               
      ""object"": ""engine"",                                                          
      ""owner"": ""openai"",                                                           
      ""permissions"": null,                                                         
      ""ready"": true                                                                
    },                                                                             
    {                                                                              
      ""created"": null,                                                             
      ""id"": ""davinci-instruct-beta"",                                               
      ""object"": ""engine"",                                                          
      ""owner"": ""openai"",                                                           
      ""permissions"": null,                                                         
      ""ready"": true                                                                
    },                                                                             
    {                                                                              
      ""created"": null,                                                             
      ""id"": ""text-similarity-curie-001"",                                           
      ""object"": ""engine"",                                                          
      ""owner"": ""openai-dev"",                                                       
      ""permissions"": null,                                                         
      ""ready"": true                                                                
    },                                                                             
    {                                                                              
      ""created"": null,                                                             
      ""id"": ""code-search-ada-code-001"",                                            
      ""object"": ""engine"",                                                          
      ""owner"": ""openai-dev"",                                                       
      ""permissions"": null,                                                         
      ""ready"": true                                                                
    },                                                                             
    {                                                                              
      ""created"": null,                                                             
      ""id"": ""ada-search-query"",                                                    
      ""object"": ""engine"",                                                          
      ""owner"": ""openai-dev"",                                                       
      ""permissions"": null,                                                         
      ""ready"": true                                                                
    },                                                                             
    {                                                                              
      ""created"": null,                                                             
      ""id"": ""text-search-davinci-query-001"",                                       
      ""object"": ""engine"",                                                          
      ""owner"": ""openai-dev"",                                                       
      ""permissions"": null,                                                         
      ""ready"": true                                                                
    },                                                                             
    {                                                                              
      ""created"": null,                                                             
      ""id"": ""davinci-search-query"",                                                
      ""object"": ""engine"",                                                          
      ""owner"": ""openai-dev"",                                                       
      ""permissions"": null,                                                         
      ""ready"": true                                                                
    },                                                                             
    {                                                                              
      ""created"": null,                                                             
      ""id"": ""text-davinci-insert-001"",                                             
      ""object"": ""engine"",                                                          
      ""owner"": ""openai"",                                                           
      ""permissions"": null,                                                         
      ""ready"": true                                                                
    },                                                                             
    {                                                                              
      ""created"": null,                                                             
      ""id"": ""babbage-search-document"",                                             
      ""object"": ""engine"",                                                          
      ""owner"": ""openai-dev"",                                                       
      ""permissions"": null,                                                         
      ""ready"": true                                                                
    },                                                                             
    {                                                                              
      ""created"": null,                                                             
      ""id"": ""ada-search-document"",                                                 
      ""object"": ""engine"",                                                          
      ""owner"": ""openai-dev"",                                                       
      ""permissions"": null,                                                         
      ""ready"": true                                                                
    },                                                                             
    {                                                                              
      ""created"": null,                                                             
      ""id"": ""text-search-babbage-doc-001"",                                         
      ""object"": ""engine"",                                                          
      ""owner"": ""openai-dev"",                                                       
      ""permissions"": null,                                                         
      ""ready"": true                                                                
    },                                                                             
    {                                                                              
      ""created"": null,                                                             
      ""id"": ""text-search-curie-doc-001"",                                           
      ""object"": ""engine"",                                                          
      ""owner"": ""openai-dev"",                                                       
      ""permissions"": null,                                                         
      ""ready"": true                                                                
    },                                                                             
    {                                                                              
      ""created"": null,                                                             
      ""id"": ""text-search-curie-query-001"",                                         
      ""object"": ""engine"",                                                          
      ""owner"": ""openai-dev"",                                                       
      ""permissions"": null,                                                         
      ""ready"": true                                                                
    },                                                                             
    {                                                                              
      ""created"": null,                                                             
      ""id"": ""babbage-search-query"",                                                
      ""object"": ""engine"",                                                          
      ""owner"": ""openai-dev"",                                                       
      ""permissions"": null,                                                         
      ""ready"": true                                                                
    },                                                                             
    {                                                                              
      ""created"": null,                                                             
      ""id"": ""text-babbage-001"",                                                    
      ""object"": ""engine"",                                                          
      ""owner"": ""openai"",                                                           
      ""permissions"": null,                                                         
      ""ready"": true                                                                
    },                                                                             
    {                                                                              
      ""created"": null,                                                             
      ""id"": ""text-search-davinci-doc-001"",                                         
      ""object"": ""engine"",                                                          
      ""owner"": ""openai-dev"",                                                       
      ""permissions"": null,                                                         
      ""ready"": true                                                                
    },                                                                             
    {                                                                              
      ""created"": null,                                                             
      ""id"": ""text-search-babbage-query-001"",                                       
      ""object"": ""engine"",                                                          
      ""owner"": ""openai-dev"",                                                       
      ""permissions"": null,                                                         
      ""ready"": true                                                                
    },                                                                             
    {                                                                              
      ""created"": null,                                                             
      ""id"": ""curie-similarity"",                                                    
      ""object"": ""engine"",                                                          
      ""owner"": ""openai-dev"",                                                       
      ""permissions"": null,                                                         
      ""ready"": true                                                                
    },                                                                             
    {                                                                              
      ""created"": null,                                                             
      ""id"": ""curie-search-document"",                                               
      ""object"": ""engine"",                                                          
      ""owner"": ""openai-dev"",                                                       
      ""permissions"": null,                                                         
      ""ready"": true                                                                
    },                                                                             
    {                                                                              
      ""created"": null,                                                             
      ""id"": ""curie"",                                                               
      ""object"": ""engine"",                                                          
      ""owner"": ""openai"",                                                           
      ""permissions"": null,                                                         
      ""ready"": true                                                                
    },                                                                             
    {                                                                              
      ""created"": null,                                                             
      ""id"": ""text-similarity-davinci-001"",                                         
      ""object"": ""engine"",                                                          
      ""owner"": ""openai-dev"",                                                       
      ""permissions"": null,                                                         
      ""ready"": true                                                                
    },                                                                             
    {                                                                              
      ""created"": null,                                                             
      ""id"": ""davinci-similarity"",                                                  
      ""object"": ""engine"",                                                          
      ""owner"": ""openai-dev"",                                                       
      ""permissions"": null,                                                         
      ""ready"": true                                                                
    }                                                                              
  ],                                                                               
  ""object"": ""list""                                                                 
} 

I can not find ChatGPT in the response. As per my understanding I should get a dedicated id for ChatGPT. I want to directly use ChatGPT like we use other models such as Davinci etc. Any help or information in this regard?
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/206","openai tools fine_tunes.prepare_data does not accept indented JSON files","2023-04-10T15:56:23Z","Closed issue","No label","To reproduce:
pip install --user openai[datalib]==0.26.4 

# Works fine:echo '[{""prompt"": ""Here is my example input 1"", ""completion"": ""Complete to 1""}, {""prompt"": ""Here is my example input 2"", ""completion"": ""Complete to 2""}]' > unindented.json
openai tools fine_tunes.prepare_data --quiet --file unindented.json

# Doesn't work:
cat > to_indented.py << EOFimport jsonwith open('unindented.json', 'rt') as f:    data = json.loads(f.read())# Simple rewrite of the ""unindented.json"": output to indented versionwith open('indented', 'wt') as f:    f.write(json.dumps(data, indent=2))EOF
python to_indented.py
openai tools fine_tunes.prepare_data -f indented.json
 The text was updated successfully, but these errors were encountered: 
👍1
GoldenretriverYT reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/openai-python/issues/205","fix yes , i just recently found a funny problem , see this","2023-01-27T01:23:20Z","Closed issue","No label","regards
 Doni ramdani
Doniramdani810@gmail.com
 The text was updated successfully, but these errors were encountered: 
👍1
doniramdani810 reacted with thumbs up emoji🎉1
doniramdani810 reacted with hooray emoji
All reactions
👍1 reaction
🎉1 reaction"
"https://github.com/openai/openai-python/issues/199","Why do I can't install openai","2023-03-11T03:57:38Z","Closed issue","No label","I had this problem,error: subprocess-exited-with-error, when I installing.
 How do I install openai?
My python is 3.8
 pip is 22.3.1
log:
Collecting openai
  Using cached openai-0.26.2.tar.gz (55 kB)
  Installing build dependencies ... done
  Getting requirements to build wheel ... error
  error: subprocess-exited-with-error

  × Getting requirements to build wheel did not run successfully.
  │ exit code: 1
  ╰─> [17 lines of output]
      Traceback (most recent call last):
        File ""D:\Python\Python38\virtual_envs\open_ai\lib\site-packages\pip\_vendor\pep517\in_process\_in_process.py"", line 351, in <module>
          main()
        File ""D:\Python\Python38\virtual_envs\open_ai\lib\site-packages\pip\_vendor\pep517\in_process\_in_process.py"", line 333, in main
          json_out['return_val'] = hook(**hook_input['kwargs'])
        File ""D:\Python\Python38\virtual_envs\open_ai\lib\site-packages\pip\_vendor\pep517\in_process\_in_process.py"", line 118, in get_requires_for_build_wheel
          return hook(config_settings)
        File ""C:\Users\user\AppData\Local\Temp\pip-build-env-qcki_ch_\overlay\Lib\site-packages\setuptools\build_meta.py"", line 338, in get_requires_for_build_wheel
          return self._get_build_requires(config_settings, requirements=['wheel'])
        File ""C:\Users\user\AppData\Local\Temp\pip-build-env-qcki_ch_\overlay\Lib\site-packages\setuptools\build_meta.py"", line 320, in _get_build_requires
          self.run_setup()
        File ""C:\Users\user\AppData\Local\Temp\pip-build-env-qcki_ch_\overlay\Lib\site-packages\setuptools\build_meta.py"", line 484, in run_setup
          super(_BuildMetaLegacyBackend,
        File ""C:\Users\user\AppData\Local\Temp\pip-build-env-qcki_ch_\overlay\Lib\site-packages\setuptools\build_meta.py"", line 335, in run_setup
          exec(code, locals())
        File ""<string>"", line 13, in <module>
      UnicodeDecodeError: 'cp950' codec can't decode byte 0xe2 in position 1030: illegal multibyte sequence
      [end of output]

  note: This error originates from a subprocess, and is likely not a problem with pip.
error: subprocess-exited-with-error

× Getting requirements to build wheel did not run successfully.
│ exit code: 1
╰─> See above for output.

note: This error originates from a subprocess, and is likely not a problem with pip.


 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/196","Regression in download, openai api fine_tunes.results behavior","2023-01-25T04:21:02Z","Closed issue","No label","To reproduce, on the older openai==0.25.0
openai api fine_tunes.results -i <job_id_here>
You'll get the output of the actual CSV file
E.g.
step,elapsed_tokens,elapsed_examples,training_loss,training_sequence_accuracy,training_token_accuracy
1,22048,32,0.011019098009307718,0.0,0.0
1,32288,32,0.007613447834811513,0.0,0.0

On the latest openai==0.26.2
You get instead
{
  ""object"": ""file"",
  ""id"": ""file-66NnpqAXzw3In27fROPIU03P"",
  ""purpose"": ""fine-tune-results"",
  ""filename"": ""compiled_results.csv"",
  ""bytes"": 386756,
  ""created_at"": 1674504271,
  ""status"": ""processed"",
  ""status_details"": null
}

This is probably due to a change in behavior in download of /api_resources/file.py. It probably affects everyone who depends on that download method, so i'm guessing wandb users who are syncing their runs too?
See https://github.com/openai/openai-python/pull/146/files#r1084884786
Probably related to #185, #183
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/192","ModuleNotFoundError: No module named 'openai' even though installed","2023-01-23T18:32:05Z","Closed issue","No label","Hello, I created a new python=3.9 conda environment, and installed openai with pip. However, I found that I am unable to import it. Would anyone be willing to advise? Many thanks! 
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/191","response is truncated from API","2023-03-17T20:29:17Z","Closed issue","No label","After pip install openai, I tried:
openai api completions.create -e text-davinci-003 -p ""what means soyboy in French?

I get
Le terme « soyboy » n'est pas courant en

The text is not finished/truncated.
What I did wrong? I see no documentation here on how to set tokens from command line if this is the issue there.
 The text was updated successfully, but these errors were encountered: 
👍2
MayCXC and hafiz031 reacted with thumbs up emoji
All reactions
👍2 reactions"
"https://github.com/openai/openai-python/issues/189","[0.26.1] Error syncing to wandb","2023-01-25T15:20:20Z","Closed issue","No label","When running the following command I receive the following warning.
$ openai wandb sync --project project
wandb: Currently logged in as: me (company). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in project/wandb/run-20230117_161328-ft-upiwOdAB20VtkGN7uHW5TFv2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ft-upiwOdAB20VtkGN7uHW5TFv2
wandb: ⭐️ View project at https://wandb.ai/me/project
wandb: 🚀 View run at https://wandb.ai/me/project/runs/ft-upiwOdAB20VtkGN7uHW5TFv2
File file-ISyZKcogrCzx59ftjGPWOUs1 could not be read as a valid JSON file
File file-3GjQ844dqjAY6hYuQKpMf7mQ could not be read as a valid JSON file
When I check in wandb, only system metrics are synced. I do not see any of the metrics related to the fine-tuning such as validation loss and token usage.
I am running the following versions.
python==3.10.8
openai==0.26.1
wandb==0.13.9

 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/186","Fine-tuning without using CLI","2023-01-23T18:37:41Z","Closed issue","No label","Hello everyone, is there any example or snippet of code on how to fine-tune using the FineTune class directly in Python script, instead of the CLI commands?
 The text was updated successfully, but these errors were encountered: 
❤️1
logankilpatrick reacted with heart emoji
All reactions
❤️1 reaction"
"https://github.com/openai/openai-python/issues/184","Json parsing issue when using openai.Completion.acreate(stream=True)","2023-02-07T22:18:38Z","Closed issue","No label","See the last line, looks like the stream iterator might be breaking up parts where it shouldn't? Unless it's an API issue. cc @ddeville
Traceback (most recent call last):
  File ""/Users/nuno/Library/Caches/pypoetry/virtualenvs/platform-api-30l3Kv3P-py3.10/lib/python3.10/site-packages/openai/api_requestor.py"", line 670, in _interpret_response_line
    data = json.loads(rbody)
  File ""/opt/homebrew/Cellar/python@3.10/3.10.9/Frameworks/Python.framework/Versions/3.10/lib/python3.10/json/__init__.py"", line 346, in loads
    return _default_decoder.decode(s)
  File ""/opt/homebrew/Cellar/python@3.10/3.10.9/Frameworks/Python.framework/Versions/3.10/lib/python3.10/json/decoder.py"", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File ""/opt/homebrew/Cellar/python@3.10/3.10.9/Frameworks/Python.framework/Versions/3.10/lib/python3.10/json/decoder.py"", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
json.decoder.JSONDecodeError: Unterminated string starting at: line 1 column 191 (char 190)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""/Users/nuno/dev/concat/server-py/concat/conversation.py"", line 87, in handle_conversation
    await run
  File ""/Users/nuno/dev/concat/server-py/concat/agent/openai/openai.py"", line 122, in __call__
    msg, _ = await asyncio.gather(
  File ""/Users/nuno/dev/concat/server-py/concat/tools/multi_prompt.py"", line 87, in __call__
    return await self.call(prompt[""prompt""], examples, input, ctx)
  File ""/Users/nuno/dev/concat/server-py/concat/tools/sql/__init__.py"", line 59, in call
    async for key, msg in ctx.stream_multi_message_async(
  File ""/Users/nuno/dev/concat/server-py/concat/context/context.py"", line 419, in stream_multi_message_async
    async for text in iterator:
  File ""/Users/nuno/dev/concat/server-py/concat/utils/openai.py"", line 37, in openai_completion_stream
    async for chunk in cast(
  File ""/Users/nuno/Library/Caches/pypoetry/virtualenvs/platform-api-30l3Kv3P-py3.10/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py"", line 230, in <genexpr>
    return (
  File ""/Users/nuno/Library/Caches/pypoetry/virtualenvs/platform-api-30l3Kv3P-py3.10/lib/python3.10/site-packages/openai/api_requestor.py"", line 320, in wrap_resp
    async for r in resp:
  File ""/Users/nuno/Library/Caches/pypoetry/virtualenvs/platform-api-30l3Kv3P-py3.10/lib/python3.10/site-packages/openai/api_requestor.py"", line 635, in <genexpr>
    self._interpret_response_line(
  File ""/Users/nuno/Library/Caches/pypoetry/virtualenvs/platform-api-30l3Kv3P-py3.10/lib/python3.10/site-packages/openai/api_requestor.py"", line 672, in _interpret_response_line
    raise error.APIError(
openai.error.APIError: HTTP code 200 from API ({""id"": ""cmpl-6YxjgLI6W6XwRj8hqhDZ6aCFz19Hp"", ""object"": ""text_completion"", ""created"": 1673790796, ""choices"": [{""text"": ""sche"", ""index"": 0, ""logprobs"": null, ""finish_reason"": null}], ""model"": "")

 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/181","Difference response between requests from openai package and api.openai.com","2023-02-06T19:00:04Z","Closed issue","No label","Hi,
 This is a simple example that return difference response between request from openai package and through the HTTP request
Human: ""Give me an example sentence for the word: python""

> package:  'The python coiled itself around the tree branch.'
> HTTP request: 'Example: I am learning to code in Python this summer.'

Sometime, the HTTP response contains other words than Example, such as Computer, AI, Bot, Robot, ...
 I wonder what is parameter to exclude these prefix in the response from HTTP request?
 Thanks
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/173","[0.26.0] openai/cli.py:440: RuntimeWarning: coroutine 'FineTune.stream_events' was never awaited","2023-01-12T05:35:32Z","Closed issue","No label","bash$ openai api fine_tunes.create -t ./train_test_prepared.jsonl -m davinci
Upload progress: 100%|██████████████████████████████████████████████████████████████████████████████| 112/112 [00:00<00:00, 88.5kit/s]
Uploaded file from ./train_test_prepared.jsonl: file-ppEDNe0p6EomteEp3JFbBoFp
Created fine-tune: ft-u9KskmmvSnBtVc4VDfbe7lyr
Streaming events until fine-tuning is complete...

(Ctrl-C will interrupt the stream, but not cancel the fine-tune)

Stream interrupted (client disconnected).
To resume the stream, run:

  openai api fine_tunes.follow -i ft-u9KskmmvSnBtVc4VDfbe7lyr

/usr/lib/python3.10/site-packages/openai/cli.py:406: RuntimeWarning: coroutine 'FineTune.stream_events' was never awaited
  cls._stream_events(resp[""id""])
RuntimeWarning: Enable tracemalloc to get the object allocation traceback


it is working fine with 0.25.0
may be related: dev-python/aiohttp-3.8.3
 The text was updated successfully, but these errors were encountered: 
👍10
sourceful-wing, evegarcianz, shirelga, prateek3255, kotikpoo000, Kcin1993, TanishqGoyal02, RaphSte, marialiguzmanadan, and asimkt reacted with thumbs up emoji
All reactions
👍10 reactions"
"https://github.com/openai/openai-python/issues/171","openai.Completion.acreate(stream=True) raises an Exception","2023-01-12T19:36:51Z","Closed issue","No label","See stack trace below
Traceback (most recent call last):
  File ""/Users/nuno/dev/concat/server-py/concat/conversation.py"", line 75, in handle_conversation
    await run
  File ""/Users/nuno/dev/concat/server-py/concat/agent/openai/openai.py"", line 122, in __call__
    msg = await ctx.stream_message_async(
  File ""/Users/nuno/dev/concat/server-py/concat/context.py"", line 287, in stream_message_async
    async for text in iterator:
  File ""/Users/nuno/dev/concat/server-py/concat/agent/openai/openai.py"", line 60, in openai_completion_stream
    iterator = await openai.Completion.acreate(*args, **kwargs, stream=True)
  File ""/Users/nuno/Library/Caches/pypoetry/virtualenvs/platform-api-30l3Kv3P-py3.10/lib/python3.10/site-packages/openai/api_resources/completion.py"", line 45, in acreate
    return await super().acreate(*args, **kwargs)
  File ""/Users/nuno/Library/Caches/pypoetry/virtualenvs/platform-api-30l3Kv3P-py3.10/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py"", line 230, in acreate
    return (
TypeError: 'async_generator' object is not iterable

I believe this should be an async for in line 239 of engine_api_resource.py. Once I make that change, then I get this error
Traceback (most recent call last):
  File ""/Users/nuno/Library/Caches/pypoetry/virtualenvs/platform-api-30l3Kv3P-py3.10/lib/python3.10/site-packages/openai/api_requestor.py"", line 645, in _interpret_response_line
    data = json.loads(rbody)
  File ""/opt/homebrew/Cellar/python@3.10/3.10.9/Frameworks/Python.framework/Versions/3.10/lib/python3.10/json/__init__.py"", line 346, in loads
    return _default_decoder.decode(s)
  File ""/opt/homebrew/Cellar/python@3.10/3.10.9/Frameworks/Python.framework/Versions/3.10/lib/python3.10/json/decoder.py"", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File ""/opt/homebrew/Cellar/python@3.10/3.10.9/Frameworks/Python.framework/Versions/3.10/lib/python3.10/json/decoder.py"", line 355, in raw_decode
    raise JSONDecodeError(""Expecting value"", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 2 column 1 (char 1)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/Users/nuno/dev/concat/server-py/concat/conversation.py"", line 75, in handle_conversation
    await run
  File ""/Users/nuno/dev/concat/server-py/concat/agent/openai/openai.py"", line 122, in __call__
    msg = await ctx.stream_message_async(
  File ""/Users/nuno/dev/concat/server-py/concat/context.py"", line 287, in stream_message_async
    async for text in iterator:
  File ""/Users/nuno/dev/concat/server-py/concat/agent/openai/openai.py"", line 61, in openai_completion_stream
    async for chunk in iterator:
  File ""/Users/nuno/Library/Caches/pypoetry/virtualenvs/platform-api-30l3Kv3P-py3.10/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py"", line 230, in <genexpr>
    return (
  File ""/Users/nuno/Library/Caches/pypoetry/virtualenvs/platform-api-30l3Kv3P-py3.10/lib/python3.10/site-packages/openai/api_requestor.py"", line 611, in <genexpr>
    self._interpret_response_line(
  File ""/Users/nuno/Library/Caches/pypoetry/virtualenvs/platform-api-30l3Kv3P-py3.10/lib/python3.10/site-packages/openai/api_requestor.py"", line 647, in _interpret_response_line
    raise error.APIError(
openai.error.APIError: HTTP code 200 from API (
)

which appears to be caused by trying to json-decode the following string \n. Once I fix that (by doing eg. data = json.loads(rbody) if rbody.strip() else None, it appears the stream never finishes, ie. the next part never arrives
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/166","Completion api injecting restart text after every prompt, no option to switch it off like in playground","2023-01-13T17:23:57Z","Closed issue","No label","I'm not sure this is an issue with the python api per se.
 When using the completion api to generate the next line of a poem using davinci, the api always starts the text with ""\n\n"". This can't be filtered with the logit parameter as the phrase ""\n\n"" is actually 4 tokens according to the tokenizer: [59, 77, 59, 77]
What is the correct workaround?
 Is there any plan to add the option ""Inject restart text"" like in the playground? The restart text adds 4 tokens to each request, which adds up fast...
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/164","How to implement continuous dialogue with api","2023-01-01T21:11:01Z","Closed issue","No label","How to use the api to achieve contextual continuous dialogue like ChatGPT , such as the first call to the API to ask you what your name is, and the second re-request to get a different result. I tried to use ChatGPT to answer this question and it told me that I could take a conversation_id parameter, but I was prompted with an error when I actually used it.please help me, This is important
 The text was updated successfully, but these errors were encountered: 
👍1
thisIsTheFoxe reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/openai-python/issues/162","getting a failure when running the search example - https://beta.openai.com/docs/guides/search","2023-01-01T21:12:06Z","Closed issue","No label","when running the Search.create call I am getting a failure
openai.Search.create(
 model=""ada"",
 query=""happy"",
 max_rerank=5,
 file=""file-xVMG7P66WxKS1zwzEBKUGGzl""
 )
here is the failure:
InvalidRequestError Traceback (most recent call last)
 in 
 ----> 1 openai.Search.create(
 2 model=""ada"",
 3 query=""happy"",
 4 max_rerank=5,
 5 file=""file-xVMG7P66WxKS1zwzEBKUGGzl""
4 frames
 /usr/local/lib/python3.8/dist-packages/openai/api_requestor.py in _interpret_response_line(self, rbody, rcode, rheaders, stream)
 427 stream_error = stream and ""error"" in resp.data
 428 if stream_error or not 200 <= rcode < 300:
 --> 429 raise self.handle_error_response(
 430 rbody, rcode, resp.data, rheaders, stream_error=stream_error
 431 )
InvalidRequestError: Invalid URL (POST /v1/search)
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/161","AttributeError: partially initialized module 'openai' has no attribute 'Completion'","2022-12-23T13:46:57Z","Closed issue","No label","No description provided.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/160","Supplying API keys per request","2023-01-06T20:27:50Z","Closed issue","No label","I'd like to supply the API key for each request separatly (for different users of the same app), instead of openai.api_key = ....
I find this strange behaviour, as documented in the following code, see in particular the _UNEXPECTED_BEHAVIOR tests.
 Edit: the following tests all pass with openai==0.25.0.
import osimport openaiimport unittest

from openai.error import AuthenticationError, RateLimitError

# No API key is provided implicitly, to avoid leakageassert os.environ.get('OPENAI_API_KEY') is None

# This is my API keyMINE = os.environ['MY_OPENAI_API_KEY']

prompt = ""Give me three anagrams of 'Randompile':""


class NoKeyError(Exception):
    pass


def inspector(f):
    def _(*a, **kw):
        try:
            result = f(*a, **kw)
        except RateLimitError:
            raise RuntimeError(""This test assumes the API key is still valid."")
        except AuthenticationError as ex:
            if ""No API key provided"" in ex.user_message:
                raise NoKeyError
            else:
                raise
        else:
            return result

    return _


@inspectordef completion(*, key_constr=None, key_member=None):
    worker = openai.Completion(api_key=key_constr)
    return worker.create(api_key=key_member, model='text-davinci-003', prompt=prompt)


@inspectordef dalleimage(*, key_constr=None, key_member=None):
    worker = openai.Image(api_key=key_constr)
    return worker.create(api_key=key_member, prompt=prompt)


class ObservedBehavior(unittest.TestCase):
    def test_completion_00(self):
        with self.assertRaises(NoKeyError):
            completion(key_constr=None, key_member=None)

    def test_completion_10_UNEXPECTED_BEHAVIOUR(self):
        with self.assertRaises(NoKeyError):
            completion(key_constr=MINE, key_member=None)

    def test_completion_01(self):
        result = completion(key_constr=None, key_member=MINE)
        # print(f""{prompt}"", result['choices'][0]['text'].strip(), sep='\n')

    def test_completion_11(self):
        result = completion(key_constr=MINE, key_member=MINE)
        # print(f""{prompt}"", result['choices'][0]['text'].strip(), sep='\n')

    def test_dalleimage_00(self):
        with self.assertRaises(NoKeyError):
            dalleimage(key_constr=None, key_member=None)

    def test_dalleimage_10_UNEXPECTED_BEHAVIOUR(self):
        with self.assertRaises(NoKeyError):
            dalleimage(key_constr=MINE, key_member=None)

    def test_dalleimage_01_UNEXPECTED_BEHAVIOUR(self):
        with self.assertRaises(NoKeyError):
            dalleimage(key_constr=None, key_member=MINE)

    def test_dalleimage_11_UNEXPECTED_BEHAVIOUR(self):
        with self.assertRaises(NoKeyError):
            dalleimage(key_constr=MINE, key_member=MINE)


if __name__ == '__main__':
    unittest.main()
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/156","1(st)","2022-12-20T17:35:12Z","Closed as not planned issue","No label","Test
 The text was updated successfully, but these errors were encountered: 
👍1
PABLOADEV reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/openai-python/issues/155","[Question] Does setting n=2 double prompt token usage?","2022-12-19T16:50:17Z","Closed issue","No label","If I use the n parameter on Completion.create, the documentation states
Note: Because this parameter generates many completions, it can quickly consume your token quota. Use carefully and ensure that you have reasonable settings for max_tokens and stop.
I recognize the model will increase the output token cost, but does it increase the input token cost as well?
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/152","Use the openai library in conjunction with the kivy library.","2023-01-01T21:11:28Z","Closed issue","No label","Hi everyone, I'm messing around with the library a bit as I try to use it in conjunction with Kivy. The project as such works when executing the venv virtualization window within Windows, but when passing the project to .APK it closes unexpectedly.
I have tried simpler projects in the conversion from .py to .apk using the Kivy library and they work perfectly. When I apply my OpenAI command lines the APK does not respond.
I have the project in this repository for possible viewing: https://github.com/FoxyCTG21/chatbotapp_public
For the conversion from .py to .apk I use the buildozer library in a Google Colab notebook: https://colab.research.google.com/github/MagnoEfren/kivy/blob/main/KivyApp_a_APK.ipynb?hl=es
I hope I can understand why I can't use the library at the end.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/151","Add types?","2023-10-18T07:18:13Z","Closed issue","fixed in v1","there is a py.typed file in the repository but it doesn't look like any of the code actually has types. Types are handy for catching bugs, but if that's not on your roadmap, maybe just remove the py.typed file?
 The text was updated successfully, but these errors were encountered: 
👍8
RevCBH, ryboe, martolini, RabeaWahab, vickyliin, stared, seabo, and krcm0209 reacted with thumbs up emoji
All reactions
👍8 reactions"
"https://github.com/openai/openai-python/issues/150","Prompt token count discrepancy","2023-12-31T00:00:12Z","Closed as not planned issue","No label","Hello,
I noticed a discrepancy in prompt (not completion) token counting. Here's a minimum working example:
import os

import openai
from transformers import GPT2TokenizerFast

os.environ[""OPENAI_API_KEY""] = ""for you""

tokenizer = GPT2TokenizerFast.from_pretrained(""gpt2"")

model_name = ""text-davinci-002""
prompt = 'Some choices are given below. It is provided in a numbered list (1 to 1),where each item in the list corresponds to a summary.\n---------------------\n(1) A serial killer is typically a person who kills three or more people, with the murders taking place over more than a month and including a significant period of time between them. The Federal Bureau of Investigation (FBI) defines serial murder as ""a series of two or more murders, committed as separate events, usually, but not always, by one offender acting alone"".   == Identified serial killers ==   == Unidentified serial killers == This is a list of unidentified serial killers who committed crimes within the United States.   == See also == List of rampage killers in the United States List of mass shootings in the United StatesInternational:  List of serial killers by country List of serial killers by number of victims   == References ==   == Bibliography ==\n\n\n---------------------\nUsing only the choices above and not prior knowledge, return the choice that is most relevant to the question: \'How many serial killers in the US are there?\'\nProvide choice in the following format: \'ANSWER: <number>\' and explain why this summary was selected in relation to the question.\n'
params = {'temperature': 0.0, 'max_tokens': 256, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'best_of': 1}

completion = openai.Completion.create(model=model_name, prompt=prompt, **params)
print(completion)

prompt_token_count = len(tokenizer(prompt)[""input_ids""])
print(f""prompt token count is {prompt_token_count}, which is 5 more tokens than the output above"")


This will print:
{
  ""choices"": [
    {
      ""finish_reason"": ""stop"",
      ""index"": 0,
      ""logprobs"": null,
      ""text"": ...
    }
  ],
  ...
  ""model"": ""text-davinci-002"",
  ""object"": ""text_completion"",
  ""usage"": {
    ""completion_tokens"": 51,
    ""prompt_tokens"": 252,              <---------------- what openai counts
    ""total_tokens"": 303
  }
}
prompt token count is 257, which is 5 more tokens than the output above

That is, openai counts 252 tokens in the prompt, but I'm counting 257. From https://beta.openai.com/tokenizer, I am tokenizing using transformers.GPT2TokenizerFast. I have also pasted prompt text above (after running it through python print()) in the url above, and I get 257 as well:

Below is my requirements.txt:
openai==0.25.0
tokenizers==0.13.2

Is there something that I am missing here? Thanks a lot!
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/149","After fine tuning gpt3 model, how to know f1 score as a evaluation?","2022-12-20T17:37:39Z","Closed as not planned issue","No label","After fine tuning gpt3 for text generation, so how I can know f1 score using openai cli ?
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/147","partially initialized module 'openai' has no attribute 'Completion'","2022-12-17T03:57:54Z","Closed as not planned issue","No label","python gpt2.py
 Traceback (most recent call last):
 File ""D:\EVE_NG\gpt2.py"", line 2, in 
 import openai
 File ""D:\EVE_NG\openai.py"", line 8, in 
 response = openai.Completion.create(
 ^^^^^^^^^^^^^^^^^
 AttributeError: partially initialized module 'openai' has no attribute 'Completion' (most likely due to a circular import)
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/145","You've made too many requests, please wait one minute and try again.","2022-12-11T17:43:42Z","Closed issue","No label","Request ID: ..
 Please contact us through our help center
 if this issue persists.
 Return to homepage
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/144","openai.Image.create_edit() requires a mask image to be passed, but the API endpoint list this parameter as optional","2022-12-05T00:02:17Z","Closed as not planned issue","No label","The Image Edit endpoint of the API can be utilized to create an entirely new, but similar image when supplied with both an image and a prompt. This feature is functionally different than the variant creation endpoint. The Edit image endpoint also has the Mask parameter, which can be used to supply an existing image mask to be edited.
The Python function for this feature is limited, in that the Mask parameter is not optional, therefore the creation of a new, non-mask-based image is not possible. Only half of the image edit functionality can be utilized.


 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/143","Encoding Error on Windows with WandB","2022-12-07T17:29:32Z","Closed issue","No label","When syncing with openai wandb sync, I get a character encoding issue on one of the run files. This happens specifically on Windows which often struggles with the default encoding on a file with open(filename).
One solution is artifact.new_file(filename, ""w"", encoding=""utf-8"") on line 279 of the wandb_logger.py which solves the problem locally for me. Alternatively, using a default of ""utf-8"" in the artifact.new_file function should work too, but might have other unintended side effects.
Here is the output:
wandb: ERROR Failed to open the provided file (UnicodeEncodeError: 'charmap' codec can't encode character '\u03bc' in position 205764: character maps to <undefined>). Please provide the proper encoding.
Traceback (most recent call last):
  File ""C:\Users\miniconda3\lib\runpy.py"", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File ""C:\Users\kaiser\miniconda3\lib\runpy.py"", line 87, in _run_code
    exec(code, run_globals)
  File ""C:\venv\Scripts\openai.exe\__main__.py"", line 7, in <module>
    sys.exit(main())
  File ""C:\venv\lib\site-packages\openai\_openai_scripts.py"", line 63, in main
    args.func(args)
  File ""C:\venv\lib\site-packages\openai\cli.py"", line 586, in sync
    resp = openai.wandb_logger.WandbLogger.sync(
  File ""C:\venv\lib\site-packages\openai\wandb_logger.py"", line 74, in sync
    fine_tune_logged = [
  File ""C:\venv\lib\site-packages\openai\wandb_logger.py"", line 75, in <listcomp>
    cls._log_fine_tune(
  File ""C:\venv\lib\site-packages\openai\wandb_logger.py"", line 172, in _log_fine_tune
    cls._log_artifacts(fine_tune, project, entity)
  File ""C:\venv\lib\site-packages\openai\wandb_logger.py"", line 236, in _log_artifacts
    cls._log_artifact_inputs(file, prefix, artifact_type, project, entity)
  File ""C:\venv\lib\site-packages\openai\wandb_logger.py"", line 280, in _log_artifact_inputs
    f.write(file_content)
  File ""C:\Users\miniconda3\lib\encodings\cp1252.py"", line 19, in encode
    return codecs.charmap_encode(input,self.errors,encoding_table)[0]
UnicodeEncodeError: 'charmap' codec can't encode character '\u03bc' in position 205764: character maps to <undefined>

 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/142","Different request methods lead to different results.","2022-12-05T02:02:59Z","Closed issue","No label","Hi,
I requested through python-client-library ""openai"" and restful request—all with default parameters.
With identical input text, it returned different results. Besides the difference in the output text, the other difference appears in ""finish_reason"". One is ""stop"", and the other is ""length"".
Could you please let me know the reason why the results were different?
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/141","Too verbose logs","2023-01-13T01:06:52Z","Closed issue","No label","Hi, is there any way to reduce these logs?
message='OpenAI API response' path=https://api.openai.com/v1/fine-tunes processing_ms=228 request_id=FOO response_code=200
INFO:openai:message='OpenAI API response' path=https://api.openai.com/v1/fine-tunes processing_ms=228 request_id=BAR response_code=200

To me, these logs should be DEBUG.
 The text was updated successfully, but these errors were encountered: 
👍1
jogardi reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/openai-python/issues/140","Connections aren't being closed","2023-06-06T00:23:48Z","Closed issue","No label","The library uses requests to fetch data from OpenAI servers, but we never call .close() on those connections which leaves dangling open file handles on the OS.
Can we clean these connections up gracefully, either after a period of time, or after making a request? There is an advantage to leaving it open as it doesn't have to do the HTTPS handshake on every request, but we should clean this up properly after a period of idleness.
This bug interacts very poorly with another bug in Docker Desktop -- moby/vpnkit#587; the openai library stops working entirely after running it in a Docker container on Mac / Windows for a few minutes. All requests to OpenAI servers time out indefinitely.
 The text was updated successfully, but these errors were encountered: 
👍4
LuladayP, zegged, domantasg, and bauefikapa reacted with thumbs up emoji
All reactions
👍4 reactions"
"https://github.com/openai/openai-python/issues/139","Compute nb of tokens for a prompt","2022-12-20T17:38:13Z","Closed as not planned issue","No label","Hi,
 Is there a way to compute the number of tokens in the prompt before sending a request (in python)?
 I'd like to avoid the ""too many tokens"" error by setting max_token to 4097 - nb_tokens(prompt)
Thanks!
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/133","Inconsistent prompt_tokens definition between text-davinci-002 and code-davinci-002","2023-12-30T23:59:45Z","Closed as not planned issue","No label","Ubuntu 18.04.6
 openai-python 0.23.1
 python 3.8.13
I'm facing unexpected results when using prompt_tokens returned from the completion API in the davinci codex model.
Consider the following function to reconstruct the prompt using the API's response:
def test_prompt_idx(prompt: str, engine=str):
    response: OpenAIObject = openai.Completion.create(
        prompt=prompt,
        stop=[""\n""],
        temperature=0.0,
        engine=engine,
        max_tokens=32,
        logprobs=5,
        echo=True,
    )
    n_prompt_tokens: int = response[""usage""][""prompt_tokens""]
    prompt_tokens = [
        {
            ""val"": response[""choices""][0][""logprobs""][""tokens""][i],
            ""options"": response[""choices""][0][""logprobs""][""top_logprobs""][i],
            ""logprob"": response[""choices""][0][""logprobs""][""token_logprobs""][i]
            if i != 0
            else 0.0,  # first token has logprob None
        }
        for i in range(n_prompt_tokens)
    ]
    reconstructed_prompt = """".join(token[""val""] for token in prompt_tokens)
    assert reconstructed_prompt == prompt
When I use text-danvinci-002, the snippet runs fine
test_prompt_idx(
        prompt=""""""import numpy as npa = np.array(object=[0,1,2])b = np.array("""""",
        engine=""text-davinci-002""
)
test_prompt_idx(
        prompt=""""""import numpy as npa = np.array(object=[0,1,2])b = np.array(object="""""",
        engine=""text-davinci-002""
)
However, when I use code-davinci-002, this snippet fails
test_prompt_idx(
        prompt=""""""import numpy as npa = np.array(object=[0,1,2])b = np.array("""""",
        engine=""code-davinci-002""
)
test_prompt_idx(
        prompt=""""""import numpy as npa = np.array(object=[0,1,2])b = np.array(object="""""",
        engine=""code-davinci-002""
)
Comparing reconstructed_prompt and prompt shows that for code-davinci-002, it seems to be that n_prompt_tokens's value is one less than expected.
 I've observed that this could lead to the codex model changing the last token of the prompt.
 For instance, if I update my test_prompt_idx() function to use n_prompt_tokens+1 instead, then the following snippet passes
test_prompt_idx(
        prompt=""""""import numpy as npa = np.array(object=[0,1,2])b = np.array("""""",
        engine=""code-davinci-002""
)
because the first completion token returned by codex (i.e.: object, with tokenid of 15252 according to the tokenizer).
 However, the following fails
test_prompt_idx(
        prompt=""""""import numpy as npa = np.array(object=[0,1,2])b = np.array(object="""""",
        engine=""code-davinci-002""
)
because the first completion token returned is =[ with token id 41888, instead of the last token of the given prompt, which is = with token id 28.
 The text was updated successfully, but these errors were encountered: 
👀1
DanielTakeshi reacted with eyes emoji
All reactions
👀1 reaction"
"https://github.com/openai/openai-python/issues/132","How are the text-search models {doc,query} different?","2022-10-30T10:15:33Z","Closed issue","No label","Seems to me, these two models should be the same?
 After all, its the same vector space.
text-search-ada-doc-001
text-search-ada-query-001

How would they be different to get a different nomenclature?
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/130","can't find finetuning example","2022-10-26T04:12:58Z","Closed issue","No label","hello
Clicking on the olympics example shows code has moved but I cannot access it either in the cookbook nor in openai-python
 This code example has moved. You can now find it in the OpenAI Cookbook at examples/fine-tuned_qa/.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/128","Inaccurate time estimation results for fine-tuning use-case","2023-12-30T23:56:50Z","Closed issue","No label","This PR introduces time estimation functionality for fine-tuning tasks. We observed in our experiments that estimated values are pretty inaccurate and have a few questions and suggestions:
Question 1: Is there any public information about where constants like 0.0515 (line 601) come from?
 My data frame which was used for fine-tuning curie model for 2 epochs contains 8236 rows. Our aim was to train an open-ended generator, that's why prompt column is completely empty. However, running memory_usage on this df gives the same values for prompt and completion columns. Please note that completion column contains pretty long text values.
If I use sys module to get the size of df on system, I get a very different result.
If I add deep=True parameter to pandas' memory_usage call, the returned value becomes very similar to sys output.
Question 2: Based on the previous trials, is there any reason why the estimator doesn't use deep=True flag to get memory consumed in the system?
Question 3: Does this estimator have any number of epochs assumption?
 Time estimator returns 1.92 hours (approximately 115 minutes) for my dataset. When I started training on the same df for 2 epochs, it took 17 minutes in total. ~9 minutes per epoch. It doesn't take this parameter into account because it's not available until the fine-tuning process call is made.
Once your model starts training, it'll approximately take 1.93 hours to train a `curie` model, and less for `ada` and `babbage`. Queue will approximately take half an hour per job ahead of you

....
[2022-10-06 15:56:26] Fine-tune enqueued. Queue number: 0
[2022-10-06 15:56:29] Fine-tune started
[2022-10-06 16:05:33] Completed epoch 1/2
[2022-10-06 16:13:42] Completed epoch 2/2

Suggestions:
 More documentation about constant values like 0.0515
 Adding deep=True flag to memory_usage call and updating constants accordingly
 Adding information about epoch count assumption to the log message, like Once your model starts training, it'll approximately take 1.93 hours to train a curie model for x epochs based on historical statistics, and less ...
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/124","plotly missing in embeddings dependencies","2022-09-28T18:22:22Z","Closed issue","bug","ModuleNotFoundError                       Traceback (most recent call last)
Cell In [34], line 1
----> 1 from openai.embeddings_utils import get_embeddings
File ~/dev/opp/chat-intents/__venv__/lib/python3.9/site-packages/openai/embeddings_utils.py:7
      5 import numpy as np
      6 import pandas as pd
----> 7 import plotly.express as px
      8 from scipy import spatial
      9 from sklearn.decomposition import PCA

ModuleNotFoundError: No module named 'plotly'

ya
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/122","Support api_key parameter in Moderation.create method","2022-09-20T15:56:07Z","Closed issue","No label","Description
create method in Moderation class initializes a new instance by calling parent constructor without any parameters. That's why it's impossible make Moderationendpoint calls without setting API key as environment variable or setting it through openai.api_key in contrast to Completion class.
How to reproduce
import openai

openai.api_key = None
moderator = openai.Moderation(api_key=api_key)

text = ""hello world""
response = moderator.create(input=text, model=""text-moderation-stable"")

> AuthenticationError: No API key provided. You can set your API key in code using 'openai.api_key = <API-KEY>',
or you can set the environment variable OPENAI_API_KEY=<API-KEY>).
If your API key is stored in a file, you can point the openai module at it with 'openai.api_key_path = <PATH>'.
You can generate API keys in the OpenAI web interface. See https://onboard.openai.com for details, or email support@openai.com if you have any questions.

Potential solution
 Accept api_key parameter in Moderation.create
 Pass api_key to super class
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/121","KeyError when removing long examples after removing duplicate rows","2022-10-16T23:01:54Z","Closed issue","bug","Error:
openai tools fine_tunes.prepare_data -f training_data_2022-09-14.jsonl
Analyzing...

- Your file contains 2446 prompt-completion pairs
Based on the analysis we will perform the following actions:
- [Recommended] Remove 1155 duplicate rows [Y/n]: y
- [Recommended] Remove 49 long examples [Y/n]: y
Traceback (most recent call last):
  File ""/Users/ser/project/project-venv/bin/openai"", line 8, in <module>
    sys.exit(main())
  File ""/Users/ser/project/project-venv/lib/python3.10/site-packages/openai/_openai_scripts.py"", line 63, in main
    args.func(args)
  File ""/Users/ser/project/project-venv/lib/python3.10/site-packages/openai/cli.py"", line 531, in prepare_data
    apply_validators(
  File ""/Users/ser/project/project-venv/lib/python3.10/site-packages/openai/validators.py"", line 851, in apply_validators
    df, optional_applied = apply_optional_remediation(
  File ""/Users/ser/project/project-venv/lib/python3.10/site-packages/openai/validators.py"", line 578, in apply_optional_remediation
    df = remediation.optional_fn(df)
  File ""/Users/ser/project/project-venv/lib/python3.10/site-packages/openai/validators.py"", line 171, in optional_fn
    return x.drop(long_indexes)
  File ""/Users/ser/project/project-venv/lib/python3.10/site-packages/pandas/util/_decorators.py"", line 311, in wrapper
    return func(*args, **kwargs)
  File ""/Users/ser/project/project-venv/lib/python3.10/site-packages/pandas/core/frame.py"", line 4957, in drop
    return super().drop(
  File ""/Users/ser/project/project-venv/lib/python3.10/site-packages/pandas/core/generic.py"", line 4267, in drop
    obj = obj._drop_axis(labels, axis, level=level, errors=errors)
  File ""/Users/ser/project/project-venv/lib/python3.10/site-packages/pandas/core/generic.py"", line 4311, in _drop_axis
    new_axis = axis.drop(labels, errors=errors)
  File ""/Users/ser/project/project-venv/lib/python3.10/site-packages/pandas/core/indexes/base.py"", line 6661, in drop
    raise KeyError(f""{list(labels[mask])} not found in axis"")
KeyError: '[330, 352, 377, 378, 422, 424, 435, 1172, 1194, 1219, 1220, 1264, 1266, 1277, 1468, 1498, 1549, 1641, 1648, 1714, 1741, 1816, 1859, 1984] not found in axis'

I believe that since the duplicate rows were removed, many of the long examples are missing, throwing this error. And thus I end up needing to apply the first recommendation and not the second one, and then use the resulting file to apply the second recommendation.
It would be great to be able to apply both changes to the same file.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/117","Confusing error message","2022-08-26T17:32:24Z","Closed issue","No label","In the function openai.embeddings_utils.get_embeddings (code), there is a check for the length of the input texts list:
assert len(list_of_text) < 2048, ""The batch size should not be larger than 2048.""
From the error message The batch size should not be larger than 2048, I expect that this function accepts a list of length 2048. However, the assertion assert len(list_of_text) < 2048 allows a maximum length of 2047. I find this quite confusing.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/113","Add a return-type flag to the CLI fine-tuning command","2022-09-18T01:09:59Z","Closed issue","enhancement","I'm using the OpenAI CLI fine-tuning command openai api fine_tunes.create within an automated data-collection + training loop. Given that this process is automated, it would be really helpful to have the ability to specify the output format from this command - specifically the ability to just return the new fine-tuned model ID rather than the whole text block (which then needs to be parsed).
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/111","Enable CI to check for formatting","2023-12-30T23:56:00Z","Closed issue","enhancement","We should enable consistent formatting and require PRs to have that formatting
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/110","Bug in the clustering jupyter example","2022-07-29T07:51:35Z","Closed issue","bug","In the following code in the clustering notebook example, at the first cell:
df['text-similarity-babbage-001'] = df.babbage_similarity.apply(eval).apply(np.array)
matrix = np.vstack(df.babbage_similarity.values)

the second line should be the following: matrix = np.vstack(df['text-similarity-babbage-001'].values)
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/108","Requests timing out","2022-07-12T02:47:39Z","Closed issue","No label","I am running a Python backend service using fastapi and a docker container and every once in a while all requests start timing out for no apparent reason. The same request can time out and suddenly work an hour later. The service is under development so my team is the only one currently using our keys and service. I have created a simple call with no fancy logic and using the ada model but this still times out (see code below). I get
""error.APIConnectionerror(""error communicating with OpenAI"")""
coming from
""requests.exceptions.ConnectionError: HTTPSConnectionPool(host='api.openai.com', port=443): Max retries exceeded with url: /v1/engines/text-ada-001/completions (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f3052475310>: Failed to establish a new connection: [Errno 110] Connection timed out'))""
Code:
def simple_call():
    openai.organization = config('ORGANIZATION')
    openai.api_key = config('API_KEY')
    print(""WARNING: performing simple call"")
    prompt = ""Tell me a fun story to cheer me up""
    completion = openai.Completion.create(**{""engine"": ""text-ada-001"", ""max_tokens"": 500, ""prompt"": prompt})
    response = {'generations': [{'prompt': prompt, 'question': ""MOCK"", 'answer': completion[""choices""][0][""text""]}]}
    return response

Any advice is appreciated
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/106","Finetuning classifier giving recommendations which are not from any of the specified categories used to train the models","2022-07-12T02:49:35Z","Closed issue","No label","Hello!
I made a finetuned classifier with 7 categories, similar to the one shown at examples/finetuning/finetuning-classification.ipynb and used the following command to create the classifier.
openai api fine_tunes.create -t ""data_prepared_train.jsonl"" -v ""data_prepared_valid.jsonl"" --compute_classification_metrics --classification_n_classes 7 -m ada
I use the following command in python when querying from the classifier
res = openai.Completion.create(model=ft_model, prompt = prompt + ""\n\n###\n\n"", max_tokens=1, temperature=0, logprobs=5)
However, when querying a completion from the classifier, the log_probs returns values which does not fall under any of the predefined categories. I have tried these with both numerical categories, and string based categories, and the same behavior is still exhibited. Was wondering if there was any way to stop these other non-predefined categories in the log_probs as they do not aid with classification in my use case. Just having the probabilities for the predefined categories would suffice. Additionally, there is also the concern that the fine-tuning model could classify the prompt into one of the undefined categories.
Here are a few examples of such occurrences:
Numerical Categories
 Predefined categories are from 0-6 inclusive
<OpenAIObject at 0x174ab8f9450> JSON: {
  ""text_offset"": [
    57
  ],
  ""token_logprobs"": [
    -0.08814435
  ],
  ""tokens"": [
    "" 1""
  ],
  ""top_logprobs"": [
    {
      "" 1"": -0.08814435,
      "" 2"": -8.10889,
      "" 5"": -7.6156697,
      "" 6"": -2.4937658,
      "" 7"": -9.455983
    }
  ]
}
Here it suggests the value "" 7"" which has never been specified in the dataset
<OpenAIObject at 0x174ab8cf360> JSON: {
  ""text_offset"": [
    28
  ],
  ""token_logprobs"": [
    -0.028152594
  ],
  ""tokens"": [
    "" 5""
  ],
  ""top_logprobs"": [
    {
      "" 0"": -8.047939,
      "" 4"": -6.4044333,
      "" 5"": -0.028152594,
      "" 6"": -3.8101103,
      "" very"": -8.138523
    }
  ]
}
Here it suggests the value "" very"" which is not an integer
<OpenAIObject at 0x174ab8cac70> JSON: {
  ""text_offset"": [
    60
  ],
  ""token_logprobs"": [
    -0.07207727
  ],
  ""tokens"": [
    "" 2""
  ],
  ""top_logprobs"": [
    {
      "" 2"": -0.07207727,
      "" 3"": -6.8346596,
      "" 4"": -6.0346746,
      "" 6"": -2.7322218,
      ""2"": -8.06412
    }
  ]
}
Here it suggests ""2"" which is different from "" 2""
String categories
 Where categories are [""background"", ""bug"", ""feature-request"" (shows up as feature), ""follow-up"", ""null"", ""painpoint"" (shows up as pain), ""usability""]
<OpenAIObject at 0x174ab91d630> JSON: {
  ""text_offset"": [
    60
  ],
  ""token_logprobs"": [
    -0.2158865
  ],
  ""tokens"": [
    "" feature""
  ],
  ""top_logprobs"": [
    {
      "" UX"": -8.75913,
      "" feature"": -0.2158865,
      "" features"": -7.479752,
      "" functionality"": -9.452176,
      "" usability"": -1.6482441
    }
  ]
}
Here it is returning other options such as "" UX"", "" features"", "" functionality"", etc over other predefined categories.
<OpenAIObject at 0x174ab8ed900> JSON: {
  ""text_offset"": [
    52
  ],
  ""token_logprobs"": [
    -0.001725924
  ],
  ""tokens"": [
    "" pain""
  ],
  ""top_logprobs"": [
    {
      "" Pain"": -6.667007,
      "" bug"": -9.389308,
      "" pain"": -0.001725924,
      "" painful"": -8.79043,
      ""pain"": -9.397688
    }
  ]
}
Here it is returning many different versions of ""pain"".
Would really appreciate any advice/support over this, as such completions which are not part of the predefined categories are not expected.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/100","Error when installing on kali-linux","2022-06-21T16:16:26Z","Closed issue","wontfix","For some reason I'm getting a build error when pip installing on Kali Linux. Correct me if I'm wrong, but I assume this is a build error. Attached is the output from my build.
txt.log
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/98","Async requests","2023-01-05T00:40:00Z","Closed issue","enhancement","Exposing async interfaces would allow using this library in a much more modern, performant, and scalable way.
Would be great if the maintainers could mention if they plan to add async methods in the future (i.e., allow for nonblocking api usage). Even specifying explicitly that this won't be added would be great, as it allows 3rd parties to release their own fork or wrapper, without the risk of being obsolete just moments later :-)
 The text was updated successfully, but these errors were encountered: 
👍45
Adam-Vandervorst, zipzopzubitybop, zalo, Rysias, Brettanda, ArtemBernatskyy, jle35, vbakhteev, MichelCarroll, lgarrison, and 35 more reacted with thumbs up emoji
All reactions
👍45 reactions"
"https://github.com/openai/openai-python/issues/96","openaipython","2022-05-23T13:36:17Z","Closed issue","No label","No description provided.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/95","HTTP Connection is not reused in stream mode","2022-05-19T21:47:00Z","Closed issue","No label","Requesting api in stream mode is not reusing HTTP Connection
Logs:
...
DEBUG:urllib3.connectionpool:Resetting dropped connection: api.openai.com
...

wireshark screenshot

minimal code to reproduce:
import openaifrom http.client import HTTPConnectionimport logging


HTTPConnection.debuglevel = 10logging.basicConfig()
logging.getLogger().setLevel(logging.DEBUG)
requests_log = logging.getLogger(""requests.packages.urllib3"")
requests_log.setLevel(logging.DEBUG)
requests_log.propagate = True

def test():
    openai_response = openai.Completion.create(
        engine=""text-davinci-002"",
        prompt=""Test prompt"",
        temperature=1.0,
        max_tokens=120,
        top_p=1.0,
        stream=True,
    )

    for resp in openai_response:
        text = resp[""choices""][0][""text""]

test()
test()
Why need to fix?
 TCP and TLS handshake can be slow and it's better to reuse connections
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/93","Check usage of a given user","2023-12-30T23:55:52Z","Closed issue","enhancement","I am testing out an application, and am using the user field to set which user has made a request. From the docs, it says that OpenAI uses this to check for abuse.
However, is there a way to get more data from the API on a per-user level? E.g. number of calls, etc.
 The text was updated successfully, but these errors were encountered: 
👍1
numpde reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/openai-python/issues/91","openai.error.APIConnectionError: Error communicating with OpenAI","2023-11-10T04:15:52Z","Closed issue","fixed in v1","Hello, we are getting this issue in our production environment, but seems to be working fine locally. Do you know what the issue might be?
Traceback (most recent call last):
  File ""/env/lib/python3.9/site-packages/openai/api_requestor.py"", line 279, in request_raw
    result = _thread_context.session.request(
  File ""/env/lib/python3.9/site-packages/requests/sessions.py"", line 529, in request
    resp = self.send(prep, **send_kwargs)
  File ""/env/lib/python3.9/site-packages/requests/sessions.py"", line 645, in send
    r = adapter.send(request, **kwargs)
  File ""/env/lib/python3.9/site-packages/requests/adapters.py"", line 519, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPSConnectionPool(host='api.openai.com', port=443): Max retries exceeded with url: /v1/engines/content-filter-alpha/completions (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f08403ee970>: Failed to establish a new connection: [Errno 110] Connection timed out'))


The above exception was the direct cause of the following exception:
Traceback (most recent call last):
  File ""/app/beni_services/usecase.py"", line 33, in execute
    response_parameters = self.do_execute()
  File ""/app/product_catalog/use_cases/get_dynamic_filters.py"", line 150, in do_execute
    product_details_using_gpt3 = self._get_product_details_using_gpt3(product_details=product_details)
  File ""/app/product_catalog/use_cases/get_dynamic_filters.py"", line 276, in _get_product_details_using_gpt3
    if self._safe_to_use_openai(openai_prompt):
  File ""/app/product_catalog/use_cases/get_dynamic_filters.py"", line 302, in _safe_to_use_openai
    response = openai.Completion.create(
  File ""/env/lib/python3.9/site-packages/openai/api_resources/completion.py"", line 31, in create
    return super().create(*args, **kwargs)
  File ""/env/lib/python3.9/site-packages/openai/api_resources/abstract/engine_api_resource.py"", line 90, in create
    response, _, api_key = requestor.request(
  File ""/env/lib/python3.9/site-packages/openai/api_requestor.py"", line 100, in request
    result = self.request_raw(
  File ""/env/lib/python3.9/site-packages/openai/api_requestor.py"", line 289, in request_raw
    raise error.APIConnectionError(""Error communicating with OpenAI"") from e
openai.error.APIConnectionError: Error communicating with OpenAI

Thank you!
 The text was updated successfully, but these errors were encountered: 
👍1
nbro10 reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/openai-python/issues/90","Completion is created only for the last prompt in the list of prompts","2022-07-28T15:35:18Z","Closed issue","bug","Hello!
Here is a sample code where the model is expected to return 4 choices for 2 prompts (2 for each prompt), according to the documentation - https://beta.openai.com/docs/api-reference/completions/create#completions/create-prompt:
import pandas as pdimport openai

openai.api_key = ""YOUR_API_KEY""

prompts = [
    ""example 1"",
    ""example 2"",
]
generated = openai.Completion.create(model='davinci',
                                     prompt=prompts,
                                     max_tokens=10,
                                     best_of=4,
                                     n=2,
                                     echo=True,
                                     )
print(generated)
But get only 2 choices for last prompt (pay attention to indices, they do not start from 0):
<OpenAIObject text_completion id=cmpl-4zR7IQzUzg8ZQQAluCUkLfkJo3DmO at 0x1885f37c630> JSON: {
  ""choices"": [
    {
      ""finish_reason"": ""length"",
      ""index"": 4,
      ""logprobs"": null,
      ""text"": ""example 2.10.1\"" , \""subtitle\""""
    },
    {
      ""finish_reason"": ""length"",
      ""index"": 5,
      ""logprobs"": null,
      ""text"": ""example 2). 5 Add meat, cover pot with tight-""
    }
  ],
  ""created"": 1650547592,
  ""id"": ""cmpl-4zR7IQzUzg8ZQQAluCUkLfkJo3DmO"",
  ""model"": ""davinci"",
  ""object"": ""text_completion""
}

Similar code, but using JS lib works as expected.
Versions:
openai==0.18.1
python==3.9.7

Does anyone have any idea what went wrong? I apologize if the error is banal, but I spent quite a lot of time looking for it and failed. Thank you!
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/89","Calculating tokens prior to prompt","2022-04-23T04:24:26Z","Closed issue","No label","hey all,
I'm often making few-shot classification models using completions and I need to be able to check how many tokens exactly are inside my prompt prior to sending it off. Right now I do something like:
# sudo code
def send_request(prompt)
    try:
        ret = make_request(prompt)
    except:
        prompt = reduce_tokens(prompt)
        ret = send_request(prompt)
    return ret

but I'd rather just have the right number of tokens calculated in my preprocessing stage. Is there a way to use OpenAI's tokenizing tool to calculate this ahead of time?
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/86","Continue fine tuning a model e.g. text-davinci-002","2022-07-29T17:49:33Z","Closed issue","enhancement","I am only able to define the base model for fine-tuning as ada, babbage, curie or davinci. I would like to be able to (1) fine tune on the official fine tuned models, e.g. text-davinci-002 to allow me to compare my fine tuning to the default latest and greatest. And of lesser importance: (2) continue a fine-tuning on my own models.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/85","Storing metadata for fine-tuned models","2023-03-07T03:17:24Z","Closed issue","enhancement,wontfix","I am fine tuning various models which have different styles of prompts. In inference I need to style the prompts adequately for each model. It would be great if it was possible to save a metadata or config json when creating the model, and retrieve it with the model. Otherwise I would need to either: (1) manage my own external database which is cumbersome when using google colab (2) hack the model suffix to encode my config params (let's not do that), or (3) reverse engineer my training jsonl to deduce the model's specific prompt styling, which is difficult. Any other options?
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/84","[FEATURE REQUEST] Add stratification on train/validation split with fine_tune.prepare_data","2023-12-30T23:54:59Z","Closed issue","enhancement","Hello,
 first of all many thanks for this great library ! 🙏
When preparing data for multiclass classification for fine-tuning and accepting the split into train and validation data, I end up with a different number of classes in both datasets with respect to those I specified.
 Error message:
[2022-03-25 10:12:57] Fine-tune failed. Errors:
The number of classes in file-LSGG6mb4lhNMqyAxN6dA63sc does not match the number of classes specified in the hyperparameters.
The number of classes in file-tRE2P9nw9pq2NtM4qpKgceI2 does not match the number of classes specified in the hyperparameters.

It seems to me a problem related to stratification while splitting. Do you think it'd be possible to include this option in the future ? I know it's not an easy task and when you have not many examples you have to manually play with test_size until you get the same number of classes in the splits but it could be automated by progressively increase the test_size until train_dataset.nunique() == test_dataset.nunique()
 The text was updated successfully, but these errors were encountered: 
👍3
Taytay, kierenj, and aperepelkin reacted with thumbs up emoji
All reactions
👍3 reactions"
"https://github.com/openai/openai-python/issues/78","AttributeError: module 'openai' has no attribute 'Embedding'","2022-03-11T07:51:43Z","Closed issue","No label","I've installed openai via:
pip install openai
pip install --upgrade openai
and yet when I try to run an example like:
import openai
print(openai.Embedding)

I get the error
Traceback (most recent call last):
  File ""/Users/travisbarton/.conda/envs/work3.6/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 3343, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-15-c5f5985add60>"", line 1, in <module>
    openai.Embedding
AttributeError: module 'openai' has no attribute 'Embedding'

the version I have is 0.8.0
this happens for full examples, but this is just the most simple case.
 any idea why?
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/73","Unable to use custom certs with OpenAI client","2022-02-22T18:35:32Z","Closed issue","No label","I'm unable to get custom cert bundle path (client side) working with OpenAI. For background, I'm on a work laptop which is behind a VPN, so we use a custom SSL cert (.pem file) for any outbound requests. The Python requests library seems to honor this with the use of the REQUESTS_CA_BUNDLE env variable, but was unable to get it working with the OpenAI client.
Here's what I tried so far:
import os

import openai

print(os.environ['REQUESTS_CA_BUNDLE'])  # confirm that it prints out correct path to .pem file

openai.verify_ssl_certs = False # disable verify but this does nothing, only logs a warning
openai.api_key = 'my-api-key'

file = '/path/to/my/file'
with open(file, 'rb') as in_file:
    openai.File.create(
        file=in_file, purpose='search')

This results in the below error being printed to console:
/Users/rnag/path/to/pem/file
/Users/rnag/.pyenv/versions/3.10.0/lib/python3.10/site-packages/openai/api_requestor.py:47: UserWarning: verify_ssl_certs is ignored; openai always verifies.
  warnings.warn(""verify_ssl_certs is ignored; openai always verifies."")
Traceback (most recent call last):
  File ""/Users/rnag/.pyenv/versions/3.10.0/lib/python3.10/site-packages/urllib3/connectionpool.py"", line 699, in urlopen
    httplib_response = self._make_request(
  File ""/Users/rnag/.pyenv/versions/3.10.0/lib/python3.10/site-packages/urllib3/connectionpool.py"", line 382, in _make_request
    self._validate_conn(conn)
  File ""/Users/rnag/.pyenv/versions/3.10.0/lib/python3.10/site-packages/urllib3/connectionpool.py"", line 1010, in _validate_conn
    conn.connect()
  File ""/Users/rnag/.pyenv/versions/3.10.0/lib/python3.10/site-packages/urllib3/connection.py"", line 416, in connect
    self.sock = ssl_wrap_socket(
  File ""/Users/rnag/.pyenv/versions/3.10.0/lib/python3.10/site-packages/urllib3/util/ssl_.py"", line 449, in ssl_wrap_socket
    ssl_sock = _ssl_wrap_socket_impl(
  File ""/Users/rnag/.pyenv/versions/3.10.0/lib/python3.10/site-packages/urllib3/util/ssl_.py"", line 493, in _ssl_wrap_socket_impl
    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)
  File ""/Users/rnag/.pyenv/versions/3.10.0/lib/python3.10/ssl.py"", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File ""/Users/rnag/.pyenv/versions/3.10.0/lib/python3.10/ssl.py"", line 1070, in _create
    self.do_handshake()
  File ""/Users/rnag/.pyenv/versions/3.10.0/lib/python3.10/ssl.py"", line 1341, in do_handshake
    self._sslobj.do_handshake()
ssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/Users/rnag/.pyenv/versions/3.10.0/lib/python3.10/site-packages/requests/adapters.py"", line 439, in send
    resp = conn.urlopen(
  File ""/Users/rnag/.pyenv/versions/3.10.0/lib/python3.10/site-packages/urllib3/connectionpool.py"", line 783, in urlopen
    return self.urlopen(
  File ""/Users/rnag/.pyenv/versions/3.10.0/lib/python3.10/site-packages/urllib3/connectionpool.py"", line 783, in urlopen
    return self.urlopen(
  File ""/Users/rnag/.pyenv/versions/3.10.0/lib/python3.10/site-packages/urllib3/connectionpool.py"", line 755, in urlopen
    retries = retries.increment(
  File ""/Users/rnag/.pyenv/versions/3.10.0/lib/python3.10/site-packages/urllib3/util/retry.py"", line 574, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='api.openai.com', port=443): Max retries exceeded with url: /v1/files (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)')))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/Users/rnag/.pyenv/versions/3.10.0/lib/python3.10/site-packages/openai/api_requestor.py"", line 255, in request_raw
    result = _thread_context.session.request(
  File ""/Users/rnag/.pyenv/versions/3.10.0/lib/python3.10/site-packages/requests/sessions.py"", line 542, in request
    resp = self.send(prep, **send_kwargs)
  File ""/Users/rnag/.pyenv/versions/3.10.0/lib/python3.10/site-packages/requests/sessions.py"", line 655, in send
    r = adapter.send(request, **kwargs)
  File ""/Users/rnag/.pyenv/versions/3.10.0/lib/python3.10/site-packages/requests/adapters.py"", line 514, in send
    raise SSLError(e, request=request)
requests.exceptions.SSLError: HTTPSConnectionPool(host='api.openai.com', port=443): Max retries exceeded with url: /v1/files (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)')))

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""/path/to/py/file"", line 11, in <module>
    openai.File.create(
  File ""/Users/rnag/.pyenv/versions/3.10.0/lib/python3.10/site-packages/openai/api_resources/file.py"", line 43, in create
    response, _, api_key = requestor.request(""post"", url, files=files)
  File ""/Users/rnag/.pyenv/versions/3.10.0/lib/python3.10/site-packages/openai/api_requestor.py"", line 98, in request
    result = self.request_raw(
  File ""/Users/rnag/.pyenv/versions/3.10.0/lib/python3.10/site-packages/openai/api_requestor.py"", line 266, in request_raw
    raise error.APIConnectionError(""Error communicating with OpenAI"") from e
openai.error.APIConnectionError: Error communicating with OpenAI

My workaround
Its not ideal but current workaround I'm using is to manually update the api_requestor.py script to pass in verify=False to request method:
timeout=TIMEOUT_SECS,
verify=False,

 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/70","openai: error: invalid choice: 'tools' (choose from 'api')","2022-01-31T04:59:41Z","Closed issue","No label","$ openai tools fine_tunes.prepare_data -f ./threat_impact_prediction/data.jsonl 
usage: openai [-h] [-v] [-b API_BASE] [-k API_KEY] [-o ORGANIZATION] {api} ...
openai: error: invalid choice: 'tools' (choose from 'api')
Anaconda Python==3.6.9
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/69","OpenAI fine-tune shows ""That model doesn't exist""","2022-01-29T20:47:46Z","Closed issue","No label","I have already trained a fine-tuned model. I am able to retrieve the details of the model using Open AI API. However, when I am using it in a Completion.create() it gives ""This model doesn't exist"".
Any reason as to why this happened ?
I have already been charged for the fine-tune training process. It also shows up on my account.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/65","Can't send files - missing purpose","2022-01-19T01:54:20Z","Closed issue","No label","Hi I am trying to send files for finetuning from my api service (not command line)
 I tried:
return json.loads(requests.post( 'https://api.openai.com/v1/files', headers={'Authorization': 'Bearer '+ os.getenv(""OPENAI_API_KEY"")}, json={""purpose"":""fine-tune""}, files={'file': ""\n"".join(data)} ).text)
 and
return json.loads(requests.post( 'https://api.openai.com/v1/files', headers={'Authorization': 'Bearer '+ os.getenv(""OPENAI_API_KEY"")}, params={""purpose"":""fine-tune""}, files={'file': ""\n"".join(data)} ).text)
but I got ""'purpose' is a required property"" error
and when I try:
return json.loads(requests.post( 'https://api.openai.com/v1/files', headers={'Authorization': 'Bearer '+ os.getenv(""OPENAI_API_KEY"")}, files={""purpose"":""fine-tune"", 'file': ""\n"".join(data)} ).text)
I got ""'' is not one of ['fine-tune', 'answers', 'search', 'classifications'] - 'purpose'"" error
What am I missing?
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/61","configure retry behavior","2021-12-23T04:49:01Z","Closed issue","No label","Is there a way to configure this SDK to retry failed API calls?
My experiments are breaking half way through because of occasional errors such as
openai.error.APIError: The server had an error while processing your request. Sorry about that! You can retry your request, or contact support@openai.com if the error persists. (Please include the request ID [...] in your email.)
These are costing money and preventing experiments from running to completion, so I'd love to find a robust way to retry failed requests 2-3 times. Thank you for your input.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/59","Engine Not found Error, when trying to get embeddings","2021-12-23T04:43:53Z","Closed issue","No label","I am trying to get embeddings using this example here -> Get_embeddings
 getting error at below line,
df['babbage_similarity'] = df.combined.apply(lambda x: get_embedding(x, engine='babbage-similarity'))

InvalidRequestError: Engine not found

The above exception was the direct cause of the following exception:

RetryError                                Traceback (most recent call last)
<ipython-input-14-7b798ddc3611> in <module>
      2 
      3 # This will take just under 10 minutes
----> 4 df['babbage_similarity'] = df.combined.apply(lambda x: get_embedding(x, engine='babbage-similarity'))

 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/56","APIConnectionError: Error communicating with OpenAI","2021-12-21T01:35:04Z","Closed issue","No label","Running into certificate errors when trying to ping the API. Happens both in virtualenv and regular Python
Traceback (most recent call last):
  File ""/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/openai/api_requestor.py"", line 255, in request_raw
    result = _thread_context.session.request(
  File ""/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/requests/sessions.py"", line 542, in request
    resp = self.send(prep, **send_kwargs)
  File ""/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/requests/sessions.py"", line 655, in send
    r = adapter.send(request, **kwargs)
  File ""/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/requests/adapters.py"", line 514, in send
    raise SSLError(e, request=request)
requests.exceptions.SSLError: HTTPSConnectionPool(host='api.openai.com', port=443): Max retries exceeded with url: /v1/engines (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self signed certificate in certificate chain (_ssl.c:1131)')))

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""search.py"", line 21, in <module>
    main()
  File ""search.py"", line 9, in main
    engines = openai.Engine.list()
  File ""/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/openai/api_resources/abstract/listable_api_resource.py"", line 27, in list
    response, _, api_key = requestor.request(
  File ""/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/openai/api_requestor.py"", line 98, in request
    result = self.request_raw(
  File ""/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/openai/api_requestor.py"", line 265, in request_raw
    raise error.APIConnectionError(""Error communicating with OpenAI"") from e
openai.error.APIConnectionError: Error communicating with OpenAI

 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/53","Embedding api not working","2021-12-23T04:50:34Z","Closed issue","No label","https://beta.openai.com/docs/guides/embeddings/what-are-embeddings documented the newly available embedding api, but the examples are not working:
response = openai.Engine(id=""babbage-similarity"").embeddings(
    input=""Sample document text goes here""
)
embeddings = response['data'][0]['embedding']

This is the error message:

Traceback (most recent call last):
  File ""/home/whao/.virtualenv/openai/lib/python3.6/site-packages/openai/openai_object.py"", line 96, in __getattr__
    return self[k]
  File ""/home/whao/.virtualenv/openai/lib/python3.6/site-packages/openai/openai_object.py"", line 136, in __getitem__
    raise err
  File ""/home/whao/.virtualenv/openai/lib/python3.6/site-packages/openai/openai_object.py"", line 124, in __getitem__
    return super(OpenAIObject, self).__getitem__(k)
KeyError: 'embeddings'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/whao/.virtualenv/openai/lib/python3.6/site-packages/openai/openai_object.py"", line 98, in __getattr__
    raise AttributeError(*err.args)
AttributeError: embeddings


 The text was updated successfully, but these errors were encountered: 
👍1
Vagif12 reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/openai-python/issues/48","Use entry_points console_scripts","2022-01-25T20:56:55Z","Closed issue","No label","Hi,
I was wondering if it might make sense to switch from scripts to entry_points console_scripts in setup.py. This is the more modern and preferred approach and recommended by PyPA nowadays.
Example:
entry_points={
    'console_scripts': [
        'sample=sample:main',
    ],
},

Guide: https://packaging.python.org/guides/distributing-packages-using-setuptools/#scripts
Ref:
https://github.com/openai/openai-python/blob/main/bin/openai
openai-python/setup.py
 Line 25 in 62f8d40
	scripts=[""bin/openai""], 
 The text was updated successfully, but these errors were encountered: 
❤️1
DutytoDevelop reacted with heart emoji
All reactions
❤️1 reaction"
"https://github.com/openai/openai-python/issues/46","How to work with maximum context length is 2049 tokens?","2022-01-04T01:09:45Z","Closed issue","No label","I'd like to send the OpenAI's API, the text from various PDF's. Specifically, the Summarize for a 2nd grader or the TL;DR summarization API's.
I can extract the text from PDF's using PyMuPDF and prepare the OpenAI prompt.
Question: How best to prepare the prompt when the token count is longer than the allowed 2049?
Do I just truncate the text?
Or is there a way to sample the text to ""compress"" it to lose key points?
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/43","[FEATURE REQUEST] Bad words list","2023-12-30T23:52:58Z","Closed as not planned issue","No label","I don't have access to the GPT-3 API yet (A guy can dream, eh?), but I have been reading through the docs and it seems like the completion module would be perfect for my use case except for the exclusion of a ""bad words list"" feature.
This feature would not allow certain words to be generated in the completion output. I am aware of the logit_bias argument, but this only stops individual tokens from being generated.
 My idea would take an arbitrary string (Or list of token IDs) as input, and then not allow the completion of this string given the words before it.
I have successfully asked for this feature from the Huggingface .generate API many moons ago. Please see my feature request for a fuller run-down of how it could be implemented (link: huggingface/transformers#3061).
It would be a useful feature for customers because it could give peace of mind that the models that they are serving are not going to output any unsavoury language. I can see that an alternative to this feature would just be to train the model not to output generally bad language (E.g. overly aggressive or xenophobic language) through thoughtful use of training data, but since everyone's definition of bad language is different, it would be nice to customise the model accordingly.
Thanks!
 The text was updated successfully, but these errors were encountered: 
👍2
badranX and py660 reacted with thumbs up emoji
All reactions
👍2 reactions"
"https://github.com/openai/openai-python/issues/40","Python 3.10 not support???","2021-12-23T04:53:09Z","Closed issue","No label","When I try to use python 3.10 to run the openai library, it returns
PS C:\Users\USER\Desktop\Gpt-3> pip install openai
 Collecting openai
 Using cached openai-0.10.5.tar.gz (157 kB)
 Collecting requests>=2.20
 Using cached requests-2.26.0-py2.py3-none-any.whl (62 kB)
 Collecting tqdm
 Using cached tqdm-4.62.3-py2.py3-none-any.whl (76 kB)
 Collecting pandas>=1.2.3
 Using cached pandas-1.3.3.tar.gz (4.7 MB)
 Installing build dependencies ... error
 ERROR: Command errored out with exit status 1:
 command: 'C:\Users\USER\Desktop\Gpt-3\venv\Scripts\python.exe' 'C:\Users\USER\AppData\Local\Temp\pip-standalone-pip-ip8y9guu_env_pip_.zip\pip' install --ignore-i
 nstalled --no-user --prefix 'C:\Users\USER\AppData\Local\Temp\pip-build-env-djvlomh4\overlay' --no-warn-script-location --no-binary :none: --only-binary :none: -i http
 s://pypi.org/simple -- 'setuptools>=51.0.0' wheel 'Cython>=0.29.21,<3' 'numpy==1.17.3; python_version=='""'""'3.7'""'""' and (platform_machine!='""'""'arm64'""'""' or platform
 _system!='""'""'Darwin'""'""') and platform_machine!='""'""'aarch64'""'""'' 'numpy==1.18.3; python_version=='""'""'3.8'""'""' and (platform_machine!='""'""'arm64'""'""' or platform_sy
 stem!='""'""'Darwin'""'""') and platform_machine!='""'""'aarch64'""'""'' 'numpy==1.19.3; python_version>='""'""'3.9'""'""' and (platform_machine!='""'""'arm64'""'""' or platform_syste
 m!='""'""'Darwin'""'""') and platform_machine!='""'""'aarch64'""'""'' 'numpy==1.19.2; python_version=='""'""'3.7'""'""' and platform_machine=='""'""'aarch64'""'""'' 'numpy==1.19.2; py
 thon_version=='""'""'3.8'""'""' and platform_machine=='""'""'aarch64'""'""'' 'numpy>=1.20.0; python_version=='""'""'3.8'""'""' and platform_machine=='""'""'arm64'""'""' and platform_s
 ystem=='""'""'Darwin'""'""'' 'numpy>=1.20.0; python_version=='""'""'3.9'""'""' and platform_machine=='""'""'arm64'""'""' and platform_system=='""'""'Darwin'""'""''
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/38","Can only get a limited number of logprobs from completions?","2021-12-23T04:56:46Z","Closed issue","No label","Hi,
I am specifying that I want the logprobs of the top 100 tokens but the API seems to give me at most 5:
(Pdb) response = openai.Completion.create(engine='ada', prompt='test', max_tokens=1, logprobs=100)
(Pdb) response
<OpenAIObject text_completion id=cmpl-3qtEQ4P8iNmTHqV8Eh4Wq59vzqnbm at 0x7fcf271ebb30> JSON: {
  ""choices"": [
    {
      ""finish_reason"": ""length"",
      ""index"": 0,
      ""logprobs"": {
        ""text_offset"": [
          4
        ],
        ""token_logprobs"": [
          -10.029023
        ],
        ""tokens"": [
          "" goal""
        ],
        ""top_logprobs"": [
          {
            "","": -3.721809,
            ""-"": -2.6293674,
            ""."": -2.6775126,
            ""/"": -2.527664,
            ""_"": -2.10408
          }
        ]
      },
      ""text"": "" goal""
    }
  ],
  ""created"": 1633734378,
  ""id"": ""cmpl-3qtEQ4P8iNmTHqV8Eh4Wq59vzqnbm"",
  ""model"": ""ada:2020-05-03"",
  ""object"": ""text_completion""
}

Is there any way I can get around this?
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/31","Example of using fine-tuned model in fine-tuning docs doesn't work with current release.","2021-08-31T08:04:54Z","Closed issue","No label","Currently using openai-0.10.2 from pypi and gettting
InvalidRequestError: Must provide an 'engine' parameter to create a <class 'openai.api_resources.completion.Completion'>
even though docs say explicitly not to provide engine.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/30","Now requires Python 3.7?","2021-08-06T08:44:41Z","Closed issue","No label","I am getting the following error pushing an app to Heroku with openai-python:
remote:        ERROR: Could not find a version that satisfies the requirement pandas>=1.2.3 (from openai->-r /tmp/build_0e7253a3/requirements.txt (line 21)) (from versions: 0.1, 0.2b0, 0.2b1, 0.2, 0.3.0b0, 0.3.0b2, 0.3.0, 0.4.0, 0.4.1, 0.4.2, 0.4.3, 0.5.0, 0.6.0, 0.6.1, 0.7.0, 0.7.1, 0.7.2, 0.7.3, 0.8.0rc1, 0.8.0, 0.8.1, 0.9.0, 0.9.1, 0.10.0, 0.10.1, 0.11.0, 0.12.0, 0.13.0, 0.13.1, 0.14.0, 0.14.1, 0.15.0, 0.15.1, 0.15.2, 0.16.0, 0.16.1, 0.16.2, 0.17.0, 0.17.1, 0.18.0, 0.18.1, 0.19.0, 0.19.1, 0.19.2, 0.20.0, 0.20.1, 0.20.2, 0.20.3, 0.21.0, 0.21.1, 0.22.0, 0.23.0, 0.23.1, 0.23.2, 0.23.3, 0.23.4, 0.24.0, 0.24.1, 0.24.2, 0.25.0, 0.25.1, 0.25.2, 0.25.3, 1.0.0, 1.0.1, 1.0.2, 1.0.3, 1.0.4, 1.0.5, 1.1.0, 1.1.1, 1.1.2, 1.1.3, 1.1.4, 1.1.5) 

remote:        ERROR: No matching distribution found for pandas>=1.2.3 (from openai->-r /tmp/build_0e7253a3/requirements.txt (line 21)) 

This is on py 3.6.10. This stopped working without any changes to the requirements.txt for the app.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/17","Building openai into an exe with pyInstaller causes a certificate error (can't find file)","2022-01-27T05:28:09Z","Closed issue","No label","Error Ranking: Unexpected error communicating with OpenAI. It looks like there's
probably a configuration issue locally.  If this problem persists, let
us know at support@openai.com.

(Network error: A OSError was raised with error message Could not find a suitable TLS CA certificate bundle, invalid path: [MASKED]\AppData\Local\Temp\_MEI122122\openai\data/ca-certificates.crt)

Is there a reason @gdb added this file in the first place to the repo? There's not commit history, so it's not clear on the context.
The file is added in to setup.py here 
openai-python/setup.py
 Line 24 in 5f8c4a8
	package_data={""openai"": [""data/ca-certificates.crt""]}, 
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/11","File api ignores api_base","2021-12-23T04:50:02Z","Closed issue","No label","cc @hallacy

openai-python/openai/api_resources/file.py
 Line 26 in cf9c04a
	api_base=openai.file_api_baseoropenai.api_base, 
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/10","Ability to check the amount of tokens remaining using the API?","2022-01-24T21:54:01Z","Closed issue","No label","I couldn't find any reference to this in the docs and the slack chatroom is quite dead, so I'm opening an issue here to suggest it if it's not implemented yet.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/6","Api key","2021-04-01T16:54:54Z","Closed issue","No label","I'm very excited with the idea of using this technology.
I've applied and fulfilled the form to get the API key.
Just need to know how long it will take to actually get one.
Thanks
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/4","Added to conda","2021-02-15T20:59:36Z","Closed issue","No label","Hi, this is more an FYI than an issue but I just wanted to let you guys know that I added this to conda-forge (https://github.com/conda-forge/openai-feedstock) so that it can be downloaded using conda.
 Let me know if someone from here wants to be added as a maintainer there, otherwise, feel free to just close this issue.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/3","AttributeError: partially initialized module 'openai' has no attribute 'Completion'","2021-03-19T00:03:04Z","Closed issue","No label","When importing openai and running the following code:
import openaiopenai.api_key = ""api key omitted 😉 ""openai.Completion.create(
  engine=""davinci"",
  prompt=""Once upon a time"",
  max_tokens=5
)
I get the the following error message:
Traceback (most recent call last):
  File ""c:\Users\Oleg\Documents\Programming\OpenAI\random.py"", line 1, in <module>
    import openai
  File ""C:\Users\Oleg\Documents\Programming\OpenAI\venv\lib\site-packages\openai\__init__.py"", line 30, in <module>
    from openai.api_resources import *  # noqa
  File ""C:\Users\Oleg\Documents\Programming\OpenAI\venv\lib\site-packages\openai\api_resources\__init__.py"", line 1, in <module>
    from openai.api_resources.branch import Branch
  File ""C:\Users\Oleg\Documents\Programming\OpenAI\venv\lib\site-packages\openai\api_resources\branch.py"", line 2, in <module>
    from openai.api_resources.abstract.engine_api_resource import EngineAPIResource
  File ""C:\Users\Oleg\Documents\Programming\OpenAI\venv\lib\site-packages\openai\api_resources\abstract\__init__.py"", line 5, in <module>
    from openai.api_resources.abstract.api_resource import APIResource
  File ""C:\Users\Oleg\Documents\Programming\OpenAI\venv\lib\site-packages\openai\api_resources\abstract\api_resource.py"", line 3, in <module>
    from openai import api_requestor, error, six, util
  File ""C:\Users\Oleg\Documents\Programming\OpenAI\venv\lib\site-packages\openai\api_requestor.py"", line 15, in <module>
    from openai import error, http_client, version, util, six
  File ""C:\Users\Oleg\Documents\Programming\OpenAI\venv\lib\site-packages\openai\http_client.py"", line 8, in <module>
    import random
  File ""c:\Users\Oleg\Documents\Programming\OpenAI\random.py"", line 13, in <module>
    response = openai.Completion.create(
AttributeError: partially initialized module 'openai' has no attribute 'Completion' (most likely due to a circular import)

I've tried re-installing it via pip install --upgrade openai but I still get the same error, however VSCode seems to know that Completion is a class within the openai module.
 The text was updated successfully, but these errors were encountered: 
👍7
Yharooer, AHaryanto, emma-ido, Amm1rr, yzhan289, GeorgeC6, and Pankajyadavup53 reacted with thumbs up emoji
All reactions
👍7 reactions"
"https://github.com/openai/openai-python/issues/2","All Playground Config Options are not available via API","2022-01-04T01:10:29Z","Closed issue","No label","There are a few config options that are available in the Playground, but not via the API. For example: Frequency Penalty, Presence Penalty, Best Of, etc. Is there a way to use these in the library now or will these eventually be added?
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-python/issues/1","Commit history was lost","2020-10-31T20:38:36Z","Closed issue","No label","Why was this repo's history reset with a new ""Initial commit"" 3 days ago? There's nothing documenting why this was done.
 The text was updated successfully, but these errors were encountered: 
All reactions"
