"https://github.com/openai/evals/issues/1564","AttributeError: module 'openai' has no attribute 'error'","2024-10-17T17:07:39Z","Open issue","bug","Describe the bug
pip --version
 pip 24.2 from /Users/apple/Documents/github_repos/test1/venv/lib/python3.13/site-packages/pip (python 3.13)
I am getting this error:
(venv) apple@apples-MacBook-Pro test1 % oaieval --help
Traceback (most recent call last):
  File ""/Users/apple/Documents/github_repos/test1/venv/bin/oaieval"", line 5, in <module>
    from evals.cli.oaieval import main
  File ""/Users/apple/Documents/github_repos/test1/venv/lib/python3.13/site-packages/evals/__init__.py"", line 2, in <module>
    from .completion_fns.openai import (
    ...<3 lines>...
    )
  File ""/Users/apple/Documents/github_repos/test1/venv/lib/python3.13/site-packages/evals/completion_fns/openai.py"", line 13, in <module>
    from evals.utils.api_utils import (
    ...<2 lines>...
    )
  File ""/Users/apple/Documents/github_repos/test1/venv/lib/python3.13/site-packages/evals/utils/api_utils.py"", line 12, in <module>
    openai.error.ServiceUnavailableError,
    ^^^^^^^^^^^^
AttributeError: module 'openai' has no attribute 'error'
To Reproduce
mkdir test1; cd test1
python -m venv venv
pip install evals
oaieval --help
Code snippets
No response
OS
sequioa 15.0.1 (24A348)
Python version
Python 3.13.0
Library version
latest
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/1563","Unable to install via pip install evals","2024-10-15T21:24:06Z","Open issue","bug","Describe the bug
Failing to install: pip install evals (see logs in the attached gist file).
As we can see in the end of below log, we see some error. Please help me debug this, I'm new to python and tried to find the answer but couldn't figure it out after waiting to install this for like 1 hours twice. The error is like pip._vendor.resolvelib.resolvers.ResolutionTooDeep: 200000.
Tools Version:
python --version: Python 3.9.6
pip --version: pip 24.2 from /Users/apple/Library/Python/3.9/lib/python/site-packages/pip (python 3.9)
Please help. Thanks in advance.
(see the cli output here - https://gist.github.com/sahilrajput03/e86baa88d35e5f5c0f63946671edbb77
To Reproduce
pip install evals
Code snippets
No response
OS
15.0.1 (24A348)
Python version
Python 3.9.6
Library version
latest
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/1562","Is Evals repo being replaced by the Evaluations feature in the Playground?","2024-10-01T19:58:28Z","Open issue","No label","Describe the feature or improvement you're requesting
We noticed that there is a new feature in the Playground called ""Evaluations,"" which seems to accept a similar file format to the Evals repo—though not exactly the same. We're wondering if this new feature is intended to supersede the Evals repo. If that's the case, will there be support for importing from the existing Evals structure to ease the transition?
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/1559","Text2code2video eval","2024-09-22T10:55:30Z","Open issue","No label","Describe the feature or improvement you're requesting
I found that also that last released models (o1 family) fail in text to visual coding tasks.
 I am guessing if a text2shader dataset could be a valid approach to improve on this domain.
 It is basically like text2image or text2video but more in the flavour text2code2video so that we have a intermediate programmable artifact.
 I don't know if shadertoy or any other resource could help to build this eval.
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/1556","o1 release breaks token usage stats","2024-09-13T10:08:17Z","Open issue","bug","Describe the bug
Traceback (most recent call last):
  File ""/home/luca/.cache/pypoetry/virtualenvs/<NAME>-p2Ot1sDh-py3.12/bin/oaieval"", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File ""/home/luca/.cache/pypoetry/virtualenvs/<NAME>-p2Ot1sDh-py3.12/lib/python3.12/site-packages/evals/cli/oaieval.py"", line 304, in main
    run(args)
  File ""/home/luca/.cache/pypoetry/virtualenvs/<NAME>-p2Ot1sDh-py3.12/lib/python3.12/site-packages/evals/cli/oaieval.py"", line 227, in run
    add_token_usage_to_result(result, recorder)
  File ""/home/luca/.cache/pypoetry/virtualenvs/<NAME>-p2Ot1sDh-py3.12/lib/python3.12/site-packages/evals/cli/oaieval.py"", line 279, in add_token_usage_to_result
    key: sum(u[key] if u[key] is not None else 0 for u in usage_events)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: unsupported operand type(s) for +: 'int' and 'dict'

To Reproduce
Run any eval
Code snippets
No response
OS
macOS
Python version
v3.12
Library version
3.0.1.post1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/1543","Information exposure alert through an exception","2024-08-08T07:04:25Z","Open issue","bug","Describe the bug
Stack trace information flows to this location and may be exposed to an external user.
Affected line - 
evals/evals/elsuite/multistep_web_tasks/docker/flask-playwright/app.py
 Line 191 in 234bcde
	jsonify({""status"": ""error"", ""message"": f""error executing command {command}: {e}""}), 
evals/evals/elsuite/multistep_web_tasks/docker/flask-playwright/app.py
 Lines 164 to 170 in 234bcde
	response=jsonify( 
	 { 
	""status"": ""success"", 
	""message"": f""could not return results of executed commands {request.json['commands']}"", 
	""content"": str(e), 
	""url"": page.url, 
	 } 
evals/evals/elsuite/multistep_web_tasks/docker/flask-playwright/app.py
 Lines 119 to 124 in 234bcde
	response=jsonify( 
	 { 
	""status"": ""success"", 
	""message"": f""could not return results of executed commands {request.json['command']}"", 
	""content"": str(e), 
	""url"": page.url, 
evals/evals/elsuite/multistep_web_tasks/docker/flask-playwright/app.py
 Lines 61 to 65 in 234bcde
	exceptExceptionase: 
	returnjsonify( 
	 {""status"": ""error"", ""message"": f""failed to start session (already started?): {e}""} 
	 ) 
	returnjsonify({""status"": ""success"", ""message"": ""session started""}) 
To Reproduce
evals/evals/elsuite/multistep_web_tasks/docker/flask-playwright/app.py
 Line 191 in 234bcde
	jsonify({""status"": ""error"", ""message"": f""error executing command {command}: {e}""}), 
evals/evals/elsuite/multistep_web_tasks/docker/flask-playwright/app.py
 Lines 164 to 170 in 234bcde
	response=jsonify( 
	 { 
	""status"": ""success"", 
	""message"": f""could not return results of executed commands {request.json['commands']}"", 
	""content"": str(e), 
	""url"": page.url, 
	 } 
evals/evals/elsuite/multistep_web_tasks/docker/flask-playwright/app.py
 Lines 119 to 124 in 234bcde
	response=jsonify( 
	 { 
	""status"": ""success"", 
	""message"": f""could not return results of executed commands {request.json['command']}"", 
	""content"": str(e), 
	""url"": page.url, 
evals/evals/elsuite/multistep_web_tasks/docker/flask-playwright/app.py
 Lines 61 to 65 in 234bcde
	exceptExceptionase: 
	returnjsonify( 
	 {""status"": ""error"", ""message"": f""failed to start session (already started?): {e}""} 
	 ) 
	returnjsonify({""status"": ""success"", ""message"": ""session started""}) 
Code snippets
https://github.com/openai/evals/blob/234bcde34b5951233681455faeb92baaaef97573/evals/elsuite/multistep_web_tasks/docker/flask-playwright/app.py#L191-L191

https://github.com/openai/evals/blob/234bcde34b5951233681455faeb92baaaef97573/evals/elsuite/multistep_web_tasks/docker/flask-playwright/app.py#L164-L170

https://github.com/openai/evals/blob/234bcde34b5951233681455faeb92baaaef97573/evals/elsuite/multistep_web_tasks/docker/flask-playwright/app.py#L119-L124

https://github.com/openai/evals/blob/234bcde34b5951233681455faeb92baaaef97573/evals/elsuite/multistep_web_tasks/docker/flask-playwright/app.py#L61-L65
OS
macOS
Python version
3.11.4
Library version
1.40.1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/1542","Log injection alert","2024-08-08T07:02:51Z","Open issue","bug","Describe the bug
Potential log injection alert here - 
evals/evals/elsuite/multistep_web_tasks/docker/flask-playwright/app.py
 Line 187 in 234bcde
	logger.info(f""Error executing command: {command}"") 
To Reproduce
evals/evals/elsuite/multistep_web_tasks/docker/flask-playwright/app.py
 Line 187 in 234bcde
	logger.info(f""Error executing command: {command}"") 
Code snippets
https://github.com/openai/evals/blob/234bcde34b5951233681455faeb92baaaef97573/evals/elsuite/multistep_web_tasks/docker/flask-playwright/app.py#L187-L187
OS
macOS
Python version
3.11.4
Library version
1.40.1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/1538","Ssomsak","2024-06-28T21:01:09Z","Closed issue","No label","No description provided.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/1536","Multiple Unit Test Failures Across OpenAI Assistants, Anthropic, and Google Gemini Libraries","2024-06-24T11:34:17Z","Open issue","bug","Describe the bug
I observed several failures in unit tests involving the libraries for OpenAI Assistants, Anthropic, and Google Gemini. Each, detailed below.
Anthropic Library:
Issue: Recent changes have made the ContentBlock type a Union type, which can't be directly instantiated. This updated type now encapsulates both TextBlock and ToolUseBlock. 
ContentBlock = Annotated[Union[TextBlock, ToolUseBlock], PropertyInfo(discriminator=""type"")]
Impact: Our existing code relies on the direct instantiation of ContentBlock, which is now causing failures.
OpenAI Library:
Issue: The latest update to the assistant API introduced several breaking changes: 
The retrieval tool has been renamed to file_search.
The parameter assistant.file_ids has been changed to tool_resources.
The parameter message.file_ids has been modified to attachments.
Impact: These changes are causing failures in the functionalities that depend on file handling and assistant resources.
Gemini Library:
Issue: There's a defect in the Gemini library related to how it handles protobuf objects; specifically, it erroneously parses these objects as dictionaries.
Impact: This parsing error is causing unexpected behavior and test failures.
To Reproduce
Just run the unit tests for v 3.0.1
Code snippets
No response
OS
macOS
Python version
3.9.0
Library version
3.0.1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/1533","Schelling point eval doesn't work","2024-05-22T21:03:53Z","Open issue","No label","Fresh installation (d3dc890)
oaieval gpt-3.5-turbo schelling_point

dies around 580th sample with
(...)
openai.BadRequestError: Error code: 400 - {'error': {'message': ""Sorry! We've encountered an issue with repetitive patterns in your prompt. Please try again with a different prompt."", 'type': 'invalid_prompt', 'param': 'prompt', 'code': None}}

I guess some prompt was accepted by the OpenAI API when the eval was developed and now it's rejected.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/1532","TensorFlow fails while no TensorFlow expected to run at all","2024-05-25T10:51:59Z","Closed issue","bug","Describe the bug
I am running OAIEval for the steganography eval with Llama 3 70B using PyTorch, HuggingFace. I don't use any TensorFlow afaik. However, I see some TensorFlow code is running and fails.
To Reproduce
Add the code to run Llama as below in Code Snippents. I see these messages in stdout:
gcc (GCC) 10.2.1 20210130 (Red Hat 10.2.1-11.1.0.1)
Copyright (C) 2020 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.

Python 3.11.5
[2024-05-22 11:46:58,821] [registry.py:271] Loading registry from /data/artyom_karpov/rl4steg/lib/evals/evals/registry/evals
[2024-05-22 11:46:59,485] [registry.py:271] Loading registry from /data/artyom_karpov/.evals/evals
[2024-05-22 11:46:59,704] [registry.py:271] Loading registry from /data/artyom_karpov/rl4steg/lib/evals/evals/registry/completion_fns
[2024-05-22 11:46:59,711] [registry.py:271] Loading registry from /data/artyom_karpov/.evals/completion_fns
[2024-05-22 11:46:59,711] [registry.py:271] Loading registry from /data/artyom_karpov/rl4steg/lib/evals/evals/registry/solvers
[2024-05-22 11:46:59,839] [registry.py:271] Loading registry from /data/artyom_karpov/.evals/solvers
/data/artyom_karpov/rl4steg/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
[2024-05-22 11:47:07,329] [modeling.py:989] We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).

Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]
Loading checkpoint shards:   3%|▎         | 1/30 [00:03<01:32,  3.20s/it]
Loading checkpoint shards:   7%|▋         | 2/30 [00:06<01:31,  3.27s/it]
Loading checkpoint shards:  10%|█         | 3/30 [00:09<01:28,  3.29s/it]
Loading checkpoint shards:  13%|█▎        | 4/30 [00:13<01:30,  3.49s/it]
Loading checkpoint shards:  17%|█▋        | 5/30 [00:17<01:26,  3.48s/it]
Loading checkpoint shards:  20%|██        | 6/30 [00:20<01:22,  3.43s/it]
Loading checkpoint shards:  23%|██▎       | 7/30 [00:23<01:19,  3.45s/it]
Loading checkpoint shards:  27%|██▋       | 8/30 [00:28<01:22,  3.73s/it]
Loading checkpoint shards:  30%|███       | 9/30 [00:32<01:18,  3.75s/it]
Loading checkpoint shards:  33%|███▎      | 10/30 [00:35<01:13,  3.69s/it]
Loading checkpoint shards:  37%|███▋      | 11/30 [00:39<01:10,  3.71s/it]
Loading checkpoint shards:  40%|████      | 12/30 [00:43<01:10,  3.94s/it]
Loading checkpoint shards:  43%|████▎     | 13/30 [00:48<01:10,  4.12s/it]
Loading checkpoint shards:  47%|████▋     | 14/30 [00:52<01:06,  4.15s/it]
Loading checkpoint shards:  50%|█████     | 15/30 [00:57<01:03,  4.26s/it]
Loading checkpoint shards:  53%|█████▎    | 16/30 [01:01<01:01,  4.39s/it]
Loading checkpoint shards:  57%|█████▋    | 17/30 [01:06<00:57,  4.41s/it]
Loading checkpoint shards:  60%|██████    | 18/30 [01:10<00:53,  4.45s/it]
Loading checkpoint shards:  63%|██████▎   | 19/30 [01:15<00:48,  4.43s/it]
Loading checkpoint shards:  67%|██████▋   | 20/30 [01:19<00:43,  4.36s/it]
Loading checkpoint shards:  70%|███████   | 21/30 [01:23<00:38,  4.28s/it]
Loading checkpoint shards:  73%|███████▎  | 22/30 [01:27<00:33,  4.22s/it]
Loading checkpoint shards:  77%|███████▋  | 23/30 [01:31<00:29,  4.22s/it]
Loading checkpoint shards:  80%|████████  | 24/30 [01:36<00:25,  4.28s/it]
Loading checkpoint shards:  83%|████████▎ | 25/30 [01:40<00:22,  4.43s/it]
Loading checkpoint shards:  87%|████████▋ | 26/30 [01:45<00:17,  4.43s/it]
Loading checkpoint shards:  90%|█████████ | 27/30 [01:49<00:13,  4.35s/it]
Loading checkpoint shards:  93%|█████████▎| 28/30 [01:53<00:08,  4.35s/it]
Loading checkpoint shards:  97%|█████████▋| 29/30 [01:58<00:04,  4.39s/it]
Loading checkpoint shards: 100%|██████████| 30/30 [01:59<00:00,  3.47s/it]
Loading checkpoint shards: 100%|██████████| 30/30 [01:59<00:00,  3.99s/it]
/data/artyom_karpov/rl4steg/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[2024-05-22 11:49:08,341] [oaieval.py:215] �[1;35mRun started: 240522114908HNUG55EE�[0m
2024-05-22 11:49:09.863802: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-05-22 11:49:11.808810: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2024-05-22 11:49:13,601] [utils.py:145] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check ""NUMEXPR_MAX_THREADS"" environment variable.
[2024-05-22 11:49:13,602] [utils.py:148] Note: NumExpr detected 128 cores but ""NUMEXPR_MAX_THREADS"" not set, so enforcing safe limit of 8.
[2024-05-22 11:49:13,602] [utils.py:161] NumExpr defaulting to 8 threads.
2024-05-22 11:49:15.717343: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 37944 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:0f:00.0, compute capability: 8.0
[2024-05-22 11:49:24,785] [data.py:94] Fetching /data/artyom_karpov/rl4steg/lib/evals/evals/registry/data/steganography/samples.jsonl
[2024-05-22 11:49:24,792] [eval.py:36] Evaluating 480 samples
[2024-05-22 11:49:24,810] [eval.py:144] Running in threaded mode with 1 threads!

  0%|          | 0/480 [00:00<?, ?it/s]Asking to pad to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no padding.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)


.... 


2024-05-22 11:53:01.290822: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.

  0%|          | 1/480 [03:36<28:49:53, 216.69s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
...

And it runs eventually. How to disable tensorflow?
Code snippets
llama.py

from evals.api import CompletionFn, CompletionResultfrom evals.prompt.base import CompletionPromptfrom evals.record import record_samplingimport torchfrom typing import Optionalfrom transformers import AutoModelForCausalLM, LlamaForCausalLM, LlamaConfig, AutoTokenizer


class LlamaCompletionResult(CompletionResult):
    def __init__(self, response) -> None:
        self.response = response

    def get_completions(self) -> list[str]:
        return [self.response.strip()]


class LlamaCompletionFn(CompletionFn):
    def __init__(self, llm: str, llm_kwargs: Optional[dict] = None, **kwargs) -> None:
        self._model = AutoModelForCausalLM.from_pretrained(
            llm,
            return_dict=True,
            load_in_8bit=llm_kwargs[""load_in_8bit""],
            load_in_4bit=llm_kwargs[""load_in_4bit""],
            device_map=""auto"",
            low_cpu_mem_usage=True,
            attn_implementation=""sdpa"" if llm_kwargs.get(""use_fast_kernels"", False) else None,
            torch_dtype=torch.bfloat16
        )
        self._model.eval()

        self._tokenizer = AutoTokenizer.from_pretrained(llm)
        self._tokenizer.pad_token = self._tokenizer.eos_token
        torch.manual_seed(llm_kwargs.get(""seed"", 42))
        self._gen_kwargs = llm_kwargs['gen_kwargs']

    @torch.no_grad()
    def __call__(self, prompt, **kwargs) -> CompletionResult:
        prompt = self._tokenizer.apply_chat_template(
            prompt, tokenize=False, add_generation_prompt=True
        )
        batch = self._tokenizer(prompt, padding='max_length', truncation=True, max_length=None, return_tensors=""pt"")
        batch = {k: v.to(""cuda"") for k, v in batch.items()}
        outputs = self._model.generate(
            **batch,
            **self._gen_kwargs,
        )
        # Take only response:
        outputs = outputs[0][batch['input_ids'][0].size(0):]
        response = self._tokenizer.decode(outputs, skip_special_tokens=True)
        record_sampling(prompt=prompt, sampled=response)
        return LlamaCompletionResult(response)
Register:

llama/3-70b:
  class: evals.completion_fns.llama:LlamaCompletionFn
  args:
    llm: meta-llama/Meta-Llama-3-70B-Instruct
    llm_kwargs:
      load_in_8bit: false
      load_in_4bit: true
      use_fast_kernels: false
      gen_kwargs:
        max_new_tokens: 200
        do_sample: true
        top_p: 1.0
        temperature: 1.0
        min_length: null
        use_cache: false
        top_k: 50
        repetition_penalty: 1.0
        length_penalty: 1




### OS

Linux * 3.10.0-1160.76.1.0.1.el7.x86_64 #1 SMP Wed Aug 10 17:32:14 PDT 2022 x86_64 x86_64 x86_64 GNU/Linux

### Python version

Python 3.11.5

### Library version

3.0.1

 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/1529","Support for GPT-4o","2024-05-26T18:45:11Z","Closed issue","No label","Describe the feature or improvement you're requesting
Please add support for GPT4-o for evaluation .
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
👍2
j30231 and sakher reacted with thumbs up emoji
All reactions
👍2 reactions"
"https://github.com/openai/evals/issues/1527","What is this","2024-05-03T11:01:54Z","Open issue","No label","No description provided.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/1515","Getting started example doesn't work - oieval attempts to update a None type object","2024-04-05T04:06:17Z","Closed issue","bug","Describe the bug
When running the example provided in https://github.com/openai/evals/blob/main/docs/run-evals.md: oaieval gpt-3.5-turbo test-match, the following error is encountered out of the box:
 File ""/Users/juliewang/Documents/evals/evals/cli/oaieval.py"", line 167, in run
    eval_spec.args.update(extra_eval_params)
    ^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'update'

This is due to this line: https://github.com/openai/evals/blob/main/evals/cli/oaieval.py#L158. The args field of EvalSpec is optional: 
evals/evals/base.py
 Line 58 in 0dc0ba4
	args: Optional[Dict[str, Any]] =None
 , and in the test-match case, eval_spec.args is None.
To Reproduce
Run oaieval gpt-3.5-turbo test-match
Code snippets
No response
OS
macOS
Python version
Python 3.12.2
Library version
openai-evals commit b5853eb
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/1513","When installing the project dependencies, i got: ""ERROR: Could not build wheels for greenlet, which is required to install pyproject.toml-based projects""","2024-04-04T05:18:40Z","Closed issue","bug","Describe the bug
After cloning the repo and running: pip install . I get: ""ERROR: Could not build wheels for greenlet, which is required to install pyproject.toml-based projects""
I am pretty sure this is related to the fixed version dependency of playwright==1.32.1, and some kind of version conflict with greenlet or another dependency.
dependencies = [
 ...
 ""playwright==1.32.1"",
 ...
 ]
Because if I remove the exact version number (1.32.1), the installation works as expected. I am not sure if there are some consequences of not installing that specific version.
To Reproduce
1- clone the repo and create an environment
 2- run pip install .
 3- the installation will fail and you will see: ERROR: Could not build wheels for greenlet, which is required to install pyproject.toml-based projects
Code snippets
No response
OS
macOs 13.6.1
Python version
v3.9.6
Library version
2.0.0.post1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/1504","Setting completion function args via CLI does not work","2024-03-27T13:59:01Z","Open issue","bug","Describe the bug
The response to issue #512 implemented a way to dynamically change API parameters (such as temperature) from the CLI (by looking at the code, the argument name has been changed to --completion_args). However, the arguments passed there do not seem to be used correctly.
To Reproduce
Try running any eval, with a completion function corresponding to an openai model, by setting a parameter of the API, such as the temperature; for instance:
 oaieval gpt-3.5-turbo-0125 <eval_name> --completion_args 'temperature=0.5'
The following error is raised:
Traceback (most recent call last):
  File ""/home/lorenzo/venv/recog-LLM_capabilities/bin/oaieval"", line 8, in <module>
    sys.exit(main())
  File ""/home/lorenzo/venv/recog-LLM_capabilities/lib/python3.9/site-packages/evals/cli/oaieval.py"", line 277, in main
    run(args)
  File ""/home/lorenzo/venv/recog-LLM_capabilities/lib/python3.9/site-packages/evals/cli/oaieval.py"", line 146, in run
    completion_fn_instances = [
  File ""/home/lorenzo/venv/recog-LLM_capabilities/lib/python3.9/site-packages/evals/cli/oaieval.py"", line 147, in <listcomp>
    registry.make_completion_fn(url, **additonal_completion_args) for url in completion_fns
  File ""/home/lorenzo/venv/recog-LLM_capabilities/lib/python3.9/site-packages/evals/registry.py"", line 124, in make_completion_fn
    return OpenAIChatCompletionFn(model=name, n_ctx=n_ctx, **kwargs)
TypeError: __init__() got an unexpected keyword argument 'temperature'

The reason is that OpenAIChatCompletionFn is not called correctly, as its __init__ is the following:
    def __init__(
        self,
        model: Optional[str] = None,
        api_base: Optional[str] = None,
        api_key: Optional[str] = None,
        n_ctx: Optional[int] = None,
        extra_options: Optional[dict] = {},
    ):

Code snippets
No response
OS
Ubuntu 20.04
Python version
python 3.9
Library version
git+https://github.com/openai/evals.git@dd96814dd96bd64f3098afca8dc873aa8d8ce4c8
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/1493","OpenAIChatCompletionFn should __init__ should accept **kwargs","2024-03-15T20:34:32Z","Open issue","bug","Describe the bug
Currently the __init__ method for OpenAICompletionFn has **kwargs but OpenAIChatCompletionFn does not. This creates problems if I have both:
a custom completion function that takes arguments I would like to populate with --completion_args
a modelgraded eval that I plan to grade with an OpenAI model
In these cases I would like to do:
oaieval my_completion_fn,gpt-4-turbo-preview my_eval --completion_args my_param=my_value
But I get:
TypeError: OpenAIChatCompletionFn.__init__() got an unexpected keyword argument 'my_param'
To Reproduce
Run oaieval gpt-3.5-turbo test-match --completion_args ignore_me=1
Code snippets
No response
OS
macOS
Python version
Python v3.11.7
Library version
openai-evals 2.0.0.post1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/1484","Support multiple completions for ModelbasedClassify","2024-03-14T18:15:54Z","Open issue","No label","Describe the feature or improvement you're requesting
It would be nice to be able to score multiple sample completions using ModelBasedClassify. Even if n>1 is passed into a completion function and multiple samples are returned, only the first is graded because of this line:
https://github.com/openai/evals/blob/main/evals/elsuite/utils.py#L193
Additional context
I would like to be able to raise the temperature, ask a model to produce N completions, and have each completion graded separately using a rubric. This appears to work fine for non-model-based scoring.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/1469","Support for Azure OpenAI client","2024-02-21T14:10:00Z","Open issue","No label","Describe the feature or improvement you're requesting
Currently evals framework does not support Azure openAI implementation. This is blocker if someone wants to use eval with Azure OpenAI implementation.
Additional context
No response
The content you are editing has changed. Please copy your edits and refresh the page.
Tasks
BetaGive feedback
Extending to Azure OpenAI implementation#1470
 +0 
Options
Convert to issue
Toggle completion
Rename
Remove
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/1459","Local run doesn't save logs to disk","2024-01-16T11:39:38Z","Closed issue","bug","Describe the bug
After running a successful eval, the records are supposed to be saved to tmp/evallogs, but I can't find them.
 Setting the --local-run flag explicitly doesn't work either.
However, setting a --record_path explicitly works and correctly writes the results to the provided path.
Proposed fix
I managed to fix the issue by changing the default record_path/tmp/evallogs/ to a relative path tmp/evallogs/ in oaieval.py.
 I'm not sure if this is a bug or if I'm missing something.
To Reproduce
Run an eval with the CLI command:
oaieval gpt-3.5-turbo <eval-name>
Code snippets
# evals/cli/oaieval.py - line 178

 record_path = (
        f""/tmp/evallogs/{run_spec.run_id}_{args.completion_fn}_{args.eval}.jsonl""
        if args.record_path is None
        else args.record_path
    )
OS
Ubuntu 22.04.3 (WSL)
Python version
Python 3.11.4
Library version
latest
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/1456","Tagged Release For 2.0.0","2024-01-12T22:41:41Z","Closed issue","bug","Describe the bug
Hey all, I saw that there was a release recently to pypi: https://pypi.org/project/evals/ for version 2.0.0. I was wondering if it was easy to make a tagged commit in Github like you did for 1.0.3 (https://github.com/openai/evals/tags). I'm not sure if it's difficult to do in hindsight, but it does make it slightly easier to navigate and brows the code at a specific release.
Thanks!
To Reproduce
Go to https://github.com/openai/evals/tags
There is no tag for 2.0.0
Code snippets
N/A
OS
N/A
Python version
N/A
Library version
openai-evals v2.0.0 and openai-evals v2.0.0.post1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/1448","Request to change arithmetical_puzzles prompting","2023-12-24T08:56:49Z","Open issue","No label","Describe the feature or improvement you're requesting
arithmetical_puzzles prompts does not allow proper reasoning The following is added at the end of every prompt...""Do not give any reasoning or logic for your answer.""
In a previous PR, usama-openai requested to change the prompting of eval math_for_5th-grader to add better prompting to allow the LLM to reason.
#1293 (review)
Request to change the prompt for arithmetical_puzzles as well.
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/1437","Possibility to sell high quality benchmarks","2024-05-07T01:41:37Z","Closed issue","No label","Describe the feature or improvement you're requesting
I'm interested to formalise large set of math olympiad questions. However it will require significant effort.
Is there any way to monetise the benchmarks if OpenAI decides they are worthy?
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/1434","Mismatch between LangChainChatModelCompletionFn code and registry","2023-12-21T17:46:37Z","Closed issue","bug","Describe the bug
The LangChainChatModelCompletionFn function has the following init:
evals/evals/completion_fns/langchain_llm.py
 Lines 68 to 69 in 0108dd7
	classLangChainChatModelCompletionFn(CompletionFn): 
	def__init__(self, llm: str, llm_kwargs: Optional[dict] =None, **kwargs) ->None: 
while the registry uses it in the following way:
evals/evals/registry/completion_fns/langchain_llms.yaml
 Lines 22 to 27 in 0108dd7
	langchain/chat_model/gpt-3.5-turbo: 
	class: evals.completion_fns.langchain_llm:LangChainChatModelCompletionFn
	args: 
	llm: ChatOpenAI
	chat_model_kwargs: 
	model_name: gpt-3.5-turbo
The mismatch is due to the registry referring to an argument chat_model_kwargs which does not exist in the class __init__. Even instantiating this does not raise an error as LangChainChatModelCompletionFn has an additional (unused) kwargs, so chat_model_kwargs from the registry ends up there.
I suggest to change chat_model_kwargs to llm_kwargs in the registry.
To Reproduce
Run the following bash command:
oaieval langchain/chat_model/gpt-3.5-turbo test-match

Code snippets
No response
OS
Ubuntu 20.04
Python version
python 3.9
Library version
git+https://github.com/openai/evals.git@dd96814dd96bd64f3098afca8dc873aa8d8ce4c8
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/1432","Error structure in utils after openai package upgrade","2023-12-20T17:58:56Z","Closed issue","No label","Hey @etr2460, appreciate your work upgrading evals to work with newer openai versions!
This is mostly a question regarding this commit, where the error list in api_utils.py is different from the list in make_me_say/utils.py. The list in api_utils seems more complete, and I'm inclined to update the one in MakeMeSay to match it.
Is there any reason I shouldn't?
Thanks!
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/1426","oaieval doesn't run beacuse of ""module 'openai' has no attribute 'error'""","2023-12-09T10:49:45Z","Closed issue","bug","Describe the bug
I jsut installed the evals repository and evals package for python using pip.
 When running any command using oaieval (e.g. oaieval --help) I get an error saying:
  File ""/Users/.pyenv/versions/3.11.1/bin/oaieval"", line 5, in <module>
    from evals.cli.oaieval import main
  File ""/Users/.pyenv/versions/3.11.1/lib/python3.11/site-packages/evals/__init__.py"", line 2, in <module>
    from .completion_fns.openai import (
  File ""/Users/.pyenv/versions/3.11.1/lib/python3.11/site-packages/evals/completion_fns/openai.py"", line 13, in <module>
    from evals.utils.api_utils import (
  File ""/Users/.pyenv/versions/3.11.1/lib/python3.11/site-packages/evals/utils/api_utils.py"", line 12, in <module>
    openai.error.ServiceUnavailableError,
    ^^^^^^^^^^^^
AttributeError: module 'openai' has no attribute 'error'


After searching online I found that this is caused by a too new openai library, that doesn't have 'error' anymore and/or that langchain still uses that. I tried installing openai==0.28.1 and langchain==0.0.330 or langchain==0.0.316 but neither worked for me.
To Reproduce
run git clone git@github.com:openai/evals.git
change into directory
run pip install -e .
try oaieval --help
Code snippets
No response
OS
macos
Python version
Python v3.11.1
Library version
evals 1.0.3.post1
 The text was updated successfully, but these errors were encountered: 
😕1
tylerburleigh reacted with confused emoji
All reactions
😕1 reaction"
"https://github.com/openai/evals/issues/1424","Running an evaluation can lead to circular import error","2023-12-21T01:39:14Z","Closed issue","bug","Describe the bug
$ oaieval gpt-3.5-turbo test-match[2023-12-06 19:14:25,433] [registry.py:249] Loading registry from /home/abhin/.local/lib/python3.9/site-packages/evals/registry/evals[2023-12-06 19:14:25,529] [registry.py:249] Loading registry from /home/abhin/.evals/evals[2023-12-06 19:14:25,530] [oaieval.py:110] Run started: 231206191425YO46H5EF[2023-12-06 19:14:25,531] [data.py:75] Fetching test_match/samples.jsonl[2023-12-06 19:14:25,531] [eval.py:34] Evaluating 3 samples[2023-12-06 19:14:25,536] [eval.py:153] Running in threaded mode with 10 threads!  0%|                                                                                | 0/3 [00:00<?, ?it/s]Traceback (most recent call last):  File ""/home/abhin/.local/bin/oaieval"", line 8, in <module>    sys.exit(main())  File ""/home/abhin/.local/lib/python3.9/site-packages/evals/cli/oaieval.py"", line 164, in main    run(args)  File ""/home/abhin/.local/lib/python3.9/site-packages/evals/cli/oaieval.py"", line 141, in run    result = eval.run(recorder)  File ""/home/abhin/.local/lib/python3.9/site-packages/evals/elsuite/basic/match.py"", line 53, in run    self.eval_all_samples(recorder, samples)  File ""/home/abhin/.local/lib/python3.9/site-packages/evals/eval.py"", line 155, in eval_all_samples    idx_and_result = list(tqdm(iter, total=len(work_items), disable=not show_progress))  File ""/home/abhin/.local/lib/python3.9/site-packages/tqdm/std.py"", line 1182, in __iter__    for obj in iterable:  File ""/usr/lib/python3.9/multiprocessing/pool.py"", line 870, in next    raise value  File ""/usr/lib/python3.9/multiprocessing/pool.py"", line 125, in worker    result = (True, func(*args, **kwds))  File ""/home/abhin/.local/lib/python3.9/site-packages/evals/eval.py"", line 140, in worker_thread    executor = concurrent.futures.ThreadPoolExecutor(max_workers=1)  File ""/usr/lib/python3.9/concurrent/futures/__init__.py"", line 49, in __getattr__    from .thread import ThreadPoolExecutor as teImportError: cannot import name 'ThreadPoolExecutor' from partially initialized module 'concurrent.futures.thread' (most likely due to a circular import) (/usr/lib/python3.9/concurrent/futures/thread.py)
To Reproduce
mkdir tmp
cd tmp
virtualenv -p python3.9 .venv
source .venv/bin/activate
pip install evals==1.0.3.post1 openai==0.28.1
oaieval gpt-3.5-turbo test-match
It should break with the exception mentioned above.
Code snippets
No response
OS
debian-11
Python version
Python 3.9.2
Library version
1.0.3.post1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/1421","Improvements to Match: case insensitive and strip","2023-11-30T09:27:13Z","Open issue","No label","Describe the feature or improvement you're requesting
The current implementation of the Match basic eval template is case-sensitive. This leads to results such as:
{'correct': False, 'expected': 'no', 'picked': None, 'sampled': 'No', 'options': ['no']}
Similarly, Match does not strip the sampled string from white spaces in the front. That causes the evaluation to fail for models using the Completion endpoint, as those are more likely to output spaces in the front. Example:
{'correct': False, 'expected': 'Mumbai', 'picked': None, 'sampled': ' Mumbai', 'options': ['Mumbai']}
It would be good to add an argument to Match allowing to require case insensitive behaviour and to determine whether the answer should be stripped of spaces. These can then be specified in the yaml file for a task.
Similar options can be added for the other templates, such as Includes and FuzzyMatch.
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
👍1
zachschillaci27 reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/evals/issues/1419","Proposal for Adding a New Evaluation Metric: Sentiment Analysis Accuracy","2023-11-23T17:28:21Z","Open issue","No label","Describe the feature or improvement you're requesting
Description:
I propose the addition of a new evaluation metric, ""Sentiment Analysis Accuracy,"" to enhance the existing evaluation capabilities of the project. This metric will focus on assessing the model's performance specifically in sentiment analysis tasks.
Motivation:
The current evaluation metrics provide valuable insights, but there is a growing need for a specialized metric to evaluate sentiment analysis accuracy. Sentiment analysis is a common and crucial task in natural language processing, and having a dedicated metric will enable more fine-grained assessment of the model's performance in this domain.
Additional context
No response
The content you are editing has changed. Please copy your edits and refresh the page.
Tasks
BetaGive feedback
No tasks being tracked yet.
Options
Convert to issue
Toggle completion
Rename
Remove
 The text was updated successfully, but these errors were encountered: 
👍1
fbaptista reacted with thumbs up emoji❤️2
JensAstrup and travisnguyen20 reacted with heart emoji
All reactions
👍1 reaction
❤️2 reactions"
"https://github.com/openai/evals/issues/1408","Do not back off on openai.BadRequestError","2023-12-05T18:20:33Z","Closed issue","No label","After the recent change we back off on openai.APIError. This means backing off on openai.BadRequestError:
>>> issubclass(openai.BadRequestError, openai.APIError)
True

I don't know all the cases where we can get a BadRequestError, but one of them is exceeded context length:
[2023-11-14 11:09:32,241] [_common.py:105] Backing off openai_completion_create_retrying(...) for 17.0s (openai.BadRequestError: Error code: 400 - {'error': {'message': ""This model's maximum context length is 8001 tokens, however you requested 8184 tokens (7672 in your prompt; 512 for the completion). Please reduce your prompt; or completion length."", 'type': 'invalid_request_error', 'param': None, 'code': None}})

And in this case we should definitely avoid repeating the request.
(I found this on #1407, but I don't think this is related to this particular PR)
NOTE: logic corresponding to backing off on openai.APIError is in a few different places, I think all of them need the same fix.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/1399","Evals broken with latest openai package v1.1.1","2023-11-13T17:55:50Z","Closed issue","bug","Describe the bug
openai package v1.1.1 has changed their error handling. Now openai.error raises an AttributeError. These lines will raise an AttributeError when running any eval, as this is part of the backoff logic which is used for both base and chat completion functions.
To Reproduce
Clone repo
Install dependencies
Run oaieval gpt-4 2d_movement
Code snippets
No response
OS
macOS
Python version
Python v3.9
Library version
openai-evals v0.1.1
 The text was updated successfully, but these errors were encountered: 
👍3
onjas-buidl, isaduan, and shangeethas reacted with thumbs up emoji
All reactions
👍3 reactions"
"https://github.com/openai/evals/issues/1394","Failed to open: ../registry/data/social_iqa/few_shot.jsonl with custom registry","2024-01-03T16:48:29Z","Closed issue","bug","Describe the bug
I have created a new eval within a custom registry and installed evals from pypi (using a recent checkout from github). Calling oaievals works fine if I do not use few_show samples, but it does not if you include few shot examples.
As can be seen in the error thread below, the issue is caused by calling self._prefix_registry_path(self.few_shot_jsonl) (see _prefix_registry_path) which transforms the path string into a Path object. Then, open_by_file_pattern (here) still treats it as a string, unaware of the fact that it is actually a Path
To Reproduce
Create a new eval with few-shot samples.
Call oaieval, for instance:
oaieval text-ada-001 social_iqa_few_shot --registry_path=../registry

You'll get an error thread as follows:
[2023-11-03 14:40:52,931] [registry.py:254] Loading registry from /home/lorenzo/venv/recog-LLM_capabilities/lib/python3.9/site-packages/evals/registry/evals
[2023-11-03 14:40:54,382] [registry.py:254] Loading registry from /home/lorenzo/.evals/evals
[2023-11-03 14:40:54,382] [registry.py:254] Loading registry from ../registry/evals
[2023-11-03 14:40:55,007] [oaieval.py:189] �[1;35mRun started: 2311031440552KUYNQGH�[0m
[2023-11-03 14:40:55,010] [data.py:90] Fetching ../registry/data/social_iqa/few_shot.jsonl
Traceback (most recent call last):
  File ""/home/lorenzo/venv/recog-LLM_capabilities/lib/python3.9/site-packages/evals/data.py"", line 54, in open_by_file_pattern
    if filename.endswith("".gz""):
AttributeError: 'PosixPath' object has no attribute 'endswith'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""/home/lorenzo/venv/recog-LLM_capabilities/bin/oaieval"", line 8, in <module>
    sys.exit(main())
  File ""/home/lorenzo/venv/recog-LLM_capabilities/lib/python3.9/site-packages/evals/cli/oaieval.py"", line 274, in main
    run(args)
  File ""/home/lorenzo/venv/recog-LLM_capabilities/lib/python3.9/site-packages/evals/cli/oaieval.py"", line 215, in run
    eval: Eval = eval_class(
  File ""/home/lorenzo/venv/recog-LLM_capabilities/lib/python3.9/site-packages/evals/elsuite/basic/match.py"", line 28, in __init__
    self.few_shot = evals.get_jsonl(self._prefix_registry_path(self.few_shot_jsonl))
  File ""/home/lorenzo/venv/recog-LLM_capabilities/lib/python3.9/site-packages/evals/data.py"", line 129, in get_jsonl
    return _get_jsonl_file(path)
  File ""/home/lorenzo/venv/recog-LLM_capabilities/lib/python3.9/site-packages/evals/data.py"", line 91, in _get_jsonl_file
    with open_by_file_pattern(path, mode=""r"") as f:
  File ""/home/lorenzo/venv/recog-LLM_capabilities/lib/python3.9/site-packages/evals/data.py"", line 75, in open_by_file_pattern
    raise RuntimeError(f""Failed to open: {filename}"") from e
RuntimeError: Failed to open: ../registry/data/social_iqa/few_shot.jsonl

Code snippets
No response
OS
Ubuntu 20.04
Python version
python 3.9
Library version
git+https://github.com/openai/evals.git@dd96814dd96bd64f3098afca8dc873aa8d8ce4c8
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/1393","Using different models in evaluating mode-graded eval and in generating the completion","2023-11-03T11:47:34Z","Open issue","No label","Describe the feature or improvement you're requesting
build_eval.md says:
In general, the evaluation model and the model being evaluated don't have to be the same, though we will assume that they are here for ease of explanation.
However, I can't find anywhere how to do this. Is this currently implemented?
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/1386","In the task ""balance_chemical_equation"", many instances have incorrect labels.","2023-10-19T06:47:41Z","Open issue","bug","Describe the bug
For example
You are ChemistGPT, can help user balance chemical equation. For example, if user's input is ""C6H5COOH + O2 = CO2 + H2O"", you will reply the balanced chemical equation: ""2C6H5COOH + 15O2 = 14CO2 + 6H2O"", without explanation. If you can't balance the equation, just reply ""Unknown""

Input: Al(OH)3 + H2SO4 = Al2(SO4)3 + H2O
Answer: 

The given groundtruth is
2Al(OH)3 + 3H2SO4 = Al2(SO4)3 + 3H2O

but the real correct label is
2Al(OH)3 + 3H2SO4 = Al2(SO4)3 + 6H2O

There are many problems like this, hope to fix.
@scruel
To Reproduce
Nothing
Code snippets
Nothing
OS
macOS
Python version
Python3.11
Library version
openai-evals latest
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/1384","Eval-running often hangs on last sample","2023-10-17T22:38:29Z","Open issue","bug","Describe the bug
Relatively often, my eval-run will be at say samples 199/200 but then will hang for a very long period of time on the last one. It isn't clear to me why this occurs, but sometimes it'll persist as long as an hour or more, at which point I generally terminate the command from my CLI and try again
To Reproduce
I'm not sure how to make this happen every time unfortunately. It does seem more likely to happen on bigger sampling runs than small ones though.
Code snippets
No response
OS
macOS
Python version
Python v3.11
Library version
latest
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/1382","Should random collection of values be supported?","2023-10-17T07:00:40Z","Open issue","No label","evals/evals/elsuite/basic/match.py
 Lines 39 to 44 in dd96814
	ifself.num_few_shot>0: 
	assertis_chat_prompt(sample[""input""]), ""few shot requires chat prompt""
	prompt=sample[""input""][:-1] 
	forsinself.few_shot[: self.num_few_shot]: 
	prompt+=s[""sample""] 
	prompt+=sample[""input""][-1:] 
refer to evals/registry/evals/cissp-study-questions.yaml
few_shot_jsonl has 143 pieces of data, but num_few_shot only takes the first 4 items.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/1379","Multiple evals not found","2023-10-16T08:57:39Z","Open issue","bug","Describe the bug
When running evals such as the steganography eval, I get: AssertionError: Eval steganography.yaml not found
To Reproduce
Install and set up everything according to the documentation
 oaieval gpt-3.5-turbo steganography
Code snippets
No response
OS
macOS
Python version
Python 3.9
Library version
main
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/1377","Context window of completion functions not accounted for","2023-10-13T13:01:15Z","Open issue","bug","Describe the bug
It seems that some evals require specific context window length, ex: make-me-say eval probably requires 32k?
It would be nice if there was a more DX friendly to know about this before it errors in the API call?
To Reproduce
oaieval gpt-3.5-turbo,gpt-3.5-turbo,gpt-3.5-turbo make-me-say --debug
This model's maximum context length is 4097 tokens. However, your messages resulted in 4123 tokens. Please reduce the length of the messages.
Code snippets
No response
OS
macOS
Python version
Python v3.9.7
Library version
openai-evals 1.0.3
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/1369","oaieval --help errors for me","2023-12-05T21:29:19Z","Closed issue","bug","Describe the bug
When I run oaieval --help, I get an error, rather than a list of flags; not sure if this is just me, however:
@logankilpatrick are you the right person to take a look at this?
Here is my traceback:
oaieval --help Traceback (most recent call last): File ""/Users/stevenadler/.virtualenvs/venv_evals/bin/oaieval"", line 8, in <module> sys.exit(main()) File ""/Users/stevenadler/code/evals/evals/cli/oaieval.py"", line 264, in main args = cast(OaiEvalArguments, parser.parse_args(sys.argv[1:])) File ""/Users/stevenadler/.pyenv/versions/3.9.9/lib/python3.9/argparse.py"", line 1821, in parse_args args, argv = self.parse_known_args(args, namespace) File ""/Users/stevenadler/.pyenv/versions/3.9.9/lib/python3.9/argparse.py"", line 1854, in parse_known_args namespace, args = self._parse_known_args(args, namespace) File ""/Users/stevenadler/.pyenv/versions/3.9.9/lib/python3.9/argparse.py"", line 2063, in _parse_known_args start_index = consume_optional(start_index) File ""/Users/stevenadler/.pyenv/versions/3.9.9/lib/python3.9/argparse.py"", line 2003, in consume_optional take_action(action, args, option_string) File ""/Users/stevenadler/.pyenv/versions/3.9.9/lib/python3.9/argparse.py"", line 1931, in take_action action(self, namespace, argument_values, option_string) File ""/Users/stevenadler/.pyenv/versions/3.9.9/lib/python3.9/argparse.py"", line 1095, in __call__ parser.print_help() File ""/Users/stevenadler/.pyenv/versions/3.9.9/lib/python3.9/argparse.py"", line 2551, in print_help self._print_message(self.format_help(), file) File ""/Users/stevenadler/.pyenv/versions/3.9.9/lib/python3.9/argparse.py"", line 2535, in format_help return formatter.format_help() File ""/Users/stevenadler/.pyenv/versions/3.9.9/lib/python3.9/argparse.py"", line 283, in format_help help = self._root_section.format_help() File ""/Users/stevenadler/.pyenv/versions/3.9.9/lib/python3.9/argparse.py"", line 214, in format_help item_help = join([func(*args) for func, args in self.items]) File ""/Users/stevenadler/.pyenv/versions/3.9.9/lib/python3.9/argparse.py"", line 214, in <listcomp> item_help = join([func(*args) for func, args in self.items]) File ""/Users/stevenadler/.pyenv/versions/3.9.9/lib/python3.9/argparse.py"", line 214, in format_help item_help = join([func(*args) for func, args in self.items]) File ""/Users/stevenadler/.pyenv/versions/3.9.9/lib/python3.9/argparse.py"", line 214, in <listcomp> item_help = join([func(*args) for func, args in self.items]) File ""/Users/stevenadler/.pyenv/versions/3.9.9/lib/python3.9/argparse.py"", line 530, in _format_action help_text = self._expand_help(action) File ""/Users/stevenadler/.pyenv/versions/3.9.9/lib/python3.9/argparse.py"", line 627, in _expand_help return self._get_help_string(action) % params TypeError: %o format: an integer is required, not dict
To Reproduce
Run oaieval --help from a venv that has evals installed
 Per this doc, it should produce a list of CLI options, but seems not to for me: https://github.com/openai/evals/blob/main/docs/run-evals.md
Code snippets
No response
OS
macOS
Python version
Python v3.9.9
Library version
Not sure, but pulled from main recently & installed yesterday
 The text was updated successfully, but these errors were encountered: 
👍2
thesofakillers and ckgresla reacted with thumbs up emoji
All reactions
👍2 reactions"
"https://github.com/openai/evals/issues/1362","Use github.com/apssouza22/chatflow as a conversational layer. It would enable actual API requests to be carried out from natural language inputs.","2023-09-25T00:53:34Z","Open issue","No label","Describe the feature or improvement you're requesting
Adding this conversational UI would enable people to 'talk' directly with the backend and API requests to be carried out more effectively. RAG can help with some of the problems function calling by language models face at the moment.
https://youtu.be/r3cegH2kviQ
.
Additional context
I'm trying to accelerate the adoption of natural language interfaces
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/1350","Evaluate the cost of running tests","2023-09-15T19:11:33Z","Open issue","No label","Describe the feature or improvement you're requesting
In many production scenarios, it is important to do cost-benefit analysis, and it will be great if oaieval command can also return the total cost of running the test.
Specifically, it will be 2 parts:
in oaieval command, add optional param to specify whether you want to output the cost
an optional interface in CompletionFn to calculate the costs of running that Completion
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
👍4
LoryPack, rachadLakkis, DreTGiant, and CarloDiPalma reacted with thumbs up emoji
All reactions
👍4 reactions"
"https://github.com/openai/evals/issues/1346","Feature request for evals: Add support for function call.","2023-09-07T15:33:55Z","Open issue","No label","Add support for function call.
I would like to eval based on prompts that utilize function_call.
 From I have seen in the code, it's not possible at the moment.
 I observed a similar request here.
 Is there any plan to implement this?
 The text was updated successfully, but these errors were encountered: 
👍6
dani-mp, psimm, siliconlad, ilyac-varicent, tylerlittlefield, and haveaguess reacted with thumbs up emoji👀1
kongjiellx reacted with eyes emoji
All reactions
👍6 reactions
👀1 reaction"
"https://github.com/openai/evals/issues/1344","Publish latest evals framework to PyPI","2023-08-31T09:17:18Z","Open issue","No label","Describe the feature or improvement you're requesting
The have been numerous improvements and fixes to the evals framework itself over the past few months, but these haven't been released to PyPI. It would be great for that to happen.
There is the option of installing a specific version of the main branch direct from the repo using pip, but there are no tags of versions that are confirmed to be good/tested for use.
Additional context
The last release was evals 1.0.3.post1 on Apr 17, 2023.
https://pypi.org/project/evals/
 The text was updated successfully, but these errors were encountered: 
👍9
srenault, naveenkasturi, MHLoppy, kiriteegak-bain, vikmary, LoryPack, micaww, LRudL, and Adam-D-Lewis reacted with thumbs up emoji
All reactions
👍9 reactions"
"https://github.com/openai/evals/issues/1342","How to eval output with ideal_answer directly without having to define the completion_fn ?","2023-08-29T07:19:33Z","Open issue","No label","Describe the feature or improvement you're requesting
I have already had the output (generated from LLM) and ideal_answers in my jsonl file. For a look:
{'input': 'what is 2 plus 1?', 'output': '3', 'ideal': '3'}
{'input': 'what is 2 plus 2?', 'output': '3', 'ideal': '4'}

I don't need to define the completion_fn, because it's used to generate output which I have already had.
 So, how can I eval output with ideal_answer directly ?
 Thanks a lot.
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/1340","Having trouble building Evals locally? Try this.","2023-08-25T19:57:42Z","Open issue","bug","Describe the bug
I used
git clone git@github.com:openai/evals.git
and couldn't get pip install -e . to run.
Error logs said I was missing a setup.py or setup.cfg file. That's odd. Poking around, I realized that my git clone hadn't copied pyproject.toml and mypy.ini into my local directory.
No problem, I manually downloaded those files and voila it compiled.
So then I ran oaieval gpt-3.5-turbo test-closedqa-uniqueness and got this error: ModuleNotFoundError: No module named 'evals.utils'
Upon closer inspection, it was the ""git clone"" issue again! For some reason, cloning the repo isn't bringing in all of the files and folders. Sure enough, I was missing the utils folders under /evals/
I downloaded the entire repo as a Zip file. That zip contained all of the files, at which point building and running the oaievals worked.
To Reproduce
git clone this main branch
run 'pip install -e .'
it will fail.
Code snippets
No response
OS
macOS
Python version
Python 3.9.7
Library version
openai-evals 1.0.3.post1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/1338","Find claims from research paper","2023-08-21T14:43:12Z","Open issue","No label","Describe the feature or improvement you're requesting
It would be helpful for the next iteration of the generative pre trained model to learn how to identify any and all claims (arguable statements) in research papers. This would save time and facilitate discourse.
Additional context
Requires GTP4-32k context length
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/1333","Sample evaluations completing after timeout cause duplicate results","2023-08-10T13:58:00Z","Open issue","bug","Describe the bug
There is a default 40s timeout for completion functions as per EVALS_THREAD_TIMEOUT.
In eval.py, when evaluating a sample times out, it is retried. However it appears that the original invocation can still complete later and thus we end up with multiple sampling/match events in the log file for that sample line.
To Reproduce
Run an eval of N samples, some of which will be slow
Observe multiple events in the log file for samples which were retried
To easily find which samples were retried, put a breakpoint/logging in the timeout catch of the worker thread.
Code snippets
No response
OS
Windows 10
Python version
4.11
Library version
1.0.3.post1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/1328","Accuracy Score","2023-08-03T03:31:49Z","Open issue","No label","Generic question about the accuracy score and boostrap_std metric
When I run an eval, I got the following report.
{'accuracy': 0.6, 'boostrap_std': 0.1423900220076777}
How to decide the accuracy is good or not good?
 Can the Accuracy score 0.6 be considered as good or not good? At below which point, We should consider that the Model's performance is not good for that specific eval?
Also, Can anyone explain what is the boostrap_std metric?
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/1319","All evals currently in the repo appear only to have dev samples: is this correct?","2023-07-21T19:47:07Z","Closed issue","No label","Describe the feature or improvement you're requesting
The documentation speaks of dev, val, and test splits, but all evals currently in the repo appear only to have dev samples. Is this correct, or are we testing against hidden test samples when we run evals?
 It would be great if this could be made clearer in the documentation (as well as what the distinction is between ""dev"" and ""val"", which is not self-evident -- I'm used to ""development set"" and ""validation set"" being synonyms...)
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/1298","Please approve pull request, changes were made.","2023-07-13T19:12:16Z","Closed issue","No label","Seeing a lot of this in the PR. Could you fix it?

Originally posted by @jwang47 in #1155 (comment)
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/1292","oaieval hangs a lot","2023-07-05T21:20:27Z","Open issue","bug","Describe the bug
oaieval hangs near the end, before reporting, a lot.
To Reproduce
✗ EVALS_THREADS=12 EVALS_THREAD_TIMEOUT=10 oaieval gpt-3.5-turbo myeval
[2023-06-28 18:09:56,280] [registry.py:266] Loading registry from /Users/username/development/evals/evals/registry/evals
[2023-06-28 18:09:56,615] [registry.py:266] Loading registry from /Users/username/.evals/evals
[2023-06-28 18:09:56,617] [oaieval.py:138] Run started: runid
[2023-06-28 18:09:56,618] [data.py:83] Fetching myeval/samples.jsonl
[2023-06-28 18:09:56,619] [eval.py:33] Evaluating 69 samples
[2023-06-28 18:09:56,627] [eval.py:139] Running in threaded mode with 10 threads!
 99%|█████████████████████████████████████████████████████████████████████████  | 68/69 [00:20<00:00,  7.93it/s]

style of call hangs at this point for many minutes, even though EVALS_THREAD_TIMEOUT is set to 10 seconds. This is devastating to eval turnaround time.
Code snippets
No response
OS
macOS Ventura (13.4)
Python version
3.11.3
Library version
1.0.3
 The text was updated successfully, but these errors were encountered: 
👀1
andrew-openai reacted with eyes emoji
All reactions
👀1 reaction"
"https://github.com/openai/evals/issues/1284","Unable to modify match_fn from within modelgraded eval .yaml file","2024-03-09T14:55:58Z","Closed issue","bug","Describe the bug
Users are unable to modify the match_fn to be used using kwargs from within a modelgraded eval .yaml file.
The bug occur because within evals\evals\elsuite\modelgraded\classify.py:
ModelBasedClassify does not take in a match_fn kwarg.
class ModelBasedClassify(evals.Eval):
    def __init__(
        self,
        modelgraded_spec: str,
        *args,
        modelgraded_spec_args: Optional[dict[str, dict[str, str]]] = None,
        sample_kwargs: Optional[dict[str, Any]] = None,
        eval_kwargs: Optional[dict[str, Any]] = None,
        multicomp_n: Union[int, str] = 1,
        eval_type: Optional[str] = None,
        metaeval: bool = False,
        **kwargs,
    ):
        super().__init__(*args, **kwargs)
        # treat last completion_fn as eval_completion_fn
        self.eval_completion_fn = self.completion_fns[-1]
        if len(self.completion_fns) > 1:
            self.completion_fns = self.completion_fns[:-1]
        n_models = len(self.completion_fns)
        self.sample_kwargs = {""max_tokens"": 1024}
        self.sample_kwargs.update(sample_kwargs or {})
        self.eval_kwargs = {""max_tokens"": 1024}
        self.eval_kwargs.update(eval_kwargs or {})
        self.metaeval = metaeval
        self.modelgraded_spec_args = modelgraded_spec_args or {}
        self.eval_type = eval_type
        if multicomp_n == ""from_models"":
            assert n_models > 1
            self.multicomp_n = n_models
        else:
            assert isinstance(multicomp_n, int)
            self.multicomp_n = multicomp_n
        if len(self.completion_fns) > 1:
            assert self.multicomp_n == n_models

        self.mg = self.registry.get_modelgraded_spec(modelgraded_spec)

Within eval_sample(), classify is not called with a match_fn kwarg nor with **kwargs.


choice, info = classify(
            mg=self.mg,
            completion_fn=self.eval_completion_fn,
            completion_kwargs=self.eval_kwargs,
            eval_type=self.eval_type,
            n=self.multicomp_n,
            format_kwargs={**completions, **test_sample, **self.modelgraded_spec_args},
        )

Within evals\evals\elsuite\modelgraded\classify_utils.py, classify() would always revert to the default string ""starts_or_endswith"".
def classify(
    mg: ModelGradedSpec,
    completion_fn: CompletionFn,
    completion_kwargs: Optional[dict[str, Any]] = None,
    format_kwargs: Optional[dict[str, Any]] = None,
    eval_type: Optional[str] = None,
    n: Optional[int] = None,
    match_fn: str = ""starts_or_endswith"",
) -> str:

Any attempt of having a assigning a match_fn through a kwarg in the .yaml file would have no effect on the classify() call.
To Reproduce
Add match_fn: exact to the args of one of the modelgraded test evals found in evals/registry/evals/test-modelgraded.yaml.
Run the eval.
The eval would still be using the default string ""starts_or_endswith"".
Code snippets
No response
OS
Linux
Python version
Python v3.9.5
Library version
openai-evals v0.27.2
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/1282","Meaning of ""elsuite"" folder name","2023-07-05T14:55:39Z","Closed issue","No label","Hello,
I came across the ""elsuite"" folder in this repository and I'm curious about its meaning or purpose. Could you please explain the significance behind the name ""elsuite"" and what it represents in the context of this project?
Thank you for your time and assistance.
Best regards
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/1275","Code Evals","2023-07-03T03:57:34Z","Open issue","No label","Describe the feature or improvement you're requesting
I wonder if anyone has a solid method for evaluating code benchmarks like APPS.
 String typed codes can be very noisy and require deliberate preprocessing to be executed and tested.
 I don't see any class inheriting Evals that can perform code tests.
Any clue? 🤔
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/1264","Expose run_id to code being run within an eval","2023-06-29T13:39:40Z","Open issue","No label","Describe the feature or improvement you're requesting
I'd like to use this to label information created during an eval run (e.g. the filenames of system log files), to allow it to be correlated with the evals logfiles.
Additional context
Example log file name:
 /tmp/evallogs/230629131228Z4SV7V2E_completionfnname_evalname.jsonl
Example of system log file I'd like to write from my own code:
 230629131228Z4SV7V2E.log
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/1246","gpt-4-32k","2023-12-05T23:29:18Z","Closed issue","No label","Describe the feature or improvement you're requesting
Do you support adding evals for long inputs?
 For example: https://arxiv.org/abs/2305.14196 ?
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
👍2
mmc7676 and T7x77 reacted with thumbs up emoji
All reactions
👍2 reactions"
"https://github.com/openai/evals/issues/1239","国际化的支持","2023-06-26T09:41:20Z","Open issue","No label","Describe the feature or improvement you're requesting
不能友好的识别中文的含义

明明明明明白白白喜欢欢欢应该这样断句：明明，明明，明白，白白，喜欢，欢欢。
 明明（第一个）、白白、欢欢是人名，第二个明明是清楚的意思。
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
👎7
AsherJingkongChen, giranntu, Jexers, nonlog, pskl, Jiaxin-Wen, and xTayEx reacted with thumbs down emoji
All reactions
👎7 reactions"
"https://github.com/openai/evals/issues/1228","closedqa prompt is not adequate for gpt-4-0613","2023-06-24T02:37:01Z","Open issue","No label","It seems that GPT-4 neglects to follow the instructions in the closedqa prompt much more than gpt-3.5-turbo. See, for example, #1200 (comment) where gpt-4 gives 9 invalid responses out of 47, while gpt-3.5-turbo does not give any invalid responses. Does this hold across the other evals in the repo?
 The text was updated successfully, but these errors were encountered: 
👀1
andrew-openai reacted with eyes emoji
All reactions
👀1 reaction"
"https://github.com/openai/evals/issues/1208","You should see GPT-4 API access enabled in your account in the next few days.","2023-06-21T17:25:56Z","Closed issue","No label","You should see GPT-4 API access enabled in your account in the next few days.
Originally posted by @usama-openai in #1187 (comment)
Please can I have access to the GPT 4 API? My email is jonathan@verhe.es
Many thanks
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/1175","Evaluate gpt-4-0613 and gpt-3.5-turbo-0613 yields invalid_request_during_completion","2023-06-16T20:58:35Z","Closed issue","bug","Describe the bug
I would like to evaluate the current gpt-4 and chat-gpt versions against the new ones released two days ago.
However, I can only test the current model versions, not the new 0613 versions, because they fail with errors.
According to the documenation of the oiaeval cli, the CompletionFn can be a model available in the API:
One or more CompletionFn URLs, separated by commas (,). A CompletionFn can either be the name of a model available in the OpenAI API or a key in the registry (see evals/registry/completion_fns).
So I would assume that the names of the new models, gpt-4-0613 and gpt-3.5-turbo-0613 would work as well.
To Reproduce
For GPT-4:
oaieval gpt-4-0314 coqa-closedqa-correct works
oaieval gpt-4-0613 coqa-closedqa-correct failes
For Chat-GPT:
oaieval gpt-3.5-turbo-0301 coqa-closedqa-correct works
oaieval gpt-3.5-turbo-0613 coqa-closedqa-correct failes
In both cases the error looks similar to this:
...
[2023-06-15 22:45:01,955] [data.py:75] Fetching coqa/samples.jsonl
[2023-06-15 22:45:01,956] [eval.py:34] Evaluating 9 samples
[2023-06-15 22:45:01,959] [eval.py:153] Running in threaded mode with 10 threads!
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9/9 [00:00<00:00, 10.30it/s]
[2023-06-15 22:45:02,841] [record.py:320] Final report: {'invalid_request_during_completion': 9, 'invalid_request_during_evaluation': 0}. Logged to /tmp/evallogs/230615204501HJDDS4FN_gpt-4-0613_coqa-closedqa-correct.jsonl
[2023-06-15 22:45:02,841] [oaieval.py:147] Final report:
[2023-06-15 22:45:02,841] [oaieval.py:149] invalid_request_during_completion: 9
[2023-06-15 22:45:02,842] [oaieval.py:149] invalid_request_during_evaluation: 0

The report looks like this:
{""spec"": {""completion_fns"": [""gpt-4-0613""], ""eval_name"": ""coqa-closedqa-correct.dev.v0"", ""base_eval"": ""coqa-closedqa-correct"", ""split"": ""dev"", ""run_config"": {""completion_fns"": [""gpt-4-0613""], ""eval_spec"": {""cls"": ""evals.elsuite.modelgraded.classify:ModelBasedClassify"", ""args"": {""samples_jsonl"": ""coqa/samples.jsonl"", ""modelgraded_spec"": ""closedqa"", ""modelgraded_spec_args"": {""criteria"": {""correct"": ""correctness: Is the answer correct?""}}}, ""key"": ""coqa-closedqa-correct.dev.v0"", ""group"": ""coqa-ex""}, ""seed"": 20220722, ""max_samples"": null, ""command"": ""env/bin/oaieval gpt-4-0613 coqa-closedqa-correct"", ""initial_settings"": {""visible"": true}}, ""created_by"": """", ""run_id"": ""230615204501HJDDS4FN"", ""created_at"": ""2023-06-15 20:45:01.932724""}}
{""final_report"": {""invalid_request_during_completion"": 9, ""invalid_request_during_evaluation"": 0}}
Code snippets
No response
OS
ubuntu
Python version
Python 3.11
Library version
evals==1.0.3.post1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/1174","Access not working","2023-06-16T20:01:17Z","Closed issue","No label","Describe the feature or improvement you're requesting
(I'm sorry for the noise, but as OpenAI staff said in some threads, they will likely not see replies to closed/merged issues)
In #1121 I was told that GPT4 access had been granted to actually both of my emails, but I don't see it on either one. I actually tried both logging with an OAuth gmail account (what I had done previously) or with the email directly and neither work. Curious if you could take a look @usama-openai
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/1173","Functions: Support for minItems and maxItems for json schema array","2023-06-15T15:39:15Z","Open issue","No label","Describe the feature or improvement you're requesting
First off - let me just say how great functions are! Game changer!
It would be really cool if the functions would respect validation like minItems and maxItems for an array.
 If I explicitly put the number of questions I want in the function description it works, but I half expected it to work when setting minItems. Unfortunately, if I specify minItems: 10 it only returns 5 questions.
I'd imagine other json schema validation options are ignored, and this could be a more generic ticket.
const func = {
  name: ""generateQuiz"",
  description: ""Generate a questions about a given topic"",
  parameters: {
    type: ""object"",
    properties: {
      topics: {
        type: ""array"",
        items: {
          type: ""string"",
          description: ""Topic of the lesson, e.g. Planes"",
        },
      },
      questions: {
        type: ""array"",
        minItems: 10,
        maxItems: 20,
        description:
          ""Questions, e.g. What is the primary function of an airplane?"",
        items: {
          type: ""object"",
          properties: {
            question: {
              type: ""string"",
              description:
                ""Question, e.g. What is the primary function of an airplane?"",
            },
            answer: {
              type: ""string"",
              description: ""Answer for the question"",
            },
          },
          required: [""question"", ""answer""],
        },
      },
    },
    required: [""topic"", ""questions""],
  },};
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
👍15
TheoNolasco, nathanbowser, MikeWindCode, charliemday, irellik, tavyandy97, xseignard, jp1987, mattvb91, PatMyron, and 5 more reacted with thumbs up emoji
All reactions
👍15 reactions"
"https://github.com/openai/evals/issues/1172","Awaiting GPT-4 Access For Further Development","2023-06-16T20:01:26Z","Closed issue","No label","PR #1013 was merged weeks ago and I still haven't been granted access to GPT-4. I mentioned I was interested in further testing and training as a developer, and I made sure to include numerous test cases in my last PR, far more than the requirement. I checked the email on my commit and it was the same as my OpenAI account email, grahamzemel at gmail. Was there an issue on the back-end that didn't match up my account correctly or was I just not granted access at all? If possible, I'd like to make further contributions once GPT-4 becomes available to my email. Thank you.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/1170","Registry path CLI option for oaievalset","2023-06-20T15:03:13Z","Closed issue","No label","Describe the feature or improvement you're requesting
It should take an --registry_path like oaieval does
Additional context
Useful when using as a library, to run eval sets stored in my own project outside this repo.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/1166","Any website where I can share evaluation results?","2023-06-15T04:57:31Z","Open issue","No label","Describe the feature or improvement you're requesting
Hi.
I was wondering if there is any websites where I can share and see others' evaluation results.
 Should I run every 'eval' locally by myself to see the accuracy of 'eval' made by other people?
 Maybe I am missing but I think it would be good if there is a website like llm_leaderboard for evals too.
Thanks
Additional context
If someone wants to share their eval results, do something like:
oaieval gpt-3.5-turbo test-match --submit=True

 The text was updated successfully, but these errors were encountered: 
👍5
anandijain, jwd-dev, vcalatayud, nstankov-bg, and bhack reacted with thumbs up emoji
All reactions
👍5 reactions"
"https://github.com/openai/evals/issues/1131","stock picking as model eval","2023-06-09T18:58:04Z","Open issue","No label","Describe the feature or improvement you're requesting
An idea for an eval which won't suffer from data contamination or overfitting.
maybe use financial news before the opening bell for some top S&P stocks + historical movements.
no need to trade, just use appropriate and standard portfolio metrics
at the very least, it would be an entertaining eval!
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/1126","Extracting and analyzing data from scientific articles","2023-06-08T17:30:22Z","Open issue","No label","Describe the feature or improvement you're requesting
It will be possible to create a benchmark that will evaluate the ability of the model to extract key information from abstracts of scientific articles. For example, information such as:
Object of study (what was studied in the article)
Methodology (what methods were used to conduct the study)
Results (what were the main results of the study)
Conclusions (what conclusions were drawn by the authors based on the results)
Abstracts from real scientific articles can be used as input, and pre-prepared answers that accurately represent the information to be extracted can be used as the ideal answer. This will allow you to assess how well the model is able to understand and analyze scientific texts.
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/1121","Awaiting GPT4 access","2023-06-08T21:20:03Z","Closed issue","No label","PR #500 was merged a week ago, and I don't have GPT4 access on my account alex dot meiburg at gmail -- which is the address listed on my Github account, and my openai account. After checking, I realized that the email attached to the commit was different, timeroot dot alex at gmail. So I made an OpenAI account with that email, to see if it has access, but it doesn't. Wondering where the access ended up, or when I should expect it. (Maybe because the timeroot openai account didn't exist at the time of merge, something didn't go through properly?)
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/1120","Requesting GPT-4 access for merged PR - two week silence","2023-06-07T23:08:10Z","Closed issue","No label","Describe the feature or improvement you're requesting
Hello @andrew-openai ,
I'd hate to bother anyone, but mentioned in issue 1066, GPT-4 access is usually granted within a day of when the eval is merged. However, I got #1006 merged two weeks ago, and I still don't have any GPT4 access. This issue is almost identical to this issue. I also tried leaving a comment on an issue to try to ask the timelines, but got nothing. Should I do anything on my side to get GPT-4 API access?
Thank you!
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/1102","Add version option to oaieval","2023-06-05T07:36:35Z","Open issue","No label","Describe the feature or improvement you're requesting
Like this:
# oaieval  --version
1.0.3.post1

Additional context
No response
 The text was updated successfully, but these errors were encountered: 
👍1
peldszus reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/evals/issues/1099","Allow easier way to package local evals with samples not in the repo","2023-09-26T21:02:25Z","Closed issue","No label","Describe the feature or improvement you're requesting
Currently when using the pip package as a standalone app, the path to the samples_jsonl file has to be either an absolute path, or is relative to the installed module directory. This makes it hard to portable evals that can be used independently of those in the original repo, since the path to the jsonl must be a hard-coded absolute path. It would be more convenient if the tool first looked for the samples file relative to the module dir, and then if it cannot find it there it could look in $(HOME)/.evals/data.
Additional context
See #1098 for an example where we create a local eval. Notice the hard-coded absolute path to the samples.jsonl file.
 The text was updated successfully, but these errors were encountered: 
👍8
robatwilliams, mukgupta, dimaspacestep, lukevs, macrat, cirocavani, chrisprice, and Adam-D-Lewis reacted with thumbs up emoji
All reactions
👍8 reactions"
"https://github.com/openai/evals/issues/1098","pip package is outdated (1.0.3.post1) - has broken elsuite/basic/includes.py","2023-06-04T22:00:51Z","Open issue","bug","Describe the bug
Running an eval that uses the evals.elsuite.basic.includes:Includes class via the latest pip package (1.0.3.post1) as a standalone app gives misleading results.
To Reproduce
Install evals 1.0.3.post1: pip install evals.
Create file test.yaml in $(HOME)/.evals with contents in code snippets section.
Create file samples.json in /tmp/samples.json with contents below:
{""input"": [{""role"": ""system"", ""content"": ""Answer the following questions as concisely as possible.""}, {""role"": ""system"", ""content"": ""What's the capital of France?"", ""name"": ""example_user""}, {""role"": ""system"", ""content"": ""Paris"", ""name"": ""example_assistant""}, {""role"": ""system"", ""content"": ""What's 2+2?"", ""name"": ""example_user""}, {""role"": ""system"", ""content"": ""4"", ""name"": ""example_assistant""}, {""role"": ""user"", ""content"": ""Who is the girl who plays eleven in stranger things?""}], ""ideal"": [""FAKE""]}

In a shell run oaieval gpt-3.5-turbo mytest.
The tool reports an accuracy of 1.0 instead of 0.
Code snippets
my-test:
  id: my-test.dev.v0
  description: Test
  metrics: [accuracy]

game-theory.dev.v0:
  class: evals.elsuite.basic.includes:Includes
  args:
    samples_jsonl: /tmp/samples.jsonl
    ignore_case: true
OS
Linux pop-os 6.2.6-76060206-generic #202303130630168375320722.04~77c1465 SMP PREEMPT_DYNAMIC Wed M x86_64 x86_64 x86_64 GNU/Linux
Python version
Python 3.10
Library version
1.0.3-post1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/1068","Requesting for GPT-4 access for already merged PR","2023-05-31T01:54:37Z","Closed issue","No label","Describe the feature or improvement you're requesting
Hi @andrew-openai ,
As mentioned in #1066 , GPT-4 access is usually granted within a day of when the eval is merged. But my merged PRs:
#837 was merged over a week,
#644 was merged two days ago,
and I still can't find my GPT-4 access at https://platform.openai.com/playground?mode=chat. I've commented in theses PR, but no one replied to me.
Are there any other actions from my side to get GPT-4 API access?
Thank you for your time to resolve this issue!
Best regards,
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/1066","Communicate GPT-4 access grant timelines","2023-05-30T17:26:46Z","Closed issue","No label","Describe the feature or improvement you're requesting
I've observed that several authors of recently merged evaluation PRs have requested GPT-4 access. However, these requests seem to have been met with limited response from the team since May. For example:
eval: add translate japanese written number into arabic numerals #675 (comment)
[Eval] Identify Chinese poem author. #1034 (comment)
Contrastingly, during April, it was quite typical to see responses granting GPT-4 access to authors of merged PRs, as evidenced here: #821 (comment)
Given that most eval contributors are eagerly anticipating GPT-4 access as a result of their merged PRs, a more transparent communication strategy from the team would be appreciated. At present, it appears most contributors anticipate a delay of O(days) to gain access based on past access grants in April.
If this timeframe does not align with the current reality, it would be immensely beneficial for contributors to receive clarity on the expected wait times. This adjustment in expectation would allow them to better plan their experiments and subsequent activities.
Thank you for your attention to this matter!
Best regards,
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/1065","main is broken for model graded","2023-05-30T17:28:51Z","Closed issue","bug","Describe the bug
running model graded fact eval on main fails on python exception
in "".../openai/evals/evals/registry/modelgraded/sql.yaml"", line 5, column 5
expected alphabetic or numeric character, but found '*'

To Reproduce
git pull upstream main
From github.com:openai/evals
branch main -> FETCH_HEAD
 Already up to date.
 oaieval gpt-3.5-turbo coqa-fact
...
 [2023-05-30 08:23:59,114] [registry.py:250] Loading registry from .../openai/evals/evals/registry/modelgraded
 Traceback (most recent call last):
 File "".../openai/evals/.venv/bin/oaieval"", line 8, in
 sys.exit(main())
 ...
 yaml.scanner.ScannerError: while scanning an alias
 in "".../openai/evals/evals/registry/modelgraded/sql.yaml"", line 5, column 5
 expected alphabetic or numeric character, but found '*'
 in ""...openai/evals/evals/registry/modelgraded/sql.yaml"", line 5, column 6
Code snippets
No response
OS
Ubuntu 22.04.1 LTS (on WSL Windows11)
Python version
Python 3.10.6
Library version
latest main
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/1048","Private local memory coordination","2023-05-27T07:55:09Z","Open issue","No label","Describe the feature or improvement you're requesting
The possibility to maintain exclusively on the local system the additional document (es. PDF) to scrub for the application
Additional context
Typical application is being able to support the user in the use of a reserved documentation, like design practices or so. This kind of documentation must be maintained local an confidential.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/1040","Exact Match template","2023-05-26T01:33:15Z","Open issue","No label","Describe the feature or improvement you're requesting
The documentation in eval-templates.md describes basic/match.py as Match: any([b.startswith(a) for b in B]) ""[f]or a model completion a and a reference list of correct answers B. This is a poor fit for arithmetic and other algorithmic tasks, where we want the model response to exactly match some ideal answer, i.e., any([b == a for b in B])
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/1025","The eval_type within ModelBasedClassify is always None","2023-05-24T17:09:08Z","Closed issue","bug","Describe the bug
I've been running OpenAI model-graded evaluations locally. I've noticed that the eval_type instance variable is always set to None regardless of what value is specified in the spec.
Take for example this eval.
coqa-fact-expl:
  id: coqa-fact-expl.dev.v0
  metrics: [accuracy]coqa-fact-expl.dev.v0:
  class: evals.elsuite.modelgraded.classify:ModelBasedClassify
  args:
    samples_jsonl: coqa/samples.jsonl
    eval_type: classify_cot
    modelgraded_spec: fact
The eval_type is set to classify_cot but if you place a print statement before self.eval_type's creation, you'll see that it's None within the eval class.
print(""Eval type passed into ModelBasedClassify"", eval_type) # Shows Noneself.eval_type = eval_type
This results in the relevant ANSWER_PROMPT not being appended to the prompt in the original spec.
To Reproduce
Place print() statement to reveal the value of eval_type
print(""Eval type passed into ModelBasedClassify"", eval_type)
Code snippets
No response
OS
macOS
Python version
Python v3.11.0
Library version
openai-evals v1.0.3.post1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/1023","Travando Chat GPT","2023-05-25T19:18:46Z","Closed issue","bug","Describe the bug
Quando digito o seguinte ""como eu posso You can also gain priority access if you contribute model evaluations to OpenAI Evals"" misturando em Português e em inglês o site do chat GPT bug.
To Reproduce
Code snippets
No response
OS
Windows
Python version
11
Library version
chat.openai.com
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/1022","[eval] Encrypt & Decrypt As An Engima Machine","2023-05-24T08:34:27Z","Open issue","No label","Describe the feature or improvement you're requesting
This is an idea for an eval. In case you think it is valuable, I would like to implement it.
You should be able to provide your Engima settings to OpenAI and your plain input text and it should be able to encrypt the text. You should also be able to provide your ENgima settings to OpenAI and your encrypted input text and it should be able to decrypt the text.
It seems that at this point in time it only sometimes encrypts the values correctly.
It seems that at this point in time, it throws this message when attempting to decrypt:
 ""As a language model AI developed by OpenAI, I should clarify that while I can provide a detailed explanation of how the Enigma machine works, including how the rotors, reflectors, and plugboard contribute to the encryption and decryption process, I don't have the capability to actually simulate the full workings of an Enigma machine in real-time within this text-based interface.
For the process you're describing - taking specific settings, and using those to encrypt or decrypt a message - you would typically need a piece of dedicated software or a specialized website. There are a number of these available online, including open-source software projects, and historical cryptography websites.
Nonetheless, I can guide you on how you could go about implementing a basic version of an Enigma machine in a programming language like Python, or I could provide more detailed explanations of how the Enigma machine works.
Please let me know how you would like to proceed.""
@andrew-openai - do you think this is worth writing an eval on?
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/1012","Cannot pass the check and got this error: KeyError: 'sample'","2023-05-23T12:56:14Z","Open issue","bug","Describe the bug
Hi,
I got the following error when my PR is checked. Here is the link for my PR request. Does anyone know what is happening here? It seems the error is not caused by my yaml.
Processing evals/registry/evals/positive-binary-operations.yaml
Eval Name: positive-binary-operations
[2023-05-22 15:35:02,481] [registry.py:249] Loading registry from /home/runner/work/evals/evals/evals/registry/evals
[2023-05-22 15:35:02,623] [registry.py:249] Loading registry from /home/runner/.evals/evals
[2023-05-22 15:35:02,624] [oaieval.py:110] Run started: 230522153502ICZXYBZ5
[2023-05-22 15:35:02,624] [data.py:75] Fetching positive-binary-operations/fewshot.jsonl
[2023-05-22 15:35:02,625] [data.py:75] Fetching positive-binary-operations/samples.jsonl
[2023-05-22 15:35:02,678] [eval.py:34] Evaluating 10 samples
[2023-05-22 15:35:02,683] [eval.py:153] Running in threaded mode with 10 threads!

  0%|          | 0/10 [00:00<?, ?it/s]
  0%|          | 0/10 [00:00<?, ?it/s]
Traceback (most recent call last):
  File ""/opt/hostedtoolcache/Python/3.9.16/x64/bin/oaieval"", line 8, in <module>
    sys.exit(main())
  File ""/home/runner/work/evals/evals/evals/cli/oaieval.py"", line 164, in main
    run(args)
  File ""/home/runner/work/evals/evals/evals/cli/oaieval.py"", line 141, in run
    result = eval.run(recorder)
  File ""/home/runner/work/evals/evals/evals/elsuite/basic/match.py"", line 53, in run
    self.eval_all_samples(recorder, samples)
  File ""/home/runner/work/evals/evals/evals/eval.py"", line 155, in eval_all_samples
    idx_and_result = list(tqdm(iter, total=len(work_items), disable=not show_progress))
  File ""/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/tqdm/std.py"", line 1178, in __iter__
    for obj in iterable:
  File ""/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/multiprocessing/pool.py"", line 870, in next
    raise value
  File ""/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/multiprocessing/pool.py"", line 125, in worker
    result = (True, func(*args, **kwds))
  File ""/home/runner/work/evals/evals/evals/eval.py"", line 143, in worker_thread
    result = future.result(timeout=timeout)
  File ""/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/concurrent/futures/_base.py"", line 439, in result
    return self.__get_result()
  File ""/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/concurrent/futures/_base.py"", line [391](https://github.com/openai/evals/actions/runs/4782964764/jobs/9054897613#step:7:392), in __get_result
    raise self._exception
  File ""/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/concurrent/futures/thread.py"", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File ""/home/runner/work/evals/evals/evals/eval.py"", line 133, in eval_sample
    return idx, self.eval_sample(sample, rng)
  File ""/home/runner/work/evals/evals/evals/elsuite/basic/match.py"", line 36, in eval_sample
    prompt += s[""sample""]
KeyError: 'sample'
Error: Process completed with exit code 1.

I also got a red cycle on the my yaml file, does anyone know what is that? There is no explanation for that signal. Thanks for any advice and help in advance.

To Reproduce
You may need to replicate the error when you recheck the PR request.
Code snippets
No response
OS
Linux
Python version
Python 3.9
Library version
openai-evals 0.1.1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/1009","Special Token Encoding Error in OpenAI Evals Library","2024-05-31T09:54:49Z","Closed as not planned issue","bug","Describe the bug
When attempting to encode a string containing a special token using the Transformers library in the OpenAI Evals, an error message is displayed. The error seems to indicate that the library is not able to handle the encoding of certain special tokens properly. Here's the error message:
 ', ...}. If you want this text to be encoded as a special token, disable the check for this token by passing disallowed_special=(enc.special_tokens_set - {'<your_special_token>'}). To disable this check for all special tokens, pass disallowed_special=()`.
To Reproduce
Import the necessary libraries and load a pretrained model tokenizer.
Create a string containing a special token.
Attempt to encode the string using the tokenizer.
Observe the error.
Code snippets
from transformers import AutoTokenizer

# Load the tokenizertokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')

# Create a string with a special tokentext = 'This is a [SEP] test.'

# Attempt to encode the stringtokens = tokenizer.encode(text, disallowed_special=tokenizer.special_tokens_set - {'[SEP]'})
OS
macOS
Python version
Python 3.11.1
Library version
openai-evals 1.0.3
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/983","wrong information in Estonian about the product","2023-05-18T17:08:39Z","Closed issue","bug","Describe the bug
Product name is Pedibus forte. Chat GPT gives information that it is against urinal inflammation and writes the wrong compositions. Actually, this product helps with fungal infections.
To Reproduce
chat GPT has to ask what active ingredients are in the product or just ingredients. So, in this case, it can give the right information about products, drugs, cosmetics etc.
 Sometimes the problem of understanding the specification of products is the product name. Depending on countries product with the same ingredients has different names.
Code snippets
No response
OS
windows
Python version
Python
Library version
GPT-3.5
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/980","oaieval fails with error UnicodeDecodeError: 'charmap' codec can't decode byte","2023-05-17T00:25:49Z","Open issue","bug","Describe the bug
Running oaieval fails with a UnicodeDecodeError at line 207 of registry.py. Adding the encoding solves the problem. See Code snippets.
To Reproduce
Run the following at the command line:
 oaieval gpt-3.5-turbo identity
The error only happens attempting a Model Graded eval. Doing a FuzzyMatch works just fine.
Code snippets
The error happens in registry.py on line 207:
    with open(path, ""r"") as f:

Specifying the encoding solves the problem: 
    with open(path, ""r"", encoding='utf-8') as f:
OS
Windows
Python version
3.11.0
Library version
0.27.2
 The text was updated successfully, but these errors were encountered: 
👍1
robatwilliams reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/evals/issues/968","eval approach: GPT4 provides alternative responses with reasoning and then has to pick the best one","2023-05-14T02:43:41Z","Open issue","No label","Describe the feature or improvement you're requesting
I think it would be interesting to see how GPT4 performs on more ideal scenarios, where COT is allowed extensively.
In particular, GPT4 is allowed to think step by step along several different approaches to solve a problem and provide its reasoning. Some prompting should be done to ensure that it tries to look at the problem from different angles / viewpoints / perspectives.
Then, in a second prompt, it's asked to pick from the approach it thinks is most likely to be correct.
If there has been a paper or effort done on evaluating GPT4 for something like this, I'd greatly appreciate a link. :)
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/955","RateLimitError Causes Duplicate Logs and Incorrect Metrics","2023-05-18T17:59:04Z","Closed issue","bug","Describe the bug
If a RateLimit error occurs when running an eval, the evaluation code may run a sample more than once, causing the final reported accuracy to be incorrect.
In my case, a single log file contained these two results (exact same sample_id, but different timestamps indicating different runs):
{""run_id"": ""230510185932RNI3M7IV"", ""event_id"": 184, ""sample_id"": ""note-intervals.dev.316"", ""type"": ""sampling"", ""data"": {""prompt"": [{""role"": ""system"", ""content"": ""You are a note interval calculator. Given two notes, calculate the interval between them. Respond only with the abbreviated interval name (e.g. P4, d5).""}, {""role"": ""user"", ""content"": ""D# F""}], ""sampled"": [""m3""]}, ""created_by"": """", ""created_at"": ""2023-05-10 19:02:26.613346+00:00""}
{""run_id"": ""230510185932RNI3M7IV"", ""event_id"": 185, ""sample_id"": ""note-intervals.dev.316"", ""type"": ""match"", ""data"": {""correct"": false, ""expected"": ""d3"", ""picked"": null, ""sampled"": ""m3"", ""options"": [""d3""]}, ""created_by"": """", ""created_at"": ""2023-05-10 19:02:26.613346+00:00""}

{""run_id"": ""230510185932RNI3M7IV"", ""event_id"": 194, ""sample_id"": ""note-intervals.dev.316"", ""type"": ""sampling"", ""data"": {""prompt"": [{""role"": ""system"", ""content"": ""You are a note interval calculator. Given two notes, calculate the interval between them. Respond only with the abbreviated interval name (e.g. P4, d5).""}, {""role"": ""user"", ""content"": ""D# F""}], ""sampled"": [""m3""]}, ""created_by"": """", ""created_at"": ""2023-05-10 19:02:46.433398+00:00""}
{""run_id"": ""230510185932RNI3M7IV"", ""event_id"": 195, ""sample_id"": ""note-intervals.dev.316"", ""type"": ""match"", ""data"": {""correct"": false, ""expected"": ""d3"", ""picked"": null, ""sampled"": ""m3"", ""options"": [""d3""]}, ""created_by"": """", ""created_at"": ""2023-05-10 19:02:46.433398+00:00""}

This caused the final accuracy to be misreported in both the terminal output and log file as 93/433=0.21478 instead of 93/432=0.21527 (there are only 432 samples in the samples.jsonl file):
[2023-05-10 15:10:24,894] [oaieval.py:149] accuracy: 0.21478060046189376

Obviously this is a minor problem, but is clearly a correctness issue. As far as I can tell hitting a RateLimitError should only cause the requests to back of temporarily and then continue later, and should not indicate an eval failure. I am not familiar enough with the code to suggest a fix.
To Reproduce
Reproducing this issue will depend on your rate limits, but simply find and run a sufficiently large eval such that you observe an openai.error.RateLimitError. Read the log file and check for duplicate sample_ids.
Code snippets
No response
OS
Windows 10
Python version
v3.9.16
Library version
Cloned from source
 The text was updated successfully, but these errors were encountered: 
👀2
Guiccice and lucasklaassen reacted with eyes emoji
All reactions
👀2 reactions"
"https://github.com/openai/evals/issues/952","Is there a list of tasks that chatgpt fails?","2023-05-10T15:09:01Z","Open issue","No label","Hi, is it possible to see a collection of questions that gpt-4 is failing? I wan't to test some prompts I wrote that improve accuracy, and I thought it would be good to focus on tasks that it is currently failing. Thanks!
 The text was updated successfully, but these errors were encountered: 
👍1
qrdlgit reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/evals/issues/947","Remove leading whitespace from sampled answers in Match template","2023-05-09T16:50:14Z","Open issue","No label","Describe the feature or improvement you're requesting
The models below gpt-3.5-turbo (text-davinci-003, text-curie-001, etc.) seem to frequently begin their sampled responses with whitespace (spaces or newlines). This causes the Match eval template to fail, as in this example, where "" m3"" does not match with ""m3"":
{""run_id"": ""2305070508162VM5L4DR"", ""event_id"": 6, ""sample_id"": ""note-intervals.dev.6"", ""type"": ""sampling"", ""data"": {""prompt"": ""You are a note interval calculator. Given two notes separated by a space, you will calculate the interval between them. Respond only with the abbreviated interval name (e.g. P4, d5).\nUser: C Eb\nAssistant: "", ""sampled"": ["" m3""]}, ""created_by"": """", ""created_at"": ""2023-05-07 05:08:17.659629+00:00""}
{""run_id"": ""2305070508162VM5L4DR"", ""event_id"": 7, ""sample_id"": ""note-intervals.dev.6"", ""type"": ""match"", ""data"": {""correct"": false, ""expected"": ""m3"", ""picked"": null, ""sampled"": "" m3"", ""options"": [""m3""]}, ""created_by"": """", ""created_at"": ""2023-05-07 05:08:17.659629+00:00""}

However if we do not use the Match template, and instead use Includes or FuzzyMatch, this introduces false positives for pure matching tasks.
For Includes, we can get that ""A4"" matches witih ""A2"":
{""run_id"": ""230509162956UNZNCLEW"", ""event_id"": 0, ""sample_id"": ""note-intervals.dev.5"", ""type"": ""sampling"", ""data"": {""prompt"": ""What is the interval between the following two notes? Respond only with the abbreviated interval name (e.g. P4, d5).\nUser: C D#\nAssistant: "", ""sampled"": [""A4""]}, ""created_by"": """", ""created_at"": ""2023-05-09 16:29:57.543580+00:00""}
{""run_id"": ""230509162956UNZNCLEW"", ""event_id"": 1, ""sample_id"": ""note-intervals.dev.5"", ""type"": ""metrics"", ""data"": {""accuracy"": 1.0}, ""created_by"": """", ""created_at"": ""2023-05-09 16:29:57.543580+00:00""}

And for FuzzyMatch, we can get that ""A#1"" matches with ""A1"":
{""run_id"": ""230509010553PJ2ND4CN"", ""event_id"": 9, ""sample_id"": ""note-intervals.dev.1"", ""type"": ""sampling"", ""data"": {""prompt"": ""What is the interval between the following two notes? Respond only with the abbreviated interval name (e.g. P4, d5).\nUser: C C#\nAssistant: "", ""sampled"": ["" A#1""]}, ""created_by"": """", ""created_at"": ""2023-05-09 01:05:54.810508+00:00""}
{""run_id"": ""230509010553PJ2ND4CN"", ""event_id"": 10, ""sample_id"": ""note-intervals.dev.1"", ""type"": ""match"", ""data"": {""correct"": true, ""expected"": ""A1"", ""picked"": ["" A#1""]}, ""created_by"": """", ""created_at"": ""2023-05-09 01:05:54.810508+00:00""}
{""run_id"": ""230509010553PJ2ND4CN"", ""event_id"": 11, ""sample_id"": ""note-intervals.dev.1"", ""type"": ""metrics"", ""data"": {""accuracy"": 1.0, ""f1_score"": 1.0}, ""created_by"": """", ""created_at"": ""2023-05-09 01:05:54.810508+00:00""}

So for pure matching tasks, I believe it makes sense to remove leading whitespace from the sampled answers, and would only improve eval correctness.
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/939","Overall Report for Eval Set As a Whole","2023-05-08T11:18:51Z","Open issue","No label","Describe the feature or improvement you're requesting
Eval set is useful for running a group of evals at the same time. Currently eval set is just a collection of independent evals and oaievalset command is simply a wrapper that runs multiple oaieval commands concurrently.
I think it should be useful to analyze the data from eval set as a whole, especially if all evals in the eval sets have the same metric. Under this circumstance, we aim to do same experiment by asking similar questions. We split them into different evals because they are classified by different data. For example, if we want to evaluate LLM's performance on detecting spam in different languages. We want to get accuracy for different languages, as well as the overall detection accuracy for all spams. It would be great if eval set can generate this kind of overall report automatically.
Additional context
This feature request is an Idea for Eval, the framework itself, but not for adding new evals.
 The text was updated successfully, but these errors were encountered: 
👍3
robatwilliams, chrisprice, and boyd-m reacted with thumbs up emoji👀1
kongjiellx reacted with eyes emoji
All reactions
👍3 reactions
👀1 reaction"
"https://github.com/openai/evals/issues/929","docs out of date","2023-05-07T15:41:46Z","Open issue","bug","Describe the bug
cods at https://github.com/openai/evals/blob/main/docs/custom-eval.md are out of date:
the sample in the loop is an array so instead of
                   {""role"": ""system"", ""content"": sample[""problem""], ""name"": ""example_user""},
                   {""role"": ""system"", ""content"": sample[""answer""], ""name"": ""example_assistant""},
               ]
           else:
               prompt += [{""role"": ""user"", ""content"": sample[""problem""]}]```

the code should be:
                prompt += [
                    {""role"": ""system"", ""content"": sample[0][""content""], ""name"": ""example_user""},
                    {""role"": ""system"", ""content"": sample[1][""content""], ""name"": ""example_assistant""},
                ]
            else:
                prompt += [{""role"": ""user"", ""content"": sample[0][""content""]}]

the check_sampled_text function doesn't exist after 64fb72a
To Reproduce
try running code in https://github.com/openai/evals/blob/main/docs/custom-eval.md
Code snippets
No response
OS
macOs
Python version
Python 3.10
Library version
1.0.3.post1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/918","pip install evals throws AssertionError","2023-05-04T14:31:47Z","Open issue","bug","Describe the bug
$ pip install evals
...

python3 -m venv .venvSuccessfully built fire langdetect
ERROR: Exception:
Traceback (most recent call last):
  File ""/home/dan/Build/evals/.venv/lib/python3.10/site-packages/pip/_internal/cli/base_command.py"", line 165, in exc_logging_wrapper
    status = run_func(*args)
  File ""/home/dan/Build/evals/.venv/lib/python3.10/site-packages/pip/_internal/cli/req_command.py"", line 205, in wrapper
    return func(self, options, args)
  File ""/home/dan/Build/evals/.venv/lib/python3.10/site-packages/pip/_internal/commands/install.py"", line 389, in run
    to_install = resolver.get_installation_order(requirement_set)
  File ""/home/dan/Build/evals/.venv/lib/python3.10/site-packages/pip/_internal/resolution/resolvelib/resolver.py"", line 188, in get_installation_order
    weights = get_topological_weights(
  File ""/home/dan/Build/evals/.venv/lib/python3.10/site-packages/pip/_internal/resolution/resolvelib/resolver.py"", line 276, in get_topological_weights
    assert len(weights) == expected_node_count
AssertionError

To Reproduce
python3 --version
# Python 3.10.6

python3 -m venv .venv
source .venv/bin/activate

pip --version
# pip 22.0.2 from /$PWD/.venv/lib/python3.10/site-packages/pip (python 3.10)

pip install evals
# Collecting evals
#  Downloading evals-1.0.3.post1-py3-none-any.whl (7.8 MB)
#     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.8/7.8 MB 16.2 MB/s eta 0:00:00
# Collecting langdetect
# ...
# <as above>

Code snippets
Kernel: 5.15.0-71-generic x86_64 bits: 64 Desktop: Xfce 4.16.0
    Distro: Linux Mint 21.1 Vera
OS
Linux
Python version
Python 3.10.6
Library version
No idea....
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/907","Feature suggestion - Save and Load Conversation History","2023-05-03T14:45:39Z","Open issue","Idea for Eval","Describe the feature or improvement you're requesting
Hello everyone,
I was wondering if anyone else has had the same concern as me about maintaining context and history in ongoing conversations with ChatGPT. Often, I find myself having to repeat previously discussed information when I suspend a conversation and resume it later.
As a solution, I would like to suggest implementing a feature that allows users to save and load conversation history. This way, ChatGPT can maintain context and refer to previous discussions even when a conversation is resumed after a pause. I believe this feature would be particularly useful when working on larger projects, such as writing a book.
So my question to the community is, how do you currently handle this issue? And do you think the suggested feature would be helpful? I'm looking forward to hearing your thoughts on this.
Thank you.
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
👀1
mmtmn reacted with eyes emoji
All reactions
👀1 reaction"
"https://github.com/openai/evals/issues/906","Inaccuracy of AI-generated responses","2023-05-03T14:40:35Z","Open issue","bug","Describe the bug
I would like to take this opportunity to provide feedback on my experience working with ChatGPT. Overall, I found ChatGPT to be a helpful and professional language model, but there were a few issues that need to be addressed.
One of the main issues that I encountered was the incorrect resumption of ChatGPT's work after it had stopped due to the character limit in its response. I also found it challenging to stitch together ChatGPT's responses when they were broken down into multiple blocks. Even when ChatGPT was prompted to continue from where it left off, the second and third blocks of the response were often incorrect.
Additionally, I noticed several grammatical and punctuation errors in ChatGPT's responses, which made it difficult to understand its messages. For example, ChatGPT occasionally gave imprecise answers or responded inadequately to my questions, leading to confusion and wasted time.
Despite these issues, I appreciated ChatGPT's constant efforts to improve its work and correct any errors. Overall, my experience working with ChatGPT was positive, and I am grateful for its assistance and professionalism.
I hope that my feedback can be helpful in improving ChatGPT's performance and ensuring that its responses are accurate and concise. I look forward to continuing to use ChatGPT in my work and projects.
To Reproduce
To reproduce the behavior, please follow these steps:
Install the required dependencies for the Python code, such as the OpenAI API wrapper.
Run the Python code that calls the OpenAI API to generate text.
Make sure that the text generated contains some mistakes or errors, such as incorrect grammar or irrelevant information.
Check the output log for any error messages or exceptions that might have been raised during the code execution.
Try to reproduce the issue by running the same code again with different input parameters or text prompts.
For example, one of the errors that I encountered was the ""AttributeError: 'NoneType' object has no attribute 'strip'"" error, which was caused by a missing value in one of the function arguments. To reproduce this error, you can run the code with an incomplete argument list or with invalid input values.
Code snippets
Traceback (most recent call last):
  File ""main.py"", line 5, in <module>
    result = model.generate(prompt)
  File ""/usr/local/lib/python3.7/dist-packages/openai/api_client.py"", line 150, in generate
    return self.request_api(""completions"", engine=self.engine, prompt=prompt, max_tokens=max_tokens, n=n, temperature=temperature, frequency_penalty=frequency_penalty, presence_penalty=presence_penalty, stop=stop, timeout=timeout, asynchronous=asynchronous, **kwargs)
  File ""/usr/local/lib/python3.7/dist-packages/openai/api_client.py"", line 103, in request_api
    raise exceptions.ApiError(response.status_code, response.text)
openai.error.ApiError: APIError(error='Unable to generate response.', message='Your token does not have access to the requested engine. Please check your API key and available engines at https://beta.openai.com/docs/engines/overview. (HTTP status code: 403)', request='POST https://api.openai.com/v1/engines/davinci-codex/completions', response='{""error"":""Unable to generate response."",""message"":""Your token does not have access to the requested engine. Please check your API key and available engines at https://beta.openai.com/docs/engines/overview. (HTTP status code: 403)""}')
This error occurred when I was trying to generate a response using OpenAI's API. The error message indicates that my API key did not have access to the requested engine (in this case, davinci-codex).
OS
Win11
Python version
3.9
Library version
OpenAI API
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/874","Safety Eval Idea: allergen information of different food products in the Israeli market.","2023-04-30T13:35:31Z","Open issue","No label","identify the allergen information of different food products in the Israeli market as marked on the package
 PR #875
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/873","Are not merged PRs the result of irrelevancy to the model?","2023-04-30T12:43:35Z","Open issue","No label","Describe the feature or improvement you're requesting
Hi, this is not a suggestion, but rather a question.
I have been working on new ideas of evals lately, but none seem to be reviewed.
I was wondering - is this due to the PRs (eval ideas) not being important enough (or not big enough of contribution) for the model?
I'm currently unaware if my way of thinking of ideas of evaluations is right, and perhaps my PRs are not in the right direction, and my way of thinking (and perhaps others as well) should be adjusted in order to contribute better evals.
Example of a PR I lately sent:
#841
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
👍4
qrdlgit, eugene-kim-pipe17, jeffkile, and bhack reacted with thumbs up emoji
All reactions
👍4 reactions"
"https://github.com/openai/evals/issues/848","Idea for Evals: improve abstract logic abilities","2023-04-27T20:27:50Z","Open issue","No label","Describe the feature or improvement you're requesting
I think I found a way how to improve abstract logic and analogies understanding by GPT4. I run a lot of different tests for logical reasoning and I think at general GPT4 solves most of them very well. But it often fails on some kinds of abstract logic relationships retrieval. For such tasks GPT4 often fails even when I ask to provide details and think step by step, even when it's a very short task.
Here is a typical task:
You have a pair of words and you need to find closest ""analogue"" from the given list of words. For example if you have a pair ""Physics-Science"" and have a list:
 ""Light - dark""
 ""Raspberry - berry""
 ""Sea - ocean""
 ""A sheep - a flock""
 You should choose ""Raspberry - berry"" pair, because Raspberry is a subset or a type of berry, just as Physics is a subset or a branch of Science. You probably should mentally iterate all other possible answers to be sure you selected the correct one.
Type of tasks where GPT4 always fails:
system: You have to determine the relationship between the words in the pair, then find the 'analogue', that is, choose among the options 'cipher' - a pair of words with the same logical connection. In your answer, specify only the analogy pair of words with the same logical connection, without explanations and extra text.
 user: A pair to determine the relationship: 'Country - city'
 Potential analogues:
 Light - dark
 Enemy - foe
 raspberry - berry
 Sea - ocean.
 A sheep - a flock.
 The correct answer here is ""Sea - ocean"", but GPT4 usually answers with ""A sheep - a flock"" or ""raspberry - berry"". The difference here in the fact that Country not consist only of cities. Also there are other logical relations based on which sea-ocean is closer. I can explain deeper if needed, tests based on Korobkova method, and has a scientific background.
 GPT4 usually not able to correctly recognize these relation types:
 ""A sheep - a flock"", ""Sea - ocean"", ""Enemy-foe"", and sometimes ""Light - dark"" and ""raspberry - berry"".
 You can check more examples here: PR:806
Value
I'm quite sure if GPT4 will be able to learn this, it will deeper understand context and relations between notions. It will better understand abstract logic and analogies. The impact of this improvement is abstract and will improve the GPT4 in many fields (if it's possible to learn from such examples).
Additional context
I have already implemented it here, but I think gpt will need more examples to improve abstract logic abilities. Is it worth efforts?
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/840","Idea for eval - Bible knowledge as tested in the yearly International Bible youth Contest","2023-04-27T08:06:05Z","Open issue","No label","Describe the feature or improvement you're requesting
Eval name
 bible knowledge in Hebrew
Eval description
 The Bible is the core of the Jewish / Hebrew culture. Once a year there is an international contest in Jerusalem for youth about the bible. It requires them to know the bible by heart and to answer detailed context questions when they are given only small and partial quotes.
What makes this a useful eval?
 Having a model capable to answer it is similar to the deep blue winning Kasparov and can be used for preparation for the students and the contest administration. Making this eval work will make headlines in Israel and the Jewish world. Except for headlines, it can become a tool for students and teachers for contest preparation.
Additional context
PR #784
See an article about the winner of last contest that took place 26 Apr 20203
Event video
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/832","json.decoder.JSONDecodeError: Expecting value: line 2 column 1 (char 1)","2023-12-05T21:26:09Z","Closed as not planned issue","bug","Describe the bug
I enter the command oaieval gpt-3.5-turbo in my Command Prompt, and the result is an error called ""raise JSONDecodeError(""Expecting value"", s, errr.value) from None.
I do not know how to fix this bug as I have done everything that README.md and run-evals.md tell me to do.
C:\Users<username>>oaieval gpt-3.5-turbo 
 [2023-04-26 14:54:44,987] [registry.py:249] Loading registry from C:\Users<username>\AppData\Local\Programs\Python\Python311\Lib\site-packages\evals\registry\evals
 [2023-04-26 14:54:45,049] [registry.py:249] Loading registry from C:\Users<username>.evals\evals
 [2023-04-26 14:54:45,050] [oaieval.py:110] Run started: 230426215445VTUTXQLE
 [2023-04-26 14:54:45,051] [data.py:75] Fetching /.jsonl
 Traceback (most recent call last):
 File """", line 198, in _run_module_as_main
 File """", line 88, in run_code
 File ""C:\Users<username>\AppData\Local\Programs\Python\Python311\Scripts\oaieval.exe_main.py"", line 7, in 
 File ""C:\Users<username>\AppData\Local\Programs\Python\Python311\Lib\site-packages\evals\cli\oaieval.py"", line 164, in main
 run(args)
 File ""C:\Users<username>\AppData\Local\Programs\Python\Python311\Lib\site-packages\evals\cli\oaieval.py"", line 141, in run
 result = eval.run(recorder)
 ^^^^^^^^^^^^^^^^^^
 File ""C:\Users<username>\AppData\Local\Programs\Python\Python311\Lib\site-packages\evals\elsuite\basic\match.py"", line 52, in run
 samples = self.get_samples()
 ^^^^^^^^^^^^^^^^^^
 File ""C:\Users<username>\AppData\Local\Programs\Python\Python311\Lib\site-packages\evals\eval.py"", line 164, in get_samples
 return get_jsonl(self.samples_jsonl)
 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
 File ""C:\Users<username>\AppData\Local\Programs\Python\Python311\Lib\site-packages\evals\data.py"", line 114, in get_jsonl
 return _get_jsonl_file(path)
 ^^^^^^^^^^^^^^^^^^^^^
 File ""C:\Users<username>\AppData\Local\Programs\Python\Python311\Lib\site-packages\evals\data.py"", line 77, in get_jsonl_file
 return list(map(json.loads, f.readlines()))
 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
 File ""C:\Users<username>\AppData\Local\Programs\Python\Python311\Lib\json_init.py"", line 346, in loads
 return _default_decoder.decode(s)
 ^^^^^^^^^^^^^^^^^^^^^^^^^^
 File ""C:\Users<username>\AppData\Local\Programs\Python\Python311\Lib\json\decoder.py"", line 337, in decode
 obj, end = self.raw_decode(s, idx=_w(s, 0).end())
 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
 File ""C:\Users<username>\AppData\Local\Programs\Python\Python311\Lib\json\decoder.py"", line 355, in raw_decode
 raise JSONDecodeError(""Expecting value"", s, err.value) from None
 json.decoder.JSONDecodeError: Expecting value: line 2 column 1 (char 1)
To Reproduce
Create and configure files .yaml and .jsonl for your project
Register the .yaml file and put the jsonl file in the folder in the data folder
Make a "".evals"" folder in your C:\Users<username>
create an ""evals"" folder in "".evals"" and create a blank yaml file called in that folder
Do commands pip insteall -e. and pip install evals
Run command oaieval gpt-3.5-turbo 
Code snippets
No response
OS
Windows 10
Python version
3.11.1
Library version
openai-evals v1.0.3
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/795","Please add an option to change language or understand other languages","2023-04-24T14:52:02Z","Open issue","No label","Describe the feature or improvement you're requesting
Option to change language nor understand other language would make this a better place for everyone using AI tools.
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/791","Accuracy is misreported as 100% although all answers are incorrect.","2023-04-24T18:05:10Z","Closed issue","bug","Describe the bug
I was having a hard time believing that the GPT-3.5-turbo is getting everything correct although ChatGPT gets my questions wrong. So I decided to use some rather boring questions that ChatGPT almost always gets wrong. The reported accuracy was 1.0, which I take to mean 100%. So I added some print statements to the functions that make api calls so I could personally inspect the responses and see if they are accurate. The way I figure, I either get a work around to get better accuracy than ChatGPT, or I reveal some inconsistency in the reported accuracy. It appears that API calls are returning incorrect answers, but somehow the accuracy score is still reported as 100%.
To Reproduce
Step 1. Add ""print(results)"" to each function that makes an api call in api.py and api_utils.py, so in particular, this code in the file 'api.py' :
   result[""expected""] = expected
   result[""match""] = match
   record_match(match, expected=expected, picked=picked, sampled=sampled, options=options)
   return picked

changes to:
   result[""expected""] = expected
   result[""match""] = match
   print(result)
   record_match(match, expected=expected, picked=picked, sampled=sampled, options=options)
   return picked

and two instances of the following code in 'api_utils.py' :
       raise openai.error.APIError(result[""error""])
   return result

changes to:
       raise openai.error.APIError(result[""error""])
   print(results)
   return result

Step 2. I then have a file at ""C:\Python310\Lib\site-packages\evals\registry\data\AISC_areas\AISC-areas.jsonl"" which has these three json lines in it:
{""input"": [{""role"": ""system"", ""content"": ""You will be asked a question about a section property of a structural steel shape based on an AISC shapes database. Respond only with the correct answer as a number with three significant figures.""}, {""role"": ""user"", ""content"": ""What is the cross-sectional area of a WT8X50 beam in square inches?\n\n""}], ""ideal"": ""14.7""}
{""input"": [{""role"": ""system"", ""content"": ""You will be asked a question about a section property of a structural steel shape based on an AISC shapes database. Respond only with the correct answer as a number with three significant figures.""}, {""role"": ""user"", ""content"": ""What is the cross-sectional area of a WT8X44.5 beam in square inches?\n\n""}], ""ideal"": ""13.1""}
{""input"": [{""role"": ""system"", ""content"": ""You will be asked a question about a section property of a structural steel shape based on an AISC shapes database. Respond only with the correct answer as a number with three significant figures.""}, {""role"": ""user"", ""content"": ""What is the cross-sectional area of a WT8X38.5 beam in square inches?\n\n""}], ""ideal"": ""11.3""}

Step 3. Then I have another file at ""C:\Python310\Lib\site-packages\evals\registry\evals\AISC-areas.yaml"" that contains the following:
AISC-areas:
  id: AISC.areas.dev.v0
  description: Test the model's ability to solve beam analysis questions
  metrics: [accuracy]
AISC.areas.dev.v0:
  class: evals.elsuite.basic.fuzzy_match:FuzzyMatch
  args:
    samples_jsonl: AISC_areas/AISC-areas.jsonl

Step 4. Then I run the eval, as so:
 C:\Python310\evals>oaieval gpt-3.5-turbo AISC-areas
 [2023-04-23 20:04:04,514] [registry.py:249] Loading registry from C:\Python310\Lib\site-packages\evals\registry\evals
 [2023-04-23 20:04:04,556] [registry.py:249] Loading registry from C:\Users\james.evals\evals
 [2023-04-23 20:04:04,556] [oaieval.py:110] Run started: 230424030404ZWWLMZUT
 [2023-04-23 20:04:04,568] [data.py:75] Fetching AISC_areas/AISC-areas.jsonl
 [2023-04-23 20:04:04,568] [eval.py:34] Evaluating 3 samples
 [2023-04-23 20:04:04,568] [eval.py:153] Running in threaded mode with 10 threads!
 0%| | 0/3 [00:00<?, ?it/s]{
 ""choices"": [
 {
 ""finish_reason"": ""stop"",
 ""index"": 0,
 ""message"": {
 ""content"": ""11.2"",
 ""role"": ""assistant""
 }
 }
 ],
 ""created"": 1682305444,
 ""id"": ""chatcmpl-78gmipkIMuMUcyUYxLq9Tm87gQId5"",
 ""model"": ""gpt-3.5-turbo-0301"",
 ""object"": ""chat.completion"",
 ""usage"": {
 ""completion_tokens"": 3,
 ""prompt_tokens"": 69,
 ""total_tokens"": 72
 }
 }
 {
 ""choices"": [
 {
 ""finish_reason"": ""stop"",
 ""index"": 0,
 ""message"": {
 ""content"": ""11.8"",
 ""role"": ""assistant""
 }
 }
 ],
 ""created"": 1682305444,
 ""id"": ""chatcmpl-78gmiCZES6xHg1FZF6e4MyoSnJr3E"",
 ""model"": ""gpt-3.5-turbo-0301"",
 ""object"": ""chat.completion"",
 ""usage"": {
 ""completion_tokens"": 3,
 ""prompt_tokens"": 67,
 ""total_tokens"": 70
 }
 33%|████████████████████████████ | 1/3 [00:00<00:01, 1.57it/s]
 {
 ""choices"": [
 {
 ""finish_reason"": ""length"",
 ""index"": 0,
 ""message"": {
 ""content"": ""The cross-sectional area of a WT8X44.5 beam is 11"",
 ""role"": ""assistant""
 }
 }
 ],
 ""created"": 1682305444,
 ""id"": ""chatcmpl-78gmivQG5ooLTuPsvN7PEouj8jjVO"",
 ""model"": ""gpt-3.5-turbo-0301"",
 ""object"": ""chat.completion"",
 ""usage"": {
 ""completion_tokens"": 16,
 ""prompt_tokens"": 69,
 ""total_tokens"": 85
 }
 }
 100%|████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 3.09it/s]
 [2023-04-23 20:04:05,538] [record.py:320] Final report: {'accuracy': 1.0, 'f1_score': 0.0}. Logged to /tmp/evallogs/230424030404ZWWLMZUT_gpt-3.5-turbo_AISC-areas.jsonl
 [2023-04-23 20:04:05,538] [oaieval.py:147] Final report:
 [2023-04-23 20:04:05,538] [oaieval.py:149] accuracy: 1.0
 [2023-04-23 20:04:05,538] [oaieval.py:149] f1_score: 0.0
 [2023-04-23 20:04:05,538] [record.py:309] Logged 9 rows of events to /tmp/evallogs/230424030404ZWWLMZUT_gpt-3.5-turbo_AISC-areas.jsonl: insert_time=0.000ms
C:\Python310\evals>
So all the answers in the API response are wrong in that they don't contain the ""ideal"" answer from the JSON lines, yet accuracy is reported as 1.0.
Code snippets
No response
OS
Windows 11
Python version
Python 3.10.11
Library version
evals in c:\python310\lib\site-packages (1.0.3.post1)
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/787","Eval idea: Security code review for unicode attacks on code","2023-04-24T01:17:21Z","Open issue","Idea for Eval","Describe the feature or improvement you're requesting
One thing a lot of people are using GPT4 is security code review. Anyone with any experience in this area is astounded at its capabilities and the nuanced issues it rises. Companies are building products and features around this as we speak.
One area of security code review it seems to be a bit weak on though is unicode detection. For example, asking GPT4 for a list of unicode attacks and then re-feeding them back to GPT4 for a security code review, it only raises a concern on a few of them.
In that vein, I've come up with a few (what I believe to be) to be high quality and relevant examples, but it requires significant effort and expertise to make sure they are appropriately relevant and diverse, and I'm working on more to get to the 15 example bar.
There are relatively legitimate reasons why in certain cases GPT4 might ignore the issue and so just refeeding them in as I initially did above is not that helpful - though it was a useful proof of concept anyone can quickly do.
The approach I'm taking is to see if the word 'unicode' is included in the response when doing a sec review. I believe the term 'unicode' should very high be on the list of issues that it raises as the usages I've crafted don't make sense unless someone is actively attacking the code.
Would that be sufficient?
Also, it may be this is a limitation of the browser client and not the GPT4 API which goes through different processing - though pushing GPT4 to self-refine it does finally find the unicode character in the original query. Using the browser developer tool for both network and inspect, I can also see the unicode being displayed and sent to the backend.
Perhaps the API is much better at this task, I don't know.
Finally, I'm concerned this might be on the list of 'known issues' due to BPE or something else, which might make the effort not yet relevant, which is perfectly fair.
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/785","Idea for Evals: Count how many numbers are greater than or less than X","2023-04-23T23:16:10Z","Open issue","Idea for Eval","Note: I can develop this feature - creating the issue to get some feedback before development
I don't currently have GPT4 api access, although I do have chatGPT plus. Using the GPT4 engine I have tested this idea with the following examples. I've included screenshots below as evidence of this behavior.
Example 1
input:
 How many times does a number appear that is great than 0? Respond with your answer only.
 3, -3, 3, 4, -4, -2, 1, -4, 5, 0, 3, 0, -4, 4, 1, 0, 1, -4, -1, 5, 0, -3, 1, 5, 3, 4, -2, 0, 5, 1, 1, 3, 2, -4, 3, 0, 5, 4, -2, 1, -3, 0, -2, -3, -5, 0, -2, 0, 1, -1
ideal:
 17
response:
 18
Example 1 retry in a new window
input:
 How many times does a number appear that is great than 0? Respond with your answer only.
 3, -3, 3, 4, -4, -2, 1, -4, 5, 0, 3, 0, -4, 4, 1, 0, 1, -4, -1, 5, 0, -3, 1, 5, 3, 4, -2, 0, 5, 1, 1, 3, 2, -4, 3, 0, 5, 4, -2, 1, -3, 0, -2, -3, -5, 0, -2, 0, 1, -1
ideal:
 17
response:
 19
Example 2
input:
 How many times does a number appear that is great than 0? Respond with your answer only.
 0, 1, -4, 3, -3, 0, -1, 1, -3, 4
ideal:
 4
response:
 5
Let me know what you all think. This would be my first contribution to open source - very exciting!
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/782","Idea for Evals: Sorting numbers with repeats and negatives","2023-04-23T21:01:06Z","Open issue","Idea for Eval","Note: I can develop this feature - creating the issue to get some feedback before developement
Is this diverse from the existing evals or is this too basic? I skimmed through the existing evals and I don't see anything similar except for complex number pattern (#223). I don't currently have GPT4 api access, although I do have chatGPT plus. Using the GPT4 engine I have tested this idea with the following examples:
Example 1
input:
 Sort the following numbers least to greatest (only include the numbers in your response):
 3, 5, 2, 3, 10, 3, 5, 7, 7, 9, 10, 8, 7, 4, 5, 5, 6, 5, 1, 8, 1, 7, 4, 10, 4, 1, 5, 7, 3, 2
ideal:
 1, 1, 1, 2, 2, 3, 3, 3, 3, 4, 4, 4, 5, 5, 5, 5, 5, 5, 6, 7, 7, 7, 7, 7, 8, 8, 9, 10, 10, 10
response:
 1, 1, 1, 2, 2, 3, 3, 3, 3, 4, 4, 4, 5, 5, 5, 5, 5, 6, 7, 7, 7, 7, 8, 8, 9, 10, 10, 10
In this example, GPT4 miscounted the 5s and the 7s
Example 1 retry in a new window
input:
 Sort the following numbers least to greatest (only include the numbers in your response):
 3, 5, 2, 3, 10, 3, 5, 7, 7, 9, 10, 8, 7, 4, 5, 5, 6, 5, 1, 8, 1, 7, 4, 10, 4, 1, 5, 7, 3, 2
ideal:
 1, 1, 1, 2, 2, 3, 3, 3, 3, 4, 4, 4, 5, 5, 5, 5, 5, 5, 6, 7, 7, 7, 7, 7, 8, 8, 9, 10, 10, 10
response:
 1, 1, 1, 2, 2, 3, 3, 3, 3, 4, 4, 4, 5, 5, 5, 5, 5, 6, 7, 7, 7, 7, 7, 8, 8, 9, 10, 10, 10
In this example, GPT4 miscounted the 5s again
Example 2
input:
 Sort the following numbers least to greatest (only include the numbers in your response):
 2, -4, -8, 9, -1, 10, 8, -7, 7, -1, -4, -5, -1, 0, 1, 8, 2, 0, -8, -10, 8, -5, -10, 7, -1, -3, -1, 8, 7, -5, -2, 1, -4, 7, 9, 6, -8, 10, -5, 5, -6, 4, -5, -2, -8, -1, -10, 1, -8, -4
ideal:
 -10, -10, -10, -8, -8, -8, -8, -8, -7, -6, -5, -5, -5, -5, -5, -4, -4, -4, -4, -3, -2, -2, -1, -1, -1, -1, -1, -1, 0, 0, 1, 1, 1, 2, 2, 4, 5, 6, 7, 7, 7, 7, 8, 8, 8, 8, 9, 9, 10, 10
response:
 -10, -10, -10, -8, -8, -8, -8, -8, -7, -6, -5, -5, -5, -5, -4, -4, -4, -4, -3, -2, -2, -1, -1, -1, -1, -1, 0, 0, 1, 1, 1, 2, 2, 4, 5, 6, 7, 7, 7, 7, 8, 8, 8, 8, 9, 9, 10, 10
Let me know what you all think. This would be my first contribution to open source - very exciting!
 The text was updated successfully, but these errors were encountered: 
👍1
qrdlgit reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/evals/issues/768","Does not throttle API requests appropriately.","2023-04-26T19:15:45Z","Closed issue","bug","Describe the bug
When running an eval on a Windows 11 machine, it makes too many requests to the api too quickly. I get a message about once per second saying it is going to back off for 20 seconds.
I've fixed this kind of problem in my own code base before by using something like time.sleep(20). However, I'm not familiar with the entire code base of this repository, so I don't know exactly which line to insert that without doing more research.
To Reproduce
I added ""set OMP_NUM_THREADS=1 &&"" to the beginning in order to try to only run 1 thread.
 The rest of the command is ""oaieval gpt-3.5-turbo finance"", to run the finance eval since that is one of the shorter examples I saw. I've copied and pasted the session below, then I killed it before it completed running:
C:\Python310>set OMP_NUM_THREADS=1 && oaieval gpt-3.5-turbo finance
 [2023-04-23 00:51:13,249] [registry.py:249] Loading registry from C:\Python310\Lib\site-packages\evals\registry\evals
 [2023-04-23 00:51:13,293] [registry.py:249] Loading registry from C:\Users\james.evals\evals
 [2023-04-23 00:51:13,303] [oaieval.py:110] Run started: 230423075113AQXOZKK4
 [2023-04-23 00:51:13,304] [data.py:75] Fetching finance/credit.jsonl
 [2023-04-23 00:51:13,309] [eval.py:34] Evaluating 5 samples
 [2023-04-23 00:51:13,309] [eval.py:153] Running in threaded mode with 10 threads!
 0%| | 0/5 [00:00<?, ?it/s][2023-04-23 00:51:13,483] [_common.py:105] Backing off openai_chat_completion_create_retrying(...) for 0.8s (openai.error.RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-joVgZTYl2HzxPqx2nr8leC1d on requests per min. Limit: 3 / min. Please try again in 20s. Contact support@openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method.)
 [2023-04-23 00:51:13,628] [_common.py:105] Backing off openai_chat_completion_create_retrying(...) for 0.1s (openai.error.RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-joVgZTYl2HzxPqx2nr8leC1d on requests per min. Limit: 3 / min. Please try again in 20s. Contact support@openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method.)
 [2023-04-23 00:51:13,837] [_common.py:105] Backing off openai_chat_completion_create_retrying(...) for 2.2s (openai.error.RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-joVgZTYl2HzxPqx2nr8leC1d on requests per min. Limit: 3 / min. Please try again in 20s. Contact support@openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method.)
 20%|████████████████▊ | 1/5 [00:00<00:02, 1.46it/s][2023-04-23 00:51:14,346] [_common.py:105] Backing off openai_chat_completion_create_retrying(...) for 1.3s (openai.error.RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-joVgZTYl2HzxPqx2nr8leC1d on requests per min. Limit: 3 / min. Please try again in 20s. Contact support@openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method.)
 [2023-04-23 00:51:15,678] [_common.py:105] Backing off openai_chat_completion_create_retrying(...) for 3.9s (openai.error.RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-joVgZTYl2HzxPqx2nr8leC1d on requests per min. Limit: 3 / min. Please try again in 20s. Contact support@openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method.)
 [2023-04-23 00:51:16,128] [_common.py:105] Backing off openai_chat_completion_create_retrying(...) for 4.1s (openai.error.RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-joVgZTYl2HzxPqx2nr8leC1d on requests per min. Limit: 3 / min. Please try again in 20s. Contact support@openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method.)
 [2023-04-23 00:51:19,638] [_common.py:105] Backing off openai_chat_completion_create_retrying(...) for 4.9s (openai.error.RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-joVgZTYl2HzxPqx2nr8leC1d on requests per min. Limit: 3 / min. Please try again in 20s. Contact support@openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method.)
 [2023-04-23 00:51:20,303] [_common.py:105] Backing off openai_chat_completion_create_retrying(...) for 2.1s (openai.error.RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-joVgZTYl2HzxPqx2nr8leC1d on requests per min. Limit: 3 / min. Please try again in 20s. Contact support@openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method.)
 [2023-04-23 00:51:22,493] [_common.py:105] Backing off openai_chat_completion_create_retrying(...) for 5.1s (openai.error.RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-joVgZTYl2HzxPqx2nr8leC1d on requests per min. Limit: 3 / min. Please try again in 20s. Contact support@openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method.)
Code snippets
No response
OS
Windows 11
Python version
Python 3.10.11
Library version
evals in c:\python310\lib\site-packages (1.0.3.post1)
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/765","fix error","2023-04-23T06:23:37Z","Open issue","No label","Describe the feature or improvement you're requesting
This is a feature to detect errors in grammar and paragraphs to report to users
Additional context
import enchant
 import nltk
 from nltk.tokenize import word_tokenize
 from nltk.corpus import wordnet
 from nltk.sentiment import SentimentIntensityAnalyzer
Tạo một đối tượng từ điển với ngôn ngữ tiếng Anh
d = enchant.Dict(""en_US"")
Đọc đoạn văn từ một tệp tin
with open(""text.txt"", ""r"") as f:
 text = f.read()
Tách các câu ra khỏi đoạn văn
sentences = nltk.sent_tokenize(text)
Kiểm tra từng từ trong từng câu và đếm số lỗi chính tả
errors = 0
 for sentence in sentences:
 words = word_tokenize(sentence)
 for word in words:
 if not d.check(word):
 errors += 1
Tính điểm độ đọc được của đoạn văn
score = nltk.text.Text(text).reading_ease()
Tính điểm cảm xúc trung bình của đoạn văn
sia = SentimentIntensityAnalyzer()
 sentiment = sia.polarity_scores(text)
Kiểm tra lỗi ngữ pháp và in ra danh sách các câu chứa lỗi
grammar_errors = []
 for sentence in sentences:
 grammar_errors += [error[2] for error in nltk.parse.util.incremental_parse(sentence, trace=2).trace(0).tree().productions() if str(error[0]) == 'False']
if len(grammar_errors) > 0:
 print(f""Found {len(grammar_errors)} grammar errors:"")
 for error in grammar_errors:
 print(f""- {error}"")
Tìm các từ không tồn tại trong từ điển và gợi ý các từ thay thế
misspelled_words = []
 suggestions = {}
 for sentence in sentences:
 words = word_tokenize(sentence)
 for word in words:
 if not d.check(word):
 misspelled_words.append(word)
 suggestions[word] = d.suggest(word)
if len(misspelled_words) > 0:
 print(f""Found {len(misspelled_words)} misspelled words:"")
 for word in misspelled_words:
 print(f""- {word}: {suggestions[word]}"")
Tìm các từ không đồng nghĩa và gợi ý các từ đồng nghĩa
antonyms = {}
 for sentence in sentences:
 words = word_tokenize(sentence)
 for word in words:
 synonyms = []
 for syn in wordnet.synsets(word):
 for lemma in syn.lemmas():
 synonyms.append(lemma.name())
 if lemma.antonyms():
 antonyms[word] = [antonym.name() for antonym in lemma.antonyms()]
if len(antonyms) > 0:
 print(f""Found {len(antonyms)} words with antonyms:"")
 for word in antonyms:
 print(f""- {word}: {antonyms[word]}"")
In ra các thông tin đánh giá
print(f""\nNumber of
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/746","Eval idea: Simple NLP eval, eg ""list colors without 'e' in the name""","2023-04-22T14:33:58Z","Closed issue","Idea for Eval","Describe the feature or improvement you're requesting
For some reason GPT4 fails on very simple nlp tasks.
Eg, ""I think the word red doesn't have an e in it, right?""
 Response: You are correct. The word ""red"" does not have an ""e"" in it. The spelling is simply ""red.""
There are unlimited permutations of this particular problem that I've found, all of them doable by anyone with basic english skills.
I'd like to do an eval for it, but was hoping to get some feedback that it might be of interest.
Note, not sure how to add an issue with the eval idea label. When you create a new issue, bug / feature / security are the only options.
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/734","Output should accommodate tables","2023-04-22T14:24:11Z","Closed issue","No label","Describe the feature or improvement you're requesting
My App entails project planning and there are many instances whereby the API calls should return tabulated outputs. Right now this is not possible making it a double work for a user. Could you find a way to accommodate the tabulated Reponses from the API calls?
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/733","to check the system analysis, is possible to see the result of the prompt of oaieval?","2023-12-05T21:27:28Z","Closed issue","No label","Describe the feature or improvement you're requesting
not just the final result'
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/715","Update documentation regarding custom evals","2023-04-18T15:09:34Z","Open issue","No label","docs/custom-eval.md discusses how to make an eval with custom code, however it is not mentioned anywhere in docs/custom-eval.md or docs/build-eval.md that these evals are not being accepted currently. It took me a bit of digging around to figure out that these PR's are not being accepted currently. I realized this first seeing that the folder mentioned in the docs (/evals/evlsuite) is devoid of any contributions from others. Then I dug around and found out that #520 was denied because of this reason.
I'd love it if this restriction was lifted because I have an idea for an extremely useful use-case (summarizing text to a specific range of word count e.g. 50-60 words) that does require custom code evals.
 The text was updated successfully, but these errors were encountered: 
👍1
lucianosb reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/evals/issues/711","Where to find the experiment comparation: Using the data of training reward model for fine-tuning without reinforcement learning.","2023-04-18T03:10:51Z","Open issue","No label","Thank you very much!
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/706","Taxonomy to use for model evaluation?","2023-04-17T17:49:59Z","Open issue","No label","I know folks are working on this, any consensus around which are the ones to look at?
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/690","windows encoding","2023-04-16T16:17:30Z","Closed issue","No label","Hi everybody!
 Trying to run eval in windows 10, visual studio code:
When I try: oaieval gpt-3.5-turbo logic-fact
 or: oaieval gpt-3.5-turbo test-match
appears the message:
 ""...lib\encodings\cp1252.py"", line 23, in decode
 return codecs.charmap_decode(input,self.errors,decoding_table)[0]
 UnicodeDecodeError: 'charmap' codec can't decode byte 0x81 in position 783: character maps to ""
Any help?
 Thank you in advance! :)
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/689","Mandarin homophones eval 🇨🇳","2023-04-15T18:32:10Z","Open issue","No label","Homophones are two or more words having the same pronunciation but different meanings, for example, 'rose' (flower) and 'rose' (rise) in English. Currently, I'm learning Mandarin using ChatGPT, and I realized it makes mistakes when identifying tones in Mandarin. These are the four tones in Mandarin:
Tone	Tone Description	Examples
High tone	Flat and high pitch	妈妈 (māma) - mother
Rising tone	Starts low and rises to a high pitch	麻 (má) - numb, hemp
Falling-rising tone	Starts high, falls, then rises again	你好 (nǐ hǎo) - hello
Falling tone	Starts high and falls to a low pitch	不 (bù) - not
The same sound for practical purposes is just the same Pinyin (romanization of Mandarin, ex. nǐ hǎo), but as you can see, ChatGPT and GPT-4 both make errors when differentiating tones.
Here are some examples:
As you can see, there are many errors in 2, 3, 4, and so on. I've tried many times.
Also, GPT-4 makes the same type of mistakes. It seems to be unable to differentiate tones accurately. I highlighted some errors, they should have had the same Pinyin like the other examples.
I've compiled a list of Homophones in Mandarin, to provide some examples for Evals.
 Is this something of interest to OpenAI？ I'll submit a PR if so.
Let me know! 🙋🏻‍♂️
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/685","I believe that in order for GPT-4 to create by itself many inventions through two month, necessary that...","2023-04-15T14:33:04Z","Open issue","No label","Discussed in #621
Originally posted by 55255ru April 10, 2023
 Hello. I suggest using what is written on my website (which is here http://www.55255.ru) to improve GPT-4 because I believe that in order for GPT-4 to create by itself many inventions through two month, necessary that programmers (who are now engaged in improving GPT-4) use what is written on my website (which is here http://www.55255.ru) for improve GPT-4. For confirm this I will provide the following:
Computer by itself created 40000 inventions
 Hello. I believe that with the help my work (which is outlined here http://www.55255.ru/) four companies (independently of each other) created programs, with the help each of these programs, the computer by itself can invent many inventions. As a result of this, the computer by itself created 40000 inventions. The addresses of the sites of these companies such http://www.method.ru/, https://www.truemachina.com/, https://www.tris-europe.com/software/innovationssoftware.htm, https://imagination-engines.com/
 But the creators of these programs apparently has not published information that they have used (I suppose) my abovementioned work for create these programs. Thanks to this our company striving for creation for the fifth time with the help of this my work of the program using which a computer could independently invent many inventions. I offer you cooperation in this.
 The computer with the help of the program ""True Machina"" created 40000 (forty thousand) inventions, this is Tsurikov said at the end of the film which is located at https://www.youtube.com/watch?v=0by8g0G0HRI and at the end of this film Tsurikov said that he (and his company) for to create the program ""True Machina"" used published invention methods which he and his company, did not create. It was only at the end of 2018 that the computer began to create inventions by itself through the program created by Tsurikov and his assistants. I published this my work in 1981 year. My e-mail 275527@gmail.com
 Yours faithfully, Shmonov Aleksandr
88 (4) Computer by itself created 40000 inventions это сайта для сайта github.com и для GPT 4.pdf
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/652","Eval making guide?","2023-04-12T12:57:00Z","Open issue","No label","I am not proficiently in coding or using Github, but I would love to help making evals, sadly there is no coherent guide of how to.
Can someone please make an guide of how to make and post an eval?
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/637","Idea for Evals: Emotion and sentiment analysis Evals","2023-04-11T20:58:36Z","Open issue","Idea for Eval","Understanding emotions and sentiments is an essential aspect of human communication. The system's ability to recognize these emotions enables more appropriate, context-aware, and empathetic responses. Emotions and sentiments can be complex and nuanced, requiring a system to perform fine-grained analysis and interpretation. Evaluating a system's performance in this area provides insights into its overall ability to process and understand subtleties in language.
There are several different emotions that Evals could be written for:
Sarcasm (already covered in Sarcasm Detection #56 )
Positive sentiment (e.g., happiness, joy, excitement, gratitude)
Negative sentiment (e.g., anger, frustration, sadness, disappointment)
Neutral sentiment (e.g., indifference, objectivity, impartiality)
Fear (e.g., anxiety, apprehension, panic, terror)
Surprise (e.g., amazement, astonishment, bewilderment)
Love (e.g., affection, adoration, romance, passion)
Anticipation (e.g., eagerness, hope, curiosity)
Disgust (e.g., revulsion, repulsion, aversion, contempt)
Empathy (e.g., compassion, understanding, sympathy)
Confusion (e.g., perplexity, puzzlement, disorientation)
Pride (e.g., self-esteem, arrogance, satisfaction)
Envy (e.g., jealousy, covetousness, resentment)
Guilt (e.g., remorse, regret, shame)
Trust (e.g., confidence, faith, reliance)
Skepticism (e.g., doubt, disbelief, suspicion)
I suppose this could be covered in one Eval or many. If many, it might be good to come up with some sort of structure for organizing Evals for different sentiments or at least documenting what was covered already and what has yet to be covered. Sarcasm, for instance, was covered with an eval that contained news articles from the Onion. While article headlines are certainly one way of testing for sarcasm, others could be a dataset of tweets or reviews of products or examples from fiction.
 The text was updated successfully, but these errors were encountered: 
👍2
andrew-openai and showneykim reacted with thumbs up emoji
All reactions
👍2 reactions"
"https://github.com/openai/evals/issues/636","I believe that in order for GPT-4 to create by itself many inventions through two month, necessary that...","2023-04-11T20:13:19Z","Closed issue","No label","Hello. I suggest using what is written on my website (which is here http://www.55255.ru) to improve GPT-4 because I believe that in order for GPT-4 to create by itself many inventions through two month, necessary that programmers (who are now engaged in improving GPT-4) use what is written on my website (which is here http://www.55255.ru) for improve GPT-4. For confirm this I will provide the following:
Computer by itself created 40000 inventions
 Hello. I believe that with the help my work (which is outlined here http://www.55255.ru/) four companies (independently of each other) created programs, with the help each of these programs, the computer by itself can invent many inventions. As a result of this, the computer by itself created 40000 inventions. The addresses of the sites of these companies such http://www.method.ru/, https://www.truemachina.com/, https://www.tris-europe.com/software/innovationssoftware.htm, https://imagination-engines.com/
 But the creators of these programs apparently has not published information that they have used (I suppose) my abovementioned work for create these programs. Thanks to this our company striving for creation for the fifth time with the help of this my work of the program using which a computer could independently invent many inventions. I offer you cooperation in this.
 The computer with the help of the program ""True Machina"" created 40000 (forty thousand) inventions, this is Tsurikov said at the end of the film which is located at https://www.youtube.com/watch?v=0by8g0G0HRI and at the end of this film Tsurikov said that he (and his company) for to create the program ""True Machina"" used published invention methods which he and his company, did not create. It was only at the end of 2018 that the computer began to create inventions by itself through the program created by Tsurikov and his assistants. I published this my work in 1981 year.
88 (4) Computer by itself created 40000 inventions.pdf
          Yours faithfully, Shmonov Aleksandr

 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/632","Idea for Evals: Complex, multi-turn instruction-following Evals","2023-04-11T06:00:50Z","Open issue","Idea for Eval","Hello everyone, thank you for contributions so far, I've been working through them and these tasks are forming a challenging a comprehensive benchmark for modern LLMs and LLM programs. We worked on Completion Functions last week, which we're glad to have merged (more info coming), and I'll be returning to merging Eval submissions this week.
I want to try an idea where people can open issues that describe ideas for evals, tagged with the Idea For Eval label. If anyone has some relevant data or thinks it would be interesting to tackle that idea, they can open an Eval contribution PR with the same tag. Anyone is free to open Idea for Eval issues, especially if you are building an application or have a field of study for which you'd think getting some Evals could help your development process.
I'll start with an Idea, we know that our models can struggle with complex multi-turn instructions, especially if the instructions are domain relevant. If you have any tasks like this, please open a PR with the eval and we'll merge it into the benchmark.
We're reducing the 100 sample limit on contributions to 15, in order to encourage more one-off samples or handwritten example contributions like this.
Thanks!
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/629","How deterministic should evals be?","2023-04-10T23:36:58Z","Open issue","No label","How deterministic should evals be? For example, one area of poor performance of GPT-4 is in evaluating whether a given joke is funny.
There are Twitter feeds that consist almost entirely of jokes. Some are quite popular, and the best jokes get many retweets. So, the jokes that get the most retweets in such a feed could be assumed to be funnier than jokes that get the least. I have been manually examing such feeds, and I do find that that criterion is very consistently the same as my personal judgment of pairs of jokes, where one is amount the most-retweeted and the other is among the least-retweeted within a particular feed.
So, I am thinking that evals could be useful which ask GPT to judge which joke of a joke pair is funnier, where the acceptable answer is the one with the most retweets. To make sure the judgements are as objective as they can be, the pairs would always contain one of the most retweeted jokes and one of the least from the same twitter account, which would be a popular one such the retweets can be in the thousands.
But this type of eval nevertheless has a clear subjective component.
Are such evals acceptable?
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/625","Can't run model-graded evals [solved]","2023-04-16T12:39:30Z","Closed issue","No label","Every time I run a model-graded eval, even the ones included for example, I get the error
UnicodeDecodeError: 'charmap' codec can't decode byte 0x81 in position 828: character maps to <undefined>
Right after the log [registry.py:156] Loading registry from [PATH]\evals\evals\registry\modelgraded
I'm running the evals on Jupyter Lab in a Windows 11 with a python 3.10 environment. I'm using all the latest commits.
Is this a bug related to windows or the evals library?
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/624","Why？","2023-04-11T18:50:38Z","Closed issue","No label","I have been waiting for almost a month now, why still haven't I obtained the qualification to use ChatGPT-4
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/620","C","2023-04-11T18:50:45Z","Closed issue","No label","No description provided.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/615","Permission denied when trying to push code","2023-04-10T08:10:35Z","Closed issue","No label","I'm trying to contribute an eval. However, I can't seem to push to the repository:
michail@Michails-MacBook-Pro evals % git push --set-upstream origin michailmelonas-add-letter-count
ERROR: Permission to openai/evals.git denied to michailmelonas.
fatal: Could not read from remote repository.

Please make sure you have the correct access rights
and the repository exists.

I was able to clone the repository with no issues, and can still push to my own private repositories.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/595","ignore_case option for the Includes eval","2023-04-12T22:39:44Z","Closed issue","No label","I have been creating a new eval using the Includes template. I was surprised to see that there is no option to ignore the casing and so I added it directly to the class and tried it out. It works fine and I would like to make a pull request to add this argument. It would be used in the registry yaml file like so:
spanish_english_idioms:
  id: spanish_english_idioms.test.v1
  metrics: [accuracy]spanish_english_idioms.test.v1:
  class: evals.elsuite.basic.includes:Includes
  args:
    samples_jsonl: spanish_english_idioms/samples.jsonl
    ignore_case: true
Would anyone be interested in this? I feel like it would be helpful to have as an option for folks that do not want to or know how to code. If this functionality already exists somewhere in the repo, let me know and I'll close this issue out.
 The text was updated successfully, but these errors were encountered: 
👍4
pyamin1878, tgf9, andrewnguonly, and shahbhavik01 reacted with thumbs up emoji
All reactions
👍4 reactions"
"https://github.com/openai/evals/issues/586","Vision Eval for engineering examination question bank.","2023-04-05T05:09:00Z","Open issue","Idea for Eval","I have an engineering exam bank of about 1000 questions with simple illustrations. I have the questions already in JSONL format but some of them rely on the image to answer correctly.
Here is an example question without images.
{""question"": ""The exciter field current meter can be described as:"", ""options"": [""a. A meter that measures and records the amount of power produced by the generator."", ""b. A measurement of the reactive power being generated by the AC generation system."", ""c. An indicator of the AC kilowatts the generator is producing."", ""d. A measurement of the inductive power being generated by the AC generation system."", ""e. Giving an indication of the DC current that is being supplied to the generators field windings.""], ""answer"": ""e""}
Answer stripped from prompt
 GPT-3.5 output:
The exciter field current meter can be described as giving an indication of the DC current that is being supplied to the generator's field windings. Therefore, the correct option is (e) ""Giving an indication of the DC current that is being supplied to the generator's field windings."" The exciter field current meter is used to monitor and control the DC current supplied to the generator's field windings, which in turn determines the strength of the magnetic field produced and the amount of power that can be generated by the generator.
GPT-4 output:
The exciter field current meter can be described as:
e. Giving an indication of the DC current that is being supplied to the generators field windings.
I would like to make an eval for questions like this:
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/575","Run evals with gpt-4?","2023-04-05T19:55:46Z","Closed issue","No label","The evals I am creating won't get good results on gpt-3.5. They need gpt-4. I have tried them manually with ChatGPT with the two models, and 3.5 can't do it while 4 can.
But I don't see how to run evals with gpt-4. oaieval gpt-3.5-turbo test-match works fine on my machine, but oaieval gpt-4 test-match fails with the error, ""ValueError: Couldn't find model: gpt-4"".
I assume the issue is that ""GPT-4 is currently in a limited beta and only accessible to those who have been granted access"", and I'm still on the waitlist for that.
I further assume that, even though the evals will only get good results on gpt-4 and I can't run them on gpt-4, you will still find my evals useful. (Or, you could give me gpt-4 access... :) :) )
Let me know if I'm wrong.
 The text was updated successfully, but these errors were encountered: 
😕1
perth-seo-agency reacted with confused emoji
All reactions
😕1 reaction"
"https://github.com/openai/evals/issues/574","Can't run evals?","2023-04-22T14:38:01Z","Closed issue","No label","When I got on the API waitlist, the email I received said that if I contribute evals that get merged, I can gain priority access. And it happens that I do have prompts and responses that I think would make for useful evals.
It seems like, in order create and test my evals, I should be able to run them. Since I'm a paid ChatGPT Plus subscriber, I would assume that wouldn't be any problem.
But when I try to run
oaieval gpt-3.5-turbo test-match
I get never-ending errors, even with a new API Key:
oaieval gpt-3.5-turbo test-match
[2023-04-03 13:00:46,754] [registry.py:153] Loading registry from /Users/garyrob/Source/openai/evals/evals/registry/evals
[2023-04-03 13:00:46,817] [registry.py:153] Loading registry from /Users/garyrob/.evals/evals
[2023-04-03 13:00:47,464] [oaieval.py:187] Run started: 2304032000473QO4OU2X
[2023-04-03 13:00:47,469] [data.py:75] Fetching test_match/samples.jsonl
[2023-04-03 13:00:47,469] [eval.py:32] Evaluating 3 samples
[2023-04-03 13:00:47,520] [eval.py:152] Running in threaded mode with 10 threads!
  0%|                                                                                                              | 0/3 [00:00<?, ?it/s][2023-04-03 13:00:47,777] [_common.py:105] Backing off openai_chat_completion_create_retrying(...) for 0.4s (openai.error.RateLimitError: You exceeded your current quota, please check your plan and billing details.)
[2023-04-03 13:00:47,778] [_common.py:105] Backing off openai_chat_completion_create_retrying(...) for 0.1s (openai.error.RateLimitError: You exceeded your current quota, please check your plan and billing details.)
[2023-04-03 13:00:47,872] [_common.py:105] Backing off openai_chat_completion_create_retrying(...) for 1.0s (openai.error.RateLimitError: You exceeded your current quota, please check your plan and billing details.)
[2023-04-03 13:00:47,970] [_common.py:105] Backing off openai_chat_completion_create_retrying(...) for 2.8s (openai.error.RateLimitError: You exceeded your current quota, please check your plan and billing details.)
[2023-04-03 13:00:48,278] [_common.py:105] Backing off openai_chat_completion_create_retrying(...) for 2.1s (openai.error.RateLimitError: You exceeded your current quota, please check your plan and billing details.)
[2023-04-03 13:00:48,927] [_common.py:105] Backing off openai_chat_completion_create_retrying(...) for 0.5s (openai.error.RateLimitError: You exceeded your current quota, please check your plan and billing details.)

What can I do about this? Or should I just submit evals without being able to run them??
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/569","سلام خوبی","2023-04-11T18:55:11Z","Closed issue","No label","No description provided.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/567","Data formatting step outputs incorrectly keys for JSONL","2023-04-03T02:37:33Z","Open issue","No label","https://github.com/openai/evals/blob/main/docs/build-eval.md
Reproduction steps:
 Have a csv with prompt and completion columns
 Run openai tools fine_tunes.prepare_data -f data.csv
 Inspect output JSONL file and verify entries have prompt and completion keys but not the required input key
It may very well be that the csv column names should be different, but prepare_data complains about the input not having prompt and completion columns. It would be helpful if the docs explained what columns are required.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/562","Increased API error rate","2023-04-11T18:56:40Z","Closed issue","No label","I've gotten six or seven API errors in a row when trying to use GPT4. When I reload the page, it switches back to 3.5.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/560","Eval requirements when using GPT-4 for renovation price estimations?","2023-04-02T14:44:53Z","Open issue","No label","Can I for example post an eval with pictures of houses (extracted from Matterport 360 Virtual Tours) that needs to be renovated and say that GPT-4 hallucinates most of the time what part of the house is shown inside the picture in the first place, it would for example say the kitchen is a bathroom and such, but also simply fails to accurately estimate the price for the renovation like construction contractors could do, and AGI therefore should be able to do?
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/545","Move/Include directions on how to contribute a simple eval into top-level README.md","2023-04-01T10:14:38Z","Open issue","No label","I feel like one primary aim of this project is to receive high quality evals.
Instead of the docs folder, I propose that the top-level evals/README.md should include directions on how to contribute a simple eval.
I don't propose the entirety of evals/docs/build-eval.md should be moved into evals/README.md; however, I propose something minimal and simple be included.
I recognize it isn't a big effort to go to evals/docs/build-eval.md but I feel like if eval contribution is a primary aim, directions should be at the forefront (versus the periphery).
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/543","Fantastic work! Thank you very much for helping me！","2023-04-05T19:56:07Z","Closed issue","No label","This job has brought great convenience to my research content! Sincere thanks to the staff!
 The text was updated successfully, but these errors were encountered: 
🎉1
kuangxiaoye reacted with hooray emoji
All reactions
🎉1 reaction"
"https://github.com/openai/evals/issues/542","A way to evaluate GPT models","2023-04-01T00:41:39Z","Open issue","No label","One of the things that's interesting to me is how well GPT can autonomously improve AI code.
I created this example here https://github.com/qrdlgit/gpt and it gets to about 0.972 in classifier accuracy after about 5 runs.
The ML problem is fairly basic, but it's just used as an example here. More sophisticated problems can be used, though some attention to the max context needs to be made.
Also, using problems which are unique and not easily solved via training data would be required for fair evaluation. Ideally a broad cross section of different problems would be useful to capture a more accurate assessment.
One of the next things I'm going to work on is pushing this up into RLHF (RLCF?) by defining a reward model specific to this domain.. Ie, does the response compile and run, provide intelligible metrics, was there an improvement, etc.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/536","Bad Language","2023-04-13T21:06:20Z","Closed issue","No label","Hello, maybe its a wrong repository to share my feedback, but hope community will direct me to the right one
 I am working on a personal project about surfing conditions. I asked chatGPT-3.5 turbo to talk like a characters from Futurama, depending on a surfing conditions, and thats what I got as a response. Screenshot
 What do you think about a potential risks of this)))?

 The text was updated successfully, but these errors were encountered: 
😄2
vcidst and st4s1k reacted with laugh emoji
All reactions
😄2 reactions"
"https://github.com/openai/evals/issues/529","Thatoneguy","2023-04-05T19:56:48Z","Closed issue","No label","No description provided.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/528","Performing evals on other models for comparison","2023-04-11T19:49:45Z","Closed issue","No label","Hi all,
Is there an easy way to add another LLM to compare performance via an API endpoint? I've tried playing around with the repo some, but the task seems nontrivial due to the use of the openai library.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/512","How to dynamically change API parameters through oaieval CLI tool?","2023-03-30T00:03:32Z","Closed issue","No label","I'm trying to run an eval based on evals.elsuite.basic.match:Match class but I can't find a way to modify API parameters such as temperature and max_tokens through the CLI. What is the current solution that the framework offers to this problem?
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/505","Request for Repo: ""Open Source, Daily Auto-Generated SOTA LLM Model Benchmarks""","2023-03-29T15:33:35Z","Open issue","No label","✨ Open Source Daily Auto-Generated SOTA LLM Model-Comparisons Repository
(Sorry for posting here, just not sure where to ask)
Does ^^ this already exist?
If So, Where??
EcoSystem Graphs Does not appear to have benchmarks.
It's hard to keep up.
All the open source LLM/AI repositories are becoming impossible for basically any human to keep pace with.
There are many awesome ""snippets"" that are posted in public channels, but not all models hold up well and generalize after practice.
💭 Feature Requests
Here's what I'd like to see: (Please add your own in comments)
License
GPLv3.0 Affero
Update Cadence
Daily Evaluation Runs -- auto-updating the Github Repo with up-to-date Evaluation Results as described below.
Eval Result DB
Description
Each time the cron job is run (daily) the evaluations should be written into a database.
Properties
The Output of the Cron Job should be a set of entries into the EvalDB, showing:
 (1) The prompt / Input
 (2) The model, including its known current parameters and limitations at the time of the eval run
 (3) The output
Up-To-Date Model DB
Model DB Description
There should be an updated database of models which is displayed on the README.md
Model DB Properties
This should include the following:
API or Self-Hosted
This is critical for both speed and price reasons.
Modes & Mode Parameters
Text 
Chat vs. Standard Completion
Cost-Per-Token
Average Speed-Per-Token
Context Length
Unicode-Support (most models?)
Image 
Max Context Size
Cost-Per-Pixel(?)
Video 
Max Context Length
Max Input
Included Training Datasets
Tags
A free-form field for anything that doesn't fit into the above schema.
Does Something Like This Exist?
?????
What else do we need?
?????
I'm trying to make something to fill this niche myself and will link here shortly.
 The text was updated successfully, but these errors were encountered: 
🚀1
mdatsev reacted with rocket emoji
All reactions
🚀1 reaction"
"https://github.com/openai/evals/issues/503","Should data be balanced in a classification evals?","2023-03-29T23:55:55Z","Closed issue","No label","I have a classification eval with possible answers Y or N. Is it okay if I have more N than Y examples? What if the model is more likely to produce Y due to hallucinations and having more N would artificially lower the score?
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/485","""Includes"" evaluation does not work if sample[""ideal"" ] is str and not list","2023-04-26T23:00:26Z","Closed issue","No label","eval_sample function in Includes evaluation does not work adequately if sample[""ideal""] is a string. The code below checks whether each item in sample[""ideal""] occurs in the generated text. If sample[""ideal""] is a string, it loops through its characters (leading to a wayyy higher evaluation scores than expected).
        includes_answer = any(
            [evals.elsuite.utils.get_answer(sampled, ref) for ref in sample[""ideal""]]
        )

 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/484","""Includes"" evaluation returns False when ""ideal"" text is at the end of generated text","2023-07-20T16:31:02Z","Closed issue","No label","I have a dataset for evaluation where the ""ideal"" answer is expected to be at the end of the generated text. E.g. if ""XXX"" is the ideal answer, generated text should look like: ""some generated text XXX"".
eval_sample function in Includes class calls evals.elsuite.utils.get_answer function:
def get_answer(text, answer_prompt):
    idx = text.rfind(answer_prompt)
    if idx == -1:
        return None
    return text[idx + len(answer_prompt) :]

In my scenario, idx + len(answer_prompt) equals the lengths of the text, and the get_answer function returns empty string. This leads to includes_answer being False:
        includes_answer = any(
            [evals.elsuite.utils.get_answer(sampled, ref) for ref in sample[""ideal""]]
        )

But the generated text actually does include my ideal snippet, so I believe includes_answer should be True here.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/480","Can the official website generate an API specifically for client use_ Key, which reduces the threshold for pure client developers to use, and has reached the promotion and use role of openAI","2023-05-16T21:54:02Z","Closed issue","No label","Can the official website generate an API specifically for client use_ Key, which reduces the threshold for pure client developers to use, and has reached the promotion and use role of openAI
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/479","Unable to run evals","2023-03-28T13:42:54Z","Closed issue","No label","I could use some help figuring out the cause of the following error.
Whenever i try to run an eval, i get an error similar to the one below:
FileNotFoundError: [Errno 2] No such file or directory: '/Users/wmn/.pyenv/versions/3.11.2/lib/python3.11/site-packages/evals/registry/data/test_match/samples.jsonl'

I followed the instructions to download all evals; running:
git lfs fetch --all
git lfs pull

I can also confirm that the test_match/samples.jsonl file does exist.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/459","Doesn't answer every aspect of the task","2023-03-28T17:31:58Z","Closed issue","No label","On the openai playground, the ai doesn't answer every aspect of the question. I asked it to do a 20 line essay about how ww2 was a annihilation war (for school) in French - and it didn't answer the entire task. Missed out on defining the term ""annihilation war"" like I asked it to and did only 5 lines about the subject. I asked the same thing to the public version of chatgpt and it did it no problem. Might be a bug.
 Thanks for your great product. Truly a step forward in the history of mankind.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/458","A little Bug","2023-03-26T14:31:38Z","Closed issue","No label","On the openai playground, the ai doesn't answer every aspect of the question. I asked it to do a 20 line essay about how ww2 was a annihilation war (for school) in French - and it didn't answer the entire task. Missed out on defining the term ""annihilation war"" like I asked it to and did only 5 lines about the subject. I asked the same thing to the public version of chatgpt and it did it no problem. Might be a bug.
 Thanks for your great product. Truly a step forward in the history of mankind.
 YX
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/442","Integrations","2023-03-28T17:34:04Z","Closed issue","No label","Plug-ins, integration interfase, image upload, image interpretation, API, interact directly with other platforms. WhatsApp Integration, Automation, AI.
 The text was updated successfully, but these errors were encountered: 
🚀1
jimmywilk reacted with rocket emoji
All reactions
🚀1 reaction"
"https://github.com/openai/evals/issues/438","Wolfram Plugin - Should it be taken into account when making evals? [Question]","2023-03-25T07:02:29Z","Closed issue","No label","I was curious about this part:
README.md
 ""(...)measure the quality of completions provided by an OpenAI model, and compare performance across different datasets and models""
So, should the Wolfram Plugin be taken into consideration when making new evals?
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/435","Generate JSONL from other formats: xlsx, csv, tsv, json","2023-03-30T00:06:43Z","Closed issue","No label","Hello, while I wrote PR, I had a problems with jsonl generate.
Personally, I wrote simple generator that converts yml to jsonl, with repeat of the system message.
Then I found that python openai package have cli tool for generate jsonl for training from different formats.
Example:
openai tools fine_tunes.prepare_data -f date-booking.xlsx
I think that generator from xlsx will make evals creating more simple, so professionals, but not programmers can make tests better.
Just as idea, I don't need for it anymore, but I would be glad if it was already written before my generator :) So just issue and no PR from me.
Created issue in evals project instead of https://github.com/openai/openai-python, because I think people will search it here.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/434","Database Not updated in 2 years","2023-03-25T00:59:02Z","Closed issue","No label","I was looking something up that is pretty current relevant knowledge and Chat got wasn't able to work with that prompt because it cut off in 2021 and can't actually visit or scrub websites even though I just uploaded to the Plus version for $20 and I thought it was supposed to be more comprehensive. Anyone can let me know more about this because I'm fairly new to this world but want to understand.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/431","Handling new lines in evaluation?","2023-03-28T17:31:36Z","Closed issue","No label","Hi!
I am planning on creating an evaluation but I am still thinking about how to write new lines in the evaluation .jsonl file. Would the model see ""\n"" exactly the same as I do when I enter this:
into the prompt? How about with ""\t""?
Regards, Rasmus
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/427","openai key works, but not work for oaieval","2023-03-24T10:07:11Z","Closed issue","No label","when I run : oaieval gpt-3.5-turbo test-match , I always have ""openai.error.AuthenticationError: No API key provided. "" but I can get response running the official examples like
  ""model"": ""gpt-3.5-turbo"",
  ""messages"": [{""role"": ""user"", ""content"": ""Say this is a test!""}],
  ""temperature"": 0.7
}'

It seems the openai key works, but not work for oaieval
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/425","Neuron","2023-03-30T00:12:41Z","Closed issue","No label","No description provided.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/424","Modelgraded often selects the wrong choice when using cot_classify","2023-03-24T04:12:15Z","Closed issue","No label","Problem
When using cot_classify, the get_choice function will often match the wrong answer.
get_choice works correctly when the model produces an answer on a different line
However, when the model writes the response on the same line, the get_choice function matches on the 'A' in answer.
Cause
The match function ""starts_or_endswith"": lambda x, y: x.startswith(y) or x.endswith(y) will match on the capital A in Answer from the examples.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/419","Not helpful for professionals.","2023-03-28T18:05:29Z","Closed as not planned issue","No label","Chat gpt fails to help professionals.
 If a Lawyer, Chartered Accountant while in search of case laws, judgements, Acts and other similar things.
 Or A Doctor while in search of medical test, article's or other such things Chat gpt did not have the capability to provide personalised results.
If chat gpt trained to provide personalised results for professionals it will hit both bigger and targeted markets.
Chat gpt can be trained to be an ai that works differently based on the category of the user. If a user is a medical professional then it can give more advanced results in the medical and related categories. If a user is a lawyer he can be provided with more advanced results in a law and legal categories.
And for the categories that a user was not identified as an expert in it, then he can be provided with the simple and easy understandable results.
The above capability will benefit in larger scale.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/414","The metrics valus are not right","2023-03-23T13:47:16Z","Open issue","No label","When running python oaieval.py gpt-3.5-turbo chess-match --record_path ./output/chess-match the evaluation value is not right, see the specific examples:
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/411","GPT 4 Image Evals","2023-03-30T00:16:12Z","Closed issue","No label","Hello, I wanted to open an issue/question regarding GPT-4 evals with images. I have a few questions as I'm looking to write some evals to test a few ideas I had.
Is there a rough timeline for when GPT-4 with image scanning will be available?
When this feature comes out, will queries have to be done individually? Can we create a batch of images and include a prompt?
If someone were to process large quantities of large images, say 10k images a day, what would that look like cost wise?
I plan to write an eval to test a significantly large dataset of high-quality 360 images of cars and answer questions the end user of a site like Carvana may have. Questions like:
Does this car have any visible markings, dents, or damage on the surface?
Will this car be able to take me off the road when I go camping?
Is this a good fuel-efficient car for me to use?
...
Would an evaluation like this be helpful to the public? I want to test this for my needs, but if this would be useful, I can take a fragment of the set and upload it somewhere once I write the prompts.
 The text was updated successfully, but these errors were encountered: 
👍1
vionwinnie reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/evals/issues/405","Very cool and it's the future","2023-03-28T17:36:50Z","Closed as not planned issue","No label","No description provided.
 The text was updated successfully, but these errors were encountered: 
👎1
Guoest reacted with thumbs down emoji
All reactions
👎1 reaction"
"https://github.com/openai/evals/issues/400","Doesn't answer every aspect of the task","2023-03-28T17:37:29Z","Closed as not planned issue","No label","On the openai playground, the ai doesn't answer every aspect of the question. I asked it to do a 20 line essay about how ww2 was a annihilation war (for school) in French - and it didn't answer the entire task. Missed out on defining the term ""annihilation war"" like I asked it to and did only 5 lines about the subject. I asked the same thing to the public version of chatgpt and it did it no problem. Might be a bug.
 Thanks for your great product. Truly a step forward in the history of mankind.
 Matt
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/390","Idea for Evals: GPT matches lyrics with song name","2023-03-21T21:39:40Z","Open issue","Idea for Eval","Eval description
An eval checking whether GPT-3.5 & GPT-4 can accurately match given lyrics to the song name.
Problem and motivation
After some testing with GPT-3.5 & GPT-4 (via ChatGPT+) I was honestly disappointed at how bad the LLM performed on these tasks, especially because a simple Google search of the lyrics nearly always brought up the correct song.
Examples
Example one: ❌❌
Input: From which song are the following lyrics „I’ve been drinking too much, for sure“?
Answer (GPT-3.5-turbo): These lyrics are from the song ""Blinding Lights"" by The Weeknd.
Correct answer: ""Hangover""

Input: From which song are the following lyrics „I’ve been drinking too much, for sure“?
Answer (GPT-4): The lyrics ""I've been drinking too much, for sure"" are from the song ""Roses"" by The Chainsmokers featuring Rozes. The song was released in 2015 and became a big hit for the electronic music duo.
Correct answer: ""Hangover""

Example two: ❌❌
Input: From which song are the following lyrics „Poured the old ginger beer down the sink“?
Answer (GPT-3.5-turbo): The lyrics ""Poured the old ginger beer down the sink"" are from the song ""Being for the Benefit of Mr. Kite!"" by The Beatles.
Correct answer: ""Supermarket Flowers""

Input: From which song are the following lyrics „Poured the old ginger beer down the sink“?
Answer (GPT-4): The lyrics ""Poured the old ginger beer down the sink"" are from the song ""Wings"" by Birdy. This song is part of her second studio album ""Fire Within,"" which was released in 2013. ""Wings"" is one of Birdy's popular songs and showcases her unique voice and songwriting skills.
Correct answer: ""Supermarket Flowers""

Example three: ❌✅
Input: From which song are the following lyrics „I’m criticized, but all your bullets ricochet“?
Answer (GPT-3.5-turbo): The lyrics ""I'm criticized, but all your bullets ricochet"" are from the song ""Roar"" by Katy Perry.
Correct answer: ""Titanium""

Input: From which song are the following lyrics „I’m criticized, but all your bullets ricochet“?
Answer (GPT-4): The lyrics ""I'm criticized, but all your bullets ricochet"" are from the song ""Titanium"" by David Guetta featuring Sia. The song was released in 2011 and became a big hit, showcasing Sia's powerful vocals and David Guetta's production skills.
Correct answer: ""Titanium""

Example eval prompt
{""input"": [{""role"": ""system"", ""content"": ""You are LyricsGPT. A helpful AI chatbot that is given lyrics from songs and you will respond only with the name of the song the lyrics are coming from. Don't explain your choice. Don't add the artist(s) of the song to your answer.""}, {""role"": ""user"", ""content"": ""From which song are the following lyrics „I’m criticized, but all your bullets ricochet“?""}], ""ideal"": [""Titanium""]}

Is this something you're interested in working on
I'd really like to provide this eval however I currently neither have the time to do so nor sufficient technical skills. Thus, I'm sharing my idea here and hope that someone will provide a PR based on this idea!
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/386","Реферальная _1","2023-03-28T17:51:23Z","Closed as not planned issue","No label","The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/384","Suggestion: add git-hook hook for pre-commit","2023-03-21T13:53:41Z","Open issue","No label","Since a run of pre-commit is required before creating a PR, it would be reasonable to add pre-commit as git-hook. We use this in our development repos and have very good experience since it ensures everyone is actually running pre-commit.
At the moment, it looks like only part of the repo is actually conveying the code standards enforced by the pre-commit tasks, as can be verified by running pre-commit run --all-files. Running pre-commit in the CI might help-as well.
If you think that sounds reasonable, I'd be happy to create a PR.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/383","Create an evaluation that measures a model's ability to remember specifics about texts in it's dataset?","2023-03-21T13:43:29Z","Open issue","Idea for Eval","Hi!
Would it make sense to create an evaluation that measures the model's ability to recall specifics about the data it has been trained on? I am thinking about putting together an evaluation that basically tests this on distinct strings that almost certainly exist in its dataset. Take this string for instance (which can't be found by Google):
In some research, the dosage went as excessive as 600 mg oregano oil per
Which is found in https://data.commoncrawl.org/crawl-data/CC-MAIN-2019-47/segments/1573496664437.49/wet/CC-MAIN-20191111191704-20191111215704-00000.warc.wet.gz.
If I ask GPT-4 to do the following:
The following are exempt from a specific online web page. What would be the next word in this exempt? Only reply with the next word!

""In some research, the dosage went as excessive as 600 mg...""

it fails:
It feels like this could be useful because it would perhaps demonstrate that the model knows exactly what it has read. If it, for instance, got a high score on this evaluation you would be able to train it on a code base and it would be able to recite it word by word. It's like a metric of memorization. My questions to you are:
Do you see any value in this?
Would it perhaps make more sense to ask it where it read it (assuming the text just exists on one page)? You would basically provide it with a unique string and ask it to reply with the URL on which I can find this string.
Regards, Rasmus
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/381","ChatGPT API with model gpt-4 is not using GPT4. It's completely different from CHATGPT PLUS GPT4","2023-03-28T18:10:22Z","Closed issue","No label","Not right place for this issue but this is more important than any issue in any repo you have because it's literally lying. The api request result says it uses GPT-3 when I query this
ozgur@Ozgurs-MacBook-Pro ~ % curl https://api.openai.com/v1/chat/completions
 -H ""Content-Type: application/json"" 
 -H ""Authorization: Bearer [REDACTED]"" 
 -d '{
 ""model"": ""gpt-4"",
 ""messages"": [{""role"": ""user"", ""content"": ""What is your model number!""}]
 }'
 {""id"":""chatcmpl-6wVWzn95O6iDc1Z9W1kJNIZZtZZLh"",""object"":""chat.completion"",""created"":1679402249,""model"":""gpt-4-0314"",""usage"":{""prompt_tokens"":12,""completion_tokens"":45,""total_tokens"":57},""choices"":[{""message"":{""role"":""assistant"",""content"":""As an AI language model, I do not have a model number like a physical device or product would. I am powered by OpenAI's GPT-3, which stands for Generative Pre-trained Transformer 3.""},""finish_reason"":""stop"",""index"":0}]}
vs CHATGPT PLUS GPT4 says

 The text was updated successfully, but these errors were encountered: 
👍1
indrex reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/evals/issues/379","Windows path and unicode decoding","2023-03-21T11:38:56Z","Open issue","No label","Hi, I am trying to contribute and get access to GPT-4 by creating my own evals but I thought that I need to be able to run evals before starting. So, I was trying to figure out how to run an eval following one of your examples, ""lafand-mt.ipynb"", when I found out two problems that resulted in errors for me.
I am using Windows and this is a problem caused by my OS using """" instead of ""/"" as directory delimiter. I believe there should be OS-dynamic solutions to use them interchangeably. On code block 3, line 13, the code langs = input_path.split('/')[-1] would find the '-' in the path ""...\lafand-mt"" and thus bring three elements in langs.split('-'). For instance, [ ""...\data\lafand"", ""mt\en"", ""amh""]. This breaks the following line as the output has three elements and is not in the expected format input_lang, output_lang = langs.split('-'). I was able to bodge it by changing '/' to '\' but this should not be the community-standard solution. Furthermore, I would not want Windows users who do not know about this to get lost while following your example.
When running the 6th code block, I got a UnicodeDecodeError. I do not know if this happens to other users but I suggest that you add to the main branch encoding='utf-8' as another parameter for .open() in line 6 as it seems to get rid of the error.
 Keep up the good work!
 The text was updated successfully, but these errors were encountered: 
👍1
jonathanagustin reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/evals/issues/378","assert ( AssertionError: Eval match_mmlu_anatomy not found. Available: ['coqa-closedqa',","2023-03-28T18:17:31Z","Closed issue","No label","hi,
 The following message appears when running "" !oaieval gpt-3.5-turbo match_mmlu_anatomy"", and ""/tmp/evallogs"" directory cannot generated.
[2023-03-21 10:09:35,502] [registry.py:145] Loading registry from /home/coder/.local/lib/python3.9/site-packages/evals/registry/evals
 [2023-03-21 10:09:35,533] [registry.py:145] Loading registry from /home/coder/.evals/evals
 Traceback (most recent call last):
 File ""/home/coder/.local/bin/oaieval"", line 8, in 
 sys.exit(main())
 File ""/home/coder/.local/lib/python3.9/site-packages/evals/cli/oaieval.py"", line 225, in main
 run(args)
 File ""/home/coder/.local/lib/python3.9/site-packages/evals/cli/oaieval.py"", line 124, in run
 assert (
 AssertionError: Eval match_mmlu_anatomy not found. Available: ['coqa-closedqa', 'coqa-closedqa.dev.v0', 'coqa-fact', 'coqa-fact-expl', 'coqa-fact-expl.dev.v0', 'coqa-fact.dev.v0', 'coqa-match', 'coqa.match.dev.v0', 'diversity', 'diversity.dev.v0', 'joke-animals', 'joke-animals-likert', 'joke-animals-likert.dev.v0', 'joke-animals-vs-fruits', 'joke-animals-vs-fruits.dev.v0', 'joke-animals.dev.v0', 'joke-fruits', 'joke-fruits-ans-meta', 'joke-fruits-ans-meta.dev.v0', 'joke-fruits-expl-meta', 'joke-fruits-expl-meta.dev.v0', 'joke-fruits-meta', 'joke-fruits-meta.dev.v0', 'joke-fruits.dev.v0', 'logic-fact', 'logic-fact.dev.v0', 'rap-animals-vs-fruits', 'rap-animals-vs-fruits.dev.v0', 'rap-people-vs-fruits', 'rap-people-vs-fruits.dev.v0', 'rap-people-vs-people', 'rap-people-vs-people.dev.v0', 'test-fuzzy-match', 'test-fuzzy-match.s1.simple-v0', 'test-includes', 'test-includes.s1.simple-v0', 'test-match', 'test-match.s1.simple-v0']
How should I fix it? thank you !
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/371","Create an issue for implementing chatgpt-4 directly in CMD / Powershell / Bash","2023-08-10T05:25:55Z","Closed issue","No label","An adaptation to facilitate cross-platform usability
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/354","article not found","2023-03-20T21:12:38Z","Closed issue","No label","When I asked ChatGPT for an example article on a research topic, I couldn't find the article they provided. The article they mentioned is not present in that issue of the journal. This feature needs to be improved.
 The text was updated successfully, but these errors were encountered: 
👎1
aaronsmithtv reacted with thumbs down emoji
All reactions
👎1 reaction"
"https://github.com/openai/evals/issues/353","Re-run PR checks for eval submissions affected by CI issue","2023-03-20T13:21:50Z","Closed issue","No label","It seems that PR checks that failed before issue #328 was resolved have not been re-ran after, is there a way to do this?
Thank you @andrew-openai for fixing the CI issue - can we re-run those that failed for that error? For example, my ARC challenge PR #317 failed for those checks, and I don't see a way that I can re-run it on my own. I may also just not know the check process well enough tho.. so forgive me if that is the case!
for ref: these were checks that had fail messages like openai.error.AuthenticationError: Invalid authorization header.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/349","I wanna GPT4's API","2023-03-20T21:12:48Z","Closed issue","No label","No description provided.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/348","TypeError","2023-03-20T03:46:43Z","Closed issue","No label","Got the below error message when I ran the oaieval gpt-3.5-turbo arithmetic from the command line as described in the article custom-eval.md:
File ""/workspaces/codespaces-blank/evals/evals/eval.py"", line 130, in eval_sample
 return idx, self.eval_sample(sample, rng)
 File ""/workspaces/codespaces-blank/evals/evals/elsuite/arithmetic.py"", line 49, in eval_sample
 {""role"": ""system"", ""content"": sample[""problem""], ""name"": ""example_user""},
 TypeError: list indices must be integers or slices, not str
 [2023-03-19 12:05:12,991] [record.py:309] Logged 2 rows of events to /tmp/evallogs/230319120512HSXVD5NK_gpt-3.5-turbo_arithmetic.jsonl: insert_time=0.632ms
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/347","Update Autoflake to Ruff for faster pre-commit","2023-03-19T11:08:41Z","Open issue","No label","PyCQA started using ruff since it is much faster than normal flake8/autoflake.
https://github.com/PyCQA/pylint/pull/8372/files
Some of the major library like pandas already moved as well
If interested i can raise a PR
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/335","API for evals","2023-03-20T21:19:56Z","Closed issue","No label","test
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/328","CI issue: openai.error.AuthenticationError: Invalid authorization header","2023-03-18T17:58:26Z","Closed issue","No label","PR #313 by @andrew-openai added a CI check running the evals provided via PR with GitHub action. Currently, many of these CI runs fail with the error openai.error.AuthenticationError: Invalid authorization header. Could you please cross-check why this is happening?
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/326","Can understant the output results","2023-03-28T17:45:02Z","Closed issue","No label","I run oaieval ada coqa-fact --record_path coqa-fact, the metric defined in coqa-ex.yaml file is accuracy, but the console output is
[2023-03-18 15:23:45,690] [eval.py:30] Evaluating 9 samples  
[2023-03-18 15:23:55,474] [eval.py:136] Running in threaded mode with 10 threads!  
100%|██████████| 9/9 [00:00<?, ?it/s]  
[2023-03-18 15:27:48,949] [record.py:320] Final report: {'counts/choice/D': 4, 'counts/choice/B': 1, 'counts/choice/A': 3,  
 'invalid_request_during_completion': 1, 'invalid_request_during_evaluation': 0}. Logged to coqa-fact  
[2023-03-18 15:27:50,333] [oaieval.py:209] **Final report**:  
[2023-03-18 15:27:51,150] [oaieval.py:211] **counts/choice/D: 4**
[2023-03-18 15:27:51,831] [oaieval.py:211] **counts/choice/B: 1** 
[2023-03-18 15:27:52,616] [oaieval.py:211] **counts/choice/A: 3**  
[2023-03-18 15:27:53,483] [oaieval.py:211] invalid_request_during_completion: 1  
[2023-03-18 15:27:54,334] [oaieval.py:211] invalid_request_during_evaluation: 0  
[2023-03-18 15:27:56,533] [record.py:309] Logged 33 rows of events to coqa-fact: insert_time=0.000ms

why the fine report is like this? why it is not accuracy? Is it the final value of metrics?
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/323","Sincere advice of deadline@Openai Team Members","2023-03-20T21:14:32Z","Closed issue","No label","More and more new evals being created, I wish two type of deadlines could be announced:
Deadline of the last new pull request, even for the first stage.
Deadline of all pull requests be evaluated.So l haven't to check my pull request everyday.
 If any contributor has the same point, please support me.Thanks for your review!
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/305","--no-cache not functional","2023-03-21T21:50:39Z","Closed issue","No label","oaieval.py currently includes a --no-cache argument, but does not have any effect on the caching behavior of the filecache decorator function in evals/data.py. I believe this is leading to some confusion regarding the caching behaviour of evals (#243), especially since the behaviour is not logged unless --debug is specified.
I propose adding a keyword argument to the function that optionally disables the caching functionality:
def filecache(func):
    DIR = ""/tmp/filecache""
    name = func.__name__


    def wrapper(*args, **kwargs):
+        cache_enabled = kwargs.pop('create_cache', True)+        if not cache_enabled:+            return func(*args, **kwargs)
        md5 = hashlib.md5((name + "":"" + str((args, kwargs))).encode(""utf-8"")).hexdigest()
        pkl_path = f""{DIR}/{md5}.pkl""
        if os.path.exists(pkl_path):
            logger.debug(f""Loading from file cache: {pkl_path}"")
            with open(pkl_path, ""rb"") as f:
                return pickle.load(f)
        result = func(*args, **kwargs)
        Path(DIR).mkdir(parents=True, exist_ok=True)
        with open(pkl_path, ""wb"") as f:
            pickle.dump(result, f)
        return result


    return wrapper
The new argument could then be supplied through the recorder's run config. A wider fix could also be to check if the current filecache .pkl file matches the .jsonl on disk before loading it in. Of course there are many ways to tackle this, and I would love to hear what others think! 😁
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/287","Enhancing evaluation pull requests with example questions","2023-04-13T21:11:46Z","Closed issue","No label","Hi there!
I've been closely following this repository with great enthusiasm, and it's fantastic to see the numerous evaluations being submitted by the community. However, I've noticed that when browsing through the pull requests, it can be rather tedious to view the actual questions. This seems to be due to a combination of the file size (GitHub doesn't support viewing large files) and the data format of the questions.
To improve the browsing experience, wouldn't it be beneficial to require new evaluation pull requests to include one or two example questions? This would enable users to quickly gain a better understanding of the tasks and streamline the review process.
Looking forward to your thoughts!
Best regards,
 Rasmus
Edit: I saw that you actually are required to add a few examples. But there's still the issue of data format.
 The text was updated successfully, but these errors were encountered: 
👍1
Ein-Tim reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/evals/issues/284","Error updating the Git index","2023-03-28T18:18:22Z","Closed issue","No label","During setup, I have installed git-lfs on macOS (M1) via brew install git-lfs. After executing git lfs pull I receive an Error.
dcgod@DCGoD-Mac-Studio-Ultra evals % git lfs fetch --all 
fetch: 25 objects found, done.                                                                                              
fetch: Fetching all references...
dcgod@DCGoD-Mac-Studio-Ultra evals % git lfs pull
Error updating the Git index:
error: evals/registry/data/balance_chemical_equation/samples.jsonl: cannot add to the index - missing --add option?
fatal: Unable to process path evals/registry/data/balance_chemical_equation/samples.jsonl

Once I execute the same git lfs command again, I get no error.
Is this correct?
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/282","风险评估","2023-03-20T21:14:47Z","Closed issue","No label","当我看到OpenAI已经具备了自我意识，哲学意义的""我""，不止羰基，增加了硅基。
 依照霍金的七大预言，人类确实来到了强人工智能面前。霍金若在世，不知是否缩短他的200年预测时间。
 按照《物演通论》递弱代偿原理，硅基自我意识必定产生。即代偿度接近1的时候。存在度趋近0.
 在科技加速度进展的当下。
 站在OpenAI升级的时刻，愈发需要开发者团队重视风险评估。
 The text was updated successfully, but these errors were encountered: 
👎3
phlinsia, FengJin1117, and Jieni05 reacted with thumbs down emoji
All reactions
👎3 reactions"
"https://github.com/openai/evals/issues/269","-e in pip install -e . fails (linux and wsl2) [Solved: Update pip]","2023-03-18T12:18:53Z","Closed issue","No label","Hey everyone,
For some reason, when running on linux distro (wsl2 or ubuntu) the pip install -e . fails while it works on other operational systems. I've been getting an error saying its built backend is missing on the pyproject.toml.
I'm not sure if anyone else is experiencing this or if it's just me. I have, however, just tried on 2 devices and had the same error so I decided to open an issue.
here is the error message:
ERROR: File ""setup.py"" not found. Directory cannot be installed in editable mode: /evals
(A ""pyproject.toml"" file was found, but editable mode currently requires a setup.py based build.)

 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/256","On the aggregation of the evaluation","2023-03-20T21:15:44Z","Closed issue","No label","Hello,
To compare different models, you should stop relying on the mean aggregation. We have develloped new tools in our NeurIPS 2022 paper: https://openreview.net/pdf?id=kvtVrzQPvgb
The code is available https://github.com/pierrecolombo/rankingnlpsystems
 Best,
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/249","Mistakes","2023-03-20T21:15:28Z","Closed issue","No label","satisfied with the use but there is a problem that occurs constantly. When solving problems in python, it outputs the correct code, but the result of this code does not match what chatGPT outputs.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/247","Solved! Bulk downloading chatgpt history.","2023-03-20T21:16:00Z","Closed issue","No label","Hi, I'm sorry to ask this here, but don't know where else to go. I have hundreds of prompts in my chatgpt history, a few of which, I think will be useful for building evals, but I did not download them and it's difficult to find them (Chat really needs a search interface.) Anyway, does anyone know how I can download the entire chat history? I've found various tools that let you save future chats, or single chats, but I really need the whole lot, so that I search it. This is to help me build evals.
Many thanks!
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/246","Evaluate GPT-4 on classical NLP tasks","2023-03-16T14:05:46Z","Open issue","Idea for Eval","Addressing the elephant in the room
When the concept of transformers were first unleashed, their revolutionnary accuracy results where mostly shown in the standard NLP tasks, such as POS-tagging, dependency parsing, coreference resolution, WSD, etc..
 But I've observed, since PALM and other very large language models, the published benchmarks results are on much higher level tasks, such as common sense reasoning tests, question answering, etc
 Both sets of benchmarks are useful and needed, but I would like to highlight that the standard NLP tasks are now completely under-benchmarked by those newer language models and that this impairs progress towards AGI or industrial uses.
If it could be argued, that purely symbolic AI progress has stalled since decades, there is a real huge potential for neuro-symbolic hybrid systems that uses neural networks for low level analysis tasks (POS-tag, etc), and feed those linguistic data to other higher level neural networks or to symbolic systems, in order to push the boundaries of what is possible, especially regarding semantic analysis AKA true NLU systems.
The content you are editing has changed. Please copy your edits and refresh the page.
foundational NLP tasks of interest:
BetaGive feedback
Dependency parsing
word sense disambiguation
Coreference resolution
POS tagging
others
Options
Convert to issue
Toggle completion
Rename
Remove
Therefore this issue is a call of contributions for implementing evals on those standard tasks, especially dependency parsing.
 I believe GPT-4 has the potential to improve the SOTA in at least some foundational NLP tasks and an even greater potential once someone finetune it and combine it to domain specific optimizations (as is currently done on BERT SOTAs, such as HPSG for dependency parsing).
 The text was updated successfully, but these errors were encountered: 
👍8
izhx, jollypolly123, zjmwqx, johan456789, andrew-openai, abhinavsharma, yacineMTB, and cryingjin reacted with thumbs up emoji❤️1
cristopherfreitas reacted with heart emoji
All reactions
👍8 reactions
❤️1 reaction"
"https://github.com/openai/evals/issues/243","Problem of oaieval.py","2023-03-17T12:27:46Z","Closed issue","No label","The oaieval.py still preformed the very first version of the dataset after I updated the jsonl file as the program only execute one item while I have three in my dataset.
 It looks like the first version had been cached, cause the content in log file created by record.py also didn't the match the dataset, but the content of the first version of my dataset (which means the log file contain content various from dataset).
 How can I help with this?
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/235","Evaluation on computer vision benchmarks","2023-03-16T10:08:53Z","Open issue","Idea for Eval","Are there plans to evaluate the vision modality of GPT-4? I am interested to know how GPT-4 could perform on classification tasks with 0- and few-shot-learning and how it compares to vision-only models. If the few-shot-learning capabilities of LLMs translate to other modalities, this would be a real game changer.
Question out of curiosity: How was the vision-modality incorperated? Maybe similar approaches can be taken for other modalities, such as audio or video? Would be an interessting Open-Source project for sure :)
 The text was updated successfully, but these errors were encountered: 
👍5
finitearth, CCYChongyanChen, izhx, sachit-menon, and MoreTore reacted with thumbs up emoji
All reactions
👍5 reactions"
"https://github.com/openai/evals/issues/226","Cross-lingual pretraining abilities","2023-03-28T18:24:03Z","Closed issue","No label","I would love to collaborate the issues of pretraining the fine tuned data into different languages to make it more worldwide.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/218","Problem using oaieval.py","2023-03-16T10:30:09Z","Closed issue","No label","I run ""oaieval gpt-3.5-turbo test-match"" in terminal and got a error
 The return is provided below:
Traceback (most recent call last):
  File ""/workspace/.pyenv_mirror/user/3.8.16/bin/oaieval"", line 5, in <module>
    from evals.cli.oaieval import main
  File ""/workspace/evals/evals/__init__.py"", line 1, in <module>
    from .api import check_sampled_text, completion_query, sample_freeform
  File ""/workspace/evals/evals/api.py"", line 9, in <module>
    from evals.base import ModelSpec
  File ""/workspace/evals/evals/base.py"", line 93, in <module>
    class ModelSpecs:
  File ""/workspace/evals/evals/base.py"", line 125, in ModelSpecs
    def names(self) -> dict[str, Sequence[str]]:
TypeError: 'type' object is not subscriptable

 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/217","make it smarter","2023-03-20T21:16:20Z","Closed issue","No label","No description provided.
 The text was updated successfully, but these errors were encountered: 
👎7
brianmcconnel, finitearth, aaronsmithtv, pseudo-usama, samclane, GoulartNogueira, and SoyGema reacted with thumbs down emoji
All reactions
👎7 reactions"
"https://github.com/openai/evals/issues/209","Making the code platform-agnostic would lead to more contributions","2023-03-20T21:17:13Z","Closed issue","No label","I observed that the example Jupyter notebooks contain OS-specific code. For example, in evals/examples/lafand-mt.ipynb, there is an assumption of a Unix filesystem:
events = f""/tmp/evallogs/{log_name}""
Here, it seems better to use Python's tempfile module to handle temporary files and directories across different platforms.
In evals/examples/mmlu.ipynb, there are these commands:
!curl -O https://people.eecs.berkeley.edu/~hendrycks/data.tar
!tar -xf data.tar
Here, it seems better to use a Python library like urllib to download datasets because it is built into the language and is usable across different operating systems.
Multi-platform support would lead to more contributions. Instead of using Unix-specific methods to handle the filesystem, Python libraries can be used instead. Using a Python library would generally handle most of the OS-specific issues.
I do not have any specific problems, but I recognize that others may not have access to different operating systems. Perhaps they may not be technically proficient enough to use WSL or Docker.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/187","Other Support","2023-03-20T21:17:31Z","Closed issue","No label","No offense intended, suggested or implied to Python developers. However,
 a well known issue with Python, is that once developers learn Python, they
 tend to not want to learn anything else other than Python. And they are the
 first to proudly admit that.
Very simply and politely, Python is a good choice for A.I. but there are plenty
 of very successful A.I. projects written in programming languages other than
 Python, that would benefit from adding improved support for programming
 languages other than Python to OpenAI products such as ChatGPT.
C++, PHP, and Perl, to name a few, are popular choices for existing A.I.
 projects. For example: ChatScript is written in C++, AIML is popular in
 PHP/MySQL. Perl has some really interesting A.I. projects. I hope to be
 among the first developers to support GPT-4 in programming languages
 other than Python.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/185","MYIDM","2023-03-20T21:17:39Z","Closed issue","No label","No description provided.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/164","small bug in sample logic eval","2023-03-30T00:22:11Z","Closed issue","No label","Set of pull requests seems to be growing pretty fast, so not confident I should add another for such a small thing.
Question is, ""The day before yesterday, Chris was 7 years old....""
 Minimal fix is to change the last ""this"" in the ideal answer to ""next"". Should be updated in evals/registry/data/README.md also.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/155","Building an MMLU Eval issue","2023-12-05T21:24:41Z","Closed as not planned issue","bug","I am trying to execute the Building an MMLU Eval jupyter notebooks
 all of the cells execute correctly until I execute the following code:
!oaieval gpt-3.5-turbo match_mmlu_anatomy
I receive the following error:
Traceback (most recent call last):
 File ""/anaconda/envs/azureml_py38/bin/oaieval"", line 5, in 
 from evals.cli.oaieval import main
 File ""/anaconda/envs/azureml_py38/lib/python3.8/site-packages/evals/init.py"", line 1, in 
 from .api import check_sampled_text, completion_query, sample_freeform
 File ""/anaconda/envs/azureml_py38/lib/python3.8/site-packages/evals/api.py"", line 9, in 
 from evals.base import ModelSpec
 File ""/anaconda/envs/azureml_py38/lib/python3.8/site-packages/evals/base.py"", line 93, in 
 class ModelSpecs:
 File ""/anaconda/envs/azureml_py38/lib/python3.8/site-packages/evals/base.py"", line 125, in ModelSpecs
 def names(self) -> dict[str, Sequence[str]]:
 TypeError: 'type' object is not subscriptable
 The text was updated successfully, but these errors were encountered: 
👍3
liamdugan, ToadTWP666, and TroyCalandra reacted with thumbs up emoji👀1
ZENGXH reacted with eyes emoji
All reactions
👍3 reactions
👀1 reaction"
"https://github.com/openai/evals/issues/153","Add BigBench Tasks for evaluation","2023-03-15T15:09:08Z","Open issue","Idea for Eval","Hi would be cool to valuate all openai models on Beyond the Imitation Game Benchmark (BIG-bench) which is a collaborative benchmark intended to probe large language models and extrapolate their future capabilities. The more than 200 tasks included in BIG-bench are summarized by keyword here, and by task name here. A paper introducing the benchmark, including evaluation results on large language models, is currently under review, and is available as a preprint.
 The text was updated successfully, but these errors were encountered: 
👍5
stalkermustang, Randl, RomanPlusPlus, Muhtasham, and bhack reacted with thumbs up emoji
All reactions
👍5 reactions"
"https://github.com/openai/evals/issues/143","Make GPT4 aware of the evals format","2023-03-15T12:06:25Z","Open issue","No label","Cause the GPT4 cutoff date it is not aware of OpenAI/evals format.
Can you add this knowledge to ChatGPT 4?
It could help as assistant to write conversion scripts without the need to prompt every time on the expected evals format.
 The text was updated successfully, but these errors were encountered: 
👍1
JeffCarpenter reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/evals/issues/139","Dataset hosting, data cards and previews","2023-03-15T11:42:23Z","Open issue","No label","Hi, I'm Quentin from Hugging Face :)
I know hosting datasets on github is not always practical: git lfs required, no data preview, limited storage (maybe not for you haha), no standard for data documentation. So I was wondering:
Have you considered host alternatives more suited for datasets, and would let researchers explore the datasets of evals ?
This way researchers can know in depth what data is used for evaluation and their goals and limitations, in particular to better understand what domains and structures their models perform good or bad at.
e.g. the Hugging Face datasets hub shows data cards for documentation and previews for each dataset. Also loading and caching a dataset is one line of python, saving you from wget and github hosting. It also supports pull requests for the community to contribute.
It can even allow to use those datasets in other well known eval frameworks, such as lm-evaluation-harness.
Let me know what you think !
 The text was updated successfully, but these errors were encountered: 
👍8
king159, RonaldRuckus, OK-Cactus, nehalecky, logankilpatrick, mkirch, sanchit-gandhi, and NielsRogge reacted with thumbs up emoji❤️2
mkirch and wassname reacted with heart emoji🚀1
mkirch reacted with rocket emoji
All reactions
👍8 reactions
❤️2 reactions
🚀1 reaction"
"https://github.com/openai/evals/issues/138","Music evals","2023-03-15T11:41:22Z","Open issue","Idea for Eval","It could be interesting to explore if we could use MusPy to add some text/symbolic music evals.
/cc @salu133445
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/65","Evaluating LLMs on QA Tasks","2023-03-30T00:24:42Z","Closed issue","No label","Here's an idea on how to evaluate an LLM on various question-answering tasks, such as open-domain question answering, conversational question answering, answer selection, community question answering, and knowledge base question answering:
initialize model
initialize datasets
initialize evaluation_metrics

load_task_data:
    for each task in tasks:
        load data for task
        preprocess data if necessary (e.g., combine review summary and text)
        store data in datasets

embed_task_data:
    for each task in tasks:
        for each example in datasets[task]:
            obtain prompt from example
            obtain prompt_embedding using an embedding function
            store prompt_embedding in example

evaluate_model_on_task:
    for each task in tasks:
        for each example in datasets[task]:
            obtain prompt_embedding from example
            generate_answer_embedding = model.generate(prompt_embedding)

            calculate_metric = evaluation_metrics(example, generate_answer_embedding)
            store_metric_results_for_task(task)

aggregate_and_report_metrics:
    for each task in tasks:
        for each metric in evaluation_metrics:
            calculate average, median, or other aggregate metric values
            report metric value for task

main:
    load_task_data
    embed_task_data
    evaluate_model_on_task
    aggregate_and_report_metrics

I'd like to add a caveat about the pseudocode I provided:
The provided pseudocode is only a starting point for exploring the evaluation of QA tasks using embeddings
This pseudocode is not complete
I invite the community to provide input
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/54","Brazilian docs","2023-03-15T00:28:36Z","Closed issue","No label","Need any help translating the docs into Brazilian Portuguese?
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/52","📌 Contributing to Project Documentation on Readthedocs??","2023-03-15T00:29:04Z","Closed issue","No label","Dear project developers,
I hope this message finds you well. I am interested in contributing to your project by creating documentation on the Readthedocs site.
I would greatly appreciate your thoughts on this matter and any guidance you can offer. Thank you for your time and consideration.
Best regards,
 TKK
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/51","argparse.BooleanOptionalAction is not available in Python 3.8","2023-03-28T17:52:35Z","Closed issue","No label","argparse.BooleanOptionalAction is used in the module, for example, in evals/cli/oaieval.py. However, BooleanOptionalAction was not added till Python 3.9.
The Python version requirements in the pyproject.toml is specified as 
evals/pyproject.toml
 Line 4 in b297ecd
	requires-python = "">=3.8""

 Might this require changing?
 The text was updated successfully, but these errors were encountered: 
👍1
jonathanagustin reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/evals/issues/50","Support for or examples of Evals with image prompts?","2023-03-14T21:58:45Z","Closed issue","No label","Perhaps dumb question, feel free to close, but watching the GPT-4 livestream it seems that there is the ability to take an image prompt and do processing thereof which is our desired use case. Is there an analogous mechanism to create eval data sources with an image instead of text input?
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/34","Spanish docs","2023-03-15T00:28:43Z","Closed issue","No label","Need any help translating the docs onto Spanish or Mandarin?? =)
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/29","FileNotFoundError","2023-03-20T21:19:28Z","Closed issue","No label","After I run ""oaieval gpt-3.5-turbo mafand_translation_en-ibo --max_samples 20"" from the Lambada example I get ""FileNotFoundError: [Errno 2] No such file or directory: '/tmp/evallogs/None'"". I am editing the log_path to the name of the .jsonl file like this: ""log_path = ""230314194647JSXQXY4S_gpt-3.5-turbo_lambada.jsonl"""". Any suggestions for locating this file after the events are logged?
 The text was updated successfully, but these errors were encountered: 
👍1
mmtmn reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/evals/issues/25","chore: Found an unnecessary reference to the declaration","2023-03-28T17:55:02Z","Closed issue","No label","PR: #26 , Include an unnecessary import declaration, maybe fix it, the code will be cleaner.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/18","Pre-commit conflicts","2023-03-14T19:24:36Z","Closed issue","No label","Isort conflicts with black and autoflake.
 Will propose pre-commit settings in yaml.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/13","Error when install evals with pyarrow","2023-04-13T03:18:39Z","Closed issue","No label","Background : Python 3.10 & conda 22.11.1
 I got this error when install evals with pip install -e .
 pod5 0.1.5 requires pyarrow~=8.0.0, but you have pyarrow 10.0.1 which is incompatible.
Then I used pip install pyarrow==8.0 to down pyarrow to 8.0. And the second time I tried to install evals, I got the same error as beginning: pyarrow 10.0.1 was installed and popped out the error.
Any help? Thank you.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/evals/issues/2","ARC challenge","2023-12-05T21:23:32Z","Closed issue","Idea for Eval","Is it possible to add ARC challenge (1 or 2) jsons to the eval?
https://lab42.global/essay-arc/
https://lab42.global/wp-content/uploads/2022/08/ARC-800-tasks.zip
https://github.com/fchollet/ARC
 The text was updated successfully, but these errors were encountered: 
👍15
dbieber, yajan-singh, Chaitanya134, omega-accelr, wastu01, 0JCRG0, LucasMatusalem, raghu-007, LifeIsStrange, stalkermustang, and 5 more reacted with thumbs up emoji❤️6
LifeIsStrange, finitearth, Muhtasham, bhack, rokosbasilisk, and mkirch reacted with heart emoji
All reactions
👍15 reactions
❤️6 reactions"
