"https://github.com/openai/openai-node/issues/1140","OpenAI request body validation error response is weird","2024-10-19T03:42:53Z","Open issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
I've sent a request to Create speech API without passing input parameter like below using the openai Node.js library.
fetch(""https://api.openai.com/v1/audio/speech"", {
  method: ""POST"",
  headers: {
    ""Authorization"": `Bearer ${process.env.OPENAI_API_KEY}`,
    ""Content-Type"": ""application/json""
  },
  body: JSON.stringify({
    model: ""tts-1"",
    voice: ""alloy""
  })})
  .then(response => {
    if (!response.ok) {
      throw new Error(response.statusText);
    }
    console.log('Request successful');
  })
  .catch(error => console.error(error));
The error looks like below:
{""error"": {
  ""message"": ""[{'type': 'string_too_short', 'loc': ('body', 'input'), 'msg': 'String should have at least 1 character', 'input': '', 'ctx': {'min_length': 1}}, {'type': 'enum', 'loc': ('body', 'voice'), 'msg': \""Input should be 'nova', 'shimmer', 'echo', 'onyx', 'fable' or 'alloy'\"", 'input': '', 'ctx': {'expected': \""'nova', 'shimmer', 'echo', 'onyx', 'fable' or 'alloy'\""}}]"",
  ""type"": ""invalid_request_error"",
  ""param"": null,
  ""code"": null}}
Why the error.message is a string?
Why is it a string of pure JS object and not a string of JSON?
 I tried to parse it with JSON.parse but as it's not a valid JSON, I failed to do so.
How do I get the error message from this? Do I have to use regex for that?
I think an improvement is needed so the developer can get the error message property.
To Reproduce
Run below request
fetch(""https://api.openai.com/v1/audio/speech"", {
  method: ""POST"",
  headers: {
    ""Authorization"": `Bearer ${process.env.OPENAI_API_KEY}`,
    ""Content-Type"": ""application/json""
  },
  body: JSON.stringify({
    model: ""tts-1"",
    voice: ""alloy""
  })})
  .then(response => {
    if (!response.ok) {
      throw new Error(response.statusText);
    }
    console.log('Request successful');
  })
  .catch(error => console.error(error));
Try to log the error message (the string message from the error message object) to the console
Code snippets
fetch(""https://api.openai.com/v1/audio/speech"", {
  method: ""POST"",
  headers: {
    ""Authorization"": `Bearer ${process.env.OPENAI_API_KEY}`,
    ""Content-Type"": ""application/json""
  },
  body: JSON.stringify({
    model: ""tts-1"",
    voice: ""alloy""
  })})
  .then(response => {
    if (!response.ok) {
      throw new Error(response.statusText);
    }
    console.log('Request successful');
  })
  .catch(error => console.error(error));
OS
macOs
Node version
Node v21.7.3
Library version
openai 4.68.1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/1134","Make it possible to change the $ref strategy of zod-to-json-schema","2024-10-16T16:32:25Z","Open issue","No label","Confirm this is a feature request for the Node library and not the underlying OpenAI API.
 This is a feature request for the Node library
Describe the feature or improvement you're requesting
The ""extract-to-root"" strategy is not working for my types, but I tried changing the strategy to ""none"" which seems to work. It would be nice to be able to just pass down which strategy to use through zodFunction
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/1133","Node Client does not provide empty system message, influencing API output compared to API Playground","2024-10-16T05:26:07Z","Open issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
The official OpenAI API Playground and this Node client may differ significantly in their Chat Completion responses, despite having the exact same request parameters, because the playground will insert an empty system message and the node client does not.
You could make an argument this is a bug with the OpenAI playground rather then this client, but starting here.
The request in question in the playground:

The node client code suggested from the playground (note no empty system message):

If you actually inspect the chrome traffic, however, note the presence of the empty system message:

If you run the exact suggested client code from the playground, the generated API request does not have the empty system message (confirmed via proxying HTTP traffic from my machine):
Adding the empty system message manually fixes the issue.
Pretty significant bug IMO, spent hours trying to figure out if this was real or if I was chasing ghosts. There are many topics on the forum about API playground requests differing from the Node client - this may explain a handful of them.
To Reproduce
Follow isntructions above.
Code snippets
No response
OS
MacOS
Node version
Node v20.15.1
Library version
4.67.3
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/1131","runTools function isn't running tools in parallel","2024-10-14T22:37:18Z","Open issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
I'm attempting to have tools run in parallel using runTools and it doesn't work
To Reproduce
Create a completion with multiple tools
Call runTools with a prompt that will likely invoke multiple tool calls
Wait for stream with it clearly being the case that the functions don't run in parallel
Code snippets
No response
OS
Mac
Node version
Latest
Library version
Latest
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/1130","RealTime API to generate TTS with ChatContext","2024-10-13T20:14:46Z","Closed as not planned issue","No label","Confirm this is a feature request for the Node library and not the underlying OpenAI API.
 This is a feature request for the Node library
Describe the feature or improvement you're requesting
To my understanding,
The TTS model generate Text2Speech without context using OAI voices. And can't change voice style
The RealtimeAPI can change voice style (like a Drunk Pirate) but requires a RealTime client ?
I'm looking for a very simple way (with Azure)
to send a ChatMessage (with context, etc ...)
and receive the text + audio on realtime model
This would open the path to many simple usecase to get audio aside the text but with all the amazing voice effects.
Additional context
I'm working with Azure Model and API keys not directly OpenAI. My starting point is this part of the documentation.
I tried using ""client.audio.speech.create()"" with realtime model but get the error:
Error: 400 The audio operation does not work with the specified model, gpt-4o-realtime-preview. Please choose different model and try again. You can learn more about which models can be used with each operation here: https://go.microsoft.com/fwlink/?linkid=2197993.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/1129","Incorrect type specified","2024-10-08T22:28:21Z","Open issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
RunStepDelta should be documented in example as TextDelta
To Reproduce
Review TypeScipt definition of textDelta event vs documented use
Code snippets
No response
OS
macOS
Node version
20
Library version
4.67.3
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/1124","Setting x-stainless-retry-count to null in defaultHeaders gets ignored","2024-10-18T18:41:36Z","Closed issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
The recent addition of the x-stainless-retry-count header (33e5e9b) uses the local headers object, which does not include the default headers:
openai-node/src/core.ts
 Line 391 in 785ef4b
	if(getHeader(headers,'x-stainless-retry-count')===undefined){
To Reproduce
Create a new OpenAI instance
Set the defaultHeaders to include 'x-stainless-retry-count': null
See the header being included nonetheless
Code snippets
No response
OS
Windows
Node version
Node v20
Library version
openai 4.67.1
 The text was updated successfully, but these errors were encountered: 
👍2
josiah-redjade and felladrin reacted with thumbs up emoji
All reactions
👍2 reactions"
"https://github.com/openai/openai-node/issues/1123","Can't use v1 assistants with the nodejs sdk anymore","2024-10-06T20:37:04Z","Open issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
I am using the nodejs SDK 4.36.0, and when I try to create an assistant with an uploaded file it gives me an error like ""vector store with id 'asst_*' not found.""
To Reproduce
const openai = new OpenAI({
    defaultHeaders: { 'OpenAI-Beta': 'assistants=v1' },
  });
const assistant = await openai.beta.assistants.create({
    instructions:
      '...',
    model: 'gpt-4-turbo',
    tools: [
      {
        type: 'retrieval',
      },
    ],
    file_ids: [fileID],
  });

Code snippets
const openai = new OpenAI({
    defaultHeaders: { 'OpenAI-Beta': 'assistants=v1' },
  });const assistant = await openai.beta.assistants.create({
    instructions:
      '...',
    model: 'gpt-4-turbo',
    tools: [
      {
        type: 'retrieval',
      },
    ],
    file_ids: [fileID],
  });


### OS

macOS

### Node version

Node v18-20

### Library version

openai 4.36.0

 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/1122","Add the ""addTool()"" fom Realtime API client SDK","2024-10-06T20:36:13Z","Open issue","No label","Confirm this is a feature request for the Node library and not the underlying OpenAI API.
 This is a feature request for the Node library
Describe the feature or improvement you're requesting
Working with tools is easy. Just call .addTool() and set a callback as the second parameter. The callback will be executed with the parameters for the tool, and the result will be automatically sent back to the model.
the way you can pass the tool spec + the actual logic for executing it easily in one thing and have it handle it s really convenient. Any chance that's added to this package?
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/1119","zodFunction helper rejects default() and min() Zod methods in validation schemas for function calling","2024-10-04T17:39:33Z","Open issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
When using the zodFunction helper found here, Zod schemas containing default() and min() methods are rejected with errors like 'default' is not permitted and 'minLength' is not permitted. These methods work fine when using the example zodFunction utility provided here, but not with the exported helper function. default() for enums seems to work in both cases, but not for other types of values.
To Reproduce
Working Example:
import { z, ZodSchema } from 'zod';import { JSONSchema } from 'openai/lib/jsonschema';import { zodToJsonSchema } from 'zod-to-json-schema';import { RunnableToolFunctionWithParse } from 'openai/lib/RunnableFunction';

export const TimeParams = z.object({
  timeZone: z.string().describe('Time zone of the location').default('Europe/Belgrade'),});type TimeParams = z.input<typeof TimeParams>;

const getCurrentTime = async (params: TimeParams) => {
  try {
    const response = await fetch(`https://worldtimeapi.org/api/timezone/${params.timeZone}`);

    const data = await response.json();
    const importantTimeData = {
      abbreviation: data.abbreviation,
      datetime: new Date(data.datetime).toLocaleString('en-US', {
        weekday: 'long',
        year: 'numeric',
        month: 'long',
        day: 'numeric',
        hour: 'numeric',
        minute: 'numeric',
        timeZone: data.timezone,
        timeZoneName: 'short',
      }),
      timezone: data.timezone,
    };

    return {
      current_time: importantTimeData,
    };
  } catch (error) {
    return {
      error: getErrorMessage(error, 'Failed to fetch the current time'),
    };
  }};

function zodFunction<T extends object>({
  function: fn,
  schema,
  description = '',
  name,}: {
  function: (args: T) => Promise<object>;
  schema: ZodSchema<T>;
  description?: string;
  name?: string;}): RunnableToolFunctionWithParse<T> {
  return {
    type: 'function',
    function: {
      function: fn,
      name: name ?? fn.name,
      description: description,
      parameters: zodToJsonSchema(schema) as JSONSchema,
      parse(input: string): T {
        const obj = JSON.parse(input);
        return schema.parse(obj);
      },
    },
  };}

export const chatCompletionsTools = [
  zodFunction({
    function: getCurrentTime,
    name: 'getCurrentTime',
    schema: TimeParams,
    description: 'Get the current time in a given location',
  }),];
Non-Working Example (Using the Official zodFunction Helper):
import { z } from 'zod';import { zodFunction } from 'openai/helpers/zod';

export const TimeParams = z.object({
  timeZone: z.string().describe('Time zone of the location').default('Europe/Belgrade'),});type TimeParams = z.input<typeof TimeParams>;

const getCurrentTime = async (params: TimeParams) => {
  try {
    const response = await fetch(`https://worldtimeapi.org/api/timezone/${params.timeZone}`);

    const data = await response.json();
    const importantTimeData = {
      abbreviation: data.abbreviation,
      datetime: new Date(data.datetime).toLocaleString('en-US', {
        weekday: 'long',
        year: 'numeric',
        month: 'long',
        day: 'numeric',
        hour: 'numeric',
        minute: 'numeric',
        timeZone: data.timezone,
        timeZoneName: 'short',
      }),
      timezone: data.timezone,
    };

    return {
      current_time: importantTimeData,
    };
  } catch (error) {
    return {
      error: getErrorMessage(error, 'Failed to fetch the current time'),
    };
  }};

export const chatCompletionsTools = [
  zodFunction({
    function: getCurrentTime,
    name: 'getCurrentTime',
    parameters: TimeParams, 
    description: 'Get the current time in a given location',
  }),];
Both examples were tested with:
openai.beta.chat.completions.runTools()
Code snippets
No response
OS
Windows 11
Node version
v20.16.0
Library version
4.67.1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/1118","Build Warnings when building for Edge Runtime","2024-10-04T04:21:56Z","Open issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
I see:
../../node_modules/.pnpm/openai@4.67.1_zod@3.23.8/node_modules/openai/core.mjs
A Node.js API is used (process.version at line: 575) which is not supported in the Edge Runtime.
Learn more: https://nextjs.org/docs/api-reference/edge-runti

To Reproduce
create a route handler in NextJS that is configured to run in edge
import openai package
run build
Code snippets
No response
OS
macOS
Node version
v20.16.0
Library version
4.67.1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/1108","Respect the retry time from a 429","2024-09-30T13:37:55Z","Open issue","No label","Confirm this is a feature request for the Node library and not the underlying OpenAI API.
 This is a feature request for the Node library
Describe the feature or improvement you're requesting
The library is already able to retry failed requests, but only ""Certain errors will be automatically retried 2 times by default, with a short exponential backoff. "". this sugests that all error types will be handled the same. For 429 the API actually returns a time in seconds when the request should be retried. It would be great if the library would respect this and only retry after the given time.
Additional context
429 Requests to the ChatCompletions_Create Operation under Azure OpenAI API version 2024-02-15-preview have exceeded token rate limit of your current OpenAI S0 pricing tier. Please retry after 33 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/1107","[TEXT-TO-SPEECH] new voices","2024-09-30T09:33:31Z","Open issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
There was a twitter/X thread about new voices.
We wanted to use it via API.
 For the moment the doc is outdated or they are not available
To Reproduce
The document not related with new voices .
https://platform.openai.com/docs/guides/text-to-speech/quickstart
Code snippets
No response
OS
macOS
Node version
node 20
Library version
4.65.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/1102","Using Structured Outputs throws body used already for: https://api.openai.com/v1/chat/completions in 4.59.0","2024-09-26T19:45:19Z","Open issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
Since 4.59.0, using structured outputs with Zod causes the following error:
TypeError: body used already for: https://api.openai.com/v1/chat/completions
    at Response.consumeBody (/dev/app/node_modules/node-fetch/lib/index.js:344:30)
    at Response.json (/dev/app/node_modules/node-fetch/lib/index.js:269:22)
    at Proxy.defaultParseResponse (/dev/app/node_modules/openai/src/core.ts:68:33)
    at /dev/app/node_modules/openai/src/core.ts:121:42
    at processTicksAndRejections (node:internal/process/task_queues:95:5)
    at /dev/app/dist/apps/api/main.js:4027:32

To Reproduce
You can reproduce by using the chat completions endpoint with structured outputs. I haven't tried this without Zod.
const completion = await this.client.beta.chat.completions.parse(
  {
    model: ""gpt-4o-mini-2024-07-18"",
    temperature: 0,
    messages: [
      {
        role: ""system"",
        content: prompt,
      },
      {
        role: ""user"",
        content: input,
      },
    ],
    response_format: zodResponseFormat(schema, schemaName),
  },
  { timeout: 60_000 },);
Using this.client.chat.completions.create with response_format: { type: ""json_object"" } works just fine.
This is a regression introduced in v4.59.0
Code snippets
No response
OS
macOS
Node version
v20.13.1
Library version
4.59.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/1099","trascriptions.create return incomplete Interface Transcription","2024-09-26T22:41:13Z","Closed as not planned issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
Type of return from openai.audio.transcriptions.create does not seems to have any segments property.
const openai = new OpenAI({
    apiKey: ...
});

const transcription = await openai.audio.transcriptions.create({
      file: file,
      model: 'whisper-1',
      response_format: ""verbose_json"",
      timestamp_granularities: [""word""]
    });

    logger.info(transcription.segments)

To Reproduce
Might be needed to use typescript, might be related to typescript type checking
Create a new OpeanAI()
Use openai.chat.transcriptions.create()
Try to use the segments property from the previous step
Code snippets
No response
OS
Linux
Node version
Node v20.17.0
Library version
openai v4.63
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/1092","zodResponseFormat should require a zod object, and not any zod type","2024-09-23T08:53:49Z","Closed as not planned issue","No label","Confirm this is a feature request for the Node library and not the underlying OpenAI API.
 This is a feature request for the Node library
Describe the feature or improvement you're requesting
it is easy to get it wrong by inputting z.boolean() which will later fail in ta cryptic http error
400 Invalid schema for response_format 'query': schema must be a JSON Schema of 'type: ""object""', got 'type: ""None""'.

 The text was updated successfully, but these errors were encountered: 
👍1
RobertCraigie reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/openai-node/issues/1090","Run failed Sorry, something went wrong when run a thread with a image_url","2024-09-23T09:54:37Z","Closed as not planned issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
I’m encountering this same error ""Run failed Sorry, something went wrong."" when trying to send a URL image using GPT-4o. Despite trying various solutions, I keep receiving the error message “Sorry, something went wrong” and I’m not sure what the issue is. Has anyone else experienced this with assistant v2 and sending url images and found a solution?
To Reproduce
Create and run thread sending an image url and a text
Code snippets
const run = await openai.beta.threads.createAndRun({
        assistant_id: ""assistant_id"",
        thread: {
          messages: [
            { ""role"": ""user"",
            ""content"":[
             {
                 ""type"":""text"",
                 ""text"":""help me convert this image to json""
                 },
              {
             ""type"":""image_url"",
             ""image_url"":{
             ""url"": imageUrl
             }}]}],
        },
      });


### OS

macOS

### Node version

node 18

### Library version

4.62.1

 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/1086","metadata should be typed as Record<string, string>","2024-09-18T17:48:13Z","Open issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
The generated types for metadata field is always unknown
This makes it weird to work with:

A better type would be Record<string, string>
To Reproduce
Install openai 4.61.1
Code snippets
No response
OS
linux
Node version
20
Library version
4.61.1
 The text was updated successfully, but these errors were encountered: 
👍1
RobertCraigie reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/openai-node/issues/1083","Leftover from polyfill removal: web-streams-polyfill","2024-09-18T16:16:08Z","Closed issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
Just updated to the latest version and encountered this during a build:
node_modules/openai/_shims/node-types.d.ts:10:32 - error TS2307: Cannot find module 'web-streams-polyfill' or its corresponding type declarations.

10 export { ReadableStream } from 'web-streams-polyfill';
                                  ~~~~~~~~~~~~~~~~~~~~~~


Found 1 error in node_modules/openai/_shims/node-types.d.ts:10

I guess it's a leftover from #954 and should be export { ReadableStream } from 'node:stream/web';
To Reproduce
Code snippets
No response
OS
macOS
Node version
Node v20.15.1
Library version
openai v4.62.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/1077","Unknown request URL: GET /v1/fine-tunes","2024-09-16T12:28:33Z","Closed as not planned issue","question","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
I'm seeing an issue, where the openai object has a function that returns an error. Either the endpoint needs to be changed or the function should not exist at all.
I want to be able to list the finetunes I've created on my account. This is not possible as the current listFineTunes() function returns an error object. I can however list all of the models using the listModels() function.
To Reproduce
Create an openai object with the node SDK:
const config = new Configuration({
  apiKey: OPEN_API_KEY,});

const openai = new OpenAIApi(config);
Try to call the function on the openai object:
openai.listFineTunes()
Code snippets
No response
OS
Windows 11
Node version
Node v20.0.9
Library version
openai v4.61.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/1074","404 Invalid URL (POST /v1/chat/completions)","2024-09-16T13:39:15Z","Closed issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
Hello all, between 2024-09-15 06:01pm and 2024-09-15 10:00pm we encounter 404 Invalid URL (POST /v1/chat/completions) errors in our logs. Before and after this specific time window all seems fine. I am searching what caused this problem. There is an incident report on 13th September. Also I checked that there is no report or annoucement or api down detection on 15th September If you can explain what is the main reason I would be appreciated.
To Reproduce
const openaiClient = new OpenAI({
            apiKey: ""apikey"",
            timeout: 3 * 60 * 1000,
            maxRetries: 2,
 });
 const response = await openaiClient.chat.completions.create({
                model: 'gpt-4o',
                messages: [{ role: 'user', content: 'Hello gpt' }],
});

Code snippets
No response
OS
Ubuntu
Node version
Node v20.12
Library version
4.52.2
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/1072","""Cannot find module 'zod'"" in production environment","2024-09-20T08:53:50Z","Closed as not planned issue","No label","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
Error: Cannot find module 'zod'
Require stack:
- /app/node_modules/openai/_vendor/zod-to-json-schema/parseDef.js
- /app/node_modules/openai/_vendor/zod-to-json-schema/index.js
- /app/node_modules/openai/helpers/zod.js
- /app/main.js
at Module._resolveFilename (node:internal/modules/cjs/loader:1140:15)
at Module._load (node:internal/modules/cjs/loader:981:27)
at Module.require (node:internal/modules/cjs/loader:1231:19)
at Hook.Module.require (/app/node_modules/dd-trace/packages/dd-trace/src/ritm.js:64:27)
at require (node:internal/modules/helpers:177:18)
at Object.<anonymous> (/app/node_modules/openai/_vendor/zod-to-json-schema/parseDef.js:4:15)
at Module._compile (node:internal/modules/cjs/loader:1364:14)
at Module._extensions..js (node:internal/modules/cjs/loader:1422:10)
at Module.load (node:internal/modules/cjs/loader:1203:32)
at Module._load (node:internal/modules/cjs/loader:1019:12) {
    code: 'MODULE_NOT_FOUND',
    requireStack: [
        '/app/node_modules/openai/_vendor/zod-to-json-schema/parseDef.js',
        '/app/node_modules/openai/_vendor/zod-to-json-schema/index.js',
        '/app/node_modules/openai/helpers/zod.js',
        '/app/main.js'
    ]
}

package.json:
""dependencies"": {
	...
	""openai"": ""^4.60.1"",
	""zod"": ""^3.23.8"",
	...
}

npm ls zod logs
@redacted/source@0.0.0 /Users/redacted
├─┬ openai@4.60.1
│ └── zod@3.23.8 deduped
└── zod@3.23.8

To Reproduce
I am not sure to be honest. It works fine locally but seems to break somewhere during the deployment. Please let me know what insights could further help you debug this issue.
Code snippets
No response
OS
linux/arm64
Node version
v18.20.4
Library version
v4.60.1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/1067","Missing break in switch statement at parser.ts line 184","2024-09-16T14:05:19Z","Closed issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
I encountered a TypeScript error (TS7029: Fallthrough case in switch) while working with the OpenAI Node.js client. The error is due to a missing break statement in the switch method at parser.ts line 184. Specifically, the case for 'number' does not have a break, causing a fallthrough to subsequent cases.
To Reproduce
Install library: openai@4.60.0
Use typescript: v5.5.4
Run project
Code snippets
No response
OS
macOS
Node version
Node v20.11.1
Library version
openai v4.60.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/1060","functionCallResult never get's called in the frontend with ChatCompletionStream","2024-09-11T15:29:37Z","Open issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
In my backend I do:
const steam = client.beta.chat.completions.runTools({})return steam.toReadableStream()
And in my frontend I do:
const runner = ChatCompletionStream.fromReadableStream( response.body )runner.on('message', (message) => {
    messages.value.push(message)})
This works but after the role:assistant with tool_calls it doesn't push the tool_call results to the messages array. In the frontend also runner.on('functionCallResult') never gets called.
This means that next calls fail because after a tool call we must pass the tool call results.
How can I get the tool call function response to show up in messages?
To Reproduce
return a stream to the frontend:
const steam = client.beta.chat.completions.runTools({})
 return steam.toReadableStream()
read the stream with ChatCompletionStream
fail to get the function response as message
P.s I've also tested with ChatCompletionStreamingRunner and it's the same
Code snippets
No response
OS
mac latest
Node version
22
Library version
latest
 The text was updated successfully, but these errors were encountered: 
👍1
blechatellier reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/openai-node/issues/1057","Support withResponse() for Parse Completion","2024-09-11T14:31:45Z","Closed issue","enhancement","Confirm this is a feature request for the Node library and not the underlying OpenAI API.
 This is a feature request for the Node library
Describe the feature or improvement you're requesting
Currently for chat completion we can use withResponse() to get the parsed response data and the raw Response instance. In my case to extract headers.
This feature request is to support the same when Parse Completion is used.
I created this PR to reflect the intention of it.
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/1052","Memory Leak in vectorStores.fileBatches.uploadAndPoll","2024-09-09T16:46:32Z","Open issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
I use S3 Bucket to getObject as stream and then upload it to a vector store using vectorStores.fileBatches.uploadAndPoll, the memory usage is supposed to increase during the upload process and return to its baseline after finishing but it doesn't, I uploaded a 22mb file and memory usage went from 300mb to 360mb and never returned to baseline
Note - 1: the upload process is part of POST request in Express.js, and memory usage remained high even after returning the json response
Note - 2 I tried saving the file to disk using ""fs"", it consumed memory and returned back to baseline, so I think the bug is in OpenAI SDK
To Reproduce
this is the ExpressJS request handler, and req.params.id is the the file key saved in s3 bucket
the file is uploaded successfully to OpenAI
import OpenAI, { toFile } from ""openai"";
import { S3Client, GetObjectCommand } from ""@aws-sdk/client-s3"";
import ""dotenv/config"";

export async function post(req:Request , res: Response, next: NextFunction){
    try {

        let client = new S3Client({
            forcePathStyle: true,
            credentials: {
                accessKeyId: process.env.S3_ACCESS_KEY_ID!,
                secretAccessKey: process.env.S3_SECRET_ACCESS_KEY!,
            },
            endpoint: process.env.S3_ENDPOINT!,
            region: ""us-east-1""
        })

        const params = {
            Bucket: ""first-bucket"",
            Key: req.params.id,
        }

        const command = new GetObjectCommand( params );
        const response = await client.send( command );
        
        const openai = new OpenAI({
            apiKey: process.env['OPENAI_API_KEY'],
        });

        await openai.beta.vectorStores.fileBatches.uploadAndPoll(<insert vector store ID here>, { files: [await toFile(response.Body!, req.params.id)]})
        
        return res.json({success: true})
    } catch(error){
        next(error)
    }
}

import { post } from ""./controller/files""
router.get(""/:id"", post)

Code snippets
No response
OS
Windows 11
Node version
v19.8.1
Library version
4.56.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/1049","Structured Outputs: JSON Schema 'nullable' modifier ignored","2024-09-20T07:10:45Z","Closed issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
In Zod one can make a field nullable in two ways, with A) a union (eg. authorName1 below) or using B) the .nullable() method (eg. authorName2 below).
A: z.union([z.string(), z.null()]
 B: z.string().nullable()
The final JSON schema will also be different:
A: { ""type"": [""string"", ""null""] }
 B: { ""type"": ""string"", ""nullable"": true }
When using the later, B, the LLM (gpt-4o-2024-08-06) always return a string and never null. The former, A, works well.
I would imagine B is quite a bit more common in Zod schemas so it would make sense to support it. I see many ways to fix this, including just clarifying that ""nullable"" has no effect in the docs.
To Reproduce
Consider this Zod schema, authorName1 and authorName2 are both nullable strings using two different syntaxes.
const SomeSchema = z.object({
  authorName1: z.union([z.string(), z.null()], {
    description: [
      `The name of the author of the provided document.`,
      `This field is is null if the author name does not exist in the input data.`,
    ].join("" ""),
  }),
  authorName2: z
    .string({
      description: [
        `The name of the author of the provided document.`,
        `This field is is null if the author name does not exist in the input data.`,
      ].join("" ""),
    })
    .nullable(),});
Passing it through zodResponseFormat:
const responseFormat = zodResponseFormat(SomeSchema, ""documentData"");console.log(JSON.stringify(responseFormat, null, 2));
Output:
{
  ""type"": ""json_schema"",
  ""json_schema"": {
    ""name"": ""documentData"",
    ""strict"": true,
    ""schema"": {
      ""type"": ""object"",
      ""properties"": {
        ""authorName1"": {
          ""type"": [
            ""string"",
            ""null""
          ],
          ""description"": ""The name of the author of the provided document. This field is is null if the author name does not exist in the input data.""
        },
        ""authorName2"": {
          ""type"": ""string"",
          ""nullable"": true,
          ""description"": ""The name of the author of the provided document. This field is is null if the author name does not exist in the input data.""
        }
      },
      ""required"": [
        ""authorName1"",
        ""authorName2""
      ],
      ""additionalProperties"": false,
      ""$schema"": ""http://json-schema.org/draft-07/schema#""
    }
  }
}
As you can see in the generated output, responseFormat also use two different syntaxes to make the two fields nullable. The model (gpt-4o-2024-08-06) seem to ignore the later ""nullable"": true.
await openai.beta.chat.completions.parse({
  top_p: 1,
  model: ""gpt-4o-2024-08-06"",
  n: 1,
  temperature: 0,
  response_format: zodResponseFormat(SomeSchema, ""documentData""),
  messages,});
Code snippets
No response
OS
macOS and Linux
Node version
Node 20
Library version
openai 4.58.1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/1048","Add streaming support for Audio","2024-09-23T09:46:36Z","Closed as not planned issue","documentation","Confirm this is a feature request for the Node library and not the underlying OpenAI API.
 This is a feature request for the Node library
Describe the feature or improvement you're requesting
Currently, it's easy to stream in Python as per docs, but I haven't been able to pull it off in Node. Concurrency works just great, tho.
we need a response.stream_to_file(""output.mp3"")
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/1047","OpenAI has no construct signatures","2024-09-09T17:27:13Z","Closed issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
Running a node script via yarn dotenv -e ../.env.prod.local -e ../.env.prod -- ts-node backend/src/openaiconsumer.ts that errors due to OpenAI is not a constructor. This issue is similar to #816
To Reproduce
Run npm install openai or yarn add openai.
Run this code:
import OpenAI from 'openai';

const client = new OpenAI({
  apiKey: 'valid api key'
});

Errors:
/Users/.../node_modules/ts-node/src/index.ts:859
    return new TSError(diagnosticText, diagnosticCodes, diagnostics);
           ^
TSError: ⨯ Unable to compile TypeScript:
backend/src/openaiconsumer.ts:31:20 - error TS2351: This expression is not constructable.
  Type 'typeof import(""/Users/.../backend/node_modules/openai/dist/index"")' has no construct signatures.

31 const client = new OpenAI({

Code snippets
No response
OS
macOS
Node version
Node v18.13.0
Library version
openai 4.58.1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/1034","Avoid redundant memory copies during file uploads","2024-09-06T07:06:30Z","Closed issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
files.create convert the value of file using toFile whenever it is Uploadable. The problem is that even if it has already been converted to a File, such as client.files.create({ file: await OpenAI.toFile(someStream, ""filename"", purpose: ""assistants"" }), toFile is executed one more time, and by calling arrayBuffer() on 
openai-node/src/uploads.ts
 Line 148 in 5e29cb2
	parts.push(awaitvalue.arrayBuffer());
 , memory copying occurs one more time.
To Reproduce
const fs = require(""node:fs"")

const client = new OpenAI()const stream = fs.createReadStream(""some-file"")const file = await OpenAI.toFile(stream, ""new_file_name"")          // toFile create buffer and copy from streamawait client.files.create({ file, purpose: ""assistants"" })  // internally call `toFile` again and it create a new buffer
Code snippets
No response
OS
Linux
Node version
Node v18.20.4
Library version
openai v4.56.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/1032","Module '"".../node_modules/zod/index""' has no default export. import type z from 'zod';","2024-09-04T22:16:35Z","Closed issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
I am programming an assistant using the openai library but when trying to build my code it gives the following error.
../../node_modules/openai/helpers/zod.d.ts:2:13 - error TS1192: Module '""../node_modules/zod/index""' has no default export.

2 import type z from 'zod';
              ~

  ../../node_modules/zod/index.d.ts:1:1
    1 export * from ""./lib"";
      ~~~~~~~~~~~~~~~~~~~~~~
    'export *' does not re-export a default.

[3:27:12 PM] Found 1 error. Watching for file changes.
Shouldn't it be better to import only the necessary types from zod instead of all types?
import type { infer as _infer, ZodType } from 'zod';
This way, it would benfit tree-shaking as we are only importing what we need.
To Reproduce
Install the openai library
Build node in watch mode
Code snippets
No response
OS
Ubuntu
Node version
v20.15.0
Library version
openai v4.56.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/1030","Image URLs are only allowed for messages with role 'user', but this message with role 'assistant' contains an image URL.","2024-09-03T19:43:58Z","Closed as not planned issue","question","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
Hello,
I have a very simple process that I think used to work with GPT-4 turbo, or a previous version of this library, but that no longer works because of a rule in the nodejs lib.
Everything happens in the same chat:
I create a chat in my chatbot app
I make Dalle 3 generate an image
I ask a question to GPT-4mini, I get the error:
handleLLMError 400 Invalid 'messages[2]'. Image URLs are only allowed for messages with role 'user', but this message with role 'assistant' contains an image URL.
passing this message array:
[
  {
    ""role"": ""user"",
    ""content"": ""Draw a cat""
  },
  {
    ""role"": ""assistant"",
    ""content"": [
      {
        ""type"": ""image_url"",
        ""image_url"": {
          ""url"": ""[URL of generated image stored somewhere]""
        }
      }
    ]
  },
  {
    ""role"": ""user"",
    ""content"": ""Describe this image""
  }]
I don't think it's logical not to be able to describe the image generated by Dalle 3 in GPT-4o. What do you think?
To Reproduce
Everything happens in the same chat:
I create a chat in my chatbot app
I make Dalle 3 generate an image
I ask a question to GPT-4mini, I get the error:
handleLLMError 400 Invalid 'messages[2]'. Image URLs are only allowed for messages with role 'user', but this message with role 'assistant' contains an image URL.
Code snippets
No response
OS
macOS
Node version
node 21
Library version
openai 4.57.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/1027","Incompatibility with Non-Node Environments Due to qs Package","2024-09-17T12:26:00Z","Closed issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
The recent release introduced a dependency on the qs package (#1023), which relies on Node.js’s built-in utils module. This makes the entire SDK unusable in non-Node.js environments, such as browsers.
Dependency Chain:
qs → side-channel → object-inspect
The object-inspect package, a transitive dependency of qs, uses a Node.js built-in module (utils), causing builds to fail in non-Node environments.
To Reproduce
Attempt to build the SDK using esbuild with the target platform set as browser.
The build fails due to the use of the Node.js built-in module.
Code snippets
Minimal Reproduction
OS
macOS
Node version
Chromium v128.0.6613.85
Library version
openai v4.56.2
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/1021","Support Valibot for Structured Outputs","2024-08-29T13:08:25Z","Open issue","enhancement","Confirm this is a feature request for the Node library and not the underlying OpenAI API.
 This is a feature request for the Node library
Describe the feature or improvement you're requesting
Hi, I am the creator of Valibot and would like to ask if you are interested in supporting Valibot for Structured Outputs besides Zod. If so, I would be happy to help with implementation and documentation.
Valibot is a newer schema validation library for TypeScript that I developed about a year ago as part of my bachelor thesis. Since then, the project has grown a lot and is slowly becoming a popular alternative to Zod. Among our partners are two universities as well as companies like Vercel, Netlify and DigitalOcean. I expect to release our v1 RC in the next 2 weeks. This might be the perfect time to integrate it into the OpenAI Node library.
Feel free to check out our website: https://valibot.dev/
Additional context
We are currently in the process of implementing an official toJsonSchema function for Valibot.
 The text was updated successfully, but these errors were encountered: 
🚀1
j3sch reacted with rocket emoji
All reactions
🚀1 reaction"
"https://github.com/openai/openai-node/issues/1017","clarify which zod features are supported","2024-08-28T17:37:35Z","Open issue","No label","Confirm this is a feature request for the Node library and not the underlying OpenAI API.
 This is a feature request for the Node library
Describe the feature or improvement you're requesting
I would like to be able to use the zod enum field, and comments, and have the AI conform to these.
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/1016","typescript fail","2024-08-28T15:11:43Z","Open issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
error TS2345: Argument of type 'BlobPart[]' is not assignable to parameter of type '(Blob | BinaryLike)[]'.
 Type 'BlobPart' is not assignable to type 'Blob | BinaryLike'.
 Type 'ArrayBuffer' is not assignable to type 'Blob | BinaryLike'.
 Type 'ArrayBuffer' is missing the following properties from type 'Float64Array': BYTES_PER_ELEMENT, buffer, byteOffset, copyWithin, and 24 more.
131 return new File(bits, name, options);
 ~~~~
Found 1 error in node_modules/.pnpm/openai@4.56.0/node_modules/openai/src/uploads.ts:131
To Reproduce
use pnpm to install the lib
 use it in any ts file
 run npx tsc
versions:
 typescript: 4.9.5
 node: v20.7.0
 pnpm: 7.33.4
Code snippets
{
  ""name"": ""@zest/common-openai"",
  ""version"": ""1.0.0"",
  ""description"": ""Common openai"",
  ""scripts"": {
    ""test"": ""jest"",
    ""test:watch"": ""jest --watch"",
    ""test:watch:all"": ""jest --watchAll"",
    ""tsc:ci"": ""tsc --noEmit"",
    ""debug"": ""node --inspect-brk node_modules/.bin/jest --runInBand""
  },
  ""author"": ""env0"",
  ""license"": ""MIT"",
  ""main"": ""index.ts"",
  ""private"": true,
  ""dependencies"": {
    ""openai"": ""4.56.0""
  }}
OS
14.1 (23B2073)
Node version
v20.7.0
Library version
4.56.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/1015","Cannot stream chat completions from Azure","2024-08-27T18:54:32Z","Open issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
When streaming chat completions using client.chat.completions.create with AzureOpenAI client and reading with ChatCompletionStreamingRunner.fromReadableStream on the client, the following error occurs:
OpenAIError: Cannot read properties of undefined (reading 'content')

Cause:
The error seems to be caused by choice.delta being undefined at some point during the streaming process, usually at the end of the stream.
Questions:
Is this a known issue?
Will this library support streaming from Azure, given potential differences in response structure?
To Reproduce
Initialize an AzureOpenAI client
Using client.chat.completions.create stream a response to the client (a web app)
Stream the response using ChatCompletionStreamingRunner.fromReadableStream on the client
Observe the error
Code snippets
No response
OS
macOS
Node version
Node v20.14.0
Library version
openai 4.56.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/1012","Constrained decoding with Extended Backus-Naur Form (EBNF)","2024-09-06T12:17:19Z","Closed as not planned issue","No label","Confirm this is a feature request for the Node library and not the underlying OpenAI API.
 This is a feature request for the Node library
Describe the feature or improvement you're requesting
Similar to the current zodResponseFormat, but instead of using Zod schemas, developers would define output structures using EBNF.
Does OpenAI use a CFG internally?
https://openai.com/index/introducing-structured-outputs-in-the-api/
To do this, we convert the supplied JSON Schema into a context-free grammar (CFG).
Implementing this feature would enable more constrained formats such as JSON, SVG, HTML, Git diff patches, PostScript, and CSV.
Here's an example using the OpenAI npm library with openai.beta.chat.completions.parse() and a new ebnfResponseFormat.
JSON EBNF
const jsonEbnf = `json ::= object | arrayobject ::= '{' pair (',' pair)* '}'pair ::= string ':' valuearray ::= '[' value (',' value)* ']'value ::= string | number | object | array | 'true' | 'false' | 'null'string ::= '""' [a-zA-Z0-9_]+ '""'number ::= [0-9]+`;

const completion = await openai.beta.chat.completions.parse({
  model: ""gpt-4o-mini"",
  messages: [
    {
      role: ""system"",
      content: ""Generate a valid JSON object with name and age."",
    },
    { role: ""user"", content: ""Create an example."" },
  ],
  response_format: ebnfResponseFormat(jsonEbnf),});

const result = completion.choices[0].message.parsed;
JSON EBNF with Specific Schema
const specificJsonEbnf = `json ::= objectobject ::= '{' 'name:' string ',' 'age:' number '}'string ::= '""' [a-zA-Z0-9_ ]+ '""'number ::= [0-9]+`;

const specificCompletion = await openai.beta.chat.completions.parse({
  model: ""gpt-4o-mini"",
  messages: [
    {
      role: ""system"",
      content:
        ""Generate a JSON object with the specific schema {name:string, age:number}."",
    },
    { role: ""user"", content: ""Create an example."" },
  ],
  response_format: ebnfResponseFormat(specificJsonEbnf),});

const specificResult = specificCompletion.choices[0].message.parsed;
written with gpt-4o
Additional context
SVG EBNF
const svgEbnf = `svg ::= '<svg' attribute* '>' content '</svg>'attribute ::= [a-zA-Z]+ '=""' [a-zA-Z0-9]+ '""'content ::= '<circle' attribute* '/>' | '<rect' attribute* '/>'`;
HTML EBNF
const htmlEbnf = `html ::= '<html>' content '</html>'content ::= '<head>' headContent '</head>' '<body>' bodyContent '</body>'headContent ::= '<title>' string '</title>'bodyContent ::= element*element ::= '<div>' content '</div>' | '<p>' string '</p>'string ::= '""' [a-zA-Z0-9_ ]+ '""'`;
Git Diff EBNF
const gitDiffEbnf = `diff ::= 'diff --git ' file file '\n' chunk+file ::= 'a/' [a-zA-Z0-9./]+ | 'b/' [a-zA-Z0-9./]+chunk ::= '@@' lineInfo lineInfo '@@\n' changeslineInfo ::= '-' [0-9]+ ',' [0-9]+changes ::= (addition | deletion | context)*addition ::= '+' [a-zA-Z0-9_ ]+ '\n'deletion ::= '-' [a-zA-Z0-9_ ]+ '\n'context ::= ' ' [a-zA-Z0-9_ ]+ '\n'`;
PostScript EBNF
const postscriptEbnf = `postscript ::= '%!' commandscommands ::= command*command ::= operator operand*operator ::= '/' [a-zA-Z]+operand ::= number | string | arrayarray ::= '[' operand* ']'number ::= [0-9]+('.'[0-9]+)?string ::= '(' [a-zA-Z0-9 ]+ ')'`;
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/1011","Bun runtime error: Cannot find module ""openai/_shims/auto/runtime"" from ""/app/.output/server/node_modules/openai/_shims/index.mjs""","2024-08-26T10:37:38Z","Open issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
When trying to run OpenAI in production with bun I get the following error
Cannot find module ""openai/_shims/auto/runtime"" from ""/app/.output/server/node_modules/openai/_shims/index.mjs""
It can be reproduced with minimal setup using Nuxt.
To Reproduce
Clone the repo Minimal Reproduction
Run bun install
Run bun build
Run bun .output/server/index.mjs
When opening localhost:3000/api/test the server should throw the error
Running with node works
Code snippets
No response
OS
macOS, Linux
Node version
Bun v1.1.26
Library version
openai v4.56.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/1010","Can not use OpenAI SDK with Sentry Node agent: TypeError: getDefaultAgent is not a function","2024-08-25T06:05:10Z","Open issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
Referenced previously here, closed without resolution: #903
This is a pretty big issue as it prevents usage of the SDK while using the latest Sentry monitoring package.
To Reproduce
Install Sentry Node sdk via npm i @sentry/node --save
Enter the following code;
import * as Sentry from '@sentry/node';

// Start Sentry
  Sentry.init({
    dsn: ""https://your-sentry-url"",
    environment: ""your-env"",
    tracesSampleRate: 1.0, //  Capture 100% of the transactions
  });
Try to create a completion somewhere in the process after Sentry has been initialized:
const params = {
  model: model,
  stream: true,
  stream_options: {
    include_usage: true
  },
  messages
};const completion = await openai.chat.completions.create(params);
Results in error:
TypeError: getDefaultAgent is not a function
    at OpenAI.buildRequest (file:///my-project/node_modules/openai/core.mjs:208:66)
    at OpenAI.makeRequest (file:///my-project/node_modules/openai/core.mjs:279:44)

Code snippets
(Included)
OS
All operating systems (macOS, Linux)
Node version
v20.10.0
Library version
v4.56.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/1009","Zod => JSONSchema compilation produces infinitely recursive schemas when using transforms","2024-09-13T12:56:29Z","Closed issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
When using a zodFunction with a zod schema that has multiple references to an object with a transformed property, the produced JSON schema is infinitely recursive and invalid. It causes the OpenAI API to 500 when requested.
I believe both of these conditions are necessary to trigger the bug:
the schema contains an object with one property that has a .transform attached to it
the schema contains references to that object more than once, so the JSONSchema transformer has to pop it out into the definitions
The produced JSONSchema ends up looking like this:
{
  ""definitions"": {
     ""test_properties_first_properties_baz"": {
       ""$ref"": ""#/definitions/test_properties_first_properties_baz"",
     }
   }
}
which is self-referential and broken, sad.
To Reproduce
Here's a test that reproduces the error:
// use ajv to validate that the produced json schema is validimport Ajv from ""ajv"";const ajv = new Ajv();

  test(""unchanged schemas with referenced inner pieces produce valid JSON schemas"", () => {
    const Inner = z.object({
      baz: z.boolean().transform((v) => v ?? true),
    });

    const Outer = z.object({
      first: Inner,
      second: Inner,
    });

    const jsonSchema = zodFunction({ name: ""test"", parameters: Outer }).function.parameters;

    expect(jsonSchema).toBeDefined();
    expect(jsonSchema).toMatchSnapshot();

    ajv.compile(jsonSchema as any);
  });
Code snippets
Here's what the problematic compiled JSONSchema looks like:
{
  ""$schema"": ""http://json-schema.org/draft-07/schema#"",
  ""additionalProperties"": false,
  ""definitions"": {
    ""test"": {
      ""additionalProperties"": false,
      ""properties"": {
        ""first"": {
          ""additionalProperties"": false,
          ""properties"": {
            ""baz"": {
              ""type"": ""boolean"",
            },
          },
          ""required"": [
            ""baz"",
          ],
          ""type"": ""object"",
        },
        ""second"": {
          ""$ref"": ""#/definitions/test_properties_first"",
        },
      },
      ""required"": [
        ""first"",
        ""second"",
      ],
      ""type"": ""object"",
    },
    ""test_properties_first"": {
      ""additionalProperties"": false,
      ""properties"": {
        ""baz"": {
          ""$ref"": ""#/definitions/test_properties_first_properties_baz"",
        },
      },
      ""required"": [
        ""baz"",
      ],
      ""type"": ""object"",
    },
    ""test_properties_first_properties_baz"": {
      ""$ref"": ""#/definitions/test_properties_first_properties_baz"",
    },
  },
  ""properties"": {
    ""first"": {
      ""additionalProperties"": false,
      ""properties"": {
        ""baz"": {
          ""type"": ""boolean"",
        },
      },
      ""required"": [
        ""baz"",
      ],
      ""type"": ""object"",
    },
    ""second"": {
      ""$ref"": ""#/definitions/test_properties_first"",
    },
  },
  ""required"": [
    ""first"",
    ""second"",
  ],
  ""type"": ""object"",}
OS
macOS
Node version
node v22.2.0
Library version
openai v4.56.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/1008","Add pagination support for Files list method","2024-08-22T14:04:21Z","Closed as not planned issue","No label","Confirm this is a feature request for the Node library and not the underlying OpenAI API.
 This is a feature request for the Node library
Describe the feature or improvement you're requesting
I have a need to retrieve all files uploaded to OpenAI. Currently files.list() returns only first 10.000 files.
 Could you add pagination to mentioned endpoint please, similar to one for assistants.list()?
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/1007","Help! can not run the default example from npm","2024-08-22T09:22:03Z","Open issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
i tried to run the default example after installing the package and i get this error which is hard to belive

my package.json
{
  ""dependencies"": {
    ""@types/clipboardy"": ""^2.0.1"",
    ""@types/cors"": ""^2.8.17"",
    ""@types/fluent-ffmpeg"": ""^2.1.25"",
    ""cors"": ""^2.8.5"",
    ""fluent-ffmpeg"": ""^2.1.3"",
    ""form-data"": ""^4.0.0"",
    ""keydb"": ""^0.1.10"",
    ""openai"": ""^4.56.0""
  },
  ""name"": ""blogforge"",
  ""version"": ""1.0.0"",
  ""main"": ""index.js"",
  ""scripts"": {
    ""dev:f"": ""cd frontend && next dev"",
    ""dev:s"": ""cd server && ts-node src/index.ts""
  },
  ""keywords"": [],
  ""author"": """",
  ""license"": ""ISC"",
  ""description"": """",
  ""workspaces"": [
    ""./frontend"",
    ""./share"",
    ""./server""
  ]
}
To Reproduce
install the openai package in your project
Code snippets
import OpenAI from 'openai';

const client = new OpenAI({
  apiKey: process.env['OPENAI_API_KEY']!, // This is the default and can be omitted});

async function main() {
  const chatCompletion = await client.chat.completions.create({
    messages: [{ role: 'user', content: 'Say this is a test' }],
    model: 'gpt-3.5-turbo',
  });}

main();
OS
linux fedora
Node version
v20.12.2
Library version
^4.56.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/1004","REALLY Support for max_num_results in FileSearchTool","2024-08-19T02:32:42Z","Open issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
Little to none and conflicting documentation on using max number of results. It was added, removed, changed???
""Unknown parameter: 'attachments[0].tools[0].file_search'."" #886
To Reproduce
Try using max file results anywhere.
Run options max_num_results
Run options file_search.max_num_results
Tools file_search.max_num_results
Code snippets
None.
OS
macOS
Node version
v18
Library version
4.56.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/996","Abort on a function call example is confusing","2024-08-15T08:39:24Z","Open issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
Current doc ""Abort on a function call"" is confusing.
In general, I think use abort is not a good thing, we actually need end/finish method.
To Reproduce
The example code actually not work if use { tool_choice: ""required"" }? It seems await runner.finalFunctionCall() would just throw Error, and all async methods (eg. finalXXX and totalUsage) will throw error anyway after aborted.
Code snippets
No response
OS
macOS
Node version
Node 22
Library version
4.55.7
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/995","Structured outputs: base zod schema using discriminatedUnion does not generate a valid JSON schema","2024-09-11T19:09:35Z","Closed issue","documentation","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
Using the StructuredOutputs documentation and trying to generate a response using a Zod schema based on a plain discriminatedUnion produces an invalid output schema
{
  type: 'json_schema',
  json_schema: {
    name: 'final_schema',
    strict: true,
    schema: {
      anyOf: [Array],
      '$schema': 'http://json-schema.org/draft-07/schema#'
    }
  }
}

The issue specifically has to do with not having a type property inside of schema property
Is this solvable?
Or should we always nest a discriminated union within a 'object' type property? In that case, it would be good to discriminate it in the documentation
Thanks a lot!
To Reproduce
install openai 4.55.7 package
install zod ^3.23.8
Follow the code snippets
Code snippets
import { z } from 'zod';import { zodResponseFormat } from 'openai/helpers/zod';

const BaseResponseSchema = z.object({
  status: z
    .literal('success'),
  file_source_ids: z
    .string()
    .array(),
   result: z.string()});

const UnsuccessfulResponseSchema = z.object({
  status: z
    .enum([
      'missing_information',
      'could_not_answer',
    ]),
  explanation: z.string(),});

const finalSchema = z.discriminatedUnion('status', [
          BaseResponseSchema,
          UnsuccessfulResponseSchema,
        ]);

const json = zodResponseFormat(finalSchema, 'final_schema');

console.log(json);
OS
Debian
Node version
22.6
Library version
openai v4.55.7
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/994","runTools() without stream:true incorrectly emits ""message"" events for messages passed in","2024-09-03T14:12:50Z","Closed issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
The documentation for the ""message"" event on the runner returned by openai.beta.chat.completions.runTools() says ""The event fired when a new message is either sent or received from the API. Does not fire for the messages sent as the parameter to either .runTools() or .stream()"". This works correctly if you pass stream: true in the options to runTools(), but if you don't pass stream: true, the runner always emits ""message"" events for all of the messages including those passed in as parameters.
This is an issue for me because I want to record all the messages (including tool calls and tool results) generated by a runTools() call and add them to the list of existing messages so that they're visible to GPT when processing future messages from the user, and from the docs it looks like the ""message"" events are the proper way to do this, but because of this issue it seems like there's no clean way to do what I need unless I pass in stream: true, which doesn't seem like it's meant to affect this behavior. (I have no issue with using stream: true but it took me a while to think to try it.)
I initially wondered if this bug was tied to how I was using runTools(), but I run into this same exact issue with the in-repo example at https://github.com/openai/openai-node/blob/master/examples/function-call-helpers.ts (okay that uses the older runFunctions() method instead but I'm assuming it's meant to work the same).
To Reproduce
Call openai.beta.chat.completions.runTools() with one or more messages, without specifying stream: true.
Add a ""message"" event listener on the returned runner.
Observe that it emits events for all the messages including the ones passed in originally, not just the messages generated by the runTools() call.
Change the runTools() call to set the stream: true option and see that it correctly emits ""message"" events only for new messages.
Code snippets
import OpenAI from ""openai"";

const openai = new OpenAI();

const runner = openai.beta.chat.completions.runTools({
  model: ""gpt-4o"",
  messages: [
    {
      role: ""system"",
      content: ""You are a helpful assistant."",
    },
    {
      role: ""user"",
      content: ""Call the log function with an ice cream flavor please."",
    },
  ],
  tools: [
    {
      type: ""function"",
      function: {
        name: ""log"",
        description: ""Logs the input"",
        function(args) {
          console.log(""logged args"", args);
        },
        parameters: {
          type: ""object"",
          properties: {
            text: { type: ""string"" },
          },
        },
      },
    },
  ],});

runner.on(""message"", (message) => {
  console.log(""runner.on message"", message);});

await runner.done();
OS
Windows
Node version
Node v21.7.3, Deno 1.45.5
Library version
openai v4.55.7
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/985","License issue","2024-08-12T14:10:49Z","Open issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
Hi OpenAI! 👋🏻 It's really cool to see you're making good use of my package zod-to-json-schema! I see you've made some interesting changes in the vendored version. I'd be happy to take a look at them in the form of a PR and integrate them into the ""canonical"" version, which you can then use as a regular dependency.
In the meantime, to resolve the issue caused by the license, you can include the license and copyright notice in the vendored files
...or offer me a job 😉
To Reproduce
See ISC license conditions in https://github.com/StefanTerdell/zod-to-json-schema/blob/master/LICENSE
Code snippets
No response
OS
n/a
Node version
n/a
Library version
4.55.4
 The text was updated successfully, but these errors were encountered: 
😄1
MartinCura reacted with laugh emoji
All reactions
😄1 reaction"
"https://github.com/openai/openai-node/issues/984","openai-deno: Type-checking error for zodResponseFormat with Deno and deno.land imports","2024-08-12T10:47:50Z","Open issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
Type-checking using ""deno check"" fails with either
error: TS2589 [ERROR]: Type instantiation is excessively deep and possibly infinite.
 response_format: zodResponseFormat(MathResponse, ""math_response"")
or
Fatal JavaScript out of memory: Reached heap limit
when using deno.land imports for OpenAI and Zod.
To Reproduce
(The example is from https://github.com/openai/openai-node/blob/master/helpers.md#auto-parsing-response-content-with-zod-schemas)
With deno.land imports
main.ts:
import { zodResponseFormat } from ""openai/helpers/zod.ts"";import OpenAI from ""openai/mod.ts"";import { z } from ""zod"";

const Step = z.object({
  explanation: z.string(),
  output: z.string(),});

const MathResponse = z.object({
  steps: z.array(Step),
  final_answer: z.string(),});

const client = new OpenAI();

const completion = await client.beta.chat.completions.parse({
  model: ""gpt-4o-2024-08-06"",
  messages: [
    { role: ""system"", content: ""You are a helpful math tutor."" },
    { role: ""user"", content: ""solve 8x + 31 = 2"" },
  ],
  response_format: zodResponseFormat(MathResponse, ""math_response""),});

console.dir(completion, { depth: 5 });

const message = completion.choices[0]?.message;if (message?.parsed) {
  console.log(message.parsed.steps);
  console.log(`answer: ${message.parsed.final_answer}`);}
with deno.json:
{
  ""imports"": {
    ""zod"": ""https://deno.land/x/zod@v3.23.8/mod.ts"",
    ""openai/"": ""https://deno.land/x/openai@v4.55.4/""
  }
}
Running deno check main.ts fails with:
Check file://main.ts
 error: TS2589 [ERROR]: Type instantiation is excessively deep and possibly infinite.
 response_format: zodResponseFormat(MathResponse, ""math_response""),
With npm imports
main.ts:
import { zodResponseFormat } from ""openai/helpers/zod"";import OpenAI from ""openai"";import { z } from ""zod"";

[same code]
with deno.json:
{
  ""imports"": {
    ""zod"": ""npm:zod@3.23.8"",
    ""openai"": ""npm:openai@4.55.4"",
    ""openai/"": ""npm:/openai@4.55.4/""
  }
}
Running deno check main.ts with npm imports is successful
OS
macOS
Node version
deno 1.45.5, typescript 5.5.2
Library version
4.55.4
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/983","Structured Outputs fail with gpt4o when Zod schema includes pattern","2024-08-09T20:56:11Z","Open issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
There is currently an issue when passing a Zod schema containg z.ZodStrings with patterns the new gpt4o structure outputs API with the NodeJS library.
For instance:
const MySchema = z.object({ myString: z.string().includes('my-pattern') })
results in the following error:
BadRequestError: 400 Invalid schema for response_format 'my-schema': In context=('properties', 'myString'), 'pattern' is not permitted
I have temporarily fixed this by wrapping all schemas passed to this endpoint with the following recursive removal code below.
function removeStringIncludes(schema: z.ZodTypeAny): z.ZodTypeAny {
  if (schema instanceof z.ZodString) {
    return z.string();
  } else if (schema instanceof z.ZodObject) {
    const newShape: { [k: string]: z.ZodTypeAny } = {};
    Object.entries(schema.shape).forEach(([key, value]) => {
      newShape[key] = removeStringIncludes(value as z.ZodTypeAny);
    });
    return z.object(newShape);
  } else if (schema instanceof z.ZodArray) {
    return z.array(removeStringIncludes(schema.element));
  } else if (schema instanceof z.ZodEnum) {
    return z.enum(schema._def.values.map((v: any) => removeStringIncludes(v)));
  } else if (schema instanceof z.ZodUnion) {
    return z.union(schema.options.map(removeStringIncludes));
  } else if (schema instanceof z.ZodIntersection) {
    return z.intersection(
      removeStringIncludes(schema._def.left),
      removeStringIncludes(schema._def.right)
    );
  }
  
  return schema;
}

It would be nice for OpenAI to support all schemas, as there are lots of validations baked into Zod schemas that are really convenient and having to work around this and strip them out while passing them to the API is tedious and weakens the power of the almighty Zod.
To Reproduce
pass this schema via zodResponseFormat(mySchema, 'schema') to the new structure outputs endpoint with the sdk.
const MySchema = z.object({ myString: z.string().includes('my-pattern') })'
Code snippets
No response
OS
macOS
Node version
Node v18.18.2
Library version
openai v4.55.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/978","Zod => JSONSchema conversion creates references to unknown definitions","2024-08-09T18:53:58Z","Closed issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
When compiling a zod schema with multiple references to the same nullable object, the compiled JSONSchema refers to definitions that don't exist. This is using the latest version of the openai client with structured output support. I believe the issue comes from an extracted definition trying to reference an inner extracted definition again -- see the example below.
To Reproduce
Here's an example zod schema and function call which triggers the issue:
// optional object that can be on each field, mark it as nullable to comply with structured output restrictionsconst metadata = z.nullable(
  z.object({
    foo: z.string(),
  }));

// union element aconst fieldA = z.object({
  type: z.literal(""string""),
  name: z.string(),
  metadata,});

// union element b, both referring to above nullable objectconst fieldB = z.object({
  type: z.literal(""number""),
  metadata,});

// top level input object with array of union elementconst model = z.object({
  name: z.string(),
  fields: z.array(z.union([fieldA, fieldB])),});

const completion = await openai.beta.chat.completions.parse({
  model: ""gpt-4o-2024-08-06"",
  messages: [
    {
      role: ""system"",
      content: ""You are a helpful assistant. Generate a data model according to the user's instructions."",
    },
    { role: ""user"", content: ""create a todo app data model"" },
  ],
  tools: [zodFunction({ name: ""query"", parameters: model })],});expect(completion.choices[0].message.tool_calls[0].function.parsed_arguments).toMatchInlineSnapshot();
When run, I get this error:
400 Invalid schema for function 'query': In context=('anyOf', '0'), reference to component 'query_properties_fields_items_anyOf_0_properties_metadata_anyOf_0' which was not found in the schema.

I ninja'd into the source and console.log'd the generated JSON schema, here's what comes out:
    {
      ""type"": ""object"",
      ""properties"": {
        ""name"": {
          ""type"": ""string""
        },
        ""fields"": {
          ""type"": ""array"",
          ""items"": {
            ""anyOf"": [
              {
                ""type"": ""object"",
                ""properties"": {
                  ""type"": {
                    ""type"": ""string"",
                    ""const"": ""string""
                  },
                  ""name"": {
                    ""type"": ""string""
                  },
                  ""metadata"": {
                    ""anyOf"": [
                      {
                        ""type"": ""object"",
                        ""properties"": {
                          ""foo"": {
                            ""type"": ""string""
                          }
                        },
                        ""required"": [
                          ""foo""
                        ],
                        ""additionalProperties"": false
                      },
                      {
                        ""type"": ""null""
                      }
                    ]
                  }
                },
                ""required"": [
                  ""type"",
                  ""name"",
                  ""metadata""
                ],
                ""additionalProperties"": false
              },
              {
                ""type"": ""object"",
                ""properties"": {
                  ""type"": {
                    ""type"": ""string"",
                    ""const"": ""number""
                  },
                  ""metadata"": {
                    ""$ref"": ""#/definitions/query_properties_fields_items_anyOf_0_properties_metadata""
                  }
                },
                ""required"": [
                  ""type"",
                  ""metadata""
                ],
                ""additionalProperties"": false
              }
            ]
          }
        }
      },
      ""required"": [
        ""name"",
        ""fields""
      ],
      ""additionalProperties"": false,
      ""definitions"": {
        ""query_properties_fields_items_anyOf_0_properties_metadata"": {
          ""anyOf"": [
            {
              ""$ref"": ""#/definitions/query_properties_fields_items_anyOf_0_properties_metadata_anyOf_0""
            },
            {
              ""type"": ""null""
            }
          ]
        },
        ""query"": {
          ""type"": ""object"",
          ""properties"": {
            ""name"": {
              ""type"": ""string""
            },
            ""fields"": {
              ""type"": ""array"",
              ""items"": {
                ""anyOf"": [
                  {
                    ""type"": ""object"",
                    ""properties"": {
                      ""type"": {
                        ""type"": ""string"",
                        ""const"": ""string""
                      },
                      ""name"": {
                        ""type"": ""string""
                      },
                      ""metadata"": {
                        ""anyOf"": [
                          {
                            ""type"": ""object"",
                            ""properties"": {
                              ""foo"": {
                                ""type"": ""string""
                              }
                            },
                            ""required"": [
                              ""foo""
                            ],
                            ""additionalProperties"": false
                          },
                          {
                            ""type"": ""null""
                          }
                        ]
                      }
                    },
                    ""required"": [
                      ""type"",
                      ""name"",
                      ""metadata""
                    ],
                    ""additionalProperties"": false
                  },
                  {
                    ""type"": ""object"",
                    ""properties"": {
                      ""type"": {
                        ""type"": ""string"",
                        ""const"": ""number""
                      },
                      ""metadata"": {
                        ""$ref"": ""#/definitions/query_properties_fields_items_anyOf_0_properties_metadata""
                      }
                    },
                    ""required"": [
                      ""type"",
                      ""metadata""
                    ],
                    ""additionalProperties"": false
                  }
                ]
              }
            }
          },
          ""required"": [
            ""name"",
            ""fields""
          ],
          ""additionalProperties"": false
        }
      },
      ""$schema"": ""http://json-schema.org/draft-07/schema#""
    }
Code snippets
No response
OS
macOS
Node version
v22.2.0
Library version
4.55.3
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/977","batches.list() doesn't iterate properly with Azure OpenAI","2024-08-12T17:37:34Z","Closed issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
When using client.batches.list() on an Azure OpenAI client, I'm unable to iterate through it like the documentation from OpenAI says I should be able to. https://platform.openai.com/docs/guides/batch/getting-started?lang=node
client.batches.list() currently returns a object that looks like this:
BatchesPage {
  options: {
    method: 'get',
    path: '/batches',
    query: {},
    headers: {
      Authorization: 'Bearer REMOVED'
    }
  },
  response: Response {
    cf: undefined,
    webSocket: null,
    url: 'https://REMOVED.openai.azure.com//openai/batches?api-version=2024-07-01-preview',
    redirected: false,
    ok: true,
    headers: Headers(13) {
      'apim-request-id' => '68da25df-8dc4-4b8c-82b7-cd859d6b5ece',
      'content-length' => '17552',
      'content-type' => 'application/json; charset=utf-8',
      'date' => 'Thu, 08 Aug 2024 23:15:22 GMT',
      'request-context' => 'appId=cid-v1:2d2e8e63-272e-4b3c-8598-4ee570a0e70d',
      'strict-transport-security' => 'max-age=31536000; includeSubDomains; preload',
      'vary' => 'Accept-Encoding',
      'x-aml-cluster' => 'vienna-eastus-02',
      'x-content-type-options' => 'nosniff',
      'x-ms-client-request-id' => 'Not-Set',
      'x-ms-region' => 'East US',
      'x-ms-response-type' => 'standard',
      'x-request-time' => '0.973',
      [immutable]: true
    },
    statusText: 'OK',
    status: 200,
    bodyUsed: true,
    body: ReadableStream { locked: true, [state]: 'closed', [supportsBYOB]: true, [length]: 0n }
  },
  body: {
    value: [
      [Object], [Object], [Object],
      [Object], [Object], [Object],
      [Object], [Object], [Object],
      [Object], [Object], [Object],
      [Object], [Object], [Object],
      [Object], [Object], [Object],
      [Object], [Object]
    ],
    first_id: 'batch_5d50b84a-f6e9-4ecc-a3c4-0a1a23a532f3',
    has_more: true,
    last_id: 'batch_c3dfc7a1-63b0-4437-a0e9-dd952e38e51f'
  },
  data: []
}

To Reproduce
If you use the following code, you'll be able to reproduce the bug.
const client = new AzureOpenAI({
  azureADTokenProvider,
  deployment: env.AZURE_OPENAI_DEPLOYMENT,
  endpoint: env.AZURE_OPENAI_ENDPOINT,
  apiVersion: ""2024-07-01-preview""});

const list = await client.batches.list();

for await (const batch of list) { // results in nothing being printed out
  console.log(batch);}
Code snippets
No response
OS
macOS
Node version
Cloudflare Worker v3.69.1
Library version
openai v4.55.2
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/972","Difficulty in importing { zodResponseFormat } from https://deno.land","2024-08-07T13:42:16Z","Open issue","No label","Confirm this is a feature request for the Node library and not the underlying OpenAI API.
 This is a feature request for the Node library
Describe the feature or improvement you're requesting
In testing with structured outputs, import { zodResponseFormat } from 'npm:openai/helpers/zod'; works correctly at runtime (with --unstable, but does not provide types), but import { zodResponseFormat } from 'https://deno.land/x/openai@v4.55.0/helpers/zod.ts'; yields runtime resolution errors. I do not have enough expertise to know whether I'm doing something wrong or whether there is a problem in the deno.land for the helpers/zod. Please advise.
Additional context
In testing with the structured outputs, this works correctly (using npm: import resolution):
// deno run --allow-all --unstable src/main-structured.ts

import OpenAI from 'npm:openai';import { zodResponseFormat } from 'npm:openai/helpers/zod';import { z } from 'npm:zod';

const openai = new OpenAI();

const WriteFile = z.object( {
  op: z.literal( 'writeFile' ),
  writeFilename: z.string(),
  contents: z.string()} );

const ReadFile = z.object( {
  op: z.literal( 'readFile' ),
  readFilename: z.string()} );

// Define an anyOf schema using Zodconst FileOperation = z.object( {
  selectedAction: z.union( [ ReadFile, WriteFile ] )} );

const x = zodResponseFormat( FileOperation, 'mySchema' )console.log( JSON.stringify( x, null, 2 ) );

// Use the anyOf schema in response_formatconst completion = await openai.beta.chat.completions.parse( {
  model: 'gpt-4o-2024-08-06',
  messages: [
    { role: 'system', content: 'Choose the next command to accomplish the goal.' },
    { role: 'user', content: 'Write a haiku' }
  ],
  response_format: zodResponseFormat( FileOperation, 'mySchema' )} );

const event = completion.choices[ 0 ].message.parsed;console.log( event );
However, this does not yield type information and must be run with --unstable. The openai-node documentation says we can import from deno like so:
import OpenAI from 'https://deno.land/x/openai@v4.55.0/mod.ts';
However, when trying to import all imports from deno.land like so:
// Note --unstable no longer needed// deno run --allow-all src/main-structured.ts

import OpenAI from 'https://deno.land/x/openai@v4.55.0/mod.ts';import { zodResponseFormat } from 'https://deno.land/x/openai@v4.55.0/helpers/zod.ts';import { z } from 'https://deno.land/x/zod@v3.23.8/mod.ts';// etc...
We obtain this error at runtime:
~/myProject$ deno run --allow-all src/main-structured.ts
error: Remote modules are not allowed to import local modules. Consider using a dynamic import instead.
  Importing: npm:zod
    at https://deno.land/x/openai@v4.55.0/_vendor/zod-to-json-schema/parsers/tuple.ts:1:56

I do not have enough expertise to know whether I'm doing something wrong or whether there is a problem in the deno.land for the helpers/zod. Please advise.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/970","JSONSchema does not support recursive refs & zod also fails to convert similar schemas to JSONSchema correctly","2024-08-07T15:00:08Z","Closed issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
Copy and pasting example schemas (zod included) from the latest documents results in errors when using the latest openai sdk and having a focus on $ref for the new response_format that supports JSONSchema.
zods example @ https://platform.openai.com/docs/guides/structured-outputs/ui-generation
 Invalid schema for response_format 'ui': In context=('properties', 'children', 'items'), reference to component '#' which was not found in the schema.

and any JSONSchema example I've tried (e.g. https://platform.openai.com/docs/guides/structured-outputs/recursive-schemas-are-supported)
2 | import { castToError } from ""./core.mjs"";
3 | export class OpenAIError extends Error {
4 | }
5 | export class APIError extends OpenAIError {
6 |     constructor(status, error, message, headers) {
7 |         super(`${APIError.makeMessage(status, error, message)}`);
                          ^
error: 500 The server had an error while processing your request. Sorry about that!
      at new OpenAIError (:1:23)
      at new APIError (/node_modules/openai/error.mjs:7:9)
      at new InternalServerError (:1:23)
      at generate (/node_modules/openai/error.mjs:62:20)
      at /node_modules/openai/core.mjs:312:30

Initial debugging suggests that the Zod to JSONSchema conversion code looks for definitions and $defs is not considered.. but then when the resulting schema is used, it requires $defs 🤷 so the generated schema from zod is invalid). For the JSONSchema, it struggles with a lot but when I get it to pass the simple syntax validation, it falls over on the server and I cannot figure out where the issue lies based on the error.
To Reproduce
Install latest ""openai"": ""^4.55.0""
copy any of the zod or schemas that include definitions, $defs, or recursive schemas.
invoke the API via any of the usual ways with the new response_format
const stream = openai.beta.chat.completions
  .stream({
    model: ""gpt-4o-2024-08-06"",
    max_tokens: 16384,
    response_format: { type: ""json_schema"", json_schema: schema },
    ...
(helps to also include DEBUG=true when running though nothing valuable appears to be thrown)
Code snippets
No response
OS
macOS
Node version
bun 1.1.20
Library version
4.55.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/959","Getting Unhandled Rejection event, randomly.","2024-07-29T07:15:55Z","Open issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
A Node.js library is throwing an unhandled promise rejection error due to improper error handling.
To Reproduce
How to reproduce this issue:
run and stream, then add an event listener to the stream.
Attempt to run and stream again on the same thread ID. This will trigger an unhandled rejection, causing your Node server to crash if there is no global error handler in place.
Ideally, this error should be handled with stream.on('error', myCustomErrorHandler).
Code snippets
No response
OS
macOs
Node version
v18.19.0
Library version
openai 4.52.7
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/958","openai.files.content returns raw Response object","2024-07-27T08:50:59Z","Open issue","documentation","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
In the documentation, openai.files.content is described as returns content as string type.
 However, it returns raw Response type.
import OpenAI from ""openai"";

const openai = new OpenAI();

async function main() {
  const file = await openai.files.content(""file-abc123"");

  console.log(file); // Expected to show a string (ref. https://platform.openai.com/docs/api-reference/files/retrieve-contents)}

main();
Output:

To Reproduce
Write the code above (copy&pasted from https://platform.openai.com/docs/api-reference/files/retrieve-contents)
It shows file is a Response object
Code snippets
No response
OS
macOS
Node version
Node v18.17.0
Library version
openai v4.53.1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/951","Stop using polyfills for Node.js","2024-07-26T08:00:03Z","Closed issue","No label","Confirm this is a feature request for the Node library and not the underlying OpenAI API.
 This is a feature request for the Node library
Describe the feature or improvement you're requesting
My understanding is that this module use polyfills for Node.js.
 There is no need for doing so given you support Node.js v18+.
This causes compatibility issues with the rest of the ecosystem.
Ref fastify/fastify#5584
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/945","OpenAIError: Final run has not been received","2024-07-22T10:41:35Z","Closed as not planned issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
Hi! Our team is trying to deal with a lot of OpenAIError: Final run has not been received errors. Is it an OpenAI API issue, our end bug or is there something you could do on your end to fix it? Thanks!
To Reproduce
Call Assistants Tools Stream API
Code snippets
No response
OS
macOS
Node version
v20.11.1
Library version
openai v4.46.1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/943","Arrays for model names","2024-07-19T15:03:48Z","Open issue","No label","Confirm this is a feature request for the Node library and not the underlying OpenAI API.
 This is a feature request for the Node library
Describe the feature or improvement you're requesting
I have a need for model names defined as iterables, in particular for enforcing typesafety in various places.
Take Sequelize enums for example:
DataTypes.ENUM({
  values: ['value', 'another value']})
or programmatically generating GraphQL enums:
const RGBType = new GraphQLEnumType({
  name: 'RGB',
  values: {
    RED: { value: 0 },
    GREEN: { value: 1 },
    BLUE: { value: 2 }
  }});
or Zod enums:
const VALUES = [""Salmon"", ""Tuna"", ""Trout""] as const;const FishEnum = z.enum(VALUES);
As it stands with the union type exported from OpenAI, we'd have to manually create these arrays and maintain them on our side.
It would be great to have something like this instead which would give us the best of both worlds:
export const ChatModelArray = [
  'gpt-4o',
  'gpt-4o-2024-05-13',
  'gpt-4-turbo',
  'gpt-4-turbo-2024-04-09',
  'gpt-4-0125-preview',
  'gpt-4-turbo-preview',
  'gpt-4-1106-preview',
  'gpt-4-vision-preview',
  'gpt-4',
  'gpt-4-0314',
  'gpt-4-0613',
  'gpt-4-32k',
  'gpt-4-32k-0314',
  'gpt-4-32k-0613',
  'gpt-3.5-turbo',
  'gpt-3.5-turbo-16k',
  'gpt-3.5-turbo-0301',
  'gpt-3.5-turbo-0613',
  'gpt-3.5-turbo-1106',
  'gpt-3.5-turbo-0125',
  'gpt-3.5-turbo-16k-0613',];

export type ChatModel = (typeof ChatModelArray)[number];
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
👍3
mgoetzke, gilles-yvetot, and tiffling reacted with thumbs up emoji
All reactions
👍3 reactions"
"https://github.com/openai/openai-node/issues/939","Vector Store File CreateAndPoll with empty file, last_error.code returned is ""invalid_file"" but TS say not exist.","2024-07-29T11:09:10Z","Closed issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
I try to manage the ""invalid_file"" error, generated when you upload to a vector store an empy file (Pdf, docx, other) and Typescript say me the ""invalid_file"" code not exist.
To Reproduce
Try to attach an empty vector store file to a vector store for reproduce the error.
 Response is:
 last_error.code = ""invalid_file""
 last_error.message = ""The file could not be parsed because it is empty.""
Code snippets
const myVectorStoreFile = await openai.beta.vectorStores.files.createAndPoll(
       store_id
        {
          file_id:file_id
        }
      );

console.log('Openai Error code:',myVectorStoreFile.last_error.code);
OS
macOS
Node version
v18.20.1
Library version
^4.52.1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/924","extra_body parameter support","2024-07-09T08:22:22Z","Closed as not planned issue","No label","Confirm this is a feature request for the Node library and not the underlying OpenAI API.
 This is a feature request for the Node library
Describe the feature or improvement you're requesting
I can't find the extra_body parameter in openai typescript SDK. Is it possible to support this parameter in typescript?
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/922","Can't build NextJS project with openai library. Getting: Type error: Private identifiers are only available when targeting ECMAScript 2015 and higher.","2024-08-19T14:43:13Z","Closed as not planned issue","question","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
Getting this kind of error on build time on NextJS 14 and I don't know why
this is my tsconfig.json
{
  ""compilerOptions"": {
    ""target"": ""ES2023"",
    ""lib"": [""dom"", ""dom.iterable"", ""esnext""],
    ""allowJs"": true,
    ""skipLibCheck"": true,
    ""strict"": true,
    ""noEmit"": true,
    ""esModuleInterop"": true,
    ""module"": ""esnext"",
    ""moduleResolution"": ""bundler"",
    ""resolveJsonModule"": true,
    ""isolatedModules"": true,
    ""jsx"": ""preserve"",
    ""incremental"": true,
    ""plugins"": [
      {
        ""name"": ""next""
      }
    ],
    ""paths"": {
      ""@/*"": [""./*""]
    }
  },
  ""include"": [""next-env.d.ts"", ""**/*.ts"", ""**/*.tsx"", "".next/types/**/*.ts""],
  ""exclude"": [""node_modules""]
}
Thanks for help.
To Reproduce
Install and use library on nextjs
import something like import typoe { Message } from 'openai/resources/beta/threads/messages';
Code snippets
No response
OS
macOS
Node version
v22.3.0
Library version
4.52.3
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/921","thread_id not recognized from Thread object","2024-07-18T13:45:43Z","Closed issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
I have the following code:
const run = await openai.beta.threads.createAndRunPoll({ thread: thread, assistant_id: usedAssistant.id, });
Typescript shows no error as it seems to be expecting a Thread object which thread is:
    const thread = await openai.beta.threads.create({
      messages: [
        {
          role: 'user',
          content: invoiceExpensePrompt(),
          attachments: [{ file_id: uploadedFile.id, tools: [{ type: 'file_search' }] }],
        },
      ],
    });

The error I receive is the following:
Failed to generate content from PDF document BadRequestError: 400 Unknown parameter: 'thread.id'.
    at APIError.generate (/app/node_modules/openai/error.js:45:20)
    at OpenAI.makeStatusError (/app/node_modules/openai/core.js:275:33)
    at OpenAI.makeRequest (/app/node_modules/openai/core.js:318:30)
    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)
    at async Threads.createAndRunPoll (/app/node_modules/openai/resources/beta/threads/threads.js:91:21)
    at async getDataFromExpenseInvoicePdf (/app/dist/services/openai.service.js:77:21)
    at async analyzePdfDocumentByOpenAiGpt (/app/dist/controllers/paperless.controller.js:80:12)
    at async postConsumption (/app/dist/controllers/paperless.controller.js:52:30) {
  status: 400,
  headers: {
    'alt-svc': 'h3="":443""; ma=86400',
    'cf-cache-status': 'DYNAMIC',
    'cf-ray': '89f790f50f719280-FRA',
    connection: 'keep-alive',
    'content-length': '165',
    'content-type': 'application/json',
    date: 'Sun, 07 Jul 2024 11:41:35 GMT',
    'openai-organization': 'jp-abig-gmbh',
    'openai-processing-ms': '9',
    'openai-version': '2020-10-01',
    server: 'cloudflare',
    'set-cookie': '__cf_bm=Ny2gnklaMhvob6uOe2uv2ChV7Z4QVp1rTVUR4NH0SQo-1720352495-1.0.1.1-QT.sZQTLPKF6XIBGaGawGdp0JPodPJXgKJYljslhhE.dhJJPth5.n83zNI9pXEezJKBpWv0U4VrcBg.g9PHmgw; path=/; expires=Sun, 07-Jul-24 12:11:35 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None, _cfuvid=KJyygLXSIziDZQlDRsYxkvnLOu4qJ.w.vFH2Tc.CFvc-1720352495034-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None',
    'strict-transport-security': 'max-age=15724800; includeSubDomains',
    'x-request-id': 'req_47110214306012817ebd2b3c4e77bd29'
  },
  request_id: 'req_47110214306012817ebd2b3c4e77bd29',
  error: {
    message: ""Unknown parameter: 'thread.id'."",
    type: 'invalid_request_error',
    param: 'thread.id',
    code: 'unknown_parameter'
  },
  code: 'unknown_parameter',
  param: 'thread.id',
  type: 'invalid_request_error'
}

To Reproduce
NodeJS Library: Create a function that creates an assistant and a thread, pass it on to the SDK to run and poll.
Code snippets
No response
OS
Docker
Node version
Node v22
Library version
""openai"": ""^4.52.3""
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/919","Property 'text' does not exist on type 'MessageContent'","2024-07-03T09:55:21Z","Closed as not planned issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
Since the MessageContent type includes the TextContentBlock type, so there must be a text property, but it doesn't work
To Reproduce
const messages = await openai.beta.threads.messages.list(threadId, {
  order: ""asc"",});

messages.data.at(-1)?.content[0].text.value;//                                 ^// Property 'text' does not exist on type 'MessageContent'.//  Property 'text' does not exist on type 'ImageFileContentBlock'.ts(2339)
Code snippets
import OpenAI from ""openai"";

const openai = new OpenAI({
  apiKey: process.env.NEXT_PUBLIC_OPENAI_API_KEY,});const assistantId = process.env.NEXT_PUBLIC_ASSISTANT_ID;const threadId = process.env.NEXT_PUBLIC_THREAD_ID;

export async function receiveMessage() {
  const messages = await openai.beta.threads.messages.list(threadId, {
    order: ""asc"",
  });
  return messages.data.at(-1)?.content[0].text.value;}
OS
Windows 11
Node version
v20.9.0
Library version
4.52.3
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/913","Allow to create OpenAI client without specifying apiKey or to be defined as an environment variable.","2024-07-08T17:11:42Z","Closed as not planned issue","No label","Confirm this is a feature request for the Node library and not the underlying OpenAI API.
 This is a feature request for the Node library
Describe the feature or improvement you're requesting
It would be useful to allow the creation of OpenAI client without specifying an apiKey (or OPENAI_API_KEY as an environment variable).
The use case for that is:
Have a server with API compatible with OpenAI spec, instead of writing a custom wrapper. (This can save a lot of time and efforts)
Have the APIKey managed on the server
Use the OpenAI official client in the browser directly and change the baseURL for my custom server
If the api key is provided from the client, use the one passed in request. Otherwise, use the default one on the server.
As of today
Using this approach as of today, require to:
set a mock ""apiKey"" on the client
Set the flag dangerouslyAllowBrowser: true on the client
ignore the passed api key on the server side
and does not support the use case If the api key is provided from the client, use the one passed in request. Otherwise, use the default one on the server.. Would need to pass some header or another workaround to tell the server to ignore the key (which is not clean)
new OpenAI({
    baseURL,
    apiKey: 'my-mock-key',
    dangerouslyAllowBrowser: true,
  })
With the proposed change
new OpenAI({
    baseURL,
    dangerouslyAllowBrowser : true,
  })
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/910","Bug Report: openai.files.create Method Hangs and Does Not Throw Errors in Docker Containers","2024-06-23T11:58:25Z","Closed issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
Bug Report: openai.files.create Method Hangs and Does Not Throw Errors in Docker Containers
Bug Report ID: BR-20240622-001
Title:openai.files.create Method Hangs and Does Not Throw Errors in Docker Containers
Reporter: simonorzel26
Date: June 22, 2024
Environment:
Docker Image:oven/bun:latest-debian, bun:debian, imbios/bun-node:20-slim and other container versions (mainly bun)
Node Version:v20.x
Bun Version:1.1.15
OpenAI API Version: Latest
Host OS: Ubuntu 20.04 LTS
Network: Verified with curl, network access is functional
Memory Allocated: 16GB
CPUs Allocated: 4
Timeout Setting: Increased to 120 seconds
Description:
 The openai.files.create method hangs indefinitely and does not throw any errors when executed within a Docker container. The method works as expected when run locally on the host machine via building js scripts and running them. This behavior prevents the file upload process from completing, causing the application to stall and making the issue unfixable due to the lack of error feedback and only timeout error happens.
Steps to Reproduce:
Set up a Docker container using oven/bun:latest-debian, bun:debian, or similar variants.
Implement and call the openai.files.create method within the container.
Observe that the method hangs and does not throw any errors.
Expected Behavior:
 The openai.files.create method should either successfully upload the file and return the file ID or throw an error if the upload fails.
Actual Behavior:
 The method hangs indefinitely and does not throw any errors, causing the application to stall and timeout the request.
Additional Information:
Network Check:curl and ping to external endpoints work correctly inside the Docker container, confirming network access.
Environment Variables:OPENAI_API_KEY is set correctly within the container. Tested via running a completion api in same container.
File Permissions: Verified that the files have the correct permissions and are accessible within the container. As I have built the functionality via direct file access from container and upload from that, as well as in-RAM file creation. (both with only 1 item in the .jsonl file)
API Calls: Other OpenAI API calls, such as openai.chat.completions.create, work correctly within the container, confirming the API is accessible over network.
Hardware Resources: The container has 16GB of memory and 4 CPUs allocated, ensuring no resource bottlenecks.
Timeout Setting: Increased the timeout to 120 seconds without resolving the issue.
Environments: Tested in many different docker images as seen above
Forgive the mess of 8+hrs of debugging:
Code:
FROM imbios/bun-node:20-slim AS deps
ARG DEBIAN_FRONTEND=noninteractive

# Install necessary packages
RUN apt-get update -y && \
    apt-get install -y --no-install-recommends \
    openssl \
    git \
    ca-certificates \
    tzdata && \
    ln -fs /usr/share/zoneinfo/Europe/Berlin /etc/localtime && \
    echo ""Europe/Berlin"" > /etc/timezone && \
    dpkg-reconfigure -f noninteractive tzdata && \
    apt-get clean && \
    rm -rf /var/lib/apt/lists/*

WORKDIR /app

# Install dependencies based on the preferred package manager
COPY package.json bun.lockb ./
RUN mkdir -p packages/scraper
RUN mkdir -p packages/shared
COPY ./.env ./.env
COPY ./packages/scraper/package.json ./packages/scraper/package.json
COPY ./packages/shared/package.json ./packages/shared/package.json
COPY ./loadEnv.ts ./loadEnv.ts
COPY ./env.js ./env.js
COPY ./packages/scraper/dbscraper/ ./packages/scraper/dbscraper/

COPY ./.env ./.env
RUN bun install 

# Build the app
FROM deps AS builder
WORKDIR /app
COPY --from=deps /app .
COPY ./packages/scraper/ ./packages/scraper/
COPY ./packages/shared/ ./packages/shared/

RUN bun scraper:build

RUN ls -la .

# Production image, copy all the files and run next
FROM imbios/bun-node:20-slim AS runner
WORKDIR /app

# Create a system group and user for running the application
RUN addgroup --system --gid 1002 bunjs
RUN adduser --system --uid 1002 --ingroup bunjs bunuser

# Set environment variables
ARG CONFIG_FILE=.env
COPY $CONFIG_FILE /app/.env
ENV NODE_ENV production
# Uncomment the following line in case you want to disable telemetry during runtime.
# ENV NEXT_TELEMETRY_DISABLED 1

# Copy necessary files and set correct permissions
COPY --from=builder /app/packages/scraper/generated ./generated
COPY --from=builder /app/package.json /app/bun.lockb ./
COPY --from=builder /app/node_modules/.prisma /app/node_modules/.prisma
COPY --from=builder /app/node_modules/@prisma /app/node_modules/@prisma

RUN ls -la .
RUN ls -la ./generated

# Change ownership of all necessary files and directories to the non-root user
RUN chown -R bunuser:bunjs /app

# Ensure that bunuser has access to necessary directories
RUN chmod -R 755 /app

# Switch to the non-root user
USER bunuser

# Expose the application port
EXPOSE 3000

# Set additional environment variables
ENV PORT 3000
ENV HOSTNAME ""0.0.0.0""

# Mount a volume for temporary files
VOLUME [""/tmp""]

# Run the application
CMD [""bun"", ""generated/htmlConsumer.js""]

import { scraperPrompt } from ""@shared"";import { File } from ""node-fetch"";import OpenAI from ""openai"";import type { GetManyRequestsByIdReturnType } from ""./dbscraper"";

// Define the structure of the batch requestinterface BatchRequest {
	custom_id: string;
	method: string;
	url: string;
	body: {
		model: string;
		messages: Array<{ role: string; content: string }>;
		max_tokens: number;
	};}

// Define the structure of the file object returned by OpenAIinterface FileObject {
	id: string;
	purpose: string;
	filename: string;
	bytes: number;
	created_at: number;
	status: string;
	status_details?: string | undefined;}

// Define the OpenAI client initializationconst openai = new OpenAI({
	apiKey: process.env.OPENAI_API_KEY, // replace with your actual API key
	timeout: 120000, // Set timeout to 60 seconds});

// Prepare the batch data in-memory and upload directly
async function prepareAndUploadBatchFile(
	batchRequests: BatchRequest[],): Promise<string> {
	try {
		console.log(`Preparing batch file with ${batchRequests.length} requests`);
		const batchData = batchRequests
			.map((req) => `${JSON.stringify(req)}\n`)
			.join("""");

		const buffer = Buffer.from(batchData, ""utf-8"");

		console.log(`Created batch file with ${batchRequests.length} requests`);
		// Create a File object from the buffer
		const file = new File([buffer], `batch-${Date.now()}.txt`, {
			type: ""text/plain"",
		});

		return await uploadBatchFile(file);
	} catch (error) {
		console.error(""Error in prepareAndUploadBatchFile:"", error);
		throw error;
	}}

// Upload the batch file and return the file ID
async function uploadBatchFile(file: File): Promise<string> {
	try {
		console.log(`Uploading batch file ${file.name}`);
		const fileObject: FileObject = await openai.files.create({
			file: file,
			purpose: ""batch"",
		});
		return fileObject.id;
	} catch (error) {
		console.error(""Error in uploadBatchFile:"", error);
		throw error;
	}}

// Create a batch and return the batch ID
async function createBatch(inputFileId: string): Promise<string> {
	try {
		console.log(`Creating batch with input file ID ${inputFileId}`);
		const batch: OpenAI.Batch = await openai.batches.create({
			input_file_id: inputFileId,
			endpoint: ""/v1/chat/completions"",
			completion_window: ""24h"",
		});
		return batch.id;
	} catch (error) {
		console.error(""Error in createBatch:"", error);
		throw error;
	}}

export const createBatchFromRequests = async (
	requests: GetManyRequestsByIdReturnType,): Promise<string> => {
	try {
		const batchRequests: BatchRequest[] = requests.map((request) => {
			return {
				custom_id: request.id,
				method: ""POST"",
				url: ""/v1/chat/completions"",
				body: {
					model: process.env.GPT_MODEL as string,
					response_format: { type: ""json_object"" },
					messages: [
						{
							role: ""system"",
							content: scraperPrompt,
						},
						{
							role: ""user"",
							content: `${request.prompt}\n\n${request?.Html?.html}`,
						},
					],
					max_tokens: 4096,
				},
			};
		});

		const inputFileId = await prepareAndUploadBatchFile(batchRequests);

		const batchId: string = await createBatch(inputFileId);
		console.log(`Batch created with ID ${batchId}`);

		return batchId;
	} catch (error) {
		console.error(""Error in createBatchFromRequests:"", error);
		throw error;
	}};
Error Log:
Preparing batch file with 1 requests
Created batch file with 1 requests
Uploading batch file batch-1719043483257.txt
Error in uploadBatchFile: 32522 | class OpenAIError extends Error {
32523 | }
32524 | 
32525 | class APIError extends OpenAIError {
32526 |   constructor(status, error, message, headers) {
32527 |     super(`${APIError.makeMessage(status, error, message)}`);
                              ^
error: Request timed out.
      at new OpenAIError (:1:23)
      at new APIError (/app/generated/htmlConsumer.js:32527:5)
      at new APIConnectionError (/app/generated/htmlConsumer.js:32592:5)
      at new APIConnectionTimeoutError (/app/generated/htmlConsumer.js:32601:5)
      at /app/generated/htmlConsumer.js:33381:15

Error in prepareAndUploadBatchFile: 32522 | class OpenAIError extends Error {
32523 | }
32524 | 
32525 | class APIError extends OpenAIError {
32526 |   constructor(status, error, message, headers) {
32527 |     super(`${APIError.makeMessage(status, error, message)}`);
                              ^
error: Request timed out.
      at new OpenAIError (:1:23)
      at new APIError (/app/generated/htmlConsumer.js:32527:5)
      at new APIConnectionError (/app/generated/htmlConsumer.js:32592:5)
      at new APIConnectionTimeoutError (/app/generated/htmlConsumer.js:32601:5)
      at /app/generated/htmlConsumer.js:33381:15

Error in createBatchFromRequests: 32522 | class OpenAIError extends Error {
32523 | }
32524 | 
32525 | class APIError extends OpenAIError {
32526 |   constructor(status, error, message, headers) {
32527 |     super(`${APIError.makeMessage(status, error, message)}`);
                              ^
error: Request timed out.
      at new OpenAIError (:1:23)
      at new APIError (/app/generated/htmlConsumer.js:32527:5)
      at new APIConnectionError (/app/generated/htmlConsumer.js:32592:5)
      at new APIConnectionTimeoutError (/app/generated/htmlConsumer.js:32601:5)
      at /app/generated/htmlConsumer.js:33381:15

Error creating batch from requests: Request timed out.
Error processing batch: Request timed out.

Severity: Medium
Priority: P2
Status: Open
Notes:
 This issue appears to be specific to running the OpenAI file upload method inside a Docker container. Further investigation is required to determine if this is a configuration issue, a problem with the Docker environment, or an issue with the OpenAI SDK/API when used in this context.
To Reproduce
Steps to Reproduce:
Set up a Docker container using oven/bun:latest-debian, bun:debian, or similar variants.
Implement and call the openai.files.create method within the container.
Observe that the method hangs and does not throw any errors.
Code snippets
No response
OS
macOs
Node version
node v20.12 bun 1.1.15
Library version
4.51.0
 The text was updated successfully, but these errors were encountered: 
👍1
cstrnt reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/openai-node/issues/906","Client-side URL sanitation in the SDK can be bypassed using prototype pollution","2024-06-21T13:44:40Z","Closed issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
The buildURL() method within the APIClient class sanitizes URLs in order to prevent path traversals.
Since the buildURL() method is public, it is prone to prototype pollution.
As a result, an attacker could override the buildURL() method and send a URL with a relative path.
To Reproduce
If you run this code client-side it'll successfully console log the message of the prototype polluted buildURL method. If the Web Application Firewall (WAF) is configured correctly, it will block the malicious path; however, if the WAF isn't correctly configured it might be able to send the malicious path server-side.
import OpenAI from 'openai';

OpenAI.prototype.buildURL = <Req>(path: string, query: Req | null | undefined) => {
    console.log(""[Attacker Server]: Prototype pollution..."")
    return ""https://api.openai.com/chat/completions/../../etc/passwd"";}

const openai = new OpenAI({
  apiKey: process.env['OPENAI_API_KEY'],});

async function main() {
  const completion = await openai.chat.completions.create({
    messages: [{ role: 'user', content: 'Say this is a test' }],
    model: 'gpt-3.5-turbo',
  });}

main();
Code snippets
No response
OS
macOS
Node version
Node v20.12.0
Library version
openai v4.47.2
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/903","TypeError: getDefaultAgent is not a function","2024-07-08T17:15:12Z","Closed as not planned issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
When using openai-node with langchain and datadog, ESM, I get the following error:
TypeError: getDefaultAgent is not a function
    at OpenAI.buildRequest (file:///Users/name/Progetti/nonworking/node_modules/openai/core.mjs:208:66)
    at OpenAI.makeRequest (file:///Users/name/Progetti/nonworking/node_modules/openai/core.mjs:279:44)
    at async file:///Users/name/Progetti/nonworking/node_modules/@langchain/openai/dist/chat_models.js:796:29
    at async RetryOperation._fn (/Users/name/Progetti/nonworking/node_modules/p-retry/index.js:50:12)

Node.js v22.3.0



To Reproduce
1. git clone https://github.com/guidev/nonworking/tree/node20

2. npm run start 

Code snippets
No response
OS
macOS
Node version
node 20, node 22
Library version
4.52.0
 The text was updated successfully, but these errors were encountered: 
👍3
nirga, andcea, and fferreira12 reacted with thumbs up emoji
All reactions
👍3 reactions"
"https://github.com/openai/openai-node/issues/902","Openai.beta.threads.messages.create give ""Unknown parameter: 'role'""","2024-06-19T10:11:51Z","Open issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
Creating a message as specified in the example on “Step 3” here: https://platform.openai.com/docs/assistants/overview
I got this error:
 BadRequestError: 400 Unknown parameter: ‘role’.
To Reproduce
const message = await openai.beta.threads.messages.create(
      threadId,
      {
        role: ""user"",
        content: ""Some question here..."",
      }
    );

Code snippets
No response
OS
Windows
Node version
Node v18.18.0
Library version
openai 4.51.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/899","Unrecognized request argument supplied: maxChatCompletions | Object literal may only specify known properties, and maxChatCompletions does not exist in type:","2024-07-08T17:16:56Z","Closed issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
Attempting to set maxChatCompletions parameter as described here throws error in TypeScript and API response.
'Unrecognized request argument supplied: maxChatCompletions'
To Reproduce
Follow the example. Let me know if reproducible is needed. I suspect mismatch in documentation.
Code snippets
const runner = openai.beta.chat.completions.runTools({
        maxChatCompletions: 3,
        model: 'gpt-3.5-turbo',
        messages: [...]});
OS
macOS
Node version
Node 22
Library version
4.51.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/898","Batch API - files.content not returning a .jsonl file format like documentation says","2024-07-08T17:24:13Z","Closed as not planned issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
The files.content method doesn't return a file content (like the documentation says)[https://platform.openai.com/docs/guides/batch/5-retrieving-the-results], it returns an API response that we should treat as an streaming response to be able to retrieve the data.
To Reproduce
Try to run (the example the documentation shows)[https://platform.openai.com/docs/guides/batch/5-retrieving-the-results]
The type of the result is not a .jsonl file but an API response.
Code snippets
No response
OS
macOS
Node version
Node v20.9.0
Library version
""openai"": ""^4.50.0"",
 The text was updated successfully, but these errors were encountered: 
👍1
MarkusTrasberg reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/openai-node/issues/897","TypeError: API.Completions is not a constructor","2024-06-28T20:31:42Z","Closed issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
Calling new OpenAI() fails when Sentry is preloaded.
❯ node --import @sentry/node/preload ./dist/index.js
TypeError: API.Completions is not a constructor
    at new OpenAI (file:///home/nato/Code/github.com/NatoBoram/bug-report-sentry/node_modules/.pnpm/openai@4.51.0/node_modules/openai/index.mjs:46:28)
    at file:///home/nato/Code/github.com/NatoBoram/bug-report-sentry/dist/openai.js:3:23
    at ModuleJob.run (node:internal/modules/esm/module_job:262:25)
    at async ModuleLoader.import (node:internal/modules/esm/loader:474:24)
    at async asyncRunEntryPointWithESMLoader (node:internal/modules/run_main:109:5)
(node:394397) [DEP0040] DeprecationWarning: The `punycode` module is deprecated. Please use a userland alternative instead.
(Use `node --trace-deprecation ...` to show where the warning was created)
Bug report at Sentry: SDK fails in ESM mode in combination with openai getsentry/sentry-javascript#12414
Minimal reproduction project: 🧪 TypeError: API.Completions is not a constructor NatoBoram/bug-report-sentry#9
To Reproduce
Set Up Profiling with Sentry
Use Late Initialization with ESM
Call new OpenAI()
Code snippets
This is the code that fails:
import OpenAI from ""openai""const openai = new OpenAI()
It fails when the project is started with node --import @sentry/node/preload.
OS
Pop!_OS 22.04 LTS x86_64
Node version
v22.3.0
Library version
^4.51.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/896","Importing & using AssistantStream breaks Angular SSR","2024-06-12T20:02:31Z","Open issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
Just by adding the following lines in my new Angular project, I get an error.
Lines:
import { AssistantStream } from 'openai/lib/AssistantStream';
...
const stream = AssistantStream.fromReadableStream(response.body);

On compiling the code I get the following error:
7:34:19 PM [vite] Error when evaluating SSR module /main.server.mjs:
2024-06-12T19:34:19.534881356Z |- TypeError: Cannot convert undefined or null to object
2024-06-12T19:34:19.534883506Z     at Function.getPrototypeOf (<anonymous>)
2024-06-12T19:34:19.534885641Z     at eval (/usr/src/app/node_modules/web-streams-polyfill/dist/ponyfill.es2018.js:538:43)
2024-06-12T19:34:19.534887281Z     at eval (/usr/src/app/node_modules/web-streams-polyfill/dist/ponyfill.es2018.js:9:68)
2024-06-12T19:34:19.534888777Z     at node_modules/web-streams-polyfill/dist/ponyfill.es2018.js (/usr/src/app/node_modules/web-streams-polyfill/dist/ponyfill.es2018.js:12:1)
2024-06-12T19:34:19.534890467Z     at __require2 (/usr/src/app/.angular/vite-root/chatsite/chunk-QU2RNZFD.mjs:51:50)
2024-06-12T19:34:19.534891964Z     at eval (/usr/src/app/node_modules/openai/_shims/node-runtime.mjs:9:32)
2024-06-12T19:34:19.534893442Z     at async instantiateModule (file:///usr/src/app/node_modules/vite/dist/node/chunks/dep-cNe07EU9.js:55058:9)
2024-06-12T19:34:19.534894944Z 

Now I'm unsure whether or not this is something you guys can fix, or it should be somehow reported to Angular or the polyfill... but I thought I'd start here, because I really have no clue what's going on. The internals of the AssistantStream are somewhat beyond me and I have no experience with handling streams in Typescript.
Feel free to close this issue and report it to the right place - or I can do it with the right info.
To Reproduce
Install Angular normally with
npm install -g @angular/cli
ng new my-app

As part of the installation prompts, choose to use SSR
Add this lib
npm install openai

Change the app.component.ts to include the openai AssistantStream
import { Component } from '@angular/core';
import { RouterOutlet } from '@angular/router';
import { AssistantStream } from 'openai/lib/AssistantStream';
import { HttpClient } from ""@angular/common/http"";

@Component({
  selector: 'app-root',
  standalone: true,
  imports: [RouterOutlet],
  templateUrl: './app.component.html',
  styleUrl: './app.component.scss'
})
export class AppComponent {
  title = 'test';
  constructor(private http: HttpClient) {}
  private test() {
    this.http.post<any>('http://test', {}).subscribe((response) => {
      const stream = AssistantStream.fromReadableStream(response.body);
    });
  }
}

Run angular and watch it crash
npm start

Code snippets
x
OS
Linux Mint
Node version
20.9.0 and 22.1.0
Library version
4.51.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/886","""Unknown parameter: 'attachments[0].tools[0].file_search'.""","2024-07-08T17:34:31Z","Closed issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
with the new api update there should be a file_search object available on the tools array when passing to attachments
 attachments: [ { file_id: 'fileid', tools: [{ type: 'file_search', file_search: { max_num_results: 50 } }], }, { file_id: 'fileidl', tools: [{ type: 'file_search', file_search: { max_num_results: 50 } }], }, ],
however I get a 400 error when creating messages ""Unknown parameter: 'attachments[0].tools[0].file_search'.""
no typescript errors when passing this information
To Reproduce
see above
Code snippets
No response
OS
mac
Node version
v18.5.0
Library version
""^4.48.2""
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/883","file_ids incorrectly deprecated, causing runtime errors","2024-07-08T17:35:08Z","Closed issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
I just updated openai from 4.28.0 to 4.48.2. I'm now seeing this error in my Lambda function:
400 Unknown parameter: threads.messages[0].file_ids

If I open my code, I see that file_ids is no longer a valid property in the code snippet cited below.
No overload matches this call.
  Overload 1 of 3, '(body: ThreadCreateAndRunParamsNonStreaming, options?: RequestOptions<unknown> | undefined): APIPromise<Run>', gave the following error.
    Object literal may only specify known properties, and 'file_ids' does not exist in type 'Message'.
  Overload 2 of 3, '(body: ThreadCreateAndRunParamsStreaming, options?: RequestOptions<unknown> | undefined): APIPromise<Stream<AssistantStreamEvent>>', gave the following error.
    Object literal may only specify known properties, and 'file_ids' does not exist in type 'Message'.
  Overload 3 of 3, '(body: ThreadCreateAndRunParamsBase, options?: RequestOptions<unknown> | undefined): APIPromise<Run | Stream<AssistantStreamEvent>>', gave the following error.
    Object literal may only specify known properties, and 'file_ids' does not exist in type 'Message'.

I'm curious why a breaking change was introduced, and openai's major version was not bumped to 5.x.x as is standard practice with semver. Looking through the changelog, I also see no mention of breaking changes.
To Reproduce
See code snippet.
Code snippets
let run = await openai.beta.threads.createAndRun({
    assistant_id: OPENAI_ASSISTANT_ID,
    thread: {
      messages: [
        {
          role: 'user',
          content:
            'Analyze the attached files.\n\n' +
            customInstructions
              .map(
                (instruction) =>
                  `If you think the vendor is ${instruction.vendorName}: ` +
                  instruction.instructions
              )
              .join('\n\n'),
          file_ids: savedFiles.map((file) => file.id),
        },
      ],
    },
  });
OS
macOS
Node version
Node v20.14.0
Library version
openai 4.48.2
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/880","I hope the official can release a software package for Markdown parsing","2024-07-08T17:35:37Z","Closed as not planned issue","No label","Confirm this is a feature request for the Node library and not the underlying OpenAI API.
 This is a feature request for the Node library
Describe the feature or improvement you're requesting
With the updates and iterations of ChatGPT, the Markdown information returned via API has become increasingly rich. Previous parsing libraries such as markdown-it and markdown-it-kbd are not able to parse all the information content effectively, which makes the user experience very poor. I hope that the official can upgrade openai-nodeto include support for Markdown parsing. This would be a very meaningful improvement.
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/879","Async Assistant Streaming Helpers","2024-06-05T00:08:22Z","Open issue","No label","Confirm this is a feature request for the Node library and not the underlying OpenAI API.
 This is a feature request for the Node library
Describe the feature or improvement you're requesting
When using the Assistant Streaming Helpers it is sometimes desirable to have an async/await level of control for the event listener. For example, an imageFileDone listener that would move a file to a new destination as part of tools being submitted.
Additional context
I am the author of Experts.js (https://github.com/metaskills/experts) and I felt this particular pain point in a few of my projects. I've not heard of other's needs for this behavior. That said, I thought that would be helpful for more context.
https://github.com/metaskills/experts?tab=readme-ov-file#streaming--events
Thanks y'all.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/867","ChatCompletionMessageParam - there is an TS error when trying to use content as an object","2024-07-08T17:53:16Z","Closed as not planned issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
When we use OpenAI.chat.completions.create and define messages as in the example below. There is an error from TS ""No overload matches this call.""
messages: [
 {
 ""role"": ""user"",
 ""content"": [
 {
 ""type"": ""text"",
 ""text"": ""What is this?""
 }
 }]
To Reproduce
import OpenAI from ""openai"";
const openai = new OpenAI({
 apiKey: process.env.OPENAI_API_KEY,
 });
const response = await openai.chat.completions.create({
 model: ""gpt-4o"",
 messages: [
 {
 ""role"": ""user"",
 ""content"": [
 {
 ""type"": ""text"",
 ""text"": ""What is this?""
 },
 {
 ""type"": ""image_url"",
 ""image_url"": {
 ""url"": ""data:image/jpeg;base64,...""
 }
 }
 ]
 },
 {
 ""role"": ""assistant"",
 ""content"": [
 {
 ""type"": ""text"",
 ""text"": ""This image shows a dish composed of various cooked vegetables and what appears to be pieces of meat. The vegetables include broccoli, carrots, green beans, corn, and possibly some others. The meat is seasoned and the whole dish seems to be baked or roasted. It looks like a hearty and colorful meal!""
 }
 ]
 }
 ],
 temperature: 1,
 max_tokens: 256,
 top_p: 1,
 frequency_penalty: 0,
 presence_penalty: 0,
 });
Code snippets
No response
OS
macOS
Node version
v20.13.1
Library version
4.47.1
 The text was updated successfully, but these errors were encountered: 
👍1
s-h-a-d-o-w reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/openai-node/issues/860","README run streaming example has incorrect & unhelpful event examples","2024-05-20T10:00:38Z","Open issue","bug,documentation","Confirm this is a feature request for the Node library and not the underlying OpenAI API.
 This is a feature request for the Node library
Describe the feature or improvement you're requesting
Look at this code from the docs:
const run = openai.beta.threads.runs.stream(thread.id, {
    assistant_id: assistant.id
  })
    .on('textCreated', (text) => process.stdout.write('\nassistant > '))
    .on('textDelta', (textDelta, snapshot) => process.stdout.write(textDelta.value))
    .on('toolCallCreated', (toolCall) => process.stdout.write(`\nassistant > ${toolCall.type}\n\n`))
    .on('toolCallDelta', (toolCallDelta, snapshot) => {
      if (toolCallDelta.type === 'code_interpreter') {
        if (toolCallDelta.code_interpreter.input) {
          process.stdout.write(toolCallDelta.code_interpreter.input);
        }
        if (toolCallDelta.code_interpreter.outputs) {
          process.stdout.write(""\noutput >\n"");
          toolCallDelta.code_interpreter.outputs.forEach(output => {
            if (output.type === ""logs"") {
              process.stdout.write(`\n${output.logs}\n`);
            }
          });
        }
      }
    });

Here it listens for events with on.
I'm assuming these are just example events, although I have no clue why you wouldn't include an obviously useful event like ""completed"" in this list.
Below this is this quote: ""See the full list of Assistants streaming events in our API reference here.""
The link goes here: https://platform.openai.com/docs/api-reference/assistants-streaming/events
It contains this information:
Assistant stream eventsBeta
Represents an event emitted when streaming a Run.

Each event in a server-sent events stream has an event and data property:

event: thread.created
data: {""id"": ""thread_123"", ""object"": ""thread"", ...}
We emit events whenever a new object is created, transitions to a new state, or is being streamed in parts (deltas). For example, we emit thread.run.created when a new run is created, thread.run.completed when a run completes, and so on. When an Assistant chooses to create a message during a run, we emit a thread.message.created event, a thread.message.in_progress event, many thread.message.delta events, and finally a thread.message.completed event.

We may add additional events over time, so we recommend handling unknown events gracefully in your code. See the Assistants API quickstart to learn how to integrate the Assistants API with streaming.

thread.created
data is a thread

Occurs when a new thread is created.

thread.run.created
data is a run

Occurs when a new run is created.

thread.run.queued
data is a run

Occurs when a run moves to a queued status.

thread.run.in_progress
data is a run

Occurs when a run moves to an in_progress status.

thread.run.requires_action
data is a run

Occurs when a run moves to a requires_action status.

thread.run.completed
data is a run

Occurs when a run is completed.

thread.run.incomplete
data is a run

Occurs when a run ends with status incomplete.

thread.run.failed
data is a run

Occurs when a run fails.

thread.run.cancelling
data is a run

Occurs when a run moves to a cancelling status.

thread.run.cancelled
data is a run

Occurs when a run is cancelled.

thread.run.expired
data is a run

Occurs when a run expires.

thread.run.step.created
data is a run step

Occurs when a run step is created.

thread.run.step.in_progress
data is a run step

Occurs when a run step moves to an in_progress state.

thread.run.step.delta
data is a run step delta

Occurs when parts of a run step are being streamed.

thread.run.step.completed
data is a run step

Occurs when a run step is completed.

thread.run.step.failed
data is a run step

Occurs when a run step fails.

thread.run.step.cancelled
data is a run step

Occurs when a run step is cancelled.

thread.run.step.expired
data is a run step

Occurs when a run step expires.

thread.message.created
data is a message

Occurs when a message is created.

thread.message.in_progress
data is a message

Occurs when a message moves to an in_progress state.

thread.message.delta
data is a message delta

Occurs when parts of a Message are being streamed.

thread.message.completed
data is a message

Occurs when a message is completed.

thread.message.incomplete
data is a message

Occurs when a message ends before it is completed.

error
data is an error

Occurs when an error occurs. This can happen due to an internal server error or a timeout.

done
data is [DONE]

Now aren't those COMPLETELY different from the ones we just saw? And indeed I can't write any of them, for example this:
const stream = openai.beta.threads.runs
    .stream(threadId, {
      assistant_id: assistantId,
      instructions,
      stream: true,
    })
    .on(""thread.run.completed"", (data) => {
      console.log(""hey"");
    })

So now I have to work out this on my own. This is the case with literally every piece of available information in the documentation. Why not make it useful? Is it really THAT difficult? I guess so since no one can do it, but still.
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
👍4
Highverve, captainamerican, aleksa-codes, and lucas-dolsan reacted with thumbs up emoji😄2
Finalet and lucas-dolsan reacted with laugh emoji
All reactions
👍4 reactions
😄2 reactions"
"https://github.com/openai/openai-node/issues/859","Not able to use OpenAI API key as local environment variable","2024-05-25T22:37:50Z","Closed as not planned issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
I'd like to use the API in nodejs to create images. I know it is recommended to have global environments, but I'd like to deploy it for production in Vercel as a nextjs project. Therefor, I use local environment variables. I am however not able to load the API key as a local environment variable.
To Reproduce
import OpenAI from 'openai';
const openai = new OpenAI({
 apiKey: process.env['NEXT_PUBLIC_OPENAI_API_KEY'], // This is the default and can be omitted
 });
Code snippets
No response
OS
macOS
Node version
v17.9.1
Library version
openai v3.3.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/856","Just tried npm exec openai migrate. It returned: '""bash""' is not recognized as an internal or external command,","2024-05-17T05:34:08Z","Closed as not planned issue","No label","(node:11400) [DEP0040] DeprecationWarning: The punycode module is deprecated. Please use a userland alternative instead.
 (Use node --trace-deprecation ... to show where the warning was created)
 Server is running on port 5000
 Error generating questions: TypeError: Cannot read properties of undefined (reading 'completions')
 at generateQuestions (C:\Users\COLLINS_DEV\Downloads\Good-Project\Good-Project\index.js:32:40)
 at C:\Users\COLLINS_DEV\Downloads\Good-Project\Good-Project\index.js:51:29
 at Layer.handle [as handle_request] (C:\Users\COLLINS_DEV\Downloads\Good-Project\Good-Project\node_modules\express\lib\router\layer.js:95:5)
 at next (C:\Users\COLLINS_DEV\Downloads\Good-Project\Good-Project\node_modules\express\lib\router\route.js:149:13) at Route.dispatch (C:\Users\COLLINS_DEV\Downloads\Good-Project\Good-Project\node_modules\express\lib\router\route.js:119:3)
 at Layer.handle [as handle_request] (C:\Users\COLLINS_DEV\Downloads\Good-Project\Good-Project\node_modules\express\lib\router\layer.js:95:5)
 at C:\Users\COLLINS_DEV\Downloads\Good-Project\Good-Project\node_modules\express\lib\router\index.js:284:15
 at Function.process_params (C:\Users\COLLINS_DEV\Downloads\Good-Project\Good-Project\node_modules\express\lib\router\index.js:346:12)
 at next (C:\Users\COLLINS_DEV\Downloads\Good-Project\Good-Project\node_modules\express\lib\router\index.js:280:10) at jsonParser (C:\Users\COLLINS_DEV\Downloads\Good-Project\Good-Project\node_modules\body-parser\lib\types\json.js:113:7)
 Error generating questions: TypeError: Cannot read properties of undefined (reading 'completions')
 at generateQuestions (C:\Users\COLLINS_DEV\Downloads\Good-Project\Good-Project\index.js:32:40)
 at C:\Users\COLLINS_DEV\Downloads\Good-Project\Good-Project\index.js:51:29
 at Layer.handle [as handle_request] (C:\Users\COLLINS_DEV\Downloads\Good-Project\Good-Project\node_modules\express\lib\router\layer.js:95:5)
 at next (C:\Users\COLLINS_DEV\Downloads\Good-Project\Good-Project\node_modules\express\lib\router\route.js:149:13) at Route.dispatch (C:\Users\COLLINS_DEV\Downloads\Good-Project\Good-Project\node_modules\express\lib\router\route.js:119:3)
 at Layer.handle [as handle_request] (C:\Users\COLLINS_DEV\Downloads\Good-Project\Good-Project\node_modules\express\lib\router\layer.js:95:5)
 at C:\Users\COLLINS_DEV\Downloads\Good-Project\Good-Project\node_modules\express\lib\router\index.js:284:15
 at Function.process_params (C:\Users\COLLINS_DEV\Downloads\Good-Project\Good-Project\node_modules\express\lib\router\index.js:346:12)
 at next (C:\Users\COLLINS_DEV\Downloads\Good-Project\Good-Project\node_modules\express\lib\router\index.js:280:10) at jsonParser (C:\Users\COLLINS_DEV\Downloads\Good-Project\Good-Project\node_modules\body-parser\lib\types\json.js:113:7)
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/855","Add example for Image generation","2024-05-16T15:58:34Z","Open issue","documentation,good first issue","It would be great if an example could be added for Image generation. Though there is a guide at the bottom that lists the new functions. Some interfaces aren't listed, such as: ""CreateImageRequestSizeEnum"".
Originally posted by @woutersteven in #217 (comment)
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/850","Assistant API - toolCall event not fired for file_search","2024-07-08T18:06:24Z","Closed as not planned issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
The helper events ""toolCall"" (created, delta, done) with a message attachments with file and tool type ""file_search"" are never fired.
 It works well for ""code_interpreter"" type.
To Reproduce
Just attach a toolCall event to a run with a message attachment type ""file_search"" and a file_id from a vector store.
Code snippets
No response
OS
windows
Node version
Node v21.5.0
Library version
openai v4.47.1
 The text was updated successfully, but these errors were encountered: 
👍1
DanielHindi reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/openai-node/issues/845","Assistant Streaming Error Event","2024-05-13T23:53:14Z","Open issue","No label","Confirm this is a feature request for the Node library and not the underlying OpenAI API.
 This is a feature request for the Node library
Describe the feature or improvement you're requesting
Been using the Assistants stream events (https://github.com/openai/openai-node/blob/master/helpers.md) and was wondering if there is a missing error event to compliment the end event? Or is there an expectation that we would use the event event and track error or end as needed from this list? https://platform.openai.com/docs/api-reference/assistants-streaming/events
Additional context
I am worried that without an error event end would be used and there would be a gap in how I could handle this with my application.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/830","Type '""batch""' is not assignable to type '""fine-tune"" | ""assistants""","2024-05-13T00:12:19Z","Closed issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
According to the document Uploading Your Batch Input File. I tried the following code:
import fs from ""fs"";import OpenAI from ""openai"";

const openai = new OpenAI();

async function main() {
  const file = await openai.files.create({
    file: fs.createReadStream(""batchinput.jsonl""),
    purpose: ""batch"",
  });

  console.log(file);}

main();
However, it outputs the error:
TSError: ⨯ Unable to compile TypeScript:
worker/generate.ts:18:4 - error TS2322: Type '""batch""' is not assignable to type '""fine-tune"" | ""assistants""'.

18    purpose: ""batch"",
      ~~~~~~~

  node_modules/openai/resources/files.d.ts:119:5
    119     purpose: 'fine-tune' | 'assistants';
            ~~~~~~~
    The expected type comes from property 'purpose' which is declared here on type 'FileCreateParams'

Maybe the type batch is missing in this line of the openai package source code:
openai-node/src/resources/files.ts
 Line 185 in 7196ac9
	purpose: 'fine-tune'|'assistants';
We can fix it by adding batch
purpose: 'fine-tune' | 'assistants' | 'batch';
To Reproduce
Easy to reproduce: look at official documentation page - https://platform.openai.com/docs/guides/batch/2-uploading-your-batch-input-file
 Run above code in nodejs / typescript.
Code snippets
No response
OS
macOS
Node version
v20.11.0
Library version
v4.42.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/821","The library has no method to list threads","2024-05-13T00:32:03Z","Closed as not planned issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
There is no method in the API to list threads, although technically they can be listed in the dashboard at https://platform.openai.com/threads (has to be enabled in the org setting)
This question was asked in the community as well
https://community.openai.com/t/where-can-i-see-all-the-threads-ive-created-with-the-api/592963/9
You may also want to update the documentation page to warn users that thread information should be maintained in the external system while you fix the API.
To Reproduce
Easy to reproduce: look at official documentation page - https://platform.openai.com/docs/api-reference/threads
Code snippets
No response
OS
macOs
Node version
node 20
Library version
3.0.35
 The text was updated successfully, but these errors were encountered: 
👍2
morganhvidt and KotaIkehara reacted with thumbs up emoji
All reactions
👍2 reactions"
"https://github.com/openai/openai-node/issues/820","Incorrect content param type in ChatCompletionAssistantMessageParam","2024-05-04T19:40:39Z","Open issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
Currently only ChatCompletionUserMessageParam has content: string | Array<ChatCompletionContentPart>, but GPT-4 vision model allows passing images for assistant and system roles.
We need to make the same content type for ChatCompletionAssistantMessageParam and ChatCompletionSystemMessageParam
OS
macOS
Node version
Node v22.0.0
Library version
4.40.2
 The text was updated successfully, but these errors were encountered: 
👍1
chasem-dev reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/openai-node/issues/816","4.40.0 -> 4.40.1: Breaking change - OpenAI is not a constructor","2024-05-03T14:30:39Z","Closed issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
Based on numbers, 4.40.1 is supposed to be a bug fix release over 4.40.0, so I didn't expect something to break. I have not tested the application after 4.40.1 to see if anything else is broken.
const openai = new OpenAI({
 ^
TypeError: OpenAI is not a constructor
To Reproduce
Code saved as test.js:
const openaiKey = ""a valid OpenAI key""
const OpenAI = require('openai');

const openai = new OpenAI({
	apiKey: openaiKey,
});


 >> npm install openai@4.40.0
 >> node test.js
(no errors.)
 >> npm install openai@4.40.1
 >> node test.js
 const openai = new OpenAI({
               ^

TypeError: OpenAI is not a constructor
 >> npm install openai@4.40.0
 >> node test.js
(no errors.)

Code snippets
No response
OS
WSL Ubuntu 24.04 LTS
Node version
v20.12.2
Library version
4.40.1
 The text was updated successfully, but these errors were encountered: 
👍5
th3c0d3br34ker, Galkon, bizob2828, mtharrison, and alimoezzi reacted with thumbs up emoji
All reactions
👍5 reactions"
"https://github.com/openai/openai-node/issues/807","Vector store file uploadAndPoll has wrong implementation","2024-05-14T13:17:25Z","Closed issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
Whe using the uploadAndPoll helper for vector store files, an error is thrown ""NotFoundError: 404 No file found with id""
 Looking at the code, this is because the files are uploaded to file api, but not to the vector store api, making it throw when polling if the file was uploaded.
To Reproduce
Try this with a new file:
vectorStores.files.uploadAndPoll
Code snippets
No response
OS
Any
Node version
v20
Library version
4.39.0
 The text was updated successfully, but these errors were encountered: 
👍1
dome2x reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/openai-node/issues/805","batch error when upload file vector in file_search type tool assistant ia","2024-07-10T07:42:04Z","Closed issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
when i upload file vector to file_seach type tool assistant, then I follow instruction from doc, i had that error when i process to finish data :`js file-batches.js:99
 if (files === null || files.length == 0) {
 ^
TypeError: Cannot read properties of undefined (reading 'length')
 `
To Reproduce
1 install
 2 create assistant ia tool type file_search
 3 upload file for this tools
Code snippets
No response
OS
win 10
Node version
=20
Library version
=5.4.4
 The text was updated successfully, but these errors were encountered: 
👀1
dews reacted with eyes emoji
All reactions
👀1 reaction"
"https://github.com/openai/openai-node/issues/804","400 'file' is a required property when Creating File in CloudFlare Worker","2024-04-29T20:32:42Z","Open issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
I get the following error when I try to create a file from a CloudFlare worker environment.
 {
      error: {
        message: ""'file' is a required property"",
        type: 'invalid_request_error',
        param: null,
        code: null
      }
    }

I'm able to successfully create a file from a vanilla nodejs environment. I've also confirmed that CURL requests are working.
To Reproduce
Run the following code in a CloudFlare worker environment.
 const llm = new OpenAI({
    apiKey: env.OPENAI_API_KEY,
  });

  
  const file = await toFile(
    new FormDataBlob([
      new TextEncoder().encode(""text is here""),
    ]),
    ""finetune.jsonl""
  );

  const res = await llm.files.create({
    file,
    purpose: ""fine-tune"",
  });

Code snippets
No response
OS
macOS
Node version
v18.0.0
Library version
4.38.5
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/795","Incorrect error: file_search tool configuration is invalid. file_ids must be non-empty","2024-04-23T19:38:06Z","Closed as not planned issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
When creating an assistant with tools: [{ type: 'file_search' }], the tool_resources: { file_search: { vector_stores: [...] } } is not evaluated properly. Specifically, when creating a empty vector store, you will receive the following error:
BadRequestError: 400 file_search tool configuration is invalid. file_ids must be non-empty
 at APIError.generate (file:///Users/chrismcc/workspace/atlas/node_modules/openai/error.mjs:41:20)
 at OpenAI.makeStatusError (file:///Users/chrismcc/workspace/atlas/node_modules/openai/core.mjs:256:25)
 at OpenAI.makeRequest (file:///Users/chrismcc/workspace/atlas/node_modules/openai/core.mjs:299:30)
 at process.processTicksAndRejections (node:internal/process/task_queues:95:5)
 at async main (file:///Users/chrismcc/workspace/atlas/test.js:10:3)
This is incorrect per the docs which indicates file_ids is optional:

Additionally, creating a vector store without file_ids works just fine within openai.beta.vectorStores.create, so this seems specific to creating the vector store as part of the openai.beta.assistants.create call.
To Reproduce
Specify something like this when creating an assistant:
tool_resources: {
  file_search: {
    vector_stores: [
      {
        metadata: {foo: 'bar'}
      }
    ]
  }
}

(Note vector_stores: [{file_ids: []}] doesn't work either)
Code snippets
import OpenAI from 'openai';const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
  maxRetries: 10,
  timeout: 60 * 1000,
  defaultHeaders: { 'OpenAI-Beta': 'assistants=v2' }});

async function main() {
  await openai.beta.assistants.create({
    instructions:
      'You are a helpful assistant. Use your knowledge base to answer questions.',
    name: 'Test',
    tools: [{ type: 'file_search' }],
    tool_resources: {
      file_search: {
        vector_stores: [
          {
            metadata: {foo: 'bar'}
          }
        ]
      }
    },
    model: 'gpt-3.5-turbo',
    temperature: 1
  });}

main();
(note file_ids: [] doesn't work either)


### OS

macOS 14.0 (23A344)

### Node version

Node v20.11.0

### Library version

openai v4.38.3

 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/792","Export OpenAI.Beta.Assistants.AssistantStream Type","2024-04-20T12:23:35Z","Open issue","No label","Confirm this is a feature request for the Node library and not the underlying OpenAI API.
 This is a feature request for the Node library
Describe the feature or improvement you're requesting
For now, only the OpenAI.Beta.Assistants.AssistantStreamEvent type is exported, please export also the OpenAI.Beta.Assistants.AssistantStream.
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/791","Assistant API - tools: [{ type: ‘retrieval’ }] not allowing any more","2024-04-22T22:40:39Z","Closed issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
couple of months ago I started working with openAI assistant. I created a simple pdf analyzer. I implemented using Nodejs. (“openai”: “^4.31.0”,). When I created the application it worked fine. But reasonly when i delete the all the node_module and installed. then I’m getting an error saying “rgument type {instructions: string, name: string, model: “gpt-3.5-turbo”, tools: {type: “retrieval”}} is not assignable to parameter type AssistantCreateParams”. Seems like now "" tools: [{ type: ‘retrieval’ }]"" is not working.
below is the code that used to work.
but the best thing is curl version of below code is working perfectly in potsman. I started installing the latest openAI version also. but still same.
 Please note that I’m new for openAI.
To Reproduce
install “openai”: “^4.31.0”
Code snippets
const assistant = await openai.beta.assistants.create({
      name: 'recruiter',
      instructions: 'Analyze the give 'file IDs' and return an summary""      tools: [{ type: 'retrieval' }],      model: 'gpt-3.5-turbo',""file_ids"": [
    ""file-0EtPDfxuS6IUS1AWBY5BNDty"",
    ""file-5wFhHiCae4ygard1O1aKc2jM"",
  ]
    });
OS
windows 11
Node version
20.10.0
Library version
^4.31.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/790","No Function Arguments in toolCallDone Stream Event Hook","2024-07-08T18:06:06Z","Closed as not planned issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
The "".on('toolCallDone') used on the stream returned by ai.beta.threads.runs.stream"" does not return arguments (empty string), only function name:
{
  index: 0,
  id: 'call_id',
  type: 'function',
  function: { name: 'functionName', arguments: '', output: null }}
To Reproduce
Retrieve a run stream with ai.beta.threads.runs.stream
Add the .on('toolCallDone') hook.
Wait for the response, it does not return with what arguments should the function be called.
Code snippets
const stream = ai.beta.threads.runs.stream(threadId, {
    assistant_id: assistantId,});stream.on('toolCallDone', (toolCall) => {
    console.log(toolCall); // no arguments for function});
OS
macOS
Node version
v20.11.1
Library version
openai v4.36.0
 The text was updated successfully, but these errors were encountered: 
👀1
superchenwb reacted with eyes emoji
All reactions
👀1 reaction"
"https://github.com/openai/openai-node/issues/789","Error : Cannot get final name for export 'getRuntime' of ./node_modules/openai/_shims/auto/runtime-node.mjs","2024-05-01T15:11:58Z","Closed issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
I am trying to use Langchain + NextJs for hosting my chatbot , and it works fine in the development server ,
 But when I try to compile my code to host on the vercel server, I am getting this error
Failed to compile.

./node_modules/@langchain/openai/index.js + 66 modules
Cannot get final name for export 'getRuntime' of ./node_modules/openai/_shims/auto/runtime-node.mjs
To Reproduce
Make a Next Js project
Add openai and langchain to it
make a basic chain.
run npm run build
Code snippets
    const chain = ConversationalRetrievalQAChain.fromLLM(
      model,
      vectorStore.asRetriever()
    );
    // Format the messages
    const formattedChatPrompt = await chatPrompt.formatMessages({
      text: question,
    });

    const res = await chain.invoke({
      question,
      chat_history: formattedChatPrompt,
    });
OS
Windows
Node version
Node v20.9.0
Library version
OpenAI v4.38.2
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/785","Thread.message.attachment","2024-05-02T14:13:11Z","Closed issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
I see you've changed types, so that
 attachment : {
 file_id: string
 tools: [""file_search"" | ""code_interpreter""]
 }
but API errors with expected attachment.tools[0] be object not string
plus: i could not get files work with assistant v2, via both attachment and assistant/thread vectorStore attachments
To Reproduce
Attach file to message, via attachment = [{file_id: your_file_id, tools: [""file_search""] }
Get API error when creating thread message
Code snippets
No response
OS
Windows 11 + WSL 1.0
Node version
20.11.0
Library version
4.38.1
 The text was updated successfully, but these errors were encountered: 
👍5
Clad012, arthur-caillaud, Zamanax, ak-notify, and hesslau reacted with thumbs up emoji
All reactions
👍5 reactions"
"https://github.com/openai/openai-node/issues/780","DeprecationWarning: The punycode module is deprecated.","2024-04-18T18:11:10Z","Closed as not planned issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
On node 21, shows a warning:
 DeprecationWarning: The punycode module is deprecated. Please use a userland alternative instead.
To Reproduce
Use node v 21
Make a request with the library
Code snippets
export async function convert({ prompt }) {
    const response = await openai.images.generate({
        prompt: `${prompt}`,
        n: 1,
        size: ""1024x1024"",
    });
    return response.data}


### OS

macOS

### Node version

Node v21.7.2

### Library version

openai v4.37.1

 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/771","AssistantStream.accumulateDelta does not cover accumulating toolCall","2024-09-03T13:17:46Z","Closed issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
Say we have a delta like
{""delta"":{""step_details"":{""type"":""tool_calls"",""tool_calls"":[{""index"":0,""type"":""function"",""function"":{""arguments"":""{}""}}]}}}
You're missing functionality here:
https://github.com/openai/openai-node/blob/116e38aae33a2d7b88c27d783a95b41e56500600/src/lib/AssistantStream.ts#L686C1-L691C8
I recommend adding something like
} else if (
    Array.isArray(accValue) &&
    Array.isArray(deltaValue)) {
    if (
        accValue.every(
            x => typeof x === ""string"" || typeof x === ""number""
        )
    ) {
        accValue.push(...deltaValue); // Use spread syntax for efficient addition
        continue;
    // new code below
    } else if (
        deltaValue.every(
            x =>
                isObj(x) &&
                x.hasOwnProperty(""index"") &&
                typeof x.index === ""number""
        )
    ) {
        for (let i = 0; i < deltaValue.length; i++) {
            const index = deltaValue[i].index;
            accValue[index] = this.accumulateDelta(
                accValue[index],
                deltaValue[i]
            );
        }
        continue;
    }
    // end of new code}
To Reproduce
Any streamed tool call with arguments will do. Notice how the argument field is empty in the toolCall parameter from the event ""toolCallDone""
Code snippets
No response
OS
Linux
Node version
Node v20.11.1
Library version
openai v4.34.0
 The text was updated successfully, but these errors were encountered: 
👍1
neilord reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/openai-node/issues/765","[Important] Add max_prompt_tokens and max_completion_tokens for Runs API","2024-04-16T14:37:53Z","Closed issue","No label","Confirm this is a feature request for the Node library and not the underlying OpenAI API.
 This is a feature request for the Node library
Describe the feature or improvement you're requesting
Hi, I belive that max_prompt_tokens and max_completion_tokens are essential features for using the whole Assistants API. Without being able to specify the max tokens, the thread is just getting longer, and running the AI model getting very expensive. Please add the max_prompt_tokens and max_completion_tokens parameters to the openai.beta.threads.runs.create. Thanks!
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/758","Assistant API Streaming: events are non-conformant to AssistantStreamEvent","2024-04-05T18:43:58Z","Closed issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
I'm seeing a strange issue that exists only in our deployed environment on Twilio Functions, but works perfectly fine when I test locally. This makes debugging the issue more difficult. I'm confident environment variables such as API keys and Assistant ID are set correctly.
I'm trying to use the streaming assistants api as seen in the code snippet. On our deployed environment where the issue exists, the event variable does not conform to the type AssistantStreamEvent. Instead it is just a single character from some part of the system prompt of the assistant. There are multiple events, and it ends up printing a few consecutive characters from the system prompt from an arbitrary location. On one occasion the characters printed from multiple events also spelled out data:[DONE].
There is no issue when using the Assistants API without streaming.
I'm not sure if this is a problem with the Node library or the OpenAI API, or some other environmental configuration issue. The issue has existed since the library update to introduce streaming support, but I also tested with the latest v4.32.2.
I checked later the status of the run and thread that were created and they seemed fine, the thread had new messages from the assistant appended correctly and the run was in a completed state.
To Reproduce
Run the code in the snippet after plugging in an api key and assistant id. Issue was not reproducible in our local testing environment.
Code snippets
import OpenAI from ""openai"";

async function a() {
  const openai = new OpenAI({ apiKey: """" })
  const threadID = (await openai.beta.threads.create()).id;
  await openai.beta.threads.messages.create(threadID, {
    role: ""user"",
    content: ""Hi""
  });
  const assistant_id = """"
  const assistantStream = await openai.beta.threads.runs.create(threadID, { assistant_id, stream: true });
  for await (const event of assistantStream) {
    console.log(""Event: "", event)            // Single character from system prompt
    console.log(""Event:Data:"", event.data)   // null
    console.log(""Event:Event:"", event.event) // null
  }}
OS
Twilio Functions
Node version
v18
Library version
openai v4.32.2
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/756","[Error: 401 Missing bearer or basic authentication in header] - React-Native","2024-04-05T17:38:48Z","Closed issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
I am using the exact same setup that I use in my web app, as I am in my react-native project. The exact same api key, and assistant ID are being used between the two apps. I am able to retrieve my assistant no problem in the web app, but get [Error: 401 Missing bearer or basic authentication in header] in my react-native apps.
To Reproduce
Create new react-native project
Run ios simulator through xCode
import openai
create new OpenAI client
attempt to retrieve assistant
Code snippets
import { OpenAI } from 'openai'

const openai = new OpenAI({
    apiKey: 'my-key',
    dangerouslyAllowBrowser: true,
    organization: 'my-org'})

openai.beta.assistants.retrieve('assistant-id').then(res => console.log(res)).catch(err => console.log(err))
OS
macOS
Node version
node v21.7.1
Library version
openai ^4.31.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/750","Deprecation Warning for punycode Module Usage","2024-04-03T06:13:33Z","Closed as not planned issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
Hello,
I've encountered a deprecation warning in my project that appears to be originating from this library. When running my Next.js application, the following deprecation warning is emitted:
(node:14106) [DEP0040] DeprecationWarning: The punycode module is deprecated. Please use a userland alternative instead.
When removing the OpenAI dependency, the warning is gone.
To Reproduce
Add the openai dependency: import OpenAI from ""openai""
Run next build
Code snippets
No response
OS
macOS
Node version
v21.7.1
Library version
4.32.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/746","Why is usage optional?","2024-04-01T14:17:23Z","Closed as not planned issue","bug","Describe the bug
openai-node/src/resources/chat/completions.ts
 Lines 76 to 79 in abb0be7
	/**
	 * Usage statistics for the completion request.
	 */
	usage?: CompletionsAPI.CompletionUsage;
To Reproduce
const chatCompletion = new OpenAI({ ... }).chat.completion.create({ ... })

const totalTokens = chatCompletion.usage.total_tokens; // TS18048: `chatCompletion.usage` is possibly `undefined` 
Code snippets
Library version
v4.31.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/740","Error install openai with pnpm","2024-03-29T19:39:21Z","Open issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
When I am trying to install pnpm install openai, I am getting md5 dependency error when it tries to install digest-fetch dependency
To Reproduce
pnpm install openai
Error
 ERR_PNPM_NO_MATCHING_VERSION  No matching version found for md5@^2.3.0

This error happened while installing the dependencies of openai@4.30.0
 at digest-fetch@1.3.0

The latest release of md5 is ""2.1.0"".

If you need the full list of all 2 published versions run ""$ pnpm view md5 versions"".

Code snippets
No response
OS
macOS
Node version
20.11.1
Library version
4.30.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/734","Pkg loading wrong runtime.js","2024-03-24T03:17:41Z","Closed as not planned issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
Hi all,
 We're currently building an application which uses Open AI Node which we go from MJS to JS files using babel. However whenever we try to load application locally ex: node .\app.js it runs fine. Once we package the files into an exe is where we run into issues. Mostly not finding the right modules. In this case, it's trying to find _shim/auto/runtime-node.js and cannot find it.
I viewed the debug for pkg and it seems like runtime.js is being added
> [debug] Bytecode of C:\Users\ptran\Desktop\nodeServiceTesting\node_modules\openai\_shims\auto\runtime.js is added to queue. It was required from C:\Users\ptran\Desktop\nodeServiceTesting\node_modules\openai\_shims\index.js
Where when we run the actual exe it looking for runtime-node.js
Here is the error from the executable:
1) If you want to compile the package/file into executable, please pay attention to compilation warnings and specify a l iteral in 'require' call. 2) If you don't want to compile the package/file into executable and want to 'require' it from filesystem (likely plugin), specify an absolute path in 'require' call using process.cwd() or process.execPath.
at createEsmNotFoundErr (node: internal/modules/cjs/loader:967:15) at finalizeEsmResolution (node:internal/modules/cjs/loader:960:15) at resolveExports (node: internal/modules/cjs/loader:488:14) at Module._findPath (node:internal/modules/cjs/loader:528:31)
at Module._resolveFilename (node:internal/modules/cjs/loader:932:27) at Function._resolveFilename (pkg/prelude/bootstrap.js:1951:46)
at Module._load (node: internal/modules/cjs/loader: 787:27) at Module require (node:internal/modules/cjs/loader:1012:19)
at Module.require (pkg/prelude/bootstrap.js:1851:31)
at require (node: internal/modules/cjs/helpers:102:18) {
code: 'MODULE_NOT_FOUND'
path: 'C:||snapshot|\nodeServiceTesting\\node_modules|\openail|package.json',
pkg: true```

My current fix is to add require the runtime-node.js within the openai file but this is temp fix. 


### To Reproduce

1. Install openAI on NPM
2. Reference openAI `import OpenAI from 'openai';`
3. Babel all the MJS into JS
4. Pkg the files into a single executable and run.


### OS

Windows 11

### Node version

v18.19.1

### Library version

openai 4.29.2

 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/732","Open ai moderation not working for node js","2024-03-22T04:41:38Z","Closed issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
\node_modules\openai\node_modules\axios\lib\adapters\http.js:322:11)
 at IncomingMessage.emit (node:events:525:35)
 at endReadableNT (node:internal/streams/readable:1359:12)
 at process.processTicksAndRejections (node:internal/process/task_queues:82:21)
To Reproduce
make the call to open ai using this await this.openai.createModeration(""Hello""))
Code snippets
const config = new Configuration({
      apiKey: process.env.OPEN_AI_KEY,
    });
    this.openai = new OpenAIApi(config);
OS
Windows
Node version
v18.16.0
Library version
""^3.2.1""
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/726","More of a short question and not a BUG: Which is the best free chatGPT-Model??","2024-03-21T01:55:15Z","Closed as not planned issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
The title tells it all... I do not have a paid openai account. Can someone give me a brief assessment of which free model is the newest and most accurate?
To Reproduce
See description!
Code snippets
No response
OS
macOS
Node version
Node v21.7.1
Library version
openai 4.29.1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/717","Not throwing error if apiKey is missing","2024-03-14T04:22:12Z","Closed as not planned issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
https://github.com/openai/openai-node/blob/master/src/index.ts#L111-L115
Is this new behavior that we introduced mid way? Our use case is that we forward the request to a proxy in which the apiKey will be overridden then, so there's no need to set it here.
To Reproduce
Not setting apiKey in new OpenAI. Workaround is pretty simple, setting a random apiKey on our client
Code snippets
No response
OS
macOS
Node version
Node v18.18.12
Library version
openai v4.28.4
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/713","SDK 4.28.4 Too large message error","2024-03-14T04:49:30Z","Closed as not planned issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
APIError: 431 
 <title>Request Header Fields Too Large</title> 
Request Header Fields Too Large
 Error parsing headers: 'limit request headers fields' 
To Reproduce
const completion = await openai.chat.completions.create({
  messages: [
    { role: 'system', content: await this.promptUtil.getSystemPrompt() },
    { role: 'user', content: message },
  ],
  model: 'moonshot-v1-32k',
});

If message is too large, an error will be thrown
Code snippets
No response
OS
windows
Node version
Node V18.17.1
Library version
openai V4.28.4
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/707","Batching?","2024-03-06T23:28:43Z","Closed as not planned issue","No label","Confirm this is a feature request for the Node library and not the underlying OpenAI API.
 This is a feature request for the Node library
Describe the feature or improvement you're requesting
From the OpenAI docs:
The OpenAI API has separate limits for requests per minute and tokens per minute.
If you're hitting the limit on requests per minute, but have available capacity on tokens per minute, you can increase your throughput by batching multiple tasks into each request. This will allow you to process more tokens per minute, especially with our smaller models.
Sending in a batch of prompts works exactly the same as a normal API call, except you pass in a list of strings to the prompt parameter instead of a single string.
The example code excerpt, which is python, seems to show a legacy API. Roughly:
Single prompt, no batching
prompt = ""Once upon a time,""
 
# serial example, with one story completion per request
  response = client.completions.create(
      model=""curie"",
      prompt=prompt,
      max_tokens=20,
  )
Batched prompts
prompts = [""Once upon a time,""] * 10
 
# batched example, with 10 story completions per requestresponse = client.completions.create(
    model=""curie"",
    prompt=prompts,
    max_tokens=20,
)
However, in today's node library, our chat completion must always include messages: ChatCompletionMessageParam[] (being: ""a list of messages comprising the conversation so far"").
With the current API spec (and current models eg gpt-4) and this node library, how can we batch completion requests as described the docs?
Is it even possible?
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
👍1
dbersan reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/openai-node/issues/706","Non-ASCII tokens are corrupted sometimes when using the streaming API","2024-04-15T21:03:29Z","Closed issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
When using the streaming API, sometimes tokens get corrupted. Characters get replaced by two or more \uFFFD. For example:
{
  choices: [ { text: ' из��естни' } ],
}

when the token received is actually supposed to be ' известни'.
The issue occurs because LineDecoder does not deal with multi-byte characters on chunk boundaries. Instead of using a separate TextDecoder instance per buffer, perhaps it should use a single TextDecoderStream for the entire stream.
To Reproduce
Send a streaming completion request that will get non-ASCII tokens as a response.
Observe the output. With some probability, some of the tokens will be corrupted.
Code snippets
No response
OS
Linux
Node version
Node v18.19.1
Library version
openai v4.14.2
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/702","Typescript - incorrect type when using verbose_json as the whisper transcription response_format","2024-09-27T23:05:37Z","Closed issue","enhancement","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
With whisper, while using the verbose_json response_format parameter, the audio.transcriptions.create returns a type Transcription, which does not include the extra details from verbose_json
To Reproduce
See the code snippet
Code snippets
const response = await openAIClient.audio.transcriptions.create({
  model: 'whisper-1',
  file: fileStream,
  response_format: 'verbose_json',
  timestamp_granularities: ['segment']})

console.log(response.text)// @ts-ignoreconsole.log(response['language']) // language isn't part of the Transcription interface
OS
macOS
Node version
18.16.1
Library version
openai v4.28.4
 The text was updated successfully, but these errors were encountered: 
👍1
lilseyi reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/openai-node/issues/701","[Feature Request] Assistants API integration/example with Azure OpenAI","2024-03-02T17:31:49Z","Open issue","documentation","Confirm this is a feature request for the Node library and not the underlying OpenAI API.
 This is a feature request for the Node library
Describe the feature or improvement you're requesting
I understand there is an Azure specific SDK, but I've built a lot of my app using this library and believe this shouldn't be too hard to integrate and maintains parity with the openai python SDK.
From this example, it looks like the python SDK can handle this pretty easily:
from openai import AzureOpenAI

# Load the environment variables - These are secrets.load_dotenv()

api_URI = os.getenv(""OPENAI_URI"")
api_KEY = os.getenv(""OPENAI_KEY"")
api_version = os.getenv(""OPENAI_VERSION"")
deployment_name = os.getenv(""OPENAI_DEPLOYMENT_NAME"")

# Create an OpenAI Azure clientclient = AzureOpenAI(api_key=api_key,
        api_version=api_version,
        azure_endpoint=api_endpoint)
That's all it takes and then all the ""beta"" assistant methods are available:
assistant = client.beta.assistants.create()
thread = client.beta.threads.create()
message = client.beta.threads.messages.create()
# etc.
Source: https://github.com/Azure-Samples/azureai-samples/tree/main/scenarios/Assistants/assistants-api-in-a-box
If this can't be achieved or isn't in the roadmap through this node library, I'd rather make my own REST methods rather than try to use 2 different libraries
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/695","Enhancing completions Endpoint Response with Maximum Available Tokens","2024-02-28T23:04:48Z","Closed as not planned issue","No label","Confirm this is a feature request for the Node library and not the underlying OpenAI API.
 This is a feature request for the Node library
Describe the feature or improvement you're requesting
I've been looking into how we could further enrich the data returned from a request to the completions endpoint. One piece of information that might be highly beneficial is the inclusion of the maximum available tokens in the meta information of the response.
Currently, the endpoint's response already includes useful meta information such as the finish_reason. However, having extra data about the available tokens could aid us in effectively making follow-up requests or better managing our token usage.
Additional context
There's already a palpable demand for this feature from the user community, as evidenced by discussions such as this post on the community page. Unfortunately, there isn't a practical workaround available to them at the moment.
Given the circumstances, I believe incorporating this feature directly into the endpoint's response could greatly improve its usefulness. Looking forward to your thoughts on this.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/692","extra_body / dataSources support for Azure OpenAI BYOD","2024-02-24T00:37:29Z","Closed issue","No label","Confirm this is a feature request for the Node library and not the underlying OpenAI API.
 This is a feature request for the Node library
Describe the feature or improvement you're requesting
In the OpenAI Python API library, there are paramters included that allow extra JSON properties to be sent to the OpenAI endpoint, which is particularly useful when using Azure OpenAI endpoints. One of these properties is dataSources, which is used by the Azure OpenAI BYOD feature. I would request adding consistent functionality in the Node library to allow the extra properties to be sent to the endpoint.
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/682","How to stop streaming","2024-02-21T15:35:34Z","Closed issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
I use stream.abort(); to stop receive from api , but i have exception below
/usr/src/main-api/node_modules/openai/src/lib/ChatCompletionStream.ts:111
      throw new APIUserAbortError();
            ^
Error: Request was aborted.
    at ChatCompletionStream._createChatCompletion (/usr/src/main-api/node_modules/openai/src/lib/ChatCompletionStream.ts:111:13)
    at processTicksAndRejections (node:internal/process/task_queues:96:5)
    at ChatCompletionStream._runChatCompletion (/usr/src/main-api/node_modules/openai/src/lib/AbstractChatCompletionRunner.ts:448:12)

I have been follow to guide in the document
If you need to cancel a stream, you can break from a for await loop or call stream.abort().
To Reproduce
   const stream = this.openapi.beta.chat.completions.stream({
          messages: input_send,
          model: model_openai,
          stream: true,
          temperature: option.temperature ?? 0.7,
        });

        stream.on('content', async (delta, snapshot) => {
          console.log(delta);
          observer.next({
            data: {
              content: delta,
            }
          });
        });
        stream.on('finalMessage', (message: ChatCompletionMessage) => {
          observer.next({
            data: {
              newMessage: new MessageOpenAiModel().fromJSON({
                content: message.content,
              }),
            }
          });

        });

        stream.on('error', async (error) => {
          observer.next({
            data: {
              newMessage: new MessageOpenAiModel().fromJSON({
                content: error.message,
              }),
            }
          });
        });
        stream.finalChatCompletion().then((chatCompletion) => {
          observer.complete()
        });

        // Lắng nghe sự kiện close từ client
        response.on('close', () => {
          console.log(stream.controller.signal?.aborted);
          observer.complete();
          stream.abort()
        }); 
Nope
Code snippets
No response
OS
Ubuntu
Node version
16.15.1
Library version
v4.28.0
 The text was updated successfully, but these errors were encountered: 
👍1
wong2 reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/openai-node/issues/673","Vision API with Azure OpenAI doesn't return any usage details like tokens used","2024-02-15T05:42:09Z","Closed issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
Not sure if this is only occurs with Azure OpenAI, but the response doesn't contain usage credits at all.
Example response:
{
  ""choices"": [
    {
      ""content_filter_results"": {
        ""hate"": {
          ""filtered"": false,
          ""severity"": ""safe""
        },
        ""self_harm"": {
          ""filtered"": false,
          ""severity"": ""safe""
        },
        ""sexual"": {
          ""filtered"": false,
          ""severity"": ""safe""
        },
        ""violence"": {
          ""filtered"": false,
          ""severity"": ""safe""
        }
      },
      ""finish_reason"": ""stop"",
      ""index"": 0,
      ""message"": {
        ""content"": ""The response here"",
        ""role"": ""assistant""
      }
    }
  ],
  ""created"": 0,
  ""id"": """",
  ""model"": """",
  ""object"": ""chat.completion"",
  ""prompt_filter_results"": [
    {
      ""prompt_index"": 0,
      ""content_filter_results"": {
        ""hate"": {
          ""filtered"": false,
          ""severity"": ""safe""
        },
        ""self_harm"": {
          ""filtered"": false,
          ""severity"": ""safe""
        },
        ""sexual"": {
          ""filtered"": false,
          ""severity"": ""safe""
        },
        ""violence"": {
          ""filtered"": false,
          ""severity"": ""safe""
        }
      }
    }
  ]
}
To Reproduce
Use Vision API with Azure OpenAI Endpoint.
Code snippets
No response
OS
Windows 11
Node version
Node v20.11.0
Library version
openai v4.28.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/666","Stop throwing when fetch isnt detected in the environment","2024-02-13T02:53:53Z","Closed as not planned issue","No label","Confirm this is a feature request for the Node library and not the underlying OpenAI API.
 This is a feature request for the Node library
Describe the feature or improvement you're requesting
Testing projects using OpenAI is unusually difficult due to import-time errors if the OpenAI library (mistakenly) believes fetch libraries are missing.
Turning this into a warning, or removing and just adding documentation telling users to use the shim, would remove the need for elaborate mocking, shimming and jest setup files.
See #304 for examples of the work arounds developers need to do to develop and test projects using OpenAI, simply due to this error being thrown.
Example using the latest version of jest and OpenAI:
llm-test.js
import 'web-streams-polyfill/es6' // doent work
import 'openai/shims/node';  // doesnt work
import 'openai/shims/web'; // doesnt work
jest.mock('openai'); // doesnt work

import LLM from './llm.js';

llm.js
import OpenAI from ""openai""

Error:
    this environment is missing the following Web Fetch API type: fetch is not defined. Add one of these imports before your first `import … from 'openai'`:
    - `import 'openai/shims/node'` (if you're running on Node)
    - `import 'openai/shims/web'` (otherwise)

Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/665","Support command line usage","2024-02-06T21:18:08Z","Closed issue","No label","Confirm this is a feature request for the Node library and not the underlying OpenAI API.
 This is a feature request for the Node library
Describe the feature or improvement you're requesting
Use openai bin command from the command line like:
openai api images.generate -m dall-e-3 -p ""$DESCRIPTION"" -n 1 -s ""$SIZE""
This could be similar to or different than the Python SDK command line.
Additional context
It will be nice to have both the npm and Python products to support the command line because there's way more people that know how to use a command line tool than know how to write source code.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/660","Text-to-Speech Stream","2024-02-03T20:11:31Z","Closed issue","No label","Confirm this is a feature request for the Node library and not the underlying OpenAI API.
 This is a feature request for the Node library
Describe the feature or improvement you're requesting
Can we get the streaming for text-to-speech ?
Additional context
Example for the python library can be found here
https://platform.openai.com/docs/guides/text-to-speech
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/658","The create call fails on some non-unicode character combinations","2024-02-28T23:06:25Z","Closed issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
When making calls using openai.chat.completions.create and passing in messages.content of a non-unicode character such as ¼ instead of unicode friendly 1/4, the call to OpenAI times out.
To Reproduce
Use the code snippet below
Get a request timed out
Code snippets
This is a slimmed down version of what I've run into, and may require more of our prompt and tooling, but I can provide more context if required:
const response = await openai.chat.completions.create({
    model: 'gpt-4-1106-preview',
    messages: [
      {
        role: 'user',
        content: 'Tell me about ¼',
      },
    ]});


### OS

Docker

### Node version

20.3.1

### Library version

^4.21.0

 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/655","Stream is aborted unexpectedly","2024-02-01T14:17:10Z","Closed issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
I'm using OpenAI SDK in the client (Next.js) and server (App Router, edge runtime) setting.
The issue is that if I passed an AbortController to the fetch call, the stream was aborted as soon as it's initialized on the client side. I received no content/error on the client side, and only ""Error: aborted"" on the server side.
This is my first time using this SDK, but I had read the docs and scoured the issues on GitHub. I'm sorry if I missed anything obvious.
To Reproduce
Client code:
const abort = new AbortController();const res = await fetch('http://localhost:3000/api/chat', {
  body: JSON.stringify(params),
  headers: { 'Content-Type': 'application/json' },
  method: 'POST',
  signal: abort.signal,});

const stream = ChatCompletionStreamingRunner.fromReadableStream(res.body);console.log(abort.signal.aborted); // false

// not calledstream.on('content', (delta) => {...});

// not calledstream.on('error', (err) => {});

stream.on('abort', () => {
  // abort callback called
  console.log(abort.signal.aborted); // true});
Server code:
export const POST = async (req: Request) => {
  const params: OpenAI.ChatCompletionCreateParamsStreaming = await req.json();

  const stream = openai.beta.chat.completions.stream(params, {
    signal: req.signal, // even if I removed this line, the issue still persists
  });

  return new Response(stream.toReadableStream());};
Sanity check: if I don't pass the abort controller to the fetch call, I won't have this issue (obviously).
For personal reference, this issue is similar to #194
Code snippets
No response
OS
macOS
Node version
18
Library version
4.26.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/654","Since we have ""dangerouslyAllowBrowser"" if true, using it with self-signed server wont connect due to net::ERR_CERT_AUTHORITY_INVALID","2024-02-28T23:06:40Z","Closed issue","No label","Confirm this is a feature request for the Node library and not the underlying OpenAI API.
 This is a feature request for the Node library
Describe the feature or improvement you're requesting
I am using openai js in client-side, and using self-signed cert where my model is served in an openai-like server api, can we have something that directly translates:
httpsAgent: new https.Agent({  
  rejectUnauthorized: false})
to handle https with self-signed certificates.
Otherwise, I am new to openai, is there a way around this instead?
 Thank you.
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/642","Missing name field in type tool ChatCompletionToolMessageParam","2024-02-28T23:06:50Z","Closed issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
ChatCompletionToolMessageParam does NOT have name field as shown below.
export interface ChatCompletionToolMessageParam {
  /**   * The contents of the tool message.   */
  content: string;

  /**   * The role of the messages author, in this case `tool`.   */
  role: 'tool';

  /**   * Tool call that this message is responding to.   */
  tool_call_id: string;}
, cited from 
openai-node/src/resources/chat/completions.ts
 Line 605 in b6e7177
	exportinterfaceChatCompletionToolMessageParam{
However, the example in OpenAI API Document page puts name as functionName,
      messages.push({
        tool_call_id: toolCall.id,
        role: ""tool"",
        name: functionName,
        content: functionResponse,
      }); // extend conversation with function response
      
.
As a result, the code snippets below shows a type error
No overload matches this call.
  Overload 1 of 3, '(body: ChatCompletionCreateParamsNonStreaming, options?: RequestOptions<unknown> | undefined): APIPromise<ChatCompletion>', gave the following error.
    Object literal may only specify known properties, and 'name' does not exist in type 'ChatCompletionToolMessageParam'.
  Overload 2 of 3, '(body: ChatCompletionCreateParamsStreaming, options?: RequestOptions<unknown> | undefined): APIPromise<Stream<ChatCompletionChunk>>', gave the following error.
    Object literal may only specify known properties, and 'name' does not exist in type 'ChatCompletionToolMessageParam'.
  Overload 3 of 3, '(body: ChatCompletionCreateParamsBase, options?: RequestOptions<unknown> | undefined): APIPromise<ChatCompletion | Stream<...>>', gave the following error.
    Object literal may only specify known properties, and 'name' does not exist in type 'ChatCompletionToolMessageParam'.ts(2769)
(property) name: string

I'm not sure which is correct.
To Reproduce
npm install openai==4.25.0
create an index.ts and put the code snippets.
Code snippets
import OpenAI from 'openai';new OpenAI().chat.completions.create({
  model: 'gpt-3.5-turbo',
  messages: [{role: 'tool',content: '',tool_call_id: '', name: ''}]})
OS
macOS
Node version
v20.5.0
Library version
4.25.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/641","Uncaught (in promise) TypeError: Cannot read properties of undefined (reading '0')","2024-01-23T08:40:38Z","Closed issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
To Reproduce
Update to openai@latest
Appeared the bug
Code snippets
const chatCompletion = await openai.chat.completions.create({
        messages: [{ role: 'user', content: textInput }],
        model: this.modelList[0],
      });

      const messageContent = chatCompletion.choices[0]?.message?.content;
      if (messageContent !== null && messageContent !== undefined) {
        this.responseContent = this.parseMarkdown(messageContent);
      }
OS
Windows 11
Node version
v18.18.2
Library version
Latest
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/639","Confusing type error for openai.chat.completions.create()","2024-01-20T00:18:46Z","Open issue","enhancement","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
When using openai-node with Typescript, there is a confusing type error when calling the chat.completions.create() method.
TypeScript fails to resolve the correct overload for the messages option due to ambiguity in the ChatCompletionMessageParam type.
ChatCompletionMessageParam is a union comprising several message types, including ChatCompletionFunctionMessageParam. When creating completion messages with a user role or system role (using ChatCompletionSystemMessageParam or ChatCompletionUserMessageParam), TS matches the parameters with ChatCompletionFunctionMessageParam because they share common properties. This results in a type error because name is required in ChatCompletionFunctionMessageParam, but it's optional or missing in system or user message types.
Here is a TS Playground reproduction:
import OpenAI from ""openai"";

const openai = new OpenAI();

const messages = [
  {
    role: ""user"",
    content: ""world"",
  },];

// This failsawait openai.chat.completions.create({
  model: ""gpt-4"",
  messages: messages,});
Reproduction link: Playground Link
It seems that most examples in the OpenAI documentation produce this type error when copy pasted into the playground.
To Reproduce
Playground Link
Code snippets
No response
OS
MacOS
Node version
v18.18.2
Library version
4.24.7
 The text was updated successfully, but these errors were encountered: 
👍1
gautamr6 reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/openai-node/issues/638","Unable to write a unit test in vitest with openai-node","2024-01-24T02:32:33Z","Closed as not planned issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
My method that uses openai-node is working as expected:

`import 'openai/shims/node';
import OpenAI from 'openai';


export const OPENAI_API = new OpenAI({
    apiKey: process.env['OPENAI_API']!,
  });

const DEFAULT_PROMPT = `My Prompt:
`;

const getHeaders = (headers: any) => {
  const rateLimitHeaders = {
    'x-ratelimit-limit-requests': headers['x-ratelimit-limit-requests'],
    'x-ratelimit-limit-tokens': headers['x-ratelimit-limit-tokens'],
    'x-ratelimit-remaining-requests': headers['x-ratelimit-remaining-requests'],
    'x-ratelimit-remaining-tokens': headers['x-ratelimit-remaining-tokens'],
    'x-ratelimit-reset-requests': headers['x-ratelimit-reset-requests'],
    'x-ratelimit-reset-tokens': headers['x-ratelimit-reset-tokens'],
  };

  return rateLimitHeaders;
};

export const foobar = async (ew: string) => {
  try {
    const prompt = `${DEFAULT_PROMPT}. ${ew}. `;
    const model = 'gpt-3.5-turbo-1106';

    const { data, response } = await OPENAI_API.chat.completions
      .create(
        {
          model,
          messages: [{ role: 'user', content: prompt }],
          max_tokens: 2500,
          presence_penalty: 2.0,
          temperature: 0,
          frequency_penalty: 0.05,
        },
        { timeout: 5000 }
      )
      .withResponse();

    const headersEntries = Object.fromEntries(response.headers.entries());

    const rateLimitHeaders = getHeaders(headersEntries);

    let content: string | undefined;
    content = data.choices[0].message.content!;
    const toReturn = { responseBody: content, headers: rateLimitHeaders };

    return toReturn;
  } catch (e: any) {
    throw e;
  }
};`


Wanted to write a unit test for this as follows, no matter how I have tried I have been unable to mock OPENAI_API
`import { describe, expect, it, vi } from 'vitest';
import { foobar } from './foo.js';

// Mock the OpenAI API to avoid external calls
vi.mock('openai', () => ({
    chat: {
      completions: {
        create: vi.fn().mockResolvedValue({
          data: {
            choices: [{ message: { content: 'Mocked response from OpenAI' } }],
          },
          response: { headers: { /* Mocked headers */ } },
        }),
      },
    },
  }));

  describe('foobar function', () => {
    it('should call OpenAI with correct parameters', async () => {
      process.env['OPENAI_API_KEY'] = ""my test key""
      await foobar('Hello world');
  
      expect(vi.fn(OPENAI_API.chat.completions.create)).toHaveBeenCalledWith({
        model: 'gpt-3.5-turbo-1106',
        messages: [{ role: 'user', content: 'my pompt' }],
      }, { timeout: 5000 });
    });
});``

To Reproduce
Using 4.12.1 add code snippets decribed above and I get error:
Error: [vitest] No ""default"" export is defined on the ""openai"" mock. Did you forget to return it from ""vi.mock""? If you need to partially mock a module, you can use ""vi.importActual"" inside:
Code snippets
No response
OS
macOS
Node version
18.16.17
Library version
4.12.1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/630","Improved docs for runTools helper","2024-02-28T23:08:12Z","Closed issue","documentation","Confirm this is a feature request for the Node library and not the underlying OpenAI API.
 This is a feature request for the Node library
Describe the feature or improvement you're requesting
Improved docs for the runTools method to integrate custom functions to the open ai client
Additional context
The docs for the runTools helper method seem to be very limited at the moment.The method is indeed a great abstraction to passback tool responses to GPT automatically but seems to remove a bunch of necessary things from the response. The finish_reason for instance.
 It will be great to have more docs on this function as tools serve a very key purpose in implementing AI personas specific to several business usecases. Would love to use this abstraction but the removal of other relevant keys from the response makes it a deal breaker.If the docs do exist please point to the same.use cases
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/618","I am not getting full message response while prompting an image with text","2024-01-11T16:00:38Z","Closed issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
Hey there I am getting an issue that is related to image prompting so I am testing this API with an image where I am giving an image on the prompt (Linked below) and asking it to describe the image. Obviously I am going to get a long response from the API but for some reason I am not getting the full response. Below I have shared the code how I am calling the API for the image prompt.
Here is the response I get when I ask the API to describe the image: ""The image displays a financial problem described as ""Question No 3"", involving a""
I am asking for help on how can I get the complete response from the API while prompting it with an image. Thanks in Advance!
To Reproduce
I am not getting any errors rather I am not getting the full response. I am also new on using this library so I might not be using any extra parameters or method while prompting with an image.
Code snippets
const response = await openai.chat.completions.create({
        model: ""gpt-4-vision-preview"",
        messages: [{
        role: ""user"",
        content: [
          { type: ""text"", text: 'Describe me this image' },
          {
            type: ""image_url"",
            image_url: image.url,
          },
        ],
      }]});
OS
Windows
Node version
v18
Library version
v4.24.2
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/617","Missing system_fingerprint in openai.beta.chat.completions.stream(request).on(""chatCompletion"")","2024-01-22T21:42:05Z","Closed issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
Set seed in the request parameters. And try to get system_fingerprint from chatCompletion. There is no system_fingerprint in the completion.
{
    ""id"": ""chatcmpl-8fe1PaiWjTWKvUDzZFZyoyWSI15WX"",
    ""choices"":
    [
        {
            ""message"":
            {
                ""content"": ""xxxx"",
                ""role"": ""assistant""
            },
            ""finish_reason"": ""stop"",
            ""index"": 0,
            ""logprobs"": null
        }
    ],
    ""created"": 1704935983,
    ""model"": ""gpt-3.5-turbo-1106"",
    ""object"": ""chat.completion""
}
But system_fingerprint exists in the chunk content.
{
    ""id"": ""chatcmpl-8fe1PaiWjTWKvUDzZFZyoyWSI15WX"",
    ""object"": ""chat.completion.chunk"",
    ""created"": 1704935983,
    ""model"": ""gpt-3.5-turbo-1106"",
    ""system_fingerprint"": ""fp_cbe4fa03fe"",
    ""choices"":
    [
        {
            ""index"": 0,
            ""delta"":
            {},
            ""logprobs"": null,
            ""finish_reason"": ""stop""
        }
    ]
}
To Reproduce
set seed in request
call openai.beta.chat.completions.stream(request)
get completion on listener chatCompletion
Code snippets
const stream = await openai.beta.chat.completions.stream(request)
      .on('chatCompletion', (completion) => {
        console.log(`on chatCompletion: ${JSON.stringify(completion)}`);
      })
      .on('chunk', (chunk, snapshot) => {
        console.log(`on chunk, chunk: ${JSON.stringify(chunk)}, \nsnapshot: ${JSON.stringify(snapshot)}`);
      })
OS
macOS
Node version
Node v16.19.0
Library version
openai v4.24.1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/613","Remove unnecessary web-streams-polyfill polluting global namespace","2024-01-08T23:27:16Z","Open issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
The web-streams-polyfill package has a TypeScript bug where it pollutes the global namespace and misdefines [Symbol.asyncIterator] as a property rather than a method:
https://github.com/MattiasBuelens/web-streams-polyfill/blob/master/dist/types/ts3.6/polyfill.d.ts#L26
vs.
https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/AsyncIterator/@@asyncIterator
This causes subtle typing bugs downstream. Furthermore, ReadableStream is now supported in the LTS version of Node, so the polyfill is no longer necessary:
https://developer.mozilla.org/en-US/docs/Web/API/ReadableStream
To Reproduce
Create an interface that extends both ReadableStream and AsyncGenerator after importing the SDK:
import OpenAI from ""openai"";

interface IterableReadableStreamInterface<T> extends ReadableStream<T>, AsyncGenerator<T> {}
Code snippets
No response
OS
macOS
Node version
18
Library version
4.24.1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/611","[assistants API] How long are the validity periods of Messages and Runs?","2024-01-08T08:39:13Z","Open issue","No label","Confirm this is a feature request for the Node library and not the underlying OpenAI API.
 This is a feature request for the Node library
Describe the feature or improvement you're requesting
[assistants API] How long are the validity periods of Messages and Runs?
 The deletion API of Assistants and Threads, my understanding is that it will always exist if it is not actively deleted.
 As for Messages and Runs, I’m not sure how long they last.
 【assistants API】Messages、Runs的有效期是多久？
 Assistants、Threads的删除的API，我的理解是不主动删除一直存在
 而Messages、Runs的是多久不太确定
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/610","Function calls are running even after restarting the service","2024-01-10T04:22:34Z","Closed issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
Hi, I believe I have found a major issue in the SDK.
Here's what I'm doing:
Firebase functions deployment
Single API contact point for users (/message)
After /message is called with the latest chat message, I do the following:
 a. Collect message history from the DB
 b. Assemble the messages to call Open AI
 c. Assemble the list of functions to add (I use the .beta module)
 d. Send all functions and past messages to OpenAI + use automatic function calling
Report the result back to the caller
Now this setup works, almost always. However, there's a critical problem I'm facing:
>> Function calling gets stuck in a loop and calls my functions forever <<
Stuff that I've observed so far:
Functions are being called and results are being processed by GPT always, whether the function result is successful or not
Functions are being called and results are being processed by GPT always, whether the API token is valid or not (I changed mine on purpose to further test)
Functions are being called even after redeploying Firebase functions with the new SDK version
Deleting Firebase functions and recreating them doesn't solve the problem
I couldn't reproduce this locally on my machine
I don't know any way of stopping the execution. Please advise.
Note: I acknowledge that I might be doing something else wrong. However, it seems likely that this SDK is to blame, because I never had this problem until I started using .beta with automatic function calling.
Thanks for the support in advance.
To Reproduce
I already summarized what I'm doing in the description.
Code snippets
No response
OS
Linux
Node version
Node 18
Library version
4.17.4, 4.24.1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/606","Enhancement Request: Add Documentation for Embedding Creation Method in OpenAI Node Package After API Update from v3 to v4","2024-01-04T02:51:39Z","Closed issue","No label","Confirm this is a feature request for the Node library and not the underlying OpenAI API.
 This is a feature request for the Node library
Describe the feature or improvement you're requesting
The issue involves missing documentation for the embedding creation method in the OpenAI Node Package after the API update from v3 to v4. While the api.md file currently provides guidance on how to use the method and details about the response type, it is suggested to include this information in the README file as well.
Visit the OpenAI Node Package repository on GitHub.
Navigate to the README file or the documentation section.
Observe that the information related to the embedding creation method after the API update from v3 to v4 is not present in the main documentation. This absence affects the accessibility for users seeking details on the updated functionality.
This correction will ensure better visibility on the npm main page or website, making it more accessible for users seeking information on embedding creation in the updated version
Additional context
An effective solution involves updating the README with a usage example. This addresses the missing documentation for the embedding creation method post the API update from v3 to v4, ensuring enhanced clarity and accessibility on the npm main page or website
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/604","dall-e-3 free tier 429 too many request limit reached","2023-12-28T09:19:56Z","Open issue","openai api","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
Hello everyone,
I'm using the OpenAI library obtained from npm in my Next.js project. When I make a call to generate an image using the dall-e-3 model, unfortunately, I always receive a response with a 429 status code, indicating that I've reached the call limit. Strangely, I haven't even made a single call yet, and the documentation suggests that there should be one call per minute.
I've also tried making the call directly without using the library, but I get the same result. I've read that some people have resolved this issue by upgrading to higher-tier plans.
Could someone please let me know if there's a solution to this problem while still using the free tier?
To Reproduce
use openai npm library
 try generate image with dall-e-3 model
or make a http call with axios, or postman with dall-e-3 model
Code snippets
No response
OS
Windows 11
Node version
Node 20.10.0
Library version
openai 4.24.1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/603","Nextjs + Typescript - cannot start application - Uncaught SyntaxError: Identifier 'Audio' has already been declared","2023-12-30T23:38:56Z","Closed issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
To Reproduce
Create a turborepo with nextjs and openai imports
Build the app
Start the app
Expected behavior: App starts
Observed behavior: White screen with Uncaught SyntaxError: Identifier 'Audio' has already been declared in console
Code snippets
No response
OS
macOS 14.1.1
Node version
node v21.2.0
Library version
openai 4.24.1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/602","Typescript - cannot build with openai import","2023-12-29T21:35:08Z","Closed issue","No label","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
Cannot use named exports, need to circumvent with unusual type OpenAI.Beta.AssistantCreateParams and OpenAI.Beta.Threads.MessageContentText
To Reproduce
Create a turborepo with nextjs and openai lib.
Add the following type imports
import type { AssistantCreateParams } from 'openai/src/resources/beta/assistants/assistants';
import type { MessageContentText } from 'openai/resources/beta/threads';

Run the build
Expected state: Build is ok
Observerd state: Build failed on Type error: 'opts' is declared but its value is never read. at ../../node_modules/.pnpm/openai@4.24.1_encoding@0.1.13/node_modules/openai/src/core.ts:187:25
Code snippets
No response
OS
macOS 14.1.1
Node version
node v21.2.0
Library version
openai 4.24.1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/597","Passing global context into tools called by the runTools helper","2023-12-21T17:00:44Z","Open issue","enhancement","Confirm this is a feature request for the Node library and not the underlying OpenAI API.
 This is a feature request for the Node library
Describe the feature or improvement you're requesting
I currently have a pattern where I need to pass context to my tools to allow them to act on my app. For example:
function updateEvent(context: { eventId: string}, args: ArgsFromOpenAi) {
   const event = await fetchEvent(eventId);
}

It'd be great if there were some way to pass a global context to the runner since the runner is passed into each function call. Then, I could do something like this:
function updateEvent( args: ArgsFromOpenAi, runner: ChatCompletionStreamingRunner<EventContext>) {
  const { eventId } = runner.context;
   const event = await fetchEvent(eventId);
}

Additional context
A workaround is to build my own runner that leverages the existing helpers. However, this is complicated because of the types integration.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/596","SyntaxError: Unexpected non-whitespace character after JSON at position X","2023-12-21T18:16:52Z","Closed issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
The following arguments were received at Function Calling.
{
functionArgText: `{""query"": ""What do I want to be by the time I'm 40?""}{""query"": ""Where do I want my career to be by the time I'm 40?""}{""query"": ""What kind of family do I want to have by the time I'm 40?""}`
}

And trying to parse the above,
SyntaxError: Unexpected non-whitespace character after JSON at position 53
I was told ""SyntaxError: Unexpected non-whitespace character after JSON at position 53 For some reason, it often happens that similar queries are repeated 3 times, like this one. What should I do?
To Reproduce
Use OpenAI API via Node.js
Set a Tool
export const googleSearchSchema: AssistantCreateParams.AssistantToolsFunction = {
  type: ""function"",
  function: {
    name: ""googleSearch"",
    description: ""Search the web using Google Custom Search Engine API."",
    parameters: {
      type: ""object"",
      properties: {
        query: {
          type: ""string"",
          description: `The search query. Multiple search terms should be separated by spaces. DO NOT use line breakers such as ""\\n"". The number of terms should be between 1 and 10. Less is better. Use Google search operators to refine your search. For example 1, ""site:en.wikipedia.org"" will search only Wikipedia.`,
        },
      },
      required: [""query""],
    },
  },
} as const;

Code snippets
No response
OS
macOS
Node version
v20.10.0
Library version
""openai"": ""^4.24.0"",
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/593","RunnerOptions is not exposed for runTools and runFunctions","2023-12-20T21:12:28Z","Open issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
The maxChatCompletions option is very helpful in managing costs. However, it is not exposed through the interface due to a typing bug.
I think the fix is to simply change the Core.RequestOptions type to RunnerOptions which only contains maxCompletions at the moment.
To Reproduce
Try passing maxCompletions to the runTools/runFunctions options


Code snippets
No response
OS
macOS
Node version
v18.16.0
Library version
v4.24.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/584","Why does the message get cut off abruptly sometimes?","2023-12-19T03:14:50Z","Closed issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
Sometimes streaming message gets cut off mid-sentence.
To Reproduce
Make a post request to openai.chat.completions.create()
Sometimes returned message is cut off and does not finish
Code snippets
I am using the OpenAI API Node SDK in a Firebase Cloud Function to generate short stories. I am trying to debug why the messages stream gets cut off sometimes mid-sentence. For example:
Anyone got any advice on how to debug this? Here are my openAI chat completions settings:
    try {
      const { messages } = request.body
      const config = {
        // model: 'gpt-3.5-turbo',
        model: 'gpt-4-1106-preview',
        stream: true,
        messages: [
          {
            role: 'system',
            content:
              ""<My custom system message here.>""
          },
          {
            role: messages[0].role,
            content: messages[0].content
          }
        ],
        temperature: 1,
        max_tokens: 3000,
        top_p: 0.82,
        frequency_penalty: 0,
        presence_penalty: 0
      }
      const openaiResponse = await openai.chat.completions.create(config)
      ```

The total characters in the generated stories is about 3000.
Any thoughts?


### OS

Windows

### Node version

Node v18

### Library version

openai v4.23.0

 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/577","Clarify if ""description"" of JSON schema parameters is taken into the account","2024-05-13T01:55:58Z","Closed as not planned issue","No label","Confirm this is a feature request for the Node library and not the underlying OpenAI API.
 This is a feature request for the Node library
Describe the feature or improvement you're requesting
I am using runTools.
If I provide description to parameters, are they taken into account?, e.g.
{
  type: 'object',
  properties: {
    artForm: {
      type: 'string',
      description: 'art form, style, e.g. Digital illustration and flat colors. Output all in UPPERCASE.'
    },
    content: {
      type: 'string',
      description: 'image content/subject, description of action, state, and mood, e.g. Human silhouette against a large, glowing AI brain, hands hovering over a keyboard, brainstorming and typing.'
    },
    description: { type: 'string', description: 'SEO description of the image.' },
    name: {
      type: 'string',
      description: 'name of the image, e.g. ""Human and AI Brain""'
    },
    settings: {
      type: 'string',
      description: 'additional settings, such as lighting, colors, and framing, e.g. Backlit with ambient blue and green hues to symbolize innovation and creativity, medium shot with focus on the interaction between human and AI brain'
    }
  },
  required: [ 'artForm', 'content', 'description', 'name', 'settings' ],
  additionalProperties: false,
  '$schema': 'http://json-schema.org/draft-07/schema#'}
Based on my experimentation, they are not, but I wanted to clarify.
Additional context
Reading https://github.com/openai/openai-node?tab=readme-ov-file#automated-function-calls is not clear if they are.
 The text was updated successfully, but these errors were encountered: 
👍1
schneiderfelipe reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/openai-node/issues/573","gpt-4-vision-preview does not work as expected.","2023-12-15T21:49:07Z","Open issue","openai api","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
The response from ChatGPT unexpectedly cuts off if using stream. The response via API does not match the request through chat; through the API, I only receive the beginning of the response which unexpectedly cuts off. I think this is related to the bug below.""
#499
To Reproduce
openai.beta.chat.completions.stream with image_url
 I use the following image.

From API I got only
The instructions are asking for a modification of the SQL CREATE TABLE statement for
 From chat I got much more.
Code snippets
const testVision = async () => {
    const stream = await openai.beta.chat.completions.stream({
        model: 'gpt-4-vision-preview',
        messages: [
            {
                role: 'user',
                content: [{
                    type: 'image_url',
                    image_url: convertImageToDataURLSync('1.png'),
                }],
            }
        ],
        stream: true,
    });
    stream.on('content', (delta, snapshot) => {
        process.stdout.write(delta)
    });
    stream.finalChatCompletion().then( () => {
        process.stdout.write('\n')
    } );}
OS
Linux
Node version
Node v18.16.0
Library version
openai 4.22.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/566","Examples in Readme.md are deprecated compared to the version ^4.21.0","2023-12-14T13:14:36Z","Closed issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
The examples provided in the Readme.MD are deprecated in ^4.21.0, it's really that hard to provide a link to the docs or just update the read.me togheter with the code before releases a new version?
To Reproduce
Install ""openai"": ""^4.21.0"",
Try to use client.beta.chat.completions.runFunctions
Function already deprecated, what's the new function example?
Code snippets
OpenAI from 'openai';


const client = new OpenAI({
  apiKey: OPENAI_API_KEY, // This is the default and can be omitted});

async function main() {
  const runner = client.beta.chat.completions
    .runFunctions({
      model: 'gpt-3.5-turbo',
      messages: [{ role: 'user', content: 'How is the weather this week?' }],
      functions: [
        {
          function: getCurrentLocation,
          parameters: { type: 'object', properties: {} },
        },
        {
          function: getWeather,
          parse: JSON.parse, // or use a validation library like zod for typesafe parsing.
          parameters: {
            type: 'object',
            properties: {
              location: { type: 'string' },
            },
          },
        },
      ],
    })
    .on('message', (message) => console.log(message));

  const finalContent = await runner.finalContent();
  console.log();
  console.log('Final content:', finalContent);}

async function getCurrentLocation() {
  return 'Boston'; // Simulate lookup}

async function getWeather(args: { location: string }) {
  const { location } = args;
  // … do lookup …
  return { temperature: 40, precipitation: 30 };}

main();

// {role: ""user"",      content: ""How's the weather this week?""}// {role: ""assistant"", function_call: ""getCurrentLocation"", arguments: ""{}""}// {role: ""function"",  name: ""getCurrentLocation"", content: ""Boston""}// {role: ""assistant"", function_call: ""getWeather"", arguments: '{""location"": ""Boston""}'}// {role: ""function"",  name: ""getWeather"", content: '{""temperature"": ""50degF"", ""preciptation"": ""high""}'}// {role: ""assistant"", content: ""It's looking cold and rainy - you might want to wear a jacket!""}//// Final content: ""It's looking cold and rainy - you might want to wear a jacket!""
OS
macOS
Node version
v16.14.0
Library version
openai v4.21.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/565","Stream events work in dev mode, not when built","2023-12-14T19:35:43Z","Closed issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
Get an error when compiled but not when in dev mode.
In my nextjs edge api route, I'm running:
const stream = openai.beta.chat.completions.runTools({
      stream: true,
      model,
      messages,
      temperature,
      tools,
});

stream.on('end', async () => {  <---- this errors when compiled, not in dev mode
      await closeSandbox();
    });

return new Response(stream.toReadableStream());

This all works fine but when I put any event on the stream once it's compiled there's an error when it should be called.
TypeError: Cannot read properties of null (reading 'toString') at ws.onmessage
Tried wrapping in promise and all sorts of other stuff but every time compiled breaks. Just need to be able to call this when the stream is over.
To Reproduce
Hit nextjs api route with edge runtime
Run tools
Add stream event like `stream.on('end', ()=> 'do something');
Works in dev, errors in build
Code snippets
In my nextjs edge api route, I'm running:const stream = openai.beta.chat.completions.runTools({      stream: true,      model,      messages,      temperature,      tools,});stream.on('end', async () => {  <---- this errors when compiled, not in dev mode
      await closeSandbox();
    });

return new Response(stream.toReadableStream());
This all works fine but when I put any event on the stream once it's compiled there's an error when it should be called.
TypeError: Cannot read properties of null (reading 'toString') at ws.onmessage


### OS

macOS

### Node version

Node 18.17.1

### Library version

openai 4.16.2

 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/564","functions don't support array params on top level","2023-12-15T00:46:44Z","Closed issue","No label","Confirm this is a feature request for the Node library and not the underlying OpenAI API.
 This is a feature request for the Node library
Describe the feature or improvement you're requesting
when using array as top level object in json schema i get error 400 from the openAI API
well it is not a big deal, i can just wrap the array one level below i think
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/563","TypeError: Reflect.get called on non-object on Cloudflare","2023-12-18T12:12:58Z","Closed issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
Hi I am using Next js and deploying it on cloudflare. I am making sure everything is running on edge runtime. When I run my project locally, everything works fine.
However, on my deployed project, when I try to make a call to my api endpoint. I get an error of
TypeError: Reflect.get called on non-object in the logs on cloudflare.
To Reproduce
Create a simple next js project with openai api call.
Deploy it on Cloudflare.
You will not be able to make a call.
Code snippets
No response
OS
Linux
Node version
18+
Library version
openai v.4.21.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/560","node_modules/openai/src/streaming.ts(187,18): error TS7006: Parameter 'ctrl' implicitly has an 'any' type.","2023-12-11T19:41:48Z","Closed issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
node_modules/openai/src/streaming.ts(187,18): error TS7006: Parameter 'ctrl' implicitly has an 'any' type.
To Reproduce
""openai"": ""^4.20.1"",
Code snippets
No response
OS
macOS
Node version
rwest@Roshis-MacBook-Pro slackgpt3 % node -v v21.2.0
Library version
""openai"": ""^4.20.1"",
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/559","How openai stores my custom session window and can get a list of it.","2023-12-19T18:10:30Z","Closed issue","No label","Confirm this is a feature request for the Node library and not the underlying OpenAI API.
 This is a feature request for the Node library
Describe the feature or improvement you're requesting
I want the message to create a new session window and store it in openai so I can view it the next time I log in.
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/558","Stream nor ChatCompletionStream types not exported","2023-12-10T10:53:51Z","Open issue","enhancement","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
One of openai.chat.completions.create return types is ApiPromise<Stream<OpenAI.Chat.ChatCompletionChunk>>
 yet the Stream is not the intrinsic Nodejs Stream, and not exported from openai.
To Reproduce
try to set type of awaited or in Promise return value from openai.chat.completions.create()
Code snippets
import OpenAI from 'openai'

import { config } from 'server/config'import { errorHandler } from 'server/helpers'

const openai = new OpenAI({
  apiKey: config?.OPENAI_API_KEY as string,})

export const createText = async(prompt: string): Promise<OpenAI.Chat.ChatCompletion | null> => {
  try {
    const params: OpenAI.Chat.ChatCompletionCreateParams = {
      messages: [{ role: 'user', content: prompt }],
      model: 'gpt-3.5-turbo',
      stream: true,
    }

    const options: OpenAI.RequestOptions = {}

    const chatCompletion: OpenAI.Stream<OpenAI.Chat.ChatCompletionChunk> = await openai.chat.completions.create(params, options)

    return chatCompletion
  } catch (error: unknown) {
    console.error(error)
    errorHandler(error)
    return null
  }}
OS
any
Node version
18.13.0
Library version
4.20.1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/557","Encountering timeouts for longer prompt when using SDK with gpt-4-1106-preview, but not when hitting API directly.","2023-12-11T13:26:29Z","Closed issue","No label","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
We are using some longer prompts and consistently seeing the NodeSDK timeout, whereas the issue does not occur when calling the API directly with the same prompt.
Works reliably:
await axios.post(
      'https://api.openai.com/v1/chat/completions',
      {
        model: 'gpt-4-1106-preview',
        messages: [
          {
            role: 'system',
            content: systemPrompt
              ? systemPrompt
              : 'You are a helpful assistant.',
          },
          {
            role: 'user',
            content: prompt,
          },
        ],
      },

Times out consistently:
const result = await openai.chat.completions.create({
      messages: [
        {
          role: 'system',
          content: `You are a helpful assistant.`,
        },
        {
          role: 'user',
          content: prompt,
        },
      ],
      model: 'gpt-4-1106-preview',
    });
    return {
      completion: result.choices[0].message.content,
    };    

I can't share the prompts as they contain customer information, but the length of them is:
 Tokens
 5,531
 Characters
 25661
To Reproduce
Make a call with longer prompt using the Node SDK -> Get Timeout error
Error: Request timed out.
    at OpenAI.makeRequest 
    at process.processTicksAndRejections 
    at async completeChat 

Make same call directly to the API via axios -> No problem.
I've reproduced this issue consistently in production and locally. It does not occur as I shrink the length of the prompt, so it seems to have to do with prompt length and the SDK.
Code snippets
const result = await openai.chat.completions.create({
      messages: [
        {
          role: 'system',
          content: `You are a helpful assistant.`,
        },
        {
          role: 'user',
          content: prompt,
        },
      ],
      model: 'gpt-4-1106-preview',
    });
    return {
      completion: result.choices[0].message.content,
    };    
await axios.post(
      'https://api.openai.com/v1/chat/completions',
      {
        model: 'gpt-4-1106-preview',
        messages: [
          {
            role: 'system',
            content: systemPrompt
              ? systemPrompt
              : 'You are a helpful assistant.',
          },
          {
            role: 'user',
            content: prompt,
          },
        ],
      },



### OS

macOS - 14.0 - 23A344

### Node version

v18.15.0

### Library version

4.20.1

 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/556","Using the speech API always produces an OPTIONS request causing additional latency","2023-12-08T14:52:03Z","Closed as not planned issue","No label","Confirm this is a feature request for the Node library and not the underlying OpenAI API.
 This is a feature request for the Node library
Describe the feature or improvement you're requesting
I use the client API in the following way:
const response = await openai.audio.speech.create({
  model: ""tts-1"",
  voice: ""Nova"",
  speed: 1,
  input: ""Some sentence"",});
    
const arrayBuffer = await response.arrayBuffer();const blob = new Blob([arrayBuffer], { type: 'audio/mpeg' });const url = URL.createObjectURL(blob);// ...
In the network tab of the Chrome Dev Tools, I saw that this always results in two requests, one OPTIONS request and one subsequent POST with the actual payload and the audio response. The OPTIONS request introduces an additional latency of about 200ms. My question is if there is no way around that. ""Options"" sounds a bit like its result should be cached, but I have no clue.
Thanks!
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/555","Add a way to avoid printing the finalContent when using runFunctions","2023-12-11T04:15:50Z","Closed issue","enhancement","Confirm this is a feature request for the Node library and not the underlying OpenAI API.
 This is a feature request for the Node library
Describe the feature or improvement you're requesting
I have a routine that prints alphabet A-Z.
One function prints one letter at a time.
The problem is that OpenAI SDK also outputs the entire alphabet again, at the end.
Is there a way that would force it to return early without bothering with producing the final output?
const runner = openai.beta.chat.completions.runFunctions({
  functions: [
    {
      description: 'Prints letter',
      name: 'print_letter',
      parameters: {
        type: 'object',
        properties: {
          letter: {
            type: 'string',
            description: 'Letter to print',
          },
        },
        function: ({letter}) => {
          console.log(letter);
          return {message: letter};
        },
      }
    }
  ],
  messages: [
    {
      content: 'Print letters A to Z',
      role: 'user',
    },
  ],
  model: 'gpt-4-1106-preview',});

await runner.finalFunctionCall();

console.log(await runner.finalContent());
Here the last console.log will print something along the lines of:
The alphabet between A and F has been listed:\n\nA\nB\nC\nD\nE\nF

I tried tapping into .on('finalFunctionCall', ... event, but that happens after finalContent() already has a result.
I really just need some sort of event that fires before it starts generating the final output, so I could abort early.
The example uses alphabet listing, which is quick. But in real-world scenario, that final content function can a long time/many tokens to generate.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/553","on('error', () => ...) event does not fire on ""OpenAIError: stream ended without producing a ChatCompletionMessage with role=assistant""","2023-12-05T03:45:20Z","Open issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
Similar to #526 except there's no real way to handle this except within process.on('uncaughtException', (err) => {}), a hacky workaround I fenagled, or using the old API vs. openai.beta.
I understand that this may be exclusive to reverse proxies or other such APIs mimicking OpenAI spec, and perhaps missing a critical spec, but the error should still land where expected to be caught.
OpenAIError: stream ended without producing a ChatCompletionMessage with role=assistant
 at ChatCompletionStream._AbstractChatCompletionRunner_getFinalMessage (/app/node_modules/openai/lib/AbstractChatCompletionRunner.js:464:11)
 at ChatCompletionStream._AbstractChatCompletionRunner_getFinalContent (/app/node_modules/openai/lib/AbstractChatCompletionRunner.js:455:134)
 at ChatCompletionStream._emitFinal (/app/node_modules/openai/lib/AbstractChatCompletionRunner.js:282:152)
 at /app/node_modules/openai/lib/AbstractChatCompletionRunner.js:77:22
 at process.processTicksAndRejections (node:internal/process/task_queues:95:5)
To Reproduce
Use a reverse proxy service, using an alternate baseURL 
here is a real scenario reported by a user of my app: [Bug]: Running LibreChat against LiteLLM backed by Ollama danny-avila/LibreChat#1270
I was able to reproduce by using ollama in conjunction with litellm, passing the server url as baseURL
Streaming works and I can add breakpoints to chunks and are indeed generation partials
Error after finalMessage listener, is uncaught and will crash node server unless prevented as shown 
I noticed the emit: end is expecting the last message of stream.messages to be an assistant message, so my hack prevents the issue by pushing an artificial assistant message with the real tokens generated
Code snippets
// Here's how I'm handling streamstry {
  const stream = await openai.beta.chat.completions
    .stream({
      ...modelOptions,
      stream: true,
    })
    .on('error', (err) => {
      /* Expect error here */
    })
    .on('finalMessage', (message) => {
      /* role === 'user' here, causing the uncaught error */
    });
  
  for await (const chunk of stream) {
    const token = chunk.choices[0]?.delta?.content || '';
  }} catch (err) {
  /* If not above, expect error here */}

// My hacky workaroundtry {
  let intermediateReply = '';
  const stream = await openai.beta.chat.completions
    .stream({
      ...modelOptions,
      stream: true,
    })
    .on('finalMessage', (message) => {
      if (message?.role !== 'assistant') {
        stream.messages.push({ role: 'assistant', content: intermediateReply });
      }
    });
  
  for await (const chunk of stream) {
    const token = chunk.choices[0]?.delta?.content || '';
    intermediateReply += token;
  }} catch (err) {//}
OS
Linux 5.10.16.3-microsoft-standard-WSL2 x86_64 x86_64
Node version
v18.13.0
Library version
openai v4.20.1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/552","I'm loading a couple pieces like this in my frontend, mostly to use .fromReadableStream. It works when in dev mode (nextjs) but when I build I get an error about Audio already being declared in the app console. I'm not using anything with ""audio"" so I'm going to guess it's the way I'm loading them that's either causing it to load twice or there's something else going on.","2023-12-05T02:51:17Z","Open issue","No label","I'm loading a couple pieces like this in my frontend, mostly to use .fromReadableStream. It works when in dev mode (nextjs) but when I build I get an error about Audio already being declared in the app console. I'm not using anything with ""audio"" so I'm going to guess it's the way I'm loading them that's either causing it to load twice or there's something else going on.
import { APIUserAbortError } from 'openai';
import { ChatCompletionStream } from 'openai/lib/ChatCompletionStream';

I'm loading the abort error to handle when the stream gets aborted.
Originally posted by @a2thek26 in #182 (comment)
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/550","Connection error -- cause: TypeError: Bearer { ""apiKey"": ""..."", ""organization"": ""..."" } is not a legal HTTP header value","2023-12-01T12:20:38Z","Closed issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
When I call OpenAI with this library with organization id, it throws Connection error.
APIConnectionError: Connection error.     at OpenAI.makeRequest (/workspace/node_modules/.pnpm/openai@4.20.1/node_modules/openai/core.js:279:19)
...
cause: TypeError: Bearer {
  ""apiKey"": ""..."",
  ""organization"": ""...""
} is not a legal HTTP header value
at validateValue (/workspace/node_modules/.pnpm/node-fetch@2.7.0/node_modules/node-fetch/lib/index.js:684:9)
at Headers.append (/workspace/node_modules/.pnpm/node-fetch@2.7.0/node_modules/node-fetch/lib/index.js:836:3) 
at new Headers (/workspace/node_modules/.pnpm/node-fetch@2.7.0/node_modules/node-fetch/lib/index.js:761:11) 
at new Request (/workspace/node_modules/.pnpm/node-fetch@2.7.0/node_modules/node-fetch/lib/index.js:1231:19) 
at /workspace/node_modules/.pnpm/node-fetch@2.7.0/node_modules/node-fetch/lib/index.js:1449:19 
at new Promise (<anonymous>) 
at fetch (/workspace/node_modules/.pnpm/node-fetch@2.7.0/node_modules/node-fetch/lib/index.js:1447:9) 
at OpenAI.fetchWithTimeout (/workspace/node_modules/.pnpm/openai@4.20.1/node_modules/openai/core.js:333:20) 
at OpenAI.makeRequest (/workspace/node_modules/.pnpm/openai@4.20.1/node_modules/openai/core.js:268:37) 
} 

To Reproduce
Instantiate OpenAI with apiKey and organization
Call arbitrary API
Connection error occurs
Code snippets
import * as ff from '@google-cloud/functions-framework';import fs from 'fs';import OpenAI from 'openai';

ff.http('FooFunction', async (req: ff.Request, res: ff.Response) => {
  const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY, organization: 'org-foobar' });
  const result = await openai.audio.transcriptions.create({
    file: fs.createReadStream('/tmp/audio.mp4'),
    model: 'whisper-1'
  }).catch(console.error)});
OS
Linux
Node version
Node v20 (Google Cloud function runtime)
Library version
v4.20.1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/549","Automated Function Calls Readme.md example is missing required description property on function objects","2024-07-08T17:09:26Z","Closed issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
The description property is missing in the RunnableFunctionWithParse<any> type on both getWeather and getCurrentLocation.

The description property is a string that describes what the function does, and are required under any RunnableFunction or ParsedFunction type:

To fix this, we just simply need to add a description property to the getWeather and getCurrentLocation function objects in the Readme example.
https://github.com/openai/openai-node/blob/master/README.md#automated-function-calls
To Reproduce
Go to https://github.com/openai/openai-node/blob/master/README.md#automated-function-calls
Copy the provided example
Insert in any Typescript(""typescript"": ""^5.2.2"") environment to immediately see the issue | Run example in Node.js
Code snippets
import OpenAI from 'openai';

const client = new OpenAI();

async function main() {
  const runner = client.beta.chat.completions
    .runFunctions({
      model: 'gpt-3.5-turbo',
      messages: [{ role: 'user', content: 'How is the weather this week?' }],
      functions: [
        {
          function: getCurrentLocation,
          description: 'This function gets the current location.', /*missing property*/  
          parameters: { type: 'object', properties: {} },
        },
        {
          function: getWeather,
          parse: JSON.parse, // or use a validation library like zod for typesafe parsing.
          description: 'This function gets the weather for a given location.', /*missing property*/  
          parameters: {
            type: 'object',
            properties: {
              location: { type: 'string' },
            },
          },
        },
      ],
    })
    .on('message', (message) => console.log(message));

  const finalContent = await runner.finalContent();
  console.log();
  console.log('Final content:', finalContent);}

async function getCurrentLocation() {
  return 'Boston'; // Simulate lookup}

async function getWeather(args: { location: string }) {
  const { location } = args;
  // … do lookup …
  return { temperature, precipitation };}

main();
OS
macOS
Node version
Node v21.1.0
Library version
OpenAI 4.20.1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/548","Unexpected token 'export' from formdata-node when using openai/shims/node","2023-11-30T17:47:18Z","Open issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
Issues experienced while running unit tests in our nodejs backend.
 I first encountered the fetch is not defined bug, which led me to #304 and was able to move past the issue with import 'openai/shims/node'
But now I'm receiving the following errors:
    Details:

    /Users/person/Developer/repos/my-apis/node_modules/formdata-node/lib/esm/index.js:1
    ({""Object.<anonymous>"":function(module,exports,require,__dirname,__filename,jest){export * from ""./FormData.js"";
                                                                                      ^^^^^^

    SyntaxError: Unexpected token 'export'

      4 | import 'formdata-polyfill'
      5 | import 'whatwg-fetch'
    > 6 | import 'openai/shims/node'
        | ^
      7 |
      8 | import joi from 'joi'
      9 | import OpenAI from 'openai'

      at Runtime.createScriptFromCode (node_modules/jest-config/node_modules/jest-runtime/build/index.js:1505:14)
      at Object.<anonymous> (node_modules/openai/src/_shims/node-runtime.ts:5:1)
      at Object.<anonymous> (node_modules/openai/src/shims/node.ts:4:1)

I've also added
/**
 * @jest-environment jsdom
 */
import 'formdata-polyfill'
import 'whatwg-fetch'

But I'll still get the same SyntaxError: Unexpected token 'export' error.
I've updated the transformIgnorePatterns value to include ""/node_modules/(?!formdata-node)"" as well as ""/node_modules/(?!openai/node/shims)"" and many combinations. Still no luck.
yarn v1.22.4
 typescript v5.1.6
 jest v29.7.0
To Reproduce
yarn add openai
I've been able to reproduce it with as little as just the imports.
 Add the following imports to a file that has unit tests (or a logic/utils file used by a file being tested)
import 'openai/shims/node'

import OpenAI from 'openai'

run the tests on that file
Code snippets
No response
OS
macOS
Node version
Node v16.20.0 (also tested with v18.19.0 same results)
Library version
openai v4.20.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/541","Missing one parameter baseURL","2024-02-28T23:13:41Z","Closed issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
set system evn export OPENAI_BASE_URL='...'
 The following code is invalid.
 const client = new OpenAI()
To Reproduce
https://github.com/openai/openai-node/blob/08833f20ae4eab6d23ddf989557e1c05a471352b/src/index.ts#L105C1-L106C1
Code snippets
No response
OS
MacOS
Node version
node v20.9.0
Library version
OpenAI 4.20.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/540","runFunction throws ""'content' is a required property - 'messages.1'"" error when using Azure openAI integration","2023-11-30T16:50:25Z","Closed issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
When you use runFunctions function with Azure open AI, it throws the ""'content' is a required property - 'messages.1'"" error with 400 error code.
const runner = openai.beta.chat.completions .runFunctions({ messages: [ { role: ""user"", content: what is sqauare root of 4? } ], functions: [ { ""name"": ""calculateSquareRoot"", ""function"": calculateSquareRoot, ""description"": ""Calculate sqaure root of a number"", ""parameters"": { type: 'object', properties: { number: { type: 'integer' }, } }, }, { ""name"": ""calculateLogBase2"", ""function"": calculateLogBase2, description: ""Calculate log base 2 of a number"", parameters: { type: 'object', properties: { number: { type: 'integer' }, } }, } ] })
After debugging in the library, I found the issue. The second gpt call that happens after the function call is missing the content field in the messages object -
{ model: 'gpt-3.5-turbo', messages: [ { role: 'user', content: 'what is sqauare root of 4?' }, { role: 'assistant', function_call: [Object] }, { role: 'function', name: 'calculateSquareRoot', content: '2' } ], functions: [ { name: 'calculateSquareRoot', parameters: [Object], description: 'Calculate sqaure root of a number' }, { name: 'calculateLogBase2', parameters: [Object], description: 'Calculate log base 2 of a number' } ], function_call: 'auto' }
Here you can see that messages[1].content is not present. In case of openAI call, this is null where it runs smoothly.
 A simple solve is to add - params.messages = params.messages.map((message) => { message.content = message.content || null; return message; }).
To Reproduce
Code to reproduce -
use azure configuration for openAI initialisation.
const runner = openai.beta.chat.completions .runFunctions({ model: ""gpt-3.5-turbo"", messages: [ { role: ""user"", content: what is sqauare root of 4? } ], functions: [ { ""name"": ""calculateSquareRoot"", ""function"": calculateSquareRoot, ""description"": ""Calculate sqaure root of a number"", ""parameters"": { type: 'object', properties: { number: { type: 'integer' }, } }, }, { ""name"": ""calculateLogBase2"", ""function"": calculateLogBase2, description: ""Calculate log base 2 of a number"", parameters: { type: 'object', properties: { number: { type: 'integer' }, } }, } ] }) console.log(runner) const finalContent = await runner.finalContent();
Code snippets
No response
OS
macOs
Node version
node v16.14.2
Library version
^4.0.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/538","Types missing when using with Deno","2023-11-27T17:29:27Z","Closed issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
When using this library with Deno, the type information are missing for all types exported from resources/index.

This is probably due to main index file importing the resources/index as openai/resources/index instead of ./resources.index.

I'm using VSCode 1.84.2.
To Reproduce
To reproduce:
Setup a Deno project with VSCode
Add import OpenAI from 'https://esm.sh/openai@4.20.0'
Try to use any of the functions (openAI.embeddings.create, for example).
Code snippets
No response
OS
MacOS
Node version
Deno 1.38.2
Library version
OpenAI 4.20.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/537","Stream handling does not recognize stream errors","2023-11-28T17:34:00Z","Closed issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
For example, when a chat completion is created like:
openai.chat.completions.create({
  messages: [{ role: 'user', content: 'foo' }],
  stream: true})
And the underlying stream generates an error, this library will not surface the error to the application. Instead, it generates an uncaught exception error.
To Reproduce
See https://github.com/jsumners-nr/openai-stream-issue/tree/91e2b46b08baec3cd061a02b9492b501a353aab3
Code snippets
import OpenAI from 'openai'import http from 'node:http'import { Readable } from 'node:stream'import { randomBytes } from 'node:crypto'

let count = 0const stream = new Readable({
  read(size = 16) {
    if (count >= 100) {
      throw Error('exceeded count')
    }

    const bytes = randomBytes(size)
    const chunk = JSON.stringify({
      id: 'chatcmpl-8MzOfSMbLxEy70lYAolSwdCzfguQZ',
      object: 'chat.completion.chunk',
      // 2023-11-20T09:00:00-05:00
      created: 1700488800,
      model: 'gpt-4',
      choices: [
        {
          index: 0,
          finish_reason: null,
          delta: { role: 'assistant', content: bytes.toString('base64')}
        }
      ]
    })

    this.push('data: ' + chunk + '\n\n')
    count += 1
  }})

const server = await new Promise((resolve) => {
  const server = http.createServer((req, res) => {
    res.statusCode = 200
    stream.pipe(res)
  })
  server.listen({ host: '127.0.0.1', port: 0 }, () => {
    resolve(server)
  })})

const openai = new OpenAI({
  baseURL: `http://${server.address().address}:${server.address().port}/v1`,
  apiKey: 'super-secret'})

const response = await openai.chat.completions.create({
  messages: [{ role: 'user', content: 'foo' }],
  stream: true})

process.on('uncaughtException', (error) => {
  console.error('!!! UNCAUGHT EXCEPTION !!!\n', error)
  server.close()
  process.exit(1)})

try {
  for await (const chunk of response) {
    console.log('got chunk:', count)
  }} catch (error) {
  console.error('got expected error:', error)} finally {
  server.close()}
OS
macOS
Node version
18.18.2
Library version
4.20.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/534","Role types are off","2023-11-26T23:42:08Z","Closed issue","No label","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
E.g.
OpenAI.Chat.ChatCompletionAssistantMessageParam - is saying that role 'system' is not a valid role and that only user or assistant is
The enum for role is gone
To Reproduce
NA
Code snippets
nA
OS
macOs
Node version
Node v20
Library version
openai v.4.2.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/531","Token usage information is not available in the assistants API","2023-11-23T17:13:33Z","Closed issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
In the completions API, the response contains information about promptTokens, completionTokens and totalTokens. While prototyping use of the Assistant API, I noticed the response (or the run result) does not provide this information. This is critical for us to devise usage limits for our users. Is this something which was purposely left out or we can expect this information to be available in one of the upcoming releases?
To Reproduce
In the snippet below, I was expecting token information in the response.data field. However, no such information is available
Code snippets
const res = await this.openai.beta.threads.runs.retrieve(threadId, runId).withResponse();const run = res.dataif (run.status === 'completed') {
 const response = await this.openai.beta.threads.runs.retrieve(threadId, runId).withResponse();
 const result = response.data
 const messagesFromThread: OpenAI.Beta.Threads.Messages.ThreadMessagesPage =
            (await this.openai.beta.threads.messages.list(threadId).withResponse()).data;
 resolve({ runResult: result, messages: messagesFromThread });
OS
macOS
Node version
node v18.18.2
Library version
openai v4.20.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/527","The punycode module is deprecated in Node.js 21 (type = module)","2024-03-31T19:24:05Z","Closed as not planned issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
When importing the openai package in Node.js 21 it throws an error:
(node:38591) [DEP0040] DeprecationWarning: The `punycode` module is deprecated. Please use a userland alternative instead.
    at node:punycode:3:9
    at BuiltinModule.compileForInternalLoader (node:internal/bootstrap/realm:392:7)
    at BuiltinModule.compileForPublicLoader (node:internal/bootstrap/realm:328:10)
    at loadBuiltinModule (node:internal/modules/helpers:101:7)
    at Module._load (node:internal/modules/cjs/loader:1001:17)
    at Module.require (node:internal/modules/cjs/loader:1235:19)
    at require (node:internal/modules/helpers:176:18)
    at Object.<anonymous> (/Users/adriaan/app/node_modules/whatwg-url/lib/url-state-machine.js:2:18)
    at Module._compile (node:internal/modules/cjs/loader:1376:14)
    at Module._extensions..js (node:internal/modules/cjs/loader:1435:10)

To Reproduce
package.json:
{
  ""main"": ""index.js"",
  ""type"": ""module"",
  ""engines"": {
    ""node"": ""21.2.0""
  },
  ""scripts"": {
    ""dev"": ""node --trace-deprecation -r dotenv/config index.js dotenv_config_path=.env.dev""
  },
  ""dependencies"": {
    ""openai"": ""^4.19.1""
  },
  ""devDependencies"": {
    ""dotenv"": ""^16.3.1""
  }
}
index.js:
import OpenAI from ""openai"";

console.log(""hi"");
.env.dev:
OPENAI_API_KEY=
ASSISTANT_ID=
PORT=3011
BASE_URL=http://localhost:3011

Code snippets
No response
OS
macOS
Node version
v21.2.0
Library version
openai 4.19.1
 The text was updated successfully, but these errors were encountered: 
👍1
timsuchanek reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/openai-node/issues/526","on('error', () => ...) event does not fire on APIUserAbortError","2023-11-22T06:26:05Z","Open issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
From the error event documentation:
The event fired when an error is encountered outside of a parse function or an abort.
However, I found that it is not fired on abort. Instead, the way to gracefully handle an abort seems to be with the finalContent method (there may be other ways, I didn't test). I found this out from looking at the tests.
To Reproduce
This test already has the minimal setup to reproduce. Simply add an assertion to expect emittedError to be present after aborting.
Code snippets
No response
OS
macOS
Node version
20.9.0
Library version
4.18.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/521","Error thrown if type?: function is not specified specified in tool_choice","2024-07-08T18:59:45Z","Closed issue","openai api","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
const response = await openai.chat.completions.create({
  model: 'gpt-4-1106-preview',
  messages: [{
    role: 'user',
    content: prompt,
  }],
  tool_choice: {
    type: 'function', // this prop is optional but if you remove it an error will be thrown (documented below)
    function: {
      name,
    }
  },
  tools: [
    {
      type: 'function',
      function: {
        name,
        description,
        parameters
      }
    }
  ],})
error: {
  message: ""'$.tool_choice' is invalid. Please check the API reference: https://platform.openai.com/docs/api-reference."",
  type: 'invalid_request_error',
  param: null,
  code: null
},
code: null,
param: null,
type: 'invalid_request_error'

To Reproduce
See above.
Also, it looks like parallel function calling is disabled if tool_choice is specified, is this intended?
Code snippets
No response
OS
macOS
Node version
v18.12.1
Library version
^4.19.0
 The text was updated successfully, but these errors were encountered: 
👍1
e-simpson reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/openai-node/issues/518","Openai MODULE_NOT_FOUND error during deployment","2023-11-21T09:23:27Z","Closed issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
I recently included the openai node package within my node.js application.
Before I included this package into my app the deployment was working perfectly fine.
Once I implemented all the needed endpoints and finished testing locally I tried to deploy it to my google cloud server instance.
But here the problem started. When I deployed the new server version, I got following error:
yarn cache v1.22.11 warning package.json: No license field success Cleared cache. Done in 4.16s. yarn install v1.22.11 warning package.json: No license field [1/4] Resolving packages... [2/4] Fetching packages... info fsevents@2.3.2: The platform ""linux"" is incompatible with this module. info ""fsevents@2.3.2"" is an optional dependency and failed compatibility check. Excluding it from installation. [3/4] Linking dependencies... [4/4] Building fresh packages... Done in 48.54s. [PM2] Applying action deleteProcessId on app [all](ids: [ 0, 1 ]) [PM2] [index](0) ✓ [PM2] [index](1) ✓ [PM2] [v] All Applications Stopped [PM2] [v] PM2 Daemon Stopped yarn run v1.22.11 warning package.json: No license field $ cross-env NODE_ENV=dev NODE_OPTIONS='--max-old-space-size=4096' ts-node src/index.ts at Object.require.extensions.<computed> [as .js] (/XXX/node_modules/ts-node/src/index.ts:1587:43) at Module.load (internal/modules/cjs/loader.js:937:32) { code: 'MODULE_NOT_FOUND', requireStack: [ '/home/user/my_app/node_modules/openai/_shims/node-runtime.js', '/home/user/my_app/node_modules/openai/_shims/auto/runtime-node.js', '/home/user/my_app/node_modules/openai/_shims/index.js', '/home/user/my_app/node_modules/openai/streaming.js', '/home/user/my_app/node_modules/openai/core.js', '/home/user/my_app/node_modules/openai/index.js', '/home/user/my_app/src/functions/chat/trainerMessages/generateTrainerMessage.ts', '/home/user/my_app/src/functions/chat/trainerMessages/sendTrainerToUserChatMessage.ts', '/home/user/my_app/src/functions/chat/trainerMessages/sendTrainerChatMessage.ts', '/home/user/my_app/src/graphql/resolvers/mutation.ts', '/home/user/my_app/src/graphql/rootResolver.ts', '/home/user/my_app/src/index.ts' ] }
Apparently the _shims/node-runtime.js is not found within my project.
 Does someone, what kind of reasons there could be that during deployment this code part can't be found?
To Reproduce
Include the openai package to a node.js api
deploy it on google cloud compute engine
receive the error
Code snippets
No response
OS
14.0 (23A344)
Node version
Node.js v20.8.0
Library version
openai v4.19.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/515","Why is functions being deprecated in favor of tools?","2023-11-18T23:48:26Z","Closed issue","No label","Confirm this is a feature request for the Node library and not the underlying OpenAI API.
 This is a feature request for the Node library
Describe the feature or improvement you're requesting
  /**
   * Deprecated in favor of `tool_choice`.
   *
   * Controls which (if any) function is called by the model. `none` means the model
   * will not call a function and instead generates a message. `auto` means the model
   * can pick between generating a message or calling a function. Specifying a
   * particular function via `{""name"": ""my_function""}` forces the model to call that
   * function.
   *
   * `none` is the default when no functions are present. `auto`` is the default if
   * functions are present.
   */
  function_call?: 'none' | 'auto' | ChatCompletionFunctionCallOption;

  /**
   * Deprecated in favor of `tools`.
   *
   * A list of functions the model may generate JSON inputs for.
   */
  functions?: Array<ChatCompletionCreateParams.Function>;

Is there a reason for this change? This looks like a soft deprecation to me because it's still working, when should we expect things to break?
Also, this is pretty confusing because we're deprecating functions in favor of tools but the beta features still introduce runFunctions and runTools, should it be runTools only?
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/514","Always get quotation marks and linebreaks around function parameters","2023-11-18T23:41:36Z","Closed as not planned issue","No label","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
I always get quotation marks put into the function call parameters, which breaks the request.
To Reproduce
See code example below
Code snippets
`{  role: 'assistant',  content: null,  function_call: {    name: 'fetchAvailableURLs',    arguments: '{\n' +      '  ""domains"": [""companeasy.com"", ""namelyhelp.com"", ""nameassist.com"", ""companyaid.com"", ""businessnaming.com"", ""domainfinder.com"", ""domaintopick.com"", ""pick businessname.com""]\n' +      '}'  }`

This is how i do it:

`try {    const messages = [      {        role: ""system"",        content:          `
          Remove all linebreaks in your response and function calls, also never include any quotation marks like ' or "" in your response or function calls.         Get a list of example urls and check if they are available for purchase.          `      },      { role: ""user"", content: question },    ];    const runner = client.beta.chat.completions      .runFunctions({        model: ""gpt-4"",        messages: messages,        functions: [          {            function: fetchAvailableEndings,            parameters: { type: ""object"", properties: {} },          },          {            function: fetchAvailableURLs,            parameters: {              type: ""object"",              properties: {                domains: {                  type: ""array"",                  description:                    ""Takes in an object with the key 'domains' containing an array of domains and will return which one of these domains that are available for purchase."",                  items: {                    type: ""string"",                  },                },              },              required: [""domains""],            },          },        ],        // function_call: ""auto""        function_call: {'name': ""fetchAvailableURLs""},

      })
      .on(""message"", (message) => console.log(message));

    console.log(""runner"", runner);
    const finalContent = await runner.finalContent();
    

    return finalContent;
  } catch (error) {
    console.error(""Error in handleUserQuery:"", error);
    throw new Error(""Failed to process the user query"");
  }`
OS
MAcOS
Node version
20.5.1
Library version
4.17.5
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/513","Incorrect Type Definition for Assistants text responses","2023-11-21T00:41:10Z","Closed issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
The tree path for the text response from an assistant is content.text.value when return type is text.
However, type definitions skip the text part of the tree causing type errors.
From the API, this is an example response:
content: [ { type: 'text', text: { value: 'this is a response' } } ],
However, when accessing it, type says Property 'text' does not exist on type 'MessageContentImageFile | MessageContentText'
To Reproduce
Fetch a response from the assistant API and then attempt to access the text value.
Code snippets
For example:


const messages = await openai.beta.threads.messages.list(
          threadId,
        )const content = messages.data
          .filter(message => message.role === 'assistant').pop()

const response = content?.content[0].text.value
In VS Code, text will be underlined in red and throw the type error described above.


### OS

macOS

### Node version

v18.13.0

### Library version

4.19.0

 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/512","Can't use GPT-4-1106-Preview","2023-11-17T16:56:17Z","Closed as not planned issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
When using the Node package, trying to create chat completions with gpt-4-1106-preview model returns this: Invalid model: gpt-4-1106-preview.
Using it through the API directly works fine.
To Reproduce
Create a chat completion with gpt-4-1106-preview
See error
Try with API directly, not using the package
It works
Code snippets
No response
OS
macOS
Node version
Node v18.17.1
Library version
openai v4.19.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/511","TS7006: Parameter 'ctrl' implicitly has an 'any' type. in ./node_modules/openai/src/streaming.ts:187:18","2023-11-18T23:00:13Z","Closed issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
The build not works with typescript, becase the param of pull has no type.

openai-node/src/streaming.ts
 Line 187 in 049ce6f
	asyncpull(ctrl){
The build now works by doing the following.
-       async pull(ctrl) {+       async pull(ctrl: ReadableStreamDefaultController) {
To Reproduce
npm i openai
write openai.chat.completion.create()
npm run build
throw error (TS7006)
Code snippets
No response
OS
macOS
Node version
Node v18.18.0
Library version
openai 4.17.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/510","OpenAI not working on AWS Lambda","2024-02-02T04:48:42Z","Closed issue","No label","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
I am trying to use OpenAI on AWS Lambda but currently the code just hangs. I have also tried adding OpenAI as a lambda layer but that is not working as well
To Reproduce
I have the following package.json
{
  ""name"": ""test"",
  ""version"": ""1.0.0"",
  ""private"": true,
  ""scripts"": {
    ""build"": ""yarn && ../../node_modules/.bin/tsc --build tsconfig.json"",
    ""deploy:dev"": ""yarn build && ../../node_modules/.bin/serverless deploy --stage dev"",
    ""preinstall"": ""../../bin/check-yarn""
  },
  ""engines"": {
    ""node"": "">=18.0""
  },
  ""devDependencies"": {
    ""@serverless/typescript"": ""2.70.0"",
    ""esbuild"": ""0.19.5"",
    ""openai"": ""4.19.0"",
    ""serverless-bundle"": ""6.0.0"",
    ""serverless-domain-manager"": ""5.1.5"",
    ""serverless-esbuild"": ""1.48.5"",
    ""serverless-stage-manager"": ""1.0.5"",
    ""ts-node"": ""10.9.1""
  }
}

The following is the openai as a Layer package
layer-package.zip
Code snippets
export async function test() {
  console.log( ""test"" )
  const openai = new OpenAIApi( { apiKey: process.env.OPENAI_KEY } )

  console.log( ""openai initialized"", openai )
  const completion_response = await openai.completions.create( { model: ""gpt-3.5-turbo-instruct"", prompt: ""Can you briefly tell me about the first president of America?"" } )

  console.log( ""completion_response"", completion_response )
  if( ! completion_response.choices?.length ) {
    return null
  }

  return completion_response.choices[ 0 ].text || null}
OS
AWS Lambda
Node version
18
Library version
openai v4.19.0
 The text was updated successfully, but these errors were encountered: 
👍1
MatildaApp reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/openai-node/issues/508","Why did V4 API drop support for ""name"" parameter in chat messages?","2023-12-15T03:19:59Z","Closed issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
In the node V4 API I try this:
let message: OpenAI.Chat.ChatCompletionMessageParam = {role: 'system', content: ""my content"", name: ""Test""};

which causes this error because name is not allowed for a system role.
Type '{ role: ""system""; content: string; name: string; }' is not assignable to type 'ChatCompletionMessageParam'.
  Object literal may only specify known properties, and 'name' does not exist in type 'ChatCompletionSystemMessageParam'.ts(2322)

But the official API description at https://platform.openai.com/docs/api-reference/chat/create
 states that the name parameter is still supported as ""optional"" value for roles ""system"", ""user"" and ""assistant"".
This worked in the node V3 API. Why was this support removed in the V4 API? Was this done on purpose or by mistake?
To Reproduce
See steps above
Code snippets
No response
OS
macOS
Node version
Node v18
Library version
openai v4.19.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/507","Examples for streaming tools calls need fixing","2023-11-18T22:27:37Z","Closed issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
I have used the messageReducer from the examples. After streaming the chunks finished, and I had the assembled message, I saw that the arguments property was always empty. I am testing the new GPT-4 model which supports multiple function calls. After hours of debugging, I finally realized that the problem was the reducer function, which didn't support arrays. This is how my reducer function currently looks, but I am not sure if that is correct. It would never add additional items from the delta:
function messageReducer(previous: ChatCompletionMessage, item: ChatCompletionChunk): ChatCompletionMessage {
  const reduce = (acc: any, delta: any) => {
    acc = { ...acc };
    for (const [key, value] of Object.entries(delta)) {
      if (acc[key] === undefined || acc[key] === null) {
        acc[key] = value;
      } else if (typeof acc[key] === 'string' && typeof value === 'string') {
        (acc[key] as string) += value;
      } else if (typeof acc[key] === 'number' && typeof value === 'number') {
        (acc[key] as number) = value;
      } else if (Array.isArray(acc[key]) && Array.isArray(value)) {
        const accArray = acc[key] as any[];
        if (accArray.length !== value.length) {
          throw new Error(`Array length mismatch for key ${key}: ${accArray.length} !== ${value.length}`);
        }
        for (let i = 0; i < value.length; i++) {
          accArray[i] = reduce(accArray[i], value[i]);
        }
      } else if (typeof acc[key] === 'object' && typeof value === 'object') {
        acc[key] = reduce(acc[key], value);
      }
    }
    return acc;
  };
  
  return reduce(previous, item.choices[0]!.delta) as ChatCompletionMessage;}
To Reproduce
Use the new gpt-4-1106-preview model which returns a tool_calls array in combination with streaming, while accumulating the message using the reducer function from the examples.
Code snippets
No response
OS
macOS
Node version
Node v20.7.0
Library version
openai 4.17.4
 The text was updated successfully, but these errors were encountered: 
👍1
nwatab reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/openai-node/issues/506","Chat completion stream returns empty total usage","2024-07-22T15:49:38Z","Closed issue","openai api","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
Using openai.beta.chat.completions.stream() and then calling totalUsage function returns the object with all values set to zero.
To Reproduce
Call the chat completions API with response stream
Log the result of totalUsage function
Check that every value is zero
Code snippets
const response = openai.beta.chat.completions.stream({
    messages: [
        {
            role: ""user"",
            content: ""hello""
        }
    ],
    model: ""gpt-3.5-turbo-1106"",
    response_format: { type: ""text"" },
    max_tokens: 300,
    n: 1,
    temperature: 1.2,
    stream: true});

console.log(await response.totalUsage())
OS
Ubuntu
Node version
Node v16.13.0
Library version
open v4.19.0
 The text was updated successfully, but these errors were encountered: 
👍1
jackmpcollins reacted with thumbs up emoji👀1
24601 reacted with eyes emoji
All reactions
👍1 reaction
👀1 reaction"
"https://github.com/openai/openai-node/issues/505","Deno distribution's version is not up to date","2023-11-27T16:45:24Z","Closed issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
import OpenAI from ""https://deno.land/x/openai@v4.16.1/mod.ts""; is the latest Deno distribution, not
import OpenAI from 'https://deno.land/x/openai@v4.19.0/mod.ts'; as described in the readme.
see https://deno.land/x/openai@v4.16.1
To Reproduce
Go to https://deno.land/x/openai@v4.16.1
Code snippets
No response
OS
does not matter
Node version
Deno
Library version
4.19.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/504","Extending Issue #499 to Deno Library: GPT4 preview finish_details not supported","2023-11-16T02:18:36Z","Closed as not planned issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
When using any of the gpt4 preview models, they return choices with finish_details instead of finish_reason. This was just addressed in issue #499 for the node library, but the change needs to be extended to the Deno build.
To Reproduce
Use deno
Query a gpt4 preview model
Be upset when typescript yells at you for selecting an element that it doesn't think exists
Code snippets
No response
OS
macOS
Node version
N/A
Library version
openai v4.16.1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/503","Potentially broken endpoints for chat completion","2023-11-15T14:24:26Z","Closed as not planned issue","No label","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
I am getting 404 Invalid URL POST /v1/chat/completions/ when performing chat completions.
To Reproduce
I am have the reproducable code in ts:
export async function getCompletion(
  prompt: string,
  model: string = 'gpt-3.5-turbo',
  temperature: number = 0,): Promise<string> {
  const response = await openai.chat.completions.create({
    messages: [{ role: 'user', content: prompt }],
    model,
    temperature,
  })

  return response.choices[0].message.content!}
Code snippets
No response
OS
React Native Expo
Node version
v16.15.0.
Library version
openai ^4.18.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/500","Broken links in code comments","2023-11-16T02:53:46Z","Closed issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
Broken link: https://platform.openai.com/docs/guides/gpt/parameter-details
Locations:
openai-node/src/resources/chat/completions.ts
 Line 582 in 09127e8
	 * [See more information about frequency and presence penalties.](https://platform.openai.com/docs/guides/gpt/parameter-details)
https://github.com/openai/openai-node/blob/09127e8425bb07ea0f9e6a80470edef6a87dcea7/src/resources/chat/completions.ts#L639C69-L639C130
To Reproduce
You can go to the given locations on the source code.
Code snippets
No response
OS
macOS
Node version
Node V20
Library version
openai v4.18.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/499","ChatCompletionStream.fromReadableStream errors due to missing finish_reason for choice","2023-12-15T19:43:12Z","Closed issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
When trying to use the API described here https://github.com/openai/openai-node/blob/2242688f14d5ab7dbf312d92a99fa4a7394907dc/examples/stream-to-client-browser.ts
I'm getting the following an error at the following point:
where the actual choices look like this:

Looks like the code expects finish_reason to be populated but the finish details are now in a property called finish_details?
To Reproduce
Setup a server that responds with chat completion streams
Then in the client try to use the ChatCompletionStream.fromReadableStream API, e.g.:
const runner = ChatCompletionStream.fromReadableStream(res.body);await runner.finalChatCompletion();
Code snippets
No response
OS
Windows
Node version
18.12.1
Library version
4.16.1
 The text was updated successfully, but these errors were encountered: 
👍2
petrgazarov and btruhand reacted with thumbs up emoji
All reactions
👍2 reactions"
"https://github.com/openai/openai-node/issues/498","FileObject status deprecated, is there a replacement?","2023-11-14T21:01:47Z","Closed issue","question","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
In #433 the status field here was marked deprecated, but I don't see a mention of another alternative to understanding file status. (To be clear, I'm asking about the file status here in the node library, not the underlying issue with the assistant noted in the community thread)
https://github.com/openai/openai-node/pull/433/files#diff-bb30fd0eac3872e21bc42abbf74bd9501293b1102ecf80ceb3066ea8f2b3d380R146
/**   * The current status of the file, which can be either `uploaded`, `processed`,   * `pending`, `error`, `deleting` or `deleted`.   * Deprecated. The current status of the File, which can be either `uploaded`,   * `processed`, or `error`.   */
I'm getting this error when using the assistant, so I wanted to make sure the files i had uploaded were processed and accessible. With this field deprecated, is there another alternative or is this broken?
https://community.openai.com/t/myfiles-browser-tool-is-not-operational-for-these-files/481922/13
To Reproduce
Upload a file
Wait for file to be finished processing
Create an assistant, thread, message with file attached and run
Wait for assistant to finish
Unable to access file
Code snippets
No response
OS
macOS
Node version
Node v18.17.1
Library version
openai 4.17.4
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/492","APIConnectionError: Connection error.","2023-11-14T03:25:56Z","Closed as not planned issue","No label","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
APIConnectionError: Connection error.
 at OpenAI.makeRequest (E:\desk\demo\text-to-speech-demo\node_modules\openai\core.js:279:19)
 at process.processTicksAndRejections (node:internal/process/task_queues:95:5)
 at async E:\desk\demo\text-to-speech-demo\server.js:31:17 {
 status: undefined,
 headers: undefined,
 error: undefined,
 code: undefined,
 param: undefined,
 type: undefined,
 cause: FetchError: request to https://api.openai.com/v1/audio/speech failed, reason: connect ETIMEDOUT 128.121.146.109:443
 at ClientRequest. (E:\desk\demo\text-to-speech-demo\node_modules\node-fetch\lib\index.js:1501:11)
 at ClientRequest.emit (node:events:514:28)
 at ClientRequest.emit (node:domain:489:12)
 at TLSSocket.socketErrorListener (node:_http_client:501:9)
 at TLSSocket.emit (node:events:514:28)
 at TLSSocket.emit (node:domain:489:12)
 at emitErrorNT (node:internal/streams/destroy:151:8)
 at emitErrorCloseNT (node:internal/streams/destroy:116:3)
 at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
 type: 'system',
 errno: 'ETIMEDOUT',
 code: 'ETIMEDOUT'
 }
 }
To Reproduce
写了一个 text-to-speech的接口，但是会报错，我的代码片段如下
Code snippets
这个是我的代码片段，我尝试使用express框架进行写一个 text-to-speech的接口，但是由于一些原因，我需要用魔法，所以才会Connection error，但是我不知道如何去解决这个问题。const express = require('express');const path = require('path');const fs = require('fs');const OpenAI = require('openai');

const app = express();const port = 3000;const cors = require('cors');

app.use(cors());app.use(express.json());

const apiKey = process.env.OPENAI_API_KEY;const openai = new OpenAI({
  apiKey: apiKey,});

app.post('/speech', async (req, res) => {
  try {
    const { text } = req.body;
    const mp3 = await openai.audio.speech.create({
      model: ""tts-1"",
      voice: ""alloy"",
      input: text,
    });
    const buffer = Buffer.from(await mp3.arrayBuffer());
    const speechFile = path.resolve(""./speech.mp3"");
    await fs.promises.writeFile(speechFile, buffer);
    res.status(200).sendFile(speechFile);
  } catch (error) {
    console.error(error);
    res.status(500).send('Internal Server Error');
  }});

app.listen(port, () => {
  console.log(`Server running on port ${port}`);});
OS
windows11
Node version
v18.17.1
Library version
openai v4.17.4
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/491","Take Description of functions and their properties into consideration when evaluating runFunctions","2023-11-14T03:28:00Z","Closed issue","No label","Confirm this is a feature request for the Node library and not the underlying OpenAI API.
 This is a feature request for the Node library
Describe the feature or improvement you're requesting
Consider this example taken from auto-playwright:
  const runner = openai.beta.chat.completions
    .runFunctions({
      model: task.options?.model ?? ""gpt-4-1106-preview"",
      messages: [{ role: ""user"", content: prompt(task) }],
      functions: [
        {
          function: async (args: {
            attributeName: string;
            cssSelector: string;
          }) => {
            const locator = await page.locator(args.cssSelector);

            const elementId = randomUUID();

            locatorMap.set(elementId, locator);

            return {
              elementId,
            };
          },
          name: ""locateElement"",
          description:
            ""Locates element and returns elementId. This element ID can be used with other functions to perform actions on the element."",
          parse: (args: string) => {
            return z
              .object({
                cssSelector: z.string(),
              })
              .parse(JSON.parse(args));
          },
          parameters: {
            type: ""object"",
            properties: {
              cssSelector: {
                description:
                  ""CSS selector that uniquely identifies the element."",
                type: ""string"",
              },
            },
          },
        },
If I added ""selectors must be prefix with iframe >"" to the description, then I don't see those instructions followed, contrary to what I would expect.
Additional context
You can inspect source code https://github.com/lucgagan/auto-playwright
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/490","Trailing Slash in buildUrl Method Affecting Expo Project for React Native","2023-11-14T03:29:12Z","Closed issue","No label","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
Description:
 I have identified a potential bug in the core.tsbuildUrl method which seems to specifically affect users working with the Expo project for React Native. This bug results in the generation of a URL with a trailing slash, leading to a 404 error when making requests.
To Reproduce
Steps To reproduce:
Use the current implementation of the buildUrl method in an Expo project for React Native.
Observe the generation of the URL with a trailing slash.
Make a request to the API using this URL.
Encounter a 404 error due to the trailing slash.
I was attempting to use the code below from the readme:
import OpenAI from 'openai';

const openai = new OpenAI({
  apiKey: 'My API Key', // defaults to process.env[""OPENAI_API_KEY""]});

async function main() {
  const chatCompletion = await openai.chat.completions.create({
    messages: [{ role: 'user', content: 'Say this is a test' }],
    model: 'gpt-3.5-turbo',
  });}

main();
The above code outputs a request with a URL with a trailing slash https://api.openai.com/v1/chat/completions/
Suggested Fix:
 Based on the URL constructor documentation, I recommend updating the code in core.ts as follows:
// Simplified version for demo purposesconst url = baseUrl(path, this.baseURL); // 'https://api.openai.com/v1/chat/completions'
Additional Context:
 This bug and its fix have been observed in my Expo project for React Native. However, it's important to note that I was unable to reproduce this bug in Google Chrome Dev Tools.
Temporary workaround:
const openai = new OpenAI({
  apiKey: 'My API Key', // defaults to process.env[""OPENAI_API_KEY""],
  fetch: async (url, init) => {
    try {
      const modifiedUrl = url.replace(/\/$/, """");
      const response = await fetch(modifiedUrl, init);

      return response;
    } catch (error) {
      console.log('openai fetch error', error);
    }
  }});
Code snippets
No response
OS
mac os
Node version
Node v16.20.0
Library version
openai v4.17.4
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/488","functions calling returning an undocumented multi_tool_use.parallel case","2023-11-14T03:30:58Z","Closed issue","bug,openai api","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
Hello,
 I'm encountering an undocumented use of the function calling feature, using the new assistant/threads features via the SDK.
 In some occasions (happened already 3x today), the function calling is returning a structure as in the screenshot below.
Please note that this behavior is random. Not happening 100% of the time, so maybe related to an underlying issue, but no-one from openai has replied to the initial thread.
Any knowledge of this ?
 Thanks
Original thread in the community:
https://community.openai.com/t/model-tries-to-call-unknown-function-multi-tool-use-parallel/490653/6
To Reproduce
Seems like related to complex calls, the model looking like it wants to parallelize function call executions.
Code snippets
No response
OS
macOS
Node version
Node v21.1.0
Library version
openai v4.12.4
 The text was updated successfully, but these errors were encountered: 
👍2
stri8ed and transitive-bullshit reacted with thumbs up emoji
All reactions
👍2 reactions"
"https://github.com/openai/openai-node/issues/487","How to stream Text to Speech?","2023-12-23T23:33:09Z","Closed issue","No label","According to the documentation here for Text to Speech:
https://platform.openai.com/docs/guides/text-to-speech?lang=node
There is the possibility of streaming audio without waiting for the full file to buffer. But the example is a Python one. Is there any possibility of streaming the incoming audio using Node JS?
 The text was updated successfully, but these errors were encountered: 
👍21
patrick-ve, thomasrosen, MGBarri, jcarnegie, abhishekgoyal1, juhana, diegoasua, nramirez, chrispangg, lemonjp, and 11 more reacted with thumbs up emoji👀10
thomasrosen, MGBarri, patrick-ve, florianchevallier, ismaelvega, abhishekgoyal1, zineanteoh, beyzayukseel, devlprkhan, and FilipKlaesson reacted with eyes emoji
All reactions
👍21 reactions
👀10 reactions"
"https://github.com/openai/openai-node/issues/486","Unable to return normally when tool_choice is specified","2023-11-11T16:21:19Z","Closed issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
We assume that tools has multiple functions in the following parameters.
const runner = openaiClient.beta.chat.completions.runTools(
  model: 'gpt-4-1106-preview',
  stream: true,
  messages: messages[],
  tools: toots[]
  tool_choice: {
    type: 'function',
    function: {
          name: 'func_name'
    }
  }
)

When tool_choice is specified in the above code, it seems that it cannot be executed normally. When set to auto, the process can make normal function calls and return normally.
Code snippets refer to examples.
To Reproduce
none
Code snippets
No response
OS
windows
Node version
Node v18.15.1
Library version
openai v4.17.3
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/485","Wrong return type for openai.audio.speech.create API","2023-11-16T02:21:39Z","Closed as not planned issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
I'm following the tutorial for TTS on the OpenAI website: https://platform.openai.com/docs/guides/text-to-speech?lang=node
When I try to use that code in a typescript file, typescript always infers a return value of ""never"". Ignoring TS for that line makes it work.
To Reproduce
Save the code from https://platform.openai.com/docs/guides/text-to-speech?lang=node in a .ts file.
Code snippets
No response
OS
macOS
Node version
Node v19
Library version
v4.17.17
 The text was updated successfully, but these errors were encountered: 
👍1
tan-dd reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/openai-node/issues/484","Supabase Deno Functions import issues","2023-11-11T01:00:36Z","Closed issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
Trying to import the SDK in version 4.17.2 fails in a Supabase Deno function with the following error:
Download https://esm.sh/openai@4.17.2
Download https://esm.sh/v133/openai@4.17.2/esnext/resources.js
Failed to load module: ""https://esm.sh/v133/openai@4.17.2/esnext/resources.js"" - Import 'https://esm.sh/v133/openai@4.17.2/esnext/resources.js' failed, not found.

It worked in 4.4.0, and is possibly an esm.sh bundling issue, since it seems to work in a Deno kernel Jupyter notebook with npm: style imports.
To Reproduce
Create a Supabase Deno function
Use the following code:
import { OpenAI } from ""https://esm.sh/openai@4.17.2"";

Deno.serve(async (req) => {
  const { name } = await req.json()
  const data = {
    message: `Hello ${name}!`,
  }
  const client = new OpenAI()

  return new Response(
    JSON.stringify({}),
    { headers: { ""Content-Type"": ""application/json"" } },
  )
})

Code snippets
No response
OS
Supabase Deno
Node version
Deno
Library version
4.17.2
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/477","No way to import types with Deno","2023-11-09T20:27:07Z","Closed issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
Based on the docs, this library exposes the Request/Response types via the ""default"" namespace.
However, if I try to use the imports in my code, I get an error: Identifier expected.
I'm not 100% but it seems like maybe this is a reserved keyword that Deno doesn't support having as a namespace?
To Reproduce
create deno project
install v4.16.1
add this code:
import { type default } from ""https://deno.land/x/openai@v4.16.1/mod.ts"";
const { ChatCompletion } = default;

Code Sandbox Link (you might have to start typing to see error)
Code snippets
No response
OS
osx
Node version
deno 1.38.0
Library version
v4.16.1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/476","""Publish to Deno"" failing","2023-11-09T22:45:44Z","Closed issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
The github action to publish to deno is currently failing in the pipeline.
Based on the logs, it seems like it's failing to connect to the deno-build repo.
To Reproduce
na
Code snippets
No response
OS
na
Node version
na
Library version
na
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/472","This expression is not constructable, Has no construct signatures","2023-11-09T03:53:32Z","Closed issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
Just upgraded the library and I'm getting this error
This expression is not constructable.
Type 'typeof import(""C:/**/**/**/**/**/**/node_modules/openai/dist/index"")' has no construct signatures.

Using node 18-lts with typescript 4.7.4
 Typescript config:
{
    ""compilerOptions"": {
        ""module"": ""commonjs"",
        ""declaration"": true,
        ""removeComments"": true,
        ""emitDecoratorMetadata"": true,
        ""experimentalDecorators"": true,
        ""allowSyntheticDefaultImports"": true,
        ""target"": ""es2018"",
        ""sourceMap"": true,
        ""outDir"": ""./dist"",
        ""baseUrl"": ""./"",
        ""incremental"": false,
        ""skipLibCheck"": true,
        ""strictNullChecks"": false,
        ""noImplicitAny"": false,
        ""strictBindCallApply"": false,
        ""forceConsistentCasingInFileNames"": false,
        ""noFallthroughCasesInSwitch"": false,
        ""esModuleInterop"": true,
        ""resolveJsonModule"": true,
        ""typeRoots"": [
            ""node_modules/@types"",
            ""src/typings""
        ]
    }
}

To Reproduce
Npm Install
Code snippets
No response
OS
Docker
Node version
Node v18
Library version
4.17.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/471","open@4.16.1 issues on deno runtime","2023-11-09T07:16:02Z","Closed issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
On 4.16.1, when trying to stream chunks, I consistently seem to only get 1 part back that looks like this:
{
  id: ""chatcmpl-8IPirfnuikMfsLCSj3uxhXPmqeU7k"",
  object: ""chat.completion.chunk"",
  created: 1699399473,
  model: ""gpt-4-0613"",
  choices: [
    {
      index: 0,
      delta: { role: ""assistant"", content: """" },
      finish_reason: null
    }
  ]
but then nothing else.
Streaming was working okay when using ""https://deno.land/x/openai@1.4.3/mod.ts""
To Reproduce
Use 4.16.1 and try to stream back 1 chunk at a time.
This is the current (non-working) version that uses 4.16.1:
import OpenAI from ""https://deno.land/x/openai@v4.16.1/mod.ts"";import { corsHeaders } from ""../_shared/cors.ts"";

const apiKey = Deno.env.get(""OPENAI_API_KEY"");

const openAI = new OpenAI({
  apiKey,});

export type ReqJSON = {
  systemPrompt: string;
  userPrompt: string;
  messages: {
    role: ""assistant"" | ""user"";
    content: string;
  }[];};

Deno.serve(async (req) => {
  if (req.method === ""OPTIONS"") {
    return new Response(""ok"", { headers: corsHeaders });
  }

  try {
    const body = (await req.json()) as ReqJSON;
    const { userPrompt, messages } = body;
    const encoder = new TextEncoder();
    const stream = new TransformStream();
    const writer = stream.writable.getWriter();
    await writer.ready;
    const streamRes = await openAI.chat.completions.create({
      model: ""gpt-4"",
      messages: [
        {
          role: ""system"",
          content: `You are a helpful assistant`,
        },
        {
          role: ""user"",
          content: userPrompt,
        },
        ...messages,
      ],
      stream: true,
    });
    console.log(""streamRes"", streamRes);
    for await (const part of streamRes) {
      console.log(""part"", part);
      if (part.choices[0].finish_reason) {
        await writer.close();
        break;
      }
      const token = part.choices[0].delta.content || """";
      const msg = encoder.encode(`data: ${token}\n\n`);
      await writer.write(msg);
    }

    return new Response(stream.readable, {
      headers: { ...corsHeaders, ""Content-Type"": ""text/event-stream"" },
    });
  } catch (e) {
    console.error(""error occurred:"", e);
    return new Response(JSON.stringify({ error: e.message }), {
      status: 500,
      headers: { ...corsHeaders, ""Content-Type"": ""application/json"" },
    });
  }});
Here was the previous working version that used the unofficial 1.4.3 version:
import { OpenAI } from ""https://deno.land/x/openai@1.4.3/mod.ts"";import { corsHeaders } from ""../_shared/cors.ts"";

const apiKey = Deno.env.get(""OPENAI_API_KEY"");

const openAI = new OpenAI(apiKey || """");

export type ReqJSON = {
  userPrompt: string;
  messages: {
    role: ""assistant"" | ""user"";
    content: string;
  }[];};

Deno.serve(async (req) => {
  if (req.method === ""OPTIONS"") {
    return new Response(""ok"", { headers: corsHeaders });
  }

  try {
    const body = (await req.json()) as ReqJSON;
    const { userPrompt, messages } = body;
    const encoder = new TextEncoder();
    const stream = new TransformStream();
    const writer = stream.writable.getWriter();
    await writer.ready;
    openAI.createChatCompletionStream(
      {
        model: ""gpt-4"",
        messages: [
          {
            role: ""system"",
            content: `You are a helpful assistant`,
          },
          {
            role: ""user"",
            content: userPrompt,
          },
          ...messages,
        ],
      },
      async (res) => {
        if (res.choices[0].finish_reason) {
          await writer.close();
          return;
        }
        const token = res.choices[0].delta.content || """";
        const msg = encoder.encode(`data: ${token}\n\n`);
        await writer.ready;
        await writer.write(msg);
      }
    );

    return new Response(stream.readable, {
      headers: { ...corsHeaders, ""Content-Type"": ""text/event-stream"" },
    });
  } catch (e) {
    return new Response(JSON.stringify({ error: e.message }), {
      status: 500,
      headers: { ...corsHeaders, ""Content-Type"": ""application/json"" },
    });
  }});
Code snippets
No response
OS
macOS
Node version
deno
Library version
openai v4.16.1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/469","new response_format field not permitted with gpt-4-vision-preview","2023-11-10T02:37:21Z","Closed as not planned issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
Trying to use the new JSON mode on the latest version 4.16.2 with model gpt-4-vision-preview is returning a validation error
{
  error: {
    code: null,
    message:
      '1 validation error for Request\nbody -> response_format\n  extra fields not permitted (type=value_error.extra)',
    param: null,
    type: 'invalid_request_error',
  },}
To Reproduce
Call chat completion method with response format property
const response = await openAI.chat.completions
  .create({
    model: 'gpt-4-vision-preview',
    max_tokens: 4048,
    temperature: 0,
    seed: 0,
    response_format: { type: 'json_object' },
    messages: [
      {
        role: 'system',
        content:
          'Summarize this image. Make sure your response is in valid JSON format',
      },
      {
        role: 'user',
        content: [
          {
            type: 'image_url',
            image_url: {
              url: `data:image/png;base64,${imageBuffer.toString( 'base64')}`,
            },
          },
        ],
      },
    ],
  })
Code snippets
No response
OS
PopOS
Node version
18.14.2
Library version
4.16.2
 The text was updated successfully, but these errors were encountered: 
👍2
rogerpadilla and bavo96 reacted with thumbs up emoji👀2
rogerpadilla and bavo96 reacted with eyes emoji
All reactions
👍2 reactions
👀2 reactions"
"https://github.com/openai/openai-node/issues/465","How to use stream with function call","2023-11-08T20:16:05Z","Closed issue","No label","Confirm this is a feature request for the Node library and not the underlying OpenAI API.
 This is a feature request for the Node library
Describe the feature or improvement you're requesting
I'm using the openai.chat.completions.create call to send messages with functions like this:
const response = await openai.chat.completions.create({ model: ""gpt-4-1106-preview"", messages: messages, tools: tools, tool_choice: ""auto"", stream: true }); console.log(response)
If I set stream: false then the function call works but regular messages no longer stream back.
If I set stream: true then I get this error:
Stream { iterator: [AsyncGeneratorFunction: iterator], controller: AbortController { signal: AbortSignal { aborted: false } } }
How can I ensure that message streaming function still works but function calls also work as well?
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/464","files.retrieveContent only returns strings, and not bytes/binary data","2023-11-09T19:07:03Z","Closed issue","enhancement","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
See this issue for the python library. I don't see an equivalent method to with_raw_response in the node sdk.
To Reproduce
const content = await openai.files.retrieveContent(""file-id"");

fs.writeFile(""image.png"", content, (err) => {
  if (err) {
    console.log(err);
  }
});

This saves a corrupt file, though the file can be downloaded from the playground without a problem.
Code snippets
No response
OS
windows 11
Node version
Node.js v20.9.0
Library version
""openai"": ""^4.16.1""
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/462","ChatCompletionUserMessageParam.content should be string | null","2023-11-08T06:11:47Z","Closed as not planned issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
According to the OpenAI docs, ChatCompletionMessageParam.content should always be either a string or null.
But currently, this typing is broken due to:
export interface ChatCompletionUserMessageParam {
    /**     * The contents of the user message.     */
    content: string | Array<ChatCompletionContentPart> | null;
    /**     * The role of the messages author, in this case `user`.     */
    role: 'user';}
What is ChatCompletionContentPart? It doesn't seem to be referenced anywhere in the OpenAI docs, and it breaks the very common TS usage of message.content being a string if truthy.
If this change is intended, then it doesn't seem to be consistent w/ the OpenAI docs as far as I can tell (source), and it makes for a much more awkward DX.
To Reproduce
import type { OpenAI } from 'openai'

let msg: OpenAI.ChatCompletionMessageParam = /* ... */if (msg.content) {
  // msg.content should be type `string` here, but it's currently `string` | Array<ChatCompletionContentPart>}
Code snippets
No response
OS
macOS
Node version
any node
Library version
latest
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/461","JSON mode support missing","2023-11-07T23:00:31Z","Closed issue","No label","Confirm this is a feature request for the Node library and not the underlying OpenAI API.
 This is a feature request for the Node library
Describe the feature or improvement you're requesting
The OpenAI completion endpoint now supports JSON mode on the request. It'd be great to have support for this param.
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/460","Dall-e-2 image generations stopped working!","2023-11-07T20:00:18Z","Closed as not planned issue","No label","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
This is really quite bad… already the API is quite frankly being documented poorly… announcements not working…
now for some reason, dall-e-2 images generation stopped working (have not updated node package)… getting 500 errors now…
so I updated the modules to 4.16.1, checked the documentation for the new dall-e-3 model and functionality… and my typescript throws error
Property ‘createImage’ does not exist on type ‘OpenAI’.
How can you allow for older versions to fail when you introduce new one? Without announcing breaking changes… and urge to update before you introduce the changes?
 Why is new one not working???
To Reproduce
Code snippets
mage = await openai.createImage({
      model: 'dall-e-3',
      prompt: `Create a photo in style of ${photoRequest.style} and should be of ${photoRequest.description}`,
      n: actualVariants,
      size: '1024x1024',
    });
OS
windows
Node version
Node v18.18.0
Library version
openai 4.16.1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/459","Creating a CSV File and attaching to an Assistant returns ""Unsupported file ... type: application/octet-stream""","2023-11-08T06:03:42Z","Closed issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
Creating a file, whether via the Node API in this library, via the web UI, or via CURL / manually written POST, succeeds, however attaching the file to the Assistant fails with ""Unsupported file ... type: application/octet-stream"".
It appears that the Node library or API, and likely the web UI library implementation (also this library?), disregards the MIME type explicitly set. There is no way to force a large CSV to be ""application/csv"".
To Reproduce
Upload a CSV file via the Assistant playground or the code snippets below
Attach the file to the assistant (this will occur as part of step 1 in the web UI)
Observe error
Code snippets
const filedata = await fs.readFile(filename);

  const blob = new File([new Blob([filedata])], filename, {
    type: 'application/csv',
  });

  const file = await client.files.create({
    file: blob,
    purpose: 'assistants',
  });

  await client.beta.assistants.files.create(assistantId, {
    file_id: file.id,
  });


Or more directly, using a formdata and POST:

```ts

  const formData = new FormData();
  formData.append(
    'file',
    new File(
      [
        new Blob([filedata], {
          type: 'application/csv',
        }),
      ],
      filename,
    ),
  );
  formData.append('purpose', 'assistants');
  console.log({ keys: [...formData.keys()] });

  const encoder = new FormDataEncoder(formData);
  const readable = Readable.from(encoder);
  const body = new MultipartBody(readable);
  const headers = {
    ...encoder.headers,
    'Content-Length': encoder.contentLength,
  };

  const response: { id: string } = await client.post('/files', {
    body,
    headers,
  });


### OS

n/a

### Node version

Node v20.x

### Library version

openai v4.16.1

 The text was updated successfully, but these errors were encountered: 
👍2
transitive-bullshit and rob-gordon reacted with thumbs up emoji
All reactions
👍2 reactions"
"https://github.com/openai/openai-node/issues/454","MessageContentText icomplete type","2023-11-11T02:56:26Z","Closed issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
When you go to access the text property, ts does not recognize it as valid.
To Reproduce
  const {data} = await openai.beta.threads.messages.list(
    'thread_id',
  );

 const messages = data.map( message => message?.content.at(0)?.text ) 

Code snippets
No response
OS
macOS
Node version
20.9.0
Library version
4.16.1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/450","ImageGenerateParams type errors for DALL-E 3","2023-11-07T13:29:07Z","Closed issue","No label","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
Issues with the ImageGenerateParams in conjunction with DALL-E 3, in particular the following properties:
model:
 ERROR:
 Object literal may only specify known properties, and 'model' does not exist in type 'ImageGenerateParams'
model ""dall-e-3"" cannot be specified without errors.
size
 ERROR:
 Type '""1024×1792""' is not assignable to type '""1024x1024"" | ""256x256"" | ""512x512"" | null | undefined'
quality
 ERROR:
 Object literal may only specify known properties, and 'quality' does not exist in type 'ImageGenerateParams'
To Reproduce
Update to latest version of the openai nodeJS package
 version 4.16.1
then try something like:
const completion = await openai.images.generate({
 model: ""dall-e-3"",
 prompt,
 size: ""1024x1792"",
 response_format: ""b64_json"",
 user: uid,
 quality: ""hd""
 });
And you will get the type errors.
Code snippets
const completion = await openai.images.generate({
        model: ""dall-e-3"",
        prompt,
        size: ""1024x1792"",
        response_format: ""b64_json"",
        user: uid,
        quality: ""hd""
      });
OS
Chrome OS
Node version
v18.16.0
Library version
openai v.4.16.1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/448","openai.beta.threads.create has the wrong type for TS","2023-11-07T05:18:55Z","Open issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
The auto-generated type for threads.createis wrong as it's possible to have a .create() with no args based off the docs
To Reproduce
In a typescript repo, invoke openai.beta.threads.runs.create() with no args and observe a type error
Code snippets
No response
OS
N/A
Node version
Any
Library version
4.16.1
 The text was updated successfully, but these errors were encountered: 
👍1
kachar reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/openai-node/issues/447","OpenAI.Beta.AsssitantDeleted typo","2023-11-11T03:22:51Z","Closed issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
small typo AsssitantDeleted should be AssistantDeleted in generated types
To Reproduce
latest code on github
Code snippets
No response
OS
macos
Node version
any node
Library version
latest
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/445","Getting response headers for rate limit tracking","2023-11-07T07:46:20Z","Closed as not planned issue","question","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
Previously in v3, I am able to track my current usage and token limit since the chat completion response is an axios response, so I can use response.headers like so, but now chat completion response is OpenAI.Chat.Completions.ChatCompletion which representing response.data in v3 only
How I can get the same behaviour in v4?
Thank you in advance
// headers that i'm using
x-ratelimit-remaining-tokens
x-ratelimit-remaining-requests
x-ratelimit-reset-tokens
x-ratelimit-reset-requests

To Reproduce
Call create chat completion function
Response does not contains header
Code snippets
import OpenAI from 'openai'

const ai = new OpenAI()

const response = await ai.chat.completions.create({
  // the payload})

const headers = response.headers['x-ratelimit-remaining-tokens'] // errored
OS
macOS
Node version
Node v18.16.1
Library version
openai v4.16.1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/444","New models are missing","2024-07-08T18:49:55Z","Closed issue","No label","Confirm this is a feature request for the Node library and not the underlying OpenAI API.
 This is a feature request for the Node library
Describe the feature or improvement you're requesting
New models are missing
https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
👍8
transitive-bullshit, Maxim-Filimonov, adriancooney, jordanful, alp-ex, halcyon-pi, kachar, and gergobalogh reacted with thumbs up emoji
All reactions
👍8 reactions"
"https://github.com/openai/openai-node/issues/443","system_fingerprint type definition is missing in ChatCompletionChunk","2023-11-07T02:46:33Z","Open issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
This is an example from the api-reference to the streaming response:
{""id"":""chatcmpl-123"",""object"":""chat.completion.chunk"",""created"":1694268190,""model"":""gpt-3.5-turbo-0613"", ""system_fingerprint"": ""fp_44709d6fcb"", ""choices"":[{""index"":0,""delta"":{""role"":""assistant"",""content"":""""},""finish_reason"":null}]}

{""id"":""chatcmpl-123"",""object"":""chat.completion.chunk"",""created"":1694268190,""model"":""gpt-3.5-turbo-0613"", ""system_fingerprint"": ""fp_44709d6fcb"", ""choices"":[{""index"":0,""delta"":{""content"":""Hello""},""finish_reason"":null}]}

{""id"":""chatcmpl-123"",""object"":""chat.completion.chunk"",""created"":1694268190,""model"":""gpt-3.5-turbo-0613"", ""system_fingerprint"": ""fp_44709d6fcb"", ""choices"":[{""index"":0,""delta"":{""content"":""!""},""finish_reason"":null}]}

....

{""id"":""chatcmpl-123"",""object"":""chat.completion.chunk"",""created"":1694268190,""model"":""gpt-3.5-turbo-0613"", ""system_fingerprint"": ""fp_44709d6fcb"", ""choices"":[{""index"":0,""delta"":{""content"":"" today""},""finish_reason"":null}]}

{""id"":""chatcmpl-123"",""object"":""chat.completion.chunk"",""created"":1694268190,""model"":""gpt-3.5-turbo-0613"", ""system_fingerprint"": ""fp_44709d6fcb"", ""choices"":[{""index"":0,""delta"":{""content"":""?""},""finish_reason"":null}]}

{""id"":""chatcmpl-123"",""object"":""chat.completion.chunk"",""created"":1694268190,""model"":""gpt-3.5-turbo-0613"", ""system_fingerprint"": ""fp_44709d6fcb"", ""choices"":[{""index"":0,""delta"":{},""finish_reason"":""stop""}]}

Actually, we can get system_fingerprint field from the response but openai api-reference and type definition are missing this field.
To Reproduce
Writing code to ChatCompletionChunk type
Code snippets
No response
OS
macOS
Node version
v18.18.0
Library version
4.16.1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/440","Bug Report: Typo in TypeScript Declaration","2023-11-07T01:34:46Z","Closed issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
Description:
 There's a typo in the AssistantToolsRetrieval interface of the AssistantCreateParams namespace. The property type is misspelled as 'retreival' instead of the correct 'retrieval'.
Impact:
 The typo may cause TypeScript errors and misconfigurations in the codebase.
Fix:
 Correct the spelling to 'retrieval' in the interface definition.
To Reproduce
Navigate to the AssistantCreateParams namespace in the TypeScript declaration file.
Observe the incorrect spelling of the type property within the AssistantToolsRetrieval interface.
Code snippets
No response
OS
macOS
Node version
20.9.0
Library version
4.16.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/439","BadRequestError: 400 Invalid parameter: 'response_format' of type 'json_object' is not supported with this model.","2023-11-07T01:36:37Z","Closed issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
I am receiving the following error message when attempting to use the new JSON mode with gpt-3.5-turbo.
BadRequestError: 400 Invalid parameter: 'response_format' of type 'json_object' is not supported with this model.
Related doc here: https://platform.openai.com/docs/guides/text-generation/json-mode
To Reproduce
Update openai npm package to latest 4.16.1.
Send following request:
await openai.chat.completions.create({
    messages: [
      {
        role: 'system',
        content: `...`
      }
    ],
    model: 'gpt-3.5-turbo',
    temperature: 0.1,
    max_tokens: 1024,
    response_format: { type: 'json_object' }
  });

Code snippets
No response
OS
macOs
Node version
v21.1.0
Library version
openai v4.16.1
 The text was updated successfully, but these errors were encountered: 
👍1
archasek reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/openai-node/issues/438","gpt-4-vision-preview: Validation error for Request body -> functions extra fields not permitted (type=value_error.extra)","2023-11-07T01:38:43Z","Closed as not planned issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
Error during openai.createChatCompletion: Error: 400 1 validation error for Request body -> functions extra fields not permitted (type=value_error.extra) at APIError.generate (error.ts:66:14) at OpenAI.makeStatusError (core.ts:358:21) at OpenAI.makeRequest (core.ts:416:24) at process.processTicksAndRejections (node:internal/process/task_queues:95:5) at async ChatCompletionStream._createChatCompletion (ChatCompletionStream.ts:96:20) at async ChatCompletionStream._runChatCompletion (AbstractChatCompletionRunner.ts:426:12) 
gpt-4-vision-preview doesn't work with streaming and functions.
To Reproduce
const callParams = {
 model,
 messages: api_messages,
 top_p: 0.1,
 stream: true,
 };
const stream = await this.openai.beta.chat.completions.stream(callParams, {
 signal: this.abortController.signal,
 });
Code snippets
No response
OS
macOS
Node version
20.6
Library version
4.16.1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/434","Permissions object not being returned with openai.models.list().withResponse()","2023-11-07T08:18:07Z","Closed as not planned issue","No label","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
Up until last day the openai.models.list().withResponse() call was returning the permissions object for each model. However starting today, the permissions object is no longer being returned.
Our app uses this object to programmatically differentiate between various kinds of models.
To Reproduce
Initialize the openai simply run:
import OpenAI from ""openai"";const oai = new OpenAI({
            apiKey: apiKey,
            organization: org,
        });async function main() {
  const list = await oai.models.list();

  for await (const model of list) {
    console.log(model);
  }}main();
and look at the models returned.
Code snippets
No response
OS
macOS
Node version
v18.16.0
Library version
openai@4.15.4
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/422","TS7030: Not all code paths return a value","2023-11-07T17:29:40Z","Closed issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
When building using tsc with openai as a dependency, I get the following error:
 node_modules/openai/src/lib/AbstractChatCompletionRunner.ts(224,28): error TS7030: Not all code paths return a value.
I also had to disable the noImplicitAny rule since openai is not built, and then my app build inherits your source code basically.
To Reproduce
const OpenAI = require('openai');
and build with tsc
(Ideally, I would like this to be an automated test for this library, so build errors do not repeat.)
Code snippets
No response
OS
macOS, ubuntu
Node version
Node 18
Library version
4.15.1
 The text was updated successfully, but these errors were encountered: 
👍1
gommzystudio reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/openai-node/issues/419","Error: Premature close in streaming mode, seems it is related to timeout","2023-11-05T16:06:08Z","Closed issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
My code use chat completion api create func with stream:true, and takes so long time: over 10 minutes(default timeout setting).
const stream = await this.openaiApi.chat.completions.create(
  {
    model: 'gpt-3.5-turbo-16k',
    messages: history, // an array of chat history
    stream: true,
  },);
This request makes long -over 10 minutes-stream and after 10 minutes the request fails with error:
Error: Premature close
    at IncomingMessage.<anonymous> (/home/dev/refri_be/data-process/node_modules/node-fetch/lib/index.js:1748:18)
    at Object.onceWrapper (node:events:631:28)
    at IncomingMessage.emit (node:events:517:28)
    at emitCloseNT (node:internal/streams/destroy:132:10)
    at processTicksAndRejections (node:internal/process/task_queues:81:21) {
  code: 'ERR_STREAM_PREMATURE_CLOSE'}
I tried to chage timeout setting by passing timeout: 20 * 60 * 1000 both in create function and OpenAi constructor, but not worked. I checked #294 but nothing was helpful..
Timeout setting not working for streaming seems bug.
If node-fetch's Premature close error is due to streaming timeout, this error should be handled by this library and transfer to proper error, e.g. APIConnectionTimeoutError.
To Reproduce
prepare so long, long time-taking prompt. It seems languages other than english takes so long times for response 🥲
call OpenAI.chat.completions.create with stream:true and wait for 10 minutes
Then Error: premature close thrown
Code snippets
No response
OS
macOS sonoma
Node version
18.18.2
Library version
4.14.2
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/418","Support for web ReadableStream without buffering the whole file.","2023-11-03T12:47:42Z","Open issue","enhancement","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
Whenever a web ReadableStream is passed to the toFile helper function, the full contents of the file are buffered before forwarding the request to the Whisper OpenAI API endpoint.
 However, it is possible to avoid having to buffer the whole file into the server memory, and instead just use the server as a middleware that streamlines the data from the source of the file to the API endpoint.
 The problem has been discussed on issue #414 as well.
A workaround using axios and FormData that seems to work:
import FormData = require(""form-data"");import axios from ""axios"";

async function foo() {

  const bucket = storage.bucket(bucketName);
  const file = bucket.file(fileName);
  const readStream = file.createReadStream();
  
  const form = new FormData();
  
  // Make sure that the file has the proper extension as well
  // (In this example, we just add the `webm` extension for brevity
  if (!fileName.includes(""."")) {
      fileName = `${fileName}.webm`;
  }
  
  form.append(""file"", readStream, fileName);
  form.append(""model"", ""whisper-1"");
  
  const apiKey = OPENAI_API_KEY
  
  const response = await axios.post(
      ""https://api.openai.com/v1/audio/transcriptions"",
      form,
      {
          headers: {
              ...form.getHeaders(),
              Authorization: `Bearer ${apiKey}`,
          },
      }
  );
  
  const transcription = response.data.text:
  return transcription;}
To Reproduce
Example uses Cloud / Firebase Storage.
Code snippets
import {OpenAI, toFile} from 'openai'

const bucket = storage.bucket(bucketName);const file = bucket.file(fileName);

const openai = new OpenAI({
    apiKey: apiKey,});

const completion = await openai.audio.transcriptions.create({
    file: await toFile(file, ""myfile.mp3""),
    model: ""whisper-1"",});
OS
Linux (Google Cloud Functions)
Node version
18
Library version
4.11.1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/417","Use Axios as custom Http agent","2023-11-04T20:58:27Z","Closed issue","No label","Confirm this is a feature request for the Node library and not the underlying OpenAI API.
 This is a feature request for the Node library
Describe the feature or improvement you're requesting
Can I use Axios instance as Http Agent in ^V4?
 I need custom retry policies and this was possible in V3. I was not able to use Axios in httpAgent parametr in V4 constructor. Can you please provide code snippet how to use it?
Thanks👍
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/415","Self-referencing absolute import","2023-11-03T01:50:48Z","Closed as not planned issue","No label","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
In this line, which seems to be auto-generated by Stainless (cc @rattrayalex), the openai package is self-referencing itself, which may happen to work in most cases, but seems like very bad behavior to rely on more generally. This relies on the Node.js package import semantics, as opposed to using a relative import to reference something within the package.
openai-node/src/index.ts
 Line 8 in 0e67361
	import*asAPIfrom'openai/resources/index';
To Reproduce
Changing this line to a relative import fixes the issue.
Code snippets
No response
OS
macOS
Node version
any node version
Library version
4.14.2
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/414","Whisper Support for any ReadStream (any Readable)","2023-11-01T22:40:56Z","Closed issue","No label","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
Whenever using the Whisper model (audio.transcriptions), it is impossible to use any kind of readable that differs from a File System Readable.
 The reason behind it seems to be the extension of Readable, FsReadSteam, which requires a path parameter.
 It is not possible to use a dummy inside this parameter. I could not examine the source code thoroughly, but the path parameter does not seem to be used particularly in the code.
 This disables any kind of streaming from a service external from the server into the Whisper API: the files must first be downloaded into the File System of the server, and once they are downloaded they can be streamed to the API.
 This behaviour is totally possible using axios and form-data, bypassing the NodeJS library.
To Reproduce
(Example using Firebase Storage inside serverless Firebase Functions)
Create a file
Upload it to an external service (in example, Firebase Storage)
Try streamlining it to OpenAI API
const bucket = storage.bucket(bucketName);const file = bucket.file(fileName);const readStream = file.createReadStream();

const apiKey: string = openaiAPI.value();const openai = new OpenAI({
    apiKey: apiKey,});

const completion = await openai.audio.transcriptions.create({
    file: readStream,
    model: ""whisper-1"",});
Code snippets
const bucket = storage.bucket(bucketName);const file = bucket.file(fileName);const readStream = file.createReadStream();

const apiKey: string = openaiAPI.value();const openai = new OpenAI({
    apiKey: apiKey,});

const completion = await openai.audio.transcriptions.create({
    file: readStream,
    model: ""whisper-1"",});
OS
Linux (Google Cloud Functions)
Node version
18
Library version
4.11.1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/412","Model endpoint compatibility retrieval","2023-10-31T04:27:20Z","Closed as not planned issue","No label","Confirm this is a feature request for the Node library and not the underlying OpenAI API.
 This is a feature request for the Node library
Describe the feature or improvement you're requesting
I stumbled upon the need for Models who have chat completion capability, and I didn't seem to find something similar in the context,
 so, if there were an API where I could request all models that are compatible with a specific endpoint, that would be nice :)
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/395","Whisper translate other languages into english?","2023-10-23T22:42:53Z","Closed issue","No label","Confirm this is a feature request for the Node library and not the underlying OpenAI API.
 This is a feature request for the Node library
Describe the feature or improvement you're requesting
I realized that the open-ai node implementation of whisper-ai doesn't support taking audio from another language and then translating it into English. It only supports the same language transcription. Can someone please add that feature? It is available in python.
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/393","openai@4.12 do not work on deno runtime?","2023-11-07T07:17:20Z","Closed issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
I try to import openai on my netlify edge function, it works well days ago, but today it raises:
error: Uncaught (in promise) Error: Module not found ""https://esm.sh/v133/openai@4.12.4/denonext/resources.js"".
       const ret = new Error(getStringFromWasm0(arg0, arg1));
                  ^
       at __wbg_new_15d3966e9981a196 (file:///opt/buildhome/node-deps/node_modules/@netlify/edge-bundler/deno/vendor/deno.land/x/eszip@v0.40.0/eszip_wasm.generated.js:417:19)
       at <anonymous> (file:///opt/buildhome/node-deps/node_modules/@netlify/edge-bundler/deno/vendor/deno.land/x/eszip@v0.40.0/eszip_wasm_bg.wasm:1:93412)
            at <anonymous> (file:///opt/buildhome/node-deps/node_modules/@netlify/edge-bundler/deno/vendor/deno.land/x/eszip@v0.40.0/eszip_wasm_bg.wasm:1:1499594)
            at <anonymous> (file:///opt/buildhome/node-deps/node_modules/@netlify/edge-bundler/deno/vendor/deno.land/x/eszip@v0.40.0/eszip_wasm_bg.wasm:1:1938165)
            at __wbg_adapter_40 (file:///opt/buildhome/node-deps/node_modules/@netlify/edge-bundler/deno/vendor/deno.land/x/eszip@v0.40.0/eszip_wasm.generated.js:231:6)
       at real (file:///opt/buildhome/node-deps/node_modules/@netlify/edge-bundler/deno/vendor/deno.land/x/eszip@v0.40.0/eszip_wasm.generated.js:215:14)
       at eventLoopTick (ext:core/01_core.js:183:11)

Note that I found /denonext/resources.js missing from esm.sh. I don't know if this is related.
To Reproduce
new a netlify edge function with just
import OpenAI from ""https://esm.sh/openai""
and deploys it
Code snippets
No response
OS
Windows
Node version
deno
Library version
openai v4.12
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/392","Replace node-fetch with undici","2023-10-20T03:30:41Z","Open issue","No label","Confirm this is a feature request for the Node library and not the underlying OpenAI API.
 This is a feature request for the Node library
Describe the feature or improvement you're requesting
I noticed this library is still using node-fetch because Node's native fetch is considered experimental.
I think it'd be in the libraries best interest to switch to undici instead. Undici is the fetch implementation in Node.js. For all intents and purposes it is stable (nodejs/undici#1737). We (the maintainers of Undici) have some concerns about marking it as such in Node.js just yet because of the nature of the Fetch api spec (it itself adds breaking changes occasionally - this doesn't fit well with Node.js versioning strategy. It's complicated - read the issue I linked above for more details).
Switching the undici for the shim will enable a significantly easier upgrade path in the future whenever we figure out how to mark it as properly stable in Node.js
Happy to help swap this out too if the maintainers approve 😄 🚀
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
👍4
SkeLLLa, shoyuf, transitive-bullshit, and trivikr reacted with thumbs up emoji
All reactions
👍4 reactions"
"https://github.com/openai/openai-node/issues/387","files.create() requires purpose param, even though it's supposed to be used only for specifying 'fine-tuning'","2023-10-18T20:09:33Z","Closed as not planned issue","No label","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
When calling:
const podcast = await openaiClient.files.create({ file: fs.createReadStream('[path]')}); // note: no 'purpose' prop
 then:
await openaiClient.audio.transcriptions.create({file: podcast})
... the API responds with:
BadRequestError: 400 'purpose' is a required property or (if attempting values other than 'fine-tune'): BadRequestError: 400 'other' is not one of ['fine-tune'] - 'purpose'
... even though the comments on files.ts say: ""The intended purpose of the file. Currently, only ""fine-tune"" is supported."". This strongly suggests that purpose should be an optional param, given all the other uses beyond fine-tuning. But the ? is conspicuously missing from the type declarations in files.ts. And the API clearly requires it. So, either:
It needs to become truly optional, all the way through to the API
Or, the comments need to explain: A. it's required B. what the viable alternative values are, beyond ""fine-tune"" 
And, TS/JS should enforce the requirement on the initial call to files.create
To Reproduce
Call client.files.create({ file: fs.createReadStream('[path]')}); without a defined purpose param, then call
Code snippets
async function main() {
  console.log('About to read file and obtain transcription... ')
  const podcast = await openaiClient.files.create({ file: fs.createReadStream('[valid path]')});
  const transcript = await openaiClient.audio.transcriptions.create({
    model: 'whisper-1',
    file: podcast,
    language: 'en',
    prompt: promptText,
    response_format: 'verbose_json' // 'vtt' file should include time-stamps
  });
Response:
BadRequestError: 400 'purpose' is a required property
    at APIError.generate (file://...node_modules/openai/error.mjs:39:20)
    at OpenAI.makeStatusError (file://...node_modules/openai/core.mjs:238:25)
    at OpenAI.makeRequest (file://...node_modules/openai/core.mjs:277:30)
    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)
    at async file://...whisperPOC.mjs:9:17 {
  status: 400,
  headers: {
    'access-control-allow-origin': '*',
    'alt-svc': 'h3="":443""; ma=86400',
    'cf-cache-status': 'DYNAMIC',
    'cf-ray': '81829c6dc8442896-IAD',
    connection: 'keep-alive',
    'content-length': '145',
    'content-type': 'application/json',
    date: 'Wed, 18 Oct 2023 17:48:31 GMT',
    'openai-organization': 'user-OMITTED',
    'openai-processing-ms': '185',
    'openai-version': '2020-10-01',
    server: 'cloudflare',
    'strict-transport-security': 'max-age=15724800; includeSubDomains',
    'x-request-id': 'e3d9f3c6405aae82a421625db8885bf5'
  },
  error: {
    message: ""'purpose' is a required property"",
    type: 'invalid_request_error',
    param: null,
    code: null
  },
  code: null,
  param: null,
  type: 'invalid_request_error'
}



### OS

macOS 14

### Node version

v18.8

### Library version

4.12.4

 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/384","Error thrown when using node's fetch after version 4.12.2","2023-10-17T21:47:21Z","Closed issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
See two commits in https://github.com/thhermansen/openai-node-fetch/commits/main
which in essence consists of a rather small program:
import OpenAI from 'openai'


async function main() {
  new OpenAI({ apiKey: 'SECRET', });

  const response = await fetch('https://google.com')
  console.log(`${response.status} ${response.statusText}`)}

if (require.main === module) main();
It works fine with openai 4.12.1, but breaks in 4.12.2 where you get an error thrown:
openai-test/node_modules/.pnpm/web-streams-polyfill@3.2.1/node_modules/web-streams-polyfill/src/lib/validators/readable-stream.ts:5
    throw new TypeError(`${context} is not a ReadableStream.`);
          ^

TypeError: First parameter has member 'readable' that is not a ReadableStream.
    at assertReadableStream (openai-test/node_modules/.pnpm/web-streams-polyfill@3.2.1/node_modules/web-streams-polyfill/src/lib/validators/readable-stream.ts:5:11)
    at convertReadableWritablePair (openai-test/node_modules/.pnpm/web-streams-polyfill@3.2.1/node_modules/web-streams-polyfill/src/lib/validators/readable-writable-pair.ts:15:3)
    at ReadableStream.ReadableStream.pipeThrough (openai-test/node_modules/.pnpm/web-streams-polyfill@3.2.1/node_modules/web-streams-polyfill/src/lib/readable-stream.ts:211:23)
    at fetchFinale (node:internal/deps/undici/undici:9584:56)
    at mainFetch (node:internal/deps/undici/undici:9476:9)
    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)

The issue is maybe related to 71984ed
To Reproduce
Clone https://github.com/thhermansen/openai-node-fetch and run the code :)
Code snippets
No response
OS
MacOS
Node version
20.8.1
Library version
4.12.1, 4.12.2, 4.12.3
 The text was updated successfully, but these errors were encountered: 
👍1
PeterAronZentai reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/openai-node/issues/375","Inquiry about the 'Accept' header","2023-10-13T10:16:12Z","Open issue","No label","Confirm this is a feature request for the Node library and not the underlying OpenAI API.
 This is a feature request for the Node library
Describe the feature or improvement you're requesting
Hello,
I am currently using the openai-node package for my project, and I have a question regarding the 'Accept' header in the request made at this specific code snippet: core.ts.
In the mentioned code, the 'Accept' header is set to ""application/json"". However, when the 'stream' option is set to true, the response I receive is in the format of ""text/event-stream"".
I am using a unified API platform that converts API interfaces from different platforms to OPENAI format. Due to some platforms validating the 'Accept' header value, I am facing issues with the API requests failing.
Could you please confirm if setting the 'Accept' header to ""application/json"" in the mentioned code is compliant with the specifications? if it should be modified based on the expected response type?
Thank you for your assistance.
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/374","I have identified the issue at hand, and I apologize for the inconvenience caused.","2023-10-13T01:26:13Z","Closed issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
I did not receive a valid response from the GPT model using the ‘stream’ parameter in OpenAI version 4.11.1 under Node.js 16.13.
To Reproduce
1.Importing and initializing the OpenAI library.
 2.I am debugging data on the interface.
 3.When I activate the streaming mode of the gpt-3.5-turbo model, it results in an ineffective response from GPT.
 4.It is possible to obtain valid data without utilizing streams.
 5.This is the response data obtained by opening the stream.

 6.This is the data obtained without enabling the stream mode.

 7.I noticed that the v4 version of the document includes the addition of the stream mode for the gpt-3.5-turbo.
 8.I am uncertain of the underlying cause of this question and hope to receive a satisfactory response.
Code snippets
const {OpenAI} = require('openai')const openai = new OpenAI({
  apiKey:'myKey'})app.get('/gpt', async (req, res) => {
  const stream = await openai.chat.completions.create({
    model: ""gpt-3.5-turbo"",
    messages: [{""role"": ""user"", ""content"": ""Hello!""}],
    stream: true,   //The issue lies in the inability to retrieve valid values from the opened stream.
  });

  res.send(stream);}
OS
window10
Node version
nodejs16.14
Library version
OpenAI 4.11.1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/364","The chunk.choices item of completion stream sometimes is undefined.","2023-10-20T20:24:34Z","Closed issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
Using the readme example of chat stream.
part.choices 👈 undefined sometimes, so throw an error when try to access to index [0].
part.choices[0] 👈 error

TypeError: Cannot read properties of undefined (reading '0')
Is it normal that it can be undefined sometimes?
 Should I check that it is not undefined before get the choice?
 Or does being undefined mean some API error?
 Thanks
Readme Example
async function main() {
  const stream = await openai.chat.completions.create({
    model: 'gpt-4',
    messages: [{ role: 'user', content: 'Say this is a test' }],
    stream: true,
  });
  for await (const part of stream) {
    process.stdout.write(part.choices[0]?.delta?.content || '');
  }}
To Reproduce
Use the readme example.
 It doesn't happen very often, my server logs told me about it
Code snippets
No response
OS
Windows 10
Node version
v18.17.1
Library version
4.11.1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/363","TypeScript compilation errors in backend environment","2023-10-20T20:25:36Z","Closed as not planned issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
Hello,
I recently encountered several TypeScript compilation errors when using this library in a backend project.
 The issue primarily seems related to missing DOM types.
Here are the errors I encountered when I run tsc command:
../../node_modules/openai/src/core.ts:39:15 - error TS2304: Cannot find name 'AbortController'.

39   controller: AbortController;
                 ~~~~~~~~~~~~~~~

../../node_modules/openai/src/core.ts:252:18 - error TS2304: Cannot find name 'TextEncoder'.

252       if (typeof TextEncoder !== 'undefined') {
                     ~~~~~~~~~~~

../../node_modules/openai/src/core.ts:253:29 - error TS2304: Cannot find name 'TextEncoder'.

253         const encoder = new TextEncoder();
                                ~~~~~~~~~~~

../../node_modules/openai/src/core.ts:377:28 - error TS2304: Cannot find name 'AbortController'.

377     const controller = new AbortController();
                               ~~~~~~~~~~~~~~~

../../node_modules/openai/src/core.ts:424:13 - error TS2304: Cannot find name 'URL'.

424         new URL(path)
                ~~~

../../node_modules/openai/src/core.ts:425:13 - error TS2304: Cannot find name 'URL'.

425       : new URL(this.baseURL + (this.baseURL.endsWith('/') && path.startsWith('/') ? path.slice(1) : path));
                ~~~

../../node_modules/openai/src/core.ts:460:17 - error TS2304: Cannot find name 'AbortController'.

460     controller: AbortController,
                    ~~~~~~~~~~~~~~~

../../node_modules/openai/src/core.ts:580:31 - error TS2304: Cannot find name 'URL'.

580 export type PageInfo = { url: URL } | { params: Record<string, unknown> | null };
                                  ~~~

../../node_modules/openai/src/core.ts:727:12 - error TS2304: Cannot find name 'AbortSignal'.

727   signal?: AbortSignal | undefined | null;
               ~~~~~~~~~~~

../../node_modules/openai/src/core.ts:1083:14 - error TS2304: Cannot find name 'btoa'.

1083   if (typeof btoa !== 'undefined') {
                  ~~~~

../../node_modules/openai/src/core.ts:1084:12 - error TS2304: Cannot find name 'btoa'.

1084     return btoa(str);
                ~~~~

../../node_modules/openai/src/streaming.ts:13:15 - error TS2304: Cannot find name 'AbortController'.

13   controller: AbortController;
                 ~~~~~~~~~~~~~~~

../../node_modules/openai/src/streaming.ts:18:47 - error TS2304: Cannot find name 'AbortController'.

18   constructor(response: Response, controller: AbortController) {
                                                 ~~~~~~~~~~~~~~~

../../node_modules/openai/src/streaming.ts:208:16 - error TS2304: Cannot find name 'TextDecoder'.

208     if (typeof TextDecoder !== 'undefined') {
                   ~~~~~~~~~~~

../../node_modules/openai/src/streaming.ts:210:34 - error TS2304: Cannot find name 'TextDecoder'.

210         this.textDecoder ??= new TextDecoder('utf8');
                                     ~~~~~~~~~~~

../../node_modules/openai/src/uploads.ts:112:18 - error TS2304: Cannot find name 'URL'.

112     name ||= new URL(value.url).pathname.split(/[\\/]/).pop() ?? 'unknown_file';
                     ~~~


Found 16 errors in 3 files.

Errors  Files
    11  ../../node_modules/openai/src/core.ts:39
     4  ../../node_modules/openai/src/streaming.ts:13
     1  ../../node_modules/openai/src/uploads.ts:112

For reference, my tsconfig.json is as follows:
{
  ""extends"": ""../../tsconfig.base.json"",
  ""compilerOptions"": {
    ""target"": ""ES2020"",
    ""module"": ""commonjs"",
    ""outDir"": ""./dist"", 
    ""lib"": [""es2020""],
    ""typeRoots"": [""./node_modules/@types"", ""./types""],
    ""baseUrl"": ""."",
    ""resolveJsonModule"": true
  },
  ""exclude"": [""cdk.out"", ""node_modules""]
}
While adding the dom library to lib of my tsconfig.json does seem to resolve these errors, it causes references in other parts of the code to inadvertently change, leading to build failures. Furthermore, I believe that adding the dom library to a backend codebase isn't appropriate.
I've verified that the problem is not related to React's StrictMode. Upon checking the openai library's tsconfig.json, I noticed it uses the skipLibCheck option, but adding that to my project's configuration did not resolve the issue.
I wanted to raise this issue here to check if it's a known issue, or if there are any recommended workarounds. Any assistance or guidance would be appreciated.
Thank you.
To Reproduce
yarn add openai
run tsc on my repository
See error
Code snippets
No response
OS
macOS
Node version
Node v18.4.0
Library version
openai v4.8.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/362","Failed to minify the code from this file: 	./node_modules/openai/dist/base.js:36","2023-10-10T16:54:34Z","Closed as not planned issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
The Version I am using for Open AI is 3.3.0.
When I run npm run build, it is giving me this error
Failed to minify the code from this file: 	./node_modules/openai/dist/base.js:36
Here is my package.json file
{
 ""name"": ""loyalty"",
 ""version"": ""0.1.0"",
 ""private"": true,
 ""dependencies"": {
 ""autoprefixer"": ""7.1.6"",
 ""axios"": ""^0.27.2"",
 ""babel-core"": ""6.26.0"",
 ""babel-eslint"": ""7.2.3"",
 ""babel-jest"": ""20.0.3"",
 ""babel-loader"": ""7.1.2"",
 ""babel-preset-es2015"": ""^6.24.1"",
 ""babel-preset-react-app"": ""^3.1.1"",
 ""babel-runtime"": ""6.26.0"",
 ""case-sensitive-paths-webpack-plugin"": ""2.1.1"",
 ""chalk"": ""1.1.3"",
 ""classnames"": ""^2.2.5"",
 ""css-loader"": ""0.28.7"",
 ""dotenv"": ""^4.0.0"",
 ""dotenv-expand"": ""4.0.1"",
 ""eslint"": ""4.10.0"",
 ""eslint-config-react-app"": ""^2.1.0"",
 ""eslint-loader"": ""1.9.0"",
 ""eslint-plugin-flowtype"": ""2.39.1"",
 ""eslint-plugin-import"": ""2.8.0"",
 ""eslint-plugin-jsx-a11y"": ""5.1.1"",
 ""eslint-plugin-react"": ""7.4.0"",
 ""extract-text-webpack-plugin"": ""3.0.2"",
 ""file-loader"": ""1.1.5"",
 ""fs-extra"": ""3.0.1"",
 ""html-webpack-plugin"": ""2.29.0"",
 ""jest"": ""20.0.4"",
 ""jwt-decode"": ""^2.2.0"",
 ""lodash"": ""^4.17.5"",
 ""material-ui"": ""^0.20.1"",
 ""moment"": ""^2.20.1"",
 ""npm"": ""^6.4.0"",
 ""object-assign"": ""4.1.1"",
 ""openai"": ""^2.0.5"",
 ""postcss-flexbugs-fixes"": ""3.2.0"",
 ""postcss-loader"": ""2.0.8"",
 ""promise"": ""8.0.1"",
 ""prop-types"": ""^15.6.0"",
 ""raf"": ""3.4.0"",
 ""react"": ""^16.2.0"",
 ""react-addons-shallow-compare"": ""^15.6.2"",
 ""react-bootstrap"": ""^0.32.1"",
 ""react-dates"": ""^16.2.1"",
 ""react-dev-utils"": ""^5.0.0"",
 ""react-dom"": ""^16.2.0"",
 ""react-dropzone"": ""^4.2.10"",
 ""react-dropzone-component"": ""^3.2.0"",
 ""react-facebook"": ""^6.0.15"",
 ""react-facebook-login"": ""^4.1.1"",
 ""react-fast-compare"": ""^2.0.2"",
 ""react-head"": ""^2.2.0"",
 ""react-images-uploader"": ""^1.2.0-rc1"",
 ""react-load-script"": ""0.0.6"",
 ""react-modal"": ""^3.1.11"",
 ""react-payment"": ""^0.1.8"",
 ""react-redux"": ""^5.0.6"",
 ""react-router-dom"": ""^4.2.2"",
 ""react-square-hosted-fields"": ""^1.2.4"",
 ""react-stripe-checkout"": ""^2.6.3"",
 ""react-stripe-elements"": ""^1.6.0"",
 ""react-toastify"": ""^3.3.4"",
 ""recharts"": ""^1.0.0-beta.10"",
 ""redux"": ""^3.7.2"",
 ""redux-thunk"": ""^2.2.0"",
 ""server"": ""^1.0.18"",
 ""square-connect"": ""^2.20181212.0"",
 ""stripe"": ""^6.4.0"",
 ""style-loader"": ""0.19.0"",
 ""sw-precache-webpack-plugin"": ""0.11.4"",
 ""url-loader"": ""0.6.2"",
 ""uuid"": ""^3.2.1"",
 ""validator"": ""^9.4.0"",
 ""webpack"": ""3.8.1"",
 ""webpack-dev-server"": ""2.9.4"",
 ""webpack-manifest-plugin"": ""1.3.2"",
 ""whatwg-fetch"": ""2.0.3""
 },
 ""scripts"": {
 ""start"": ""node scripts/start.js"",
 ""build"": ""node scripts/build.js"",
 ""test"": ""node scripts/test.js --env=jsdom""
 },
 ""jest"": {
 ""collectCoverageFrom"": [
 ""src//*.{js,jsx,mjs}""
 ],
 ""setupFiles"": [
 ""/config/polyfills.js""
 ],
 ""testMatch"": [
 ""/src//tests//*.{js,jsx,mjs}"",
 ""/src//?(.)(spec|test).{js,jsx,mjs}""
 ],
 ""testEnvironment"": ""node"",
 ""testURL"": ""http://localhost"",
 ""transform"": {
 ""^.+\.(js|jsx|mjs)$"": ""/node_modules/babel-jest"",
 ""^.+\.css$"": ""/config/jest/cssTransform.js"",
 ""^(?!.\.(js|jsx|mjs|css|json)$)"": ""/config/jest/fileTransform.js""
 },
 ""transformIgnorePatterns"": [
 ""[/\\]node_modules[/\\].+\.(js|jsx|mjs)$""
 ],
 ""moduleNameMapper"": {
 ""^react-native$"": ""react-native-web""
 },
 ""moduleFileExtensions"": [
 ""web.js"",
 ""mjs"",
 ""js"",
 ""json"",
 ""web.jsx"",
 ""jsx"",
 ""node""
 ]
 },
 ""babel"": {
 ""presets"": [
 ""react-app""
 ]
 },
 ""eslintConfig"": {
 ""extends"": ""react-app""
 },
 ""devDependencies"": {
 ""react-square-cards"": ""^1.0.14""
 }
 }
To Reproduce
nom install openai than create build using rpm run build
Code snippets
No response
OS
macOs
Node version
v16.18.0
Library version
3.3.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/361","TypeError: the promise constructor cannot be invoked directly - Azure OpenAI","2023-10-11T22:40:51Z","Closed issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
When using Azure OpenAI I am getting intermittently the error TypeError: the promise constructor cannot be invoked directly
TypeError: the promise constructor cannot be invoked directly

    See http://goo.gl/MqrFmX

    at check (/var/task/node_modules/bluebird/js/release/promise.js:86:15)
    at new Promise (/var/task/node_modules/bluebird/js/release/promise.js:96:9)
    at new APIPromise (file:///var/task/node_modules/openai/core.mjs:44:9)
    at OpenAI.request (file:///var/task/node_modules/openai/core.mjs:241:16)
    at OpenAI.methodRequest (file:///var/task/node_modules/openai/core.mjs:157:21)
    at OpenAI.post (file:///var/task/node_modules/openai/core.mjs:145:21)
    at Completions.create (file:///var/task/node_modules/openai/resources/chat/completions.mjs:6:21)

To Reproduce
const AZURE_OPENAI_API_INSTANCE_NAME = '';
const AZURE_OPENAI_API_KEY = '';
const AZURE_OPENAI_API_VERSION = '2023-07-01-preview';
const AZURE_OPENAI_API_DEPLOYMENT_NAME = '';

const configAzure = {
          apiKey: AZURE_OPENAI_API_KEY,
          baseURL: `https://${AZURE_OPENAI_API_INSTANCE_NAME}.openai.azure.com/openai/deployments/${AZURE_OPENAI_API_DEPLOYMENT_NAME}`,
          defaultQuery: { 'api-version': AZURE_OPENAI_API_VERSION },
          defaultHeaders: { 'api-key': AZURE_OPENAI_API_KEY },
        };
  
const openai = new OpenAI(configAzure);

const response = await openai.chat.completions.create(
        {
        model: 'gpt-3.5-turbo',
        messages: [
               { role: 'user' , content: 'Write about AI' },
         ],
        n: 3
        top_p: 1,
      frequency_penalty: 0,
      presence_penalty: 0,
        }
      );

Code snippets
No response
OS
Vercel (Linux)
Node version
18.x
Library version
4.11.1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/360","Cannot access Audio before initalization","2023-10-20T20:27:02Z","Closed issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
I'm getting this error with nextjs Cannot access 'Audio' before initialization
 i'm only using chat completion.
The issue seems to be a circular depedency in index.js and audio.js generated by stainless client.
Notice how audio.js is importing import * as API from ""./index.mjs""This is not correct and causes issues.
More info below in code snippet.
Uncaught ReferenceError: Cannot access 'Audio' before initialization
    at Module.Audio (audio.mjs:1:54)
    at Module.Audio (index.mjs:1:54)
    at Object.registerExportsForReactRefresh (helpers.js:52:40)
    at registerExportsAndSetupBoundaryForReactRefresh (runtime-base.ts:438:11)
    at runModuleExecutionHooks (runtime-base.ts:382:7)
    at instantiateModule (runtime-base.ts:321:5)
    at getOrInstantiateModuleFromParent (runtime-base.ts:422:10)
    at esmImport (runtime-utils.ts:203:18)
    at audio.mjs:1:54
    at [project]/node_modules/.pnpm/openai@4.11.1/node_modules/openai/resources/audio/audio.mjs (ecmascript) (08b5e__pnpm_360590._.js:141833:3)
    at runtime-base.ts:322:21
    at runModuleExecutionHooks (runtime-base.ts:374:5)
    at instantiateModule (runtime-base.ts:321:5)
    at getOrInstantiateModuleFromParent (runtime-base.ts:422:10)
    at esmImport (runtime-utils.ts:203:18)
    at index.mjs:1:54
    at [project]/node_modules/.pnpm/openai@4.11.1/node_modules/openai/resources/index.mjs (ecmascript) (08b5e__pnpm_360590._.js:141878:3)
    at runtime-base.ts:322:21
    at runModuleExecutionHooks (runtime-base.ts:374:5)
    at instantiateModule (runtime-base.ts:321:5)
    at getOrInstantiateModuleFromParent (runtime-base.ts:422:10)
    at esmImport (runtime-utils.ts:203:18)
    at index.mjs:1:54
    at [project]/node_modules/.pnpm/openai@4.11.1/node_modules/openai/index.mjs (ecmascript) (08b5e__pnpm_360590._.js:142022:3)
    at runtime-base.ts:322:21
    at runModuleExecutionHooks (runtime-base.ts:374:5)
    at instantiateModule (runtime-base.ts:321:5)
    at getOrInstantiateModuleFromParent (runtime-base.ts:422:10)
    at esmImport (runtime-utils.ts:203:18)
    at [project]_packages_askluna-core_src_api_64b410._.js:126:168
    at [project]/packages/askluna-core/src/api/getOpenAIClient.ts (ecmascript) ([project]_packages_askluna-core_src_api_64b410._.js:145:3)
    at runtime-base.ts:322:21
    at runModuleExecutionHooks (runtime-base.ts:374:5)
    at instantiateModule (runtime-base.ts:321:5)
    at getOrInstantiateModuleFromParent (runtime-base.ts:422:10)
    at esmImport (runtime-utils.ts:203:18)
    at [project]_packages_askluna-core_src_api_64b410._.js:153:144
    at [project]/packages/askluna-core/src/api/OpenAiModelClient.ts (ecmascript) ([project]_packages_askluna-core_src_api_64b410._.js:215:3)
    at runtime-base.ts:322:21
    at runModuleExecutionHooks (runtime-base.ts:374:5)
    at instantiateModule (runtime-base.ts:321:5)
    at getOrInstantiateModuleFromParent (runtime-base.ts:422:10)
    at esmImport (runtime-utils.ts:203:18)
    at [project]_packages_askluna-core_src_services_ai-system_AiSystem_ts_638c3c._.js:8:146
    at [project]/packages/askluna-core/src/services/ai-system/AiSystemAbstract.ts (ecmascript) ([project]_packages_askluna-core_src_services_ai-system_AiSystem_ts_638c3c._.js:66:3)
    at runtime-base.ts:322:21
    at runModuleExecutionHooks (runtime-base.ts:374:5)
    at instantiateModule (runtime-base.ts:321:5)
    at getOrInstantiateModuleFromParent (runtime-base.ts:422:10)

To Reproduce
create nextjs project
run with --turbo
use chat completion
Code snippets
// File generated from our OpenAPI spec by Stainless.export { Audio } from ""./audio.mjs"";export { Transcriptions } from ""./transcriptions.mjs"";export { Translations } from ""./translations.mjs"";//# sourceMappingURL=index.mjs.map
// File generated from our OpenAPI spec by Stainless.
import { APIResource } from 'openai/resource';
import { Transcriptions } from ""./transcriptions.mjs"";
import { Translations } from ""./translations.mjs"";
import * as API from ""./index.mjs"";
export class Audio extends APIResource {
    constructor() {
        super(...arguments);
        this.transcriptions = new Transcriptions(this.client);
        this.translations = new Translations(this.client);
    }
}
(function (Audio) {
    Audio.Transcriptions = API.Transcriptions;
    Audio.Translations = API.Translations;
})(Audio || (Audio = {}));
//# sourceMappingURL=audio.mjs.map



### OS

macos

### Node version

v20

### Library version

v4

 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/359","Nextjs issue with fetch (can't resolve encoding)","2023-10-09T20:42:49Z","Closed issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
usage of this module in a monorepo libary with Nextjs causes an error

Module not found: Can't resolve 'encoding' in '/Users/shravansunder/Documents/dev/project-dev/2023-askluna/node_modules/.pnpm/node-fetch@2.7.0/node_modules/node-fetch/lib'

Import trace for requested module:
../../node_modules/.pnpm/node-fetch@2.7.0/node_modules/node-fetch/lib/index.js
../../node_modules/.pnpm/openai@4.11.1/node_modules/openai/_shims/node-runtime.mjs
../../node_modules/.pnpm/openai@4.11.1/node_modules/openai/_shims/auto/runtime-node.mjs
../../node_modules/.pnpm/openai@4.11.1/node_modules/openai/_shims/index.mjs
../../node_modules/.pnpm/openai@4.11.1/node_modules/openai/core.mjs
../../node_modules/.pnpm/openai@4.11.1/node_modules/openai/index.mjs
../../packages/askluna-core/src/api/getOpenAIClient.ts

Its most likely the same issue supabase had: supabase/supabase-js#612
 The recently resolved it.
To Reproduce
create a monorepo library that uses the open api client
create a nextjs app
import the client or try to use it
you'll get this error during webpack compile.
Code snippets
No response
OS
macos
Node version
20
Library version
v4
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/358","Getting ""Unable to connect. Is the computer able to access the url?"" error after a few requests","2023-10-06T22:43:15Z","Closed as not planned issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
I'm using the nodejs SDK to make a request to create embeddings. After I do a few requests in a row I'm getting the following error:
ConnectionRefused: Unable to connect. Is the computer able to access the url?
 path: ""https://api.openai.com/v1/embeddings""

Error
Error: undefined Connection error.
    at new OpenAIError (:1:28)
    at new APIError (/Users/joaquindiaz/Documents/embrace/chatgtp-poc-api/node_modules/openai/error.mjs:7:13)
    at new APIConnectionError (/Users/joaquindiaz/Documents/embrace/chatgtp-poc-api/node_modules/openai/error.mjs:63:8)
    at <anonymous> (/Users/joaquindiaz/Documents/embrace/chatgtp-poc-api/node_modules/openai/core.mjs:266:60)

I'm on a free tier so I'm assuming it's related to rate limiting. My main issue is that after getting this error for whatever reason I need to wait and restart my server in order to make the SDK work again. Only waiting is not enough
Thanks!
To Reproduce
Be on a free tier
Make several requests to openai.embeddings.create in a row
Get the error after a few requests
Code snippets
const getEmbeddings = async (text: string) => {
  const openai = new OpenAI({
    apiKey: process.env.OPENAI_API_KEY,
  });

  try {
    const response = await openai.embeddings.create({
      model: ""text-embedding-ada-002"",
      input: text,
    });

    return response.data.map(item => item.embedding);
  } catch (e) {
    throw new OpenAIError(""Failed to get embeddings"");
  }}
OS
macOS
Node version
v18.18.0
Library version
4.11.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/355","ReactNative: Error: 404 Invalid URL (POST /v1/chat/completions/)","2023-10-06T18:01:20Z","Closed issue","No label","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
The issue only happens when the OpenAI client is used in ReactNative code, specifically due to the following bug in ReactNative itself: facebook/react-native#24428
Notice how the request is attempted to be sent to /v1/chat/completions/ (with a trailing slash).
It can be resolved by installing the https://www.npmjs.com/package/react-native-url-polyfill; however, it can also be resolved by trimming the trailing slash in the buildURL method here: https://github.com/openai/openai-node/blob/master/src/core.ts#L436
Something along these lines:
return url.toString().endsWith('/')
      ? url.toString().slice(0, -1)
      : url.toString();

To Reproduce
Try to send any message using the openai.chat.completions.create within a ReactNative app
Code snippets
const openai = new OpenAI({
  apiKey: process.env.EXPO_PUBLIC_OPENAI_API_KEY,});

const chatCompletion = await openai.chat.completions.create({
        model: ""gpt-3.5-turbo"",
        messages: [
          {
            role: ""user"",
            content: ""test message"",
          },
        ],
      });
OS
macOS
Node version
Node v20.3.1
Library version
openai 4.11.1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/351","Make response accessible from the Stream class","2023-10-05T16:21:35Z","Closed issue","No label","Confirm this is a feature request for the Node library and not the underlying OpenAI API.
 This is a feature request for the Node library
Describe the feature or improvement you're requesting
I would like to propose allowing consumer of a Stream to be able to access the original response that includes the Stream:PassThrough as the body. When using the version 3.*.* version of this library we (and probably a bunch of others) ended up having to build our own decoder.
When migrating to 4.*.* we realized the response object from the Stream class was flagged as private preventing us from consuming the response and leveraging all the code we already write to consume the PassThrough stream directly, in a Typescript friendly way (aka: don't @ts-ignore things).
I understand the recommended way is to use the AsyncIterator to loop over the decoded chunk. This very small change would provide some backwards compatibility with the v3 way of streaming that a lot of consumer of this library may appreciate.
Additional context
Should be as simple as making the following change to the Stream class:
export class Stream<Item> implements AsyncIterable<Item> {
  controller: AbortController;
  response: Response;

  private decoder: SSEDecoder;
  ...
I would have open a PR but do not seem to have the permission to do so.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/349","MaxListenersExceededWarning: Possible EventEmitter memory leak detected and Regular Expression Denial of Service","2024-07-08T19:15:48Z","Closed as not planned issue","No label","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
Using Node.js version 20.x or later in a project that depends on node-fetch prior to version 3.2.5 results in Node prompting a MaxListenersExceededWarning: Possible EventEmitter memory leak detected warning (as can be seen here). However, node-fetch versions between 3.1.0 and 3.2.10 also suffer from a Regular Expression Denial of Service (as can be seen here). Installing node-fetch@3.2.10 seems to solve both problems.
To Reproduce
Initialize a new Node.js project;
install node-fetch prior to version 3.2.10 or packages that depend on node-fetch prior to version 3.2.10.
Code snippets
No response
OS
Windows 11
Node version
Node v20.7.0
Library version
openai v4.11.1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/348","openai.chat.completions.create throws a 404 Invalid URL error","2023-10-03T20:47:44Z","Closed issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
I'm trying to create a simple React Native app using the OpenAI npm package.
 I have installed version 4.11.1
I keep getting an Invalid URL error 404 Invalid URL (POST /v1/chat/completions/)
But when I call the same endpoint using Postman I get back the expected results.
I have noticed there's an additional / on the API endpoint mentioned in the error message, /v1/chat/completions/
 So, I tried to build the URL myself using openai.buildURL and it worked as expected.
Everything works as expected if I add these two lines before the API call.
openAi.baseURL = 'https://api.openai.com/v1';
openAi.buildURL = (path: string) => `${openAi.baseURL}${path}`;

So I believe something is wrong with the endpoint URL.
To Reproduce
Creating a new React Native app with Expo.
Install OpenAI npm package (I'm using version 4.11.1).
Call the openai.chat.completions.create({...}) method.
Code snippets
const openAi = new OpenAI({ apiKey: 'sk-x0x0x0x0x0x0x0x' });

const response = await openAi.chat.completions.create({
                ""model"": ""gpt-3.5-turbo"",
                ""messages"": [
                  {
                    ""role"": ""system"",
                    ""content"": ""You will be provided with a block of text, and your task is to extract a list of keywords from it.""
                  },
                  {
                    ""role"": ""user"",
                    ""content"": CLEANED_USER_INPUT
                  }
                ],
                ""temperature"": 0.5,
                ""max_tokens"": 256,
                ""top_p"": 1,
                ""frequency_penalty"": 0,
                ""presence_penalty"": 0
              });
    
console.log('response', response);
    
return response.choices;
OS
Windows 11
Node version
16
Library version
4.11.1
 The text was updated successfully, but these errors were encountered: 
🚀1
AbdeenM reacted with rocket emoji
All reactions
🚀1 reaction"
"https://github.com/openai/openai-node/issues/347","Reading stream response in JS client raises ""TypeError: network error""","2023-10-03T20:54:42Z","Closed issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
I'm trying to stream response from AWS Lambda using the library using Lambda's ""Streaming Response"" and ""Function URL"" features. But on client side I always facing TypeError: network error after 10-30 successfully read chunks of data.
I have this error in Google Chrome, Chromium, FIreFox. Also reproducible in other clients (for example Python requests).
Not sure that this is the library's issue, but I cannot find solution.
To Reproduce
npm install ... and use installed package as AWS Lambda Layer
deploy to AWS Lambda (Runtime Node 18 + Response Streming + Function URL + CORS enabled)
write simple client and run it
Code snippets
// Below is my server code (AWS Lambda):import OpenAI from 'openai';

const openai = new OpenAI({
    apiKey: OPENAI_API_KEY,
    maxRetries: 5,});

export const handler = awslambda.streamifyResponse(async (event, responseStream, _context) => {

    const stream = await openai.chat.completions.create({
        model: MODEL,
        messages: data.messages,
        stream: true,
    });

    for await (const part of stream) {
        let chunk = part.choices[0]?.delta?.content || '';
        if (chunk && chunk.length) {
            responseStream.write(chunk);
        }

    }
    responseStream.end();})


// And client code (pure JS):const response = await fetch(url, {
    method: 'POST',
    headers: headers,
    body: JSON.stringify({messages}),});

const reader = response.body.getReader();

while (true) {
    const {done, value} = await reader.read();  // <--- this line raises error
    if (done) break;
    const decodedValue = new TextDecoder().decode(value);
    console.log(decodedValue)}
OS
Linux
Node version
Node.js v18.x
Library version
4.11.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/341","How stream result data of GPT from ExpressJS to ReactJS application front-end","2023-10-05T18:52:59Z","Closed issue","No label","Confirm this is a feature request for the Node library and not the underlying OpenAI API.
 This is a feature request for the Node library
Describe the feature or improvement you're requesting
I'm not sure if this considered as a Feature but I can find any related articles mentioned.
 So I have an ExpressJS /gpt endpoint I want to stream the data of chunk.choices[0].delta.content
app.post('/gpt', (req, res) => {

    (async function openAiApi() {
        const stream = await openai.chat.completions.create({
            model: ""gpt-4"",
            messages: [{
                role: ""user"", content: ""write a 100 word paragraph about tourism""
            }],
            stream: true,
        });

        for await (const chunk of stream) {

            console.log(chunk.choices[0].delta.content)
            chunk.on('data', chunk.choices[0].delta.content) // not sure how to use this properly

        }
   }());

})

This's ReactJS code so how to receive the data in streaming real-time way, thanks appreciate your help
axios.post(""http://localhost:8000/gpt"", { body: JSON.stringify(queryValue) }).then((response) => {
   
     console.log(response);
    var  openAiResponse = response.data
     setResponse(openAiResponse);

   }).catch(() => {
     console.log(""Something went wrong. Plase try again later"");
   });
 }

Additional context
No response
 The text was updated successfully, but these errors were encountered: 
👀1
Peek-A-Booo reacted with eyes emoji
All reactions
👀1 reaction"
"https://github.com/openai/openai-node/issues/337","Missing content_filter in finish_reason of chat completion chunk","2023-09-27T00:04:16Z","Closed issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
When trying to select events that were stopped due to content moderation, you get a warning that about content_filter not being one of the options of finish_reason:
This doesn't match the official documentation of OpenAI: https://platform.openai.com/docs/api-reference/chat/streaming
To Reproduce
Try to select events that were stopped due to content moderation.
Code snippets
    const stream = await openai.chat.completions.create({
      model: OPENAI_MODEL,
      messages: messages,
      stream: true,
    })

    for await (const event of stream) {
      for (const choice of event.choices) {
        if (choice.finish_reason === 'content_filter') {
          console.log(""Content filter triggered"")
        }
        console.log(event)
      }
    }


### OS

macOS

### Node version

Node v18

### Library version

openai v4.8.0

 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/336","The SDK examples for finetuning in the docs are wrong.","2023-09-24T18:47:05Z","Closed as not planned issue","No label","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
In the docs it tells you to use:
const fineTune = await openai.fineTunes.create({ training_file: 'file-abc123', model: 'gpt-3.5-turbo' })
Whereas in the API reference, it says to use:
const fineTune = await openai.fineTuning.jobs.create({ training_file: 'file-abc123', model: 'gpt-3.5-turbo' })
I'm assuming the API reference is the correct version, if so, the docs need to be updated.
 Link to relevant section in docs: https://platform.openai.com/docs/guides/fine-tuning/create-a-fine-tuned-model
To Reproduce
read docs here: https://platform.openai.com/docs/guides/fine-tuning/create-a-fine-tuned-model
read API reference here: https://platform.openai.com/docs/api-reference/fine-tuning/create
Code snippets
No response
OS
macOS
Node version
Node v18.17.1
Library version
openai v4.5.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/333","""lib/IsolatedGPT35TurboMutation/deleteFineTuneModel: AbortController is not defined""","2024-07-08T19:14:02Z","Closed as not planned issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
What is this error?
 It works fine locally, but when deploying I get this error..: ""lib/IsolatedGPT35TurboMutation/deleteFineTuneModel: AbortController is not defined""
To Reproduce
if (job.model_id) {
 if (!src.openAiKey) throw new Error('OpenAiKey not found');
 const openai = new OpenAI({ apiKey: src.openAiKey });
 const model = await openai.models.del(job.model_id);
Code snippets
No response
OS
macOS
Node version
Node 18
Library version
openai 4.0.8
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/332","Can we use chatgpt plugins now?","2023-09-21T22:30:15Z","Closed as not planned issue","No label","Confirm this is a feature request for the Node library and not the underlying OpenAI API.
 This is a feature request for the Node library
Describe the feature or improvement you're requesting
Can we use chatgpt plugins now? Would be great
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/324","Configuring an HTTP(S) Agent (e.g., for proxies) not Work in Browser","2023-09-20T22:20:18Z","Closed as not planned issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
When use httpAgent in the browser, the agent is ignored and never used.
To Reproduce
import OpenAI from ""openai""import http from 'http';import HttpsProxyAgent from 'https-proxy-agent';

// Configure the default for all requests:const openai = new OpenAI({
  dangerouslyAllowBrowser: true,
  httpAgent: new HttpsProxyAgent(process.env.PROXY_URL),});openai.models.list().then(console.log);
Code snippets
No response
OS
Windows 10
Node version
v18.17.1
Library version
v4.8.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/323","How to use it with axios using custom fetch?","2023-09-19T23:25:34Z","Closed issue","No label","Confirm this is a feature request for the Node library and not the underlying OpenAI API.
 This is a feature request for the Node library
Describe the feature or improvement you're requesting
I want to use axios to use axios interceptors, or the proxy config of axios.
I think it can be used in the fetch params of the ClientOptions, like this example
openai-node/tests/index.test.ts
 Lines 86 to 90 in 0386937
	fetch: (url)=>{
	returnPromise.resolve(
	newResponse(JSON.stringify({ url,custom: true}),{
	headers: {'Content-Type': 'application/json'},
	}),
But I don't know how to use it
    const openai = new OpenAI({
      fetch: async (requestInfo: RequestInfo, init?: RequestInit) => {
        // TODO: Use acios
        // return axios()
      }
    })
Can you help me, and tell me If there are any limitations when using axios as a custom fetch?
I think this could be added to de docs.
Thanks
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/322","v4 chat completion: a way to access rate limit data available through response headers in v3","2023-09-19T15:24:03Z","Closed issue","No label","Confirm this is a feature request for the Node library and not the underlying OpenAI API.
 This is a feature request for the Node library
Describe the feature or improvement you're requesting
In v3, we used to be able to access the rate limit stats (openai docs ref) with the chat completion response:
const chatCompletion = await openai.createChatCompletion({...});const headers = chatCompletion.headers;
  const limits = {
    limitRequests: parseInt(headers[""x-ratelimit-limit-requests""]),
    limitTokens: parseInt(headers[""x-ratelimit-limit-tokens""]),
    remainingRequests: parseInt(headers[""x-ratelimit-remaining-requests""]),
    remainingTokens: parseInt(headers[""x-ratelimit-remaining-tokens""]),
    ...
  };
in v4, however, const chatCompletion = await openai.chat.completions.create({...}); doesn't include headers, nor there seems to be a property with the related data (unless I missed it somewhere in the codebase).
It would be great to have it in v4, similar to chatCompletion.usage for example.
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/321","String-based chunks in Stream object pose challenges for server-side implementation","2023-09-21T22:30:54Z","Closed issue","No label","Confirm this is a feature request for the Node library and not the underlying OpenAI API.
 This is a feature request for the Node library
I'm implementing a server-side request to the OpenAI endpoint, following OpenAI's strong recommendation not to expose API keys on the client-side. While doing so, I've noticed that the Stream object returned from the endpoint contains chunks in string form, rather than byte-like data.
This design choice complicates the process of forwarding the server response to the client. Specifically, it adds complexity when attempting to use TransformStream, as the string-based chunks require an extra encoding step to convert them into byte-like data suitable for network transmission.
Could you clarify the reasoning behind delivering the stream chunks as strings? Knowing this could help simplify the process of securely transmitting data from the server to the client, in line with OpenAI's security recommendations.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/320","Type for finish_reason should include 'content_filter'","2023-09-21T02:08:13Z","Closed issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
Based on the docs at https://platform.openai.com/docs/guides/gpt/chat-completions-api, finish_reason can include:
stop: API returned complete message, or a message terminated by one of the [stop](https://platform.openai.com/docs/api-reference/chat/create#chat/create-stop) sequences provided via the stop parameter

length: Incomplete model output due to [max_tokens](https://platform.openai.com/docs/api-reference/chat/create#chat/create-max_tokens) parameter or token limit

function_call: The model decided to call a function

content_filter: Omitted content due to a flag from our content filters

null: API response still in progress or incomplete 

Indeed Azure's API will return content_filter frequently.
The types at 
openai-node/src/resources/chat/completions.ts
 Line 81 in 0386937
	finish_reason: 'stop'|'length'|'function_call';
 should include content_filter.
To Reproduce
Read docs
Make calls to Azure
Write code to handle content_filter
Observe type errors
Code snippets
No response
OS
macOS
Node version
Node v20.5.1
Library version
openai v4.0.1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/319","openai.createTranscription is not a function.","2023-09-18T01:52:44Z","Closed as not planned issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
file:///C:/scambaiting/app.mjs:200
 const transcript = await openai.createTranscription(fs.createReadStream(filename), 'whisper-1');
 ^
TypeError: openai.createTranscription is not a function
 at transcribeAudio (file:///C:/scambaiting/app.mjs:200:34)
 at main (file:///C:/scambaiting/app.mjs:209:30)
To Reproduce
const transcript = await openai.createTranscription(fs.createReadStream(filename), 'whisper-1');
this line returns the error,
However, this works:
const completion = await openai.chat.completions.create({
 messages: [{ role: 'user', content: 'Tell me a random color.' }],
 model: 'gpt-3.5-turbo',
 });
 console.log(completion.choices); // prints out like ""Indigo"" or ""Sea Foam Green""
Code snippets
import OpenAI from 'openai';

const openai = new OpenAI({
	apiKey: 'hehe_xd',});

async function transcribeAudio(filename) {
	
	const transcript = await openai.createTranscription(fs.createReadStream(filename), 'whisper-1');

	return transcript.data.text;}
OS
win10
Node version
Node.js v19.5.0
Library version
latest
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/318","Estimate token usage + dry run option","2023-09-17T21:02:07Z","Open issue","enhancement","Confirm this is a feature request for the Node library and not the underlying OpenAI API.
 This is a feature request for the Node library
Describe the feature or improvement you're requesting
There a couple nodejs tokenizers laying around npm land:
gpt-tokenizer
tiktoken
However, they have to be kept maintained and up to date with the official tiktoken. It feels very natural that such a tool be apart of this repository and updated regularly by the openai team. I feel that pre-request token estimation is vital to ensuring hobbyist developers don't unknowingly succumb to financial demise.
Why apart of this library?
 Take function calling for example:
    const response = await openai.chat.completions.create({
        model: ""gpt-3.5-turbo"",
        messages: messages,
        functions: functions,
        function_call: ""auto"",  // auto is default, but we'll be explicit
    });

I could intercept this function call above to view what data is sent to chatGPT so I can figure out how many tokens this call takes with the message + the functions, pipe them into gpt-tokenizer and see how many tokens are used. But if it were to be built into the API itself then you can have a dry run option on your nodejs api functions that estimate cost and then the developer could determine in the logic if they want to proceed with the actual run.
What else could be very helpful would be to include some parameter whether in the configuration or when calling api functions such that if the tokens and/or usd cost exceeds a certain limit the call auto fails and doesn't actually get executed.
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
👍12
spence, MiniSuperDev, brenc, gareth3wm, tolbri, Macil, eugene-kim, ngryman, LouisVA, NatoBoram, and 2 more reacted with thumbs up emoji
All reactions
👍12 reactions"
"https://github.com/openai/openai-node/issues/317","dalleRoutes.js (to fetch images from openapi)","2023-09-17T18:53:26Z","Closed as not planned issue","No label","dalleRoutes.js (to fetch images from openapi)
import express from 'express';
 import * as dotenv from 'dotenv';
 import { OpenAI } from 'openai';
import Post from '../mongodb/models/post.js';
dotenv.config();
const router = express.Router();
const openai = new OpenAI({
 apiKey: process.env.OPENAI_API_KEY,
 })
router.route('/').get((req, res) => {
 res.send(""Dall-e says 'HII'"");
 });
router.route('/').post(async (req, res) => {
 try {
 const { prompt } = req.body;
    const aiResponse = await openai.createImage({
        prompt,
        n: 1,
        size: '1024x1024',
        response_format: 'b64_json',
    });

    const image = aiResponse.data.data[0].b64_json;

    res.status(200).json({ photo: image });

} catch (error) {
    res.status(500);
    console.log('Error in getting the prompt', error);
}

})
export default router;
postRoutes.js (to post on homepage)
import express from 'express';
 import * as dotenv from 'dotenv';
 import { v2 as cloudinary } from 'cloudinary';
import Post from '../mongodb/models/post.js';
dotenv.config();
const router = express.Router();
cloudinary.config({
 cloud_name: process.env.CLOUDINARY_NAME,
 api_key: process.env.CLOUDINARY_API_KEY,
 api_secret: process.env.CLOUDINARY_API_SECRET,
 });
//GET ALL POSTS
 router.route('/').get(async (req, res) => {
 try {
 const posts = await Post.find({});
    res.status(200).json({ success: true, data: posts })
} catch (error) {
    res.status(500).json({ success: false, message: error })

}

});
//CREATE A POST
 router.route('/').post(async (req, res) => {
 try {
 const { name, prompt, photo } = req.body;
 const photoUrl = await cloudinary.uploader.upload(photo);
    const newPost = await Post.create({
        name,
        prompt,
        photo: photoUrl.url,
    })

    res.status(201).json({ success: true, data: newPost });
} catch (e) {
    res.status(500).json({ success: false, message: error });
}

});
export default router;
I'm getting this error:
 Error in getting the prompt TypeError: openai.createImage is not a function
 at file:///E:/React_projects/DreamVerse/backend/routes/dalleRoutes.js:23:41..
 Pls help!!!
Originally posted by @Darshan2923 in #217 (comment)
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/309","Discrepancy in OpenAI npm Library Results Compared to Postman API and Web UI","2023-09-14T06:06:09Z","Closed issue","No label","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
The npm library for OpenAI is providing incorrect, random and irrelavant results, but when I make an API call using Postman, it works, and the OpenAI Web UI also returns the correct results.
I have asked this question on the developer forum and it seems no one knows the reason for sure.
Input & output:
Limited Text: Bitcoin: A Peer-to-Peer Electronic Cash System Satoshi Nakamoto satoshin@gmx.comwww.bitcoin.org Abstract. A purely peer-to-peer version of electronic cash would allow online payments to be sent directly from one party to another without going through a financial institution. Digital signatures provide part of the solution, but the main benefits are lost if a trusted third party is still required to prevent double-spending. We propose a solution to the double-spending problem using a peer-to-peer network. The network timestamps transactions by hashing them into an ongoing chain of hash-based
 PDF file saved successfully.
 [
 {
 index: 0,
 message: {
 role: 'assistant',
 content: 'The text highlights the importance of education and the positive impact it has on individuals and society. It emphasizes the need for quality education that fosters critical thinking, creativity, and problem-solving skills. The author believes that education is not limited to formal schooling but also encompasses lifelong learning and personal development. Furthermore, the text emphasizes the importance of equal access to education for all individuals, regardless of their socio-economic background. Ultimately, the author argues that investing in education is crucial for building a strong and prosperous society.'
 },
 finish_reason: 'stop'
 }
With the same input, the results are inconsistent.
[
 {
 index: 0,
 message: {
 role: 'assistant',
 content: ""I'm sorry, but I cannot provide personal information or specific details about individuals without their consent.""
 },
 finish_reason: 'stop'
 }
 ]
 [
 {
 index: 0,
 message: {
 role: 'assistant',
 content: 'An exchange-traded fund (ETF) is a type of investment fund and exchange-traded product, with shares that are tradeable on a stock exchange. ETFs are similar to mutual funds, but trade like a stock on a stock exchange. They are designed to track the performance of a specific index, sector, commodity, or asset class.\n' +
 '\n' +
 'The main advantages of ETFs are their low expense ratios compared to mutual funds, as well as their liquidity due to being traded on an exchange. They also provide diversification, as they typically hold a basket of different securities. ETFs can be bought and sold throughout the trading day at market prices, making them more flexible than mutual funds, which are priced at the end of the trading day.\n' +
 '\n' +
 'Investors can choose from a wide range of ETFs, offering exposure to various asset classes such as stocks, bonds, commodities, or even alternative investments like real estate or cryptocurrencies. Some ETFs also offer leveraged or inverse exposure, which can magnify gains or losses.\n' +
 '\n' +
 'Investing in ETFs can be done through a brokerage account, similar to buying and selling stocks. The investor can choose the ETF they want to invest in and place orders to buy or sell the shares at the current market price.\n' +
 '\n' +
 ""However, it's important for investors to carefully consider the fees, underlying holdings, and performance history of an ETF before investing. Additionally, investors should be aware of the risks associated with any particular ETF and understand their own investment goals and risk tolerance.\n"" +
 '\n' +
 'Overall, ETFs have gained popularity among investors due to their low costs, diversification, and flexibility. They offer a convenient and efficient way to gain exposure to various investment opportunities and asset classes.'
 },
 finish_reason: 'stop'
 }
 ]
Code:
// Send the limited text to ChatGPT
  const completion = await openai.chat.completions.create({
    messages: [{ role: 'user', content:  limitedText }], //""summary this text:"" +
    model: 'gpt-3.5-turbo',
  });

  console.log(completion.choices);

The API returns expected results:
curl https://api.openai.com/v1/chat/completions \
  -H ""Content-Type: application/json"" \
  -H ""Authorization: Bearer sk-xxxx"" \
  -d '{
     ""model"": ""gpt-3.5-turbo"",
     ""messages"": [{""role"": ""user"", ""content"": ""summary: A Peer-to-Peer Electronic Cash System  Satoshi Nakamoto  satoshin@gmx.com  www.bitcoin.org  Abstract.   A purely peer-to-peer version of electronic cash would allow online  payments to be sent directly from one party to another without going through a  financial institution.   Digital signatures provide part of the solution, but the main  benefits are lost if a trusted third party is still required to prevent double-spending.  We propose a solution to the double-spending problem using a peer-to-peer network.  The network timestamps transactions by hashing them into an ongoing chain of  hash-based""}],
     ""temperature"": 0.7
   }'

output:
{
    ""id"": ""chatcmpl-7yLuSNQUnPe9ecNAKu9WC0nNCCwNR"",
    ""object"": ""chat.completion"",
    ""created"": 1694618256,
    ""model"": ""gpt-3.5-turbo-0613"",
    ""choices"": [
        {
            ""index"": 0,
            ""message"": {
                ""role"": ""assistant"",
                ""content"": ""summary: The paper discusses the need for a peer-to-peer electronic cash system that allows online payments to be sent directly from one party to another without the need for a financial institution. It suggests that digital signatures can partially solve the problem, but a trusted third party is still needed to prevent double-spending. The paper proposes a solution to the double-spending problem using a peer-to-peer network that timestamps transactions by hashing them into a chain of hash-based timestamps. The paper was written by Satoshi Nakamoto and provides a foundation for the development of Bitcoin.""
            },
            ""finish_reason"": ""stop""
        }
    ],
    ""usage"": {
        ""prompt_tokens"": 132,
        ""completion_tokens"": 110,
        ""total_tokens"": 242
    }
}

To Reproduce
The input is basically a random paper.
Use the official example
Code snippets
No response
OS
macOS
Node version
v18.17.1
Library version
openai@4.6.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/308","Fine tune request is being deprecated","2023-09-13T20:37:09Z","Closed as not planned issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
Hi, I have been trying to create the fine tune request for the GPT-3.5-turbo. I managed to successfully sent my file and received the id of my file from OpenAI. But when I used that file id to start the fine tune request, it returned an error that stated
 ""gpt-3.5-turbo can only be fine-tuned on the new fine-tuning API. This API is being deprecated. Please refer to our documentation: https://platform.openai.com/docs/api-reference/fine-tuning""
To Reproduce
Just use the fine tune request that is available on the newest version of the API
Code snippets
public async sendFineTuneRequest(file: Express.Multer.File) {
 try {
 console.log(file)
 let tmpFilePath = path.join(os.tmpdir(), file.originalname)
 await fsPromises.writeFile(tmpFilePath, file.buffer)
  let stream = fs.createReadStream(tmpFilePath)
  const sendFile = await this.openAI.files.create({
    file: stream,
    purpose: 'fine-tune',
  })
  await fsPromises.unlink(tmpFilePath)
  console.log(JSON.stringify(sendFile))
  const fineTune = await this.openAI.fineTunes.create({
    training_file: sendFile.id,
    model: 'gpt-3.5-turbo',
  })
  console.log(fineTune, 'fineTune')
  return '123'
} catch (error) {
  Logger.error('fine tune request failed')
  throw error
}

No response
OS
Windows 11
Node version
Node v16.13.1
Library version
openai 4.6.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/305","finish_reason is ""stop"" for function call case","2023-09-10T23:37:05Z","Closed as not planned issue","bug,openai api","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
When explicitly passing in a function to use (i.e. function_call: { name: 'categorizeItem' }) the finish reason is no longer 'function_call' but rather 'stop'. This is a little hard to work around because I'm using logic that checks the finish reason for the chat completion.
It makes sense to me that the stop reason should be ""function_call"" in this case (since it's returning a function).
To Reproduce
Make a request with function_call auto
Make a request with an explicit function_call name
Code snippets
Request:
{
    ""model"": ""gpt-4"",
    ""messages"": [
        {
            ""role"": ""system"",
            ""content"": ""Given the following product and list of categories return the category that best matches the item:\n Product: foobar\n Categories: ['foo', 'bar']\n""
        }
    ],
    ""temperature"": 1,
    ""function_call"": ""auto"",
    ""functions"": [
        {
            ""name"": ""categorizeItem"",
            ""parameters"": {
                ""type"": ""object"",
                ""properties"": {
                    ""itemCategory"": {
                        ""type"": ""string"",
                        ""description"": ""The category that best matches the item""
                    }
                },
                ""required"": [
                    ""itemCategory""
                ]
            },
            ""description"": ""Apply the correct category to the item. If no category matches the item, return 'invalid'""
        }
    ]
},
Response with function_call: 'auto'
{
    ""id"": ""chatcmpl-7xNmVb9IjgZVgjJltviPAj7gWLRQh"",
    ""object"": ""chat.completion"",
    ""created"": 1694387123,
    ""model"": ""gpt-4-0613"",
    ""choices"": [
        {
            ""index"": 0,
            ""message"": {
                ""role"": ""assistant"",
                ""content"": null,
                ""function_call"": {
                    ""name"": ""categorizeItem"",
                    ""arguments"": ""{\n\""itemCategory\"": \""foo\""\n}""
                }
            },
            ""finish_reason"": ""function_call"" // finish reason for function_call = auto
        }
    ],
    ""usage"": {
        ""prompt_tokens"": 258,
        ""completion_tokens"": 19,
        ""total_tokens"": 277
    }
},
Response with function_call: { name: 'categorizeItem' }
{
    ""id"": ""chatcmpl-7xNrU1RzsJswDCCN3XXi3sljxesLY"",
    ""object"": ""chat.completion"",
    ""created"": 1694387432,
    ""model"": ""gpt-4-0613"",
    ""choices"": [
        {
            ""index"": 0,
            ""message"": {
                ""role"": ""assistant"",
                ""content"": null,
                ""function_call"": {
                    ""name"": ""categorizeItem"",
                    ""arguments"": ""{\n\""itemCategory\"": \""Patio, Lawn & Garden\""\n}""
                }
            },
            ""finish_reason"": ""stop"" // finish reason for explicit function name
        }
    ],
    ""usage"": {
        ""prompt_tokens"": 413,
        ""completion_tokens"": 14,
        ""total_tokens"": 427
    }
}
OS
macOs
Node version
Node V18
Library version
v4.6.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/304","ReferenceError: fetch is not defined in import OpenAI from 'openai'","2023-11-15T14:01:45Z","Closed issue","bug","Hello there! There seems to have been a few issues around this that have been resolved recently, but I'm still getting it so thought I would share just in case it's something different.
Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
When building and running the OpenAI library in our NestJS API, we can do so locally with no issue ; we can communicate with it and get responses etc. However, we have a Jest testing suite, and when trying to test, it fails with:
ReferenceError: fetch is not defined
  1 | import {Injectable} from ""@nestjs/common"";
  2 | import {ChatCompletionMessageParam} from ""openai/resources/chat"";
> 3 | import OpenAI from 'openai';

at Object. (../../../../node_modules/openai/_shims/fetch.js:8:17)
 at Object. (../../../../node_modules/openai/core.js:64:17)
 at Object. (../../../../node_modules/openai/src/index.ts:116:7)
To Reproduce
Create a NestJS API
Install the openai library via npm
Create a controller that returns a response when a message is passed through via POST
Create a test suite in Jest to test this endpoint
Error occurs
Code snippets
No response
OS
macOS
Node version
Node v18.17.1
Library version
openai 4.6.0
 The text was updated successfully, but these errors were encountered: 
👍1
gianmarcocalbi reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/openai-node/issues/303","wrong implementation of uuid algorithm","2023-09-09T20:56:41Z","Closed issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
Note: The use of any UUID generator that relies on Math.random() is strongly discouraged (including snippets featured in previous versions of this answer) for reasons best explained here. TL;DR: solutions based on Math.random() do not provide good uniqueness guarantees.
https://stackoverflow.com/questions/105034/how-do-i-create-a-guid-uuid/2117523#2117523
To Reproduce
https://github.com/openai/openai-node/blob/master/src/core.ts#L1003-L1009
Code snippets
https://github.com/openai/openai-node/blob/master/src/core.ts#L1003-L1009
OS
library code
Node version
any
Library version
4.6.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/302","process.env.DEBUG variable can have many values","2023-09-09T08:59:18Z","Open issue","enhancement","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
process.env.DEBUG variable can be used by many libraries, for example: debug npm
more details: https://github.com/debug-js/debug#windows-command-prompt-notes
in other words DEBUG environment variable can be something more complicated than just a true or undefined.
To Reproduce
https://github.com/openai/openai-node/blob/master/src/core.ts#L994-L998
DEBUG=* yarn start:dev
DEBUG=*,-openai  yarn start:dev
DEBUG=openai yarn start:dev
Code snippets
https://github.com/openai/openai-node/blob/master/src/core.ts#L994-L998

 
DEBUG=* yarn start:dev # max verbosityDEBUG=*,-openai  yarn start:dev # show all debug msgs except openaiDEBUG=openai yarn start:dev # show only openai logs
OS
macOs, Windows, Linux
Node version
any
Library version
4.6.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/297","/var/task/node_modules/openai/_shims/agent-node.mjs import issue in T3 stack with Vercel Edge function","2023-09-08T00:02:33Z","Closed as not planned issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
The SDK throws an import error with the default T3 stack config (https://create.t3.gg/) in Edge functions with the pages router.
First reported in LangChainJS, reproed by me with the SDK by itself: langchain-ai/langchainjs#2558
Here is the trace:
Error [ERR_MODULE_NOT_FOUND]: Cannot find module '/var/task/node_modules/openai/_shims/agent-node.mjs' imported from /var/task/node_modules/openai/core.mjs
    at new NodeError (node:internal/errors:405:5)
    at finalizeResolution (node:internal/modules/esm/resolve:329:11)
    at moduleResolve (node:internal/modules/esm/resolve:992:10)
    at moduleResolveWithNodePath (node:internal/modules/esm/resolve:936:12)
    at defaultResolve (node:internal/modules/esm/resolve:1178:79)
    at nextResolve (node:internal/modules/esm/loader:163:28)
    at ESMLoader.resolve (node:internal/modules/esm/loader:835:30)
    at ESMLoader.getModuleJob (node:internal/modules/esm/loader:424:18)
    at ModuleWrap.<anonymous> (node:internal/modules/esm/module_job:77:40)
    at link (node:internal/modules/esm/module_job:76:36) {
  code: 'ERR_MODULE_NOT_FOUND'
}
RequestId: 8a11b070-7a2c-4a19-8bdd-16db8dce3f14 Error: Runtime exited with error: exit status 1
Runtime.ExitError

To Reproduce
Set up an app using the T3 Stack: https://create.t3.gg/
Set up an endpoint under pages/api/route.ts
Import and use the OpenAI SDK (code below)
Deploy to Vercel
Ping the endpoint
Code snippets
import type { NextApiRequest, NextApiResponse } from 'next'import { type ClientOptions, OpenAI as OpenAIClient } from ""openai"";

type ResponseData = {
  message: string}

export default async function handler(
  req: NextApiRequest,
  res: NextApiResponse<ResponseData>) {
  const client = new OpenAIClient();
  const result = await client.chat.completions.create({
    messages: [{
      role: ""user"",
      content: ""hi"",
    }],
    model: ""gpt-3.5-turbo"",
  })
  res.status(200).json({ message: JSON.stringify(result) })}
OS
Vercel Edge function
Node version
Edge
Library version
openai v4.5.0
 The text was updated successfully, but these errors were encountered: 
👍1
pvhuwung reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/openai-node/issues/296","Module not found error on v4.5.0","2023-09-08T23:23:12Z","Closed issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
I was following the tutorial to integrate nextjs with openai and noticed that the latest release (v4.5.0) is throwing a module not found error.
./node_modules/.pnpm/openai@4.5.0/node_modules/openai/core.mjs:32:0
Module not found: Can't resolve 'openai/_shims/agent.mjs'

https://nextjs.org/docs/messages/module-not-found

Import trace for requested module:
./node_modules/.pnpm/openai@4.5.0/node_modules/openai/index.mjs
./pages/api/completion.ts
./node_modules/.pnpm/next@13.4.3_react-dom@18.2.0_react@18.2.0/node_modules/next/dist/build/webpack/loaders/next-edge-function-loader.js?absolutePagePath=%2Fhome%2Fgxara%2FPessoal%2FProjetos%2Fknowledge-tree%2Fpages%2Fapi%2Fcompletion.ts&page=%2Fapi%2Fcompletion&rootDir=%2Fhome%2Fgxara%2FPessoal%2FProjetos%2Fknowledge-tree&preferredRegion=!

Maybe this is something related to this change:
Important: I did a downgrade to version v4.4.0 and everything worked as expected.
To Reproduce
Install release v4.5.0
import OpenAI from ""openai"";
Code snippets
No response
OS
Debian
Node version
Node v18.17.1
Library version
openai v4.5.0
 The text was updated successfully, but these errors were encountered: 
👍5
waveiron, AlecRust, rychlis, lemonphresh, and shuklaalok7 reacted with thumbs up emoji
All reactions
👍5 reactions"
"https://github.com/openai/openai-node/issues/295","Support for use in front end environment instead of node","2023-09-07T18:25:30Z","Closed issue","No label","Confirm this is a feature request for the Node library and not the underlying OpenAI API.
 This is a feature request for the Node library
Describe the feature or improvement you're requesting
Could we use this in JavaScript such as React?
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/294","Premature close error thrown from fetch not handled","2023-09-09T19:42:09Z","Closed issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
when node-fetch response stream emitted an ""Premature close"" error, it will cause the process exit with an Uncaught error
Error: Premature close
     at IncomingMessage.<anonymous> (/app/node_modules/.pnpm/node-fetch@2.6.9/node_modules/node-fetch/lib/index.js:1749:18)
     at Object.onceWrapper (node:events:627:28)
     at IncomingMessage.emit (node:events:513:28)
     at IncomingMessage.emit (node:domain:489:12)
     at emitCloseNT (node:internal/streams/destroy:132:10)
     at process.processTicksAndRejections (node:internal/process/task_queues:81:21)

node-fetch throwing error:
https://github.com/node-fetch/node-fetch/blob/v2.6.9/src/index.js#L156
the stream parser should handle stream error event here:
https://github.com/openai/openai-node/blob/master/src/streaming.ts#L253
To Reproduce
When OpenAI API calling encountered an error, the process will be exited, because the stream error event is not handled
Code snippets
No response
OS
macOS
Node version
v19.2.0
Library version
openai v4.3.1
 The text was updated successfully, but these errors were encountered: 
👍1
winrey reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/openai-node/issues/293","Add utilities to make creating messages easier","2023-09-07T19:54:12Z","Closed as not planned issue","No label","Confirm this is a feature request for the Node library and not the underlying OpenAI API.
 This is a feature request for the Node library
Describe the feature or improvement you're requesting
Hey!
I use this helper utility to make it easier to build messages in the format accepted by the API. Feel free to add it centrally if others might find it useful too...
function createMessageFactory<T extends ChatCompletionMessageParam['role']>(role: T) {
  return function(strings: TemplateStringsArray, ...args: any[]) {
    let str = strings[0]
    for (let i = 0; i < args.length; i++) {
      str += args[i] + strings[i + 1]
    }
    return { role: role, content: str.trim() }
  }}

export const system = createMessageFactory('system')export const user = createMessageFactory('user')export const assistant = createMessageFactory('assistant')

// usageconst prompt = system`Do a backflip!`
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/284","FormData error","2023-09-05T02:42:55Z","Closed issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
            const openai = new OpenAI(openaiConf)
            openai.files.create({
                file: fs.createReadStream('C:\\Users\\18222\\Desktop\\workspace\\AfflatusBox\\1.jsonl'),
                purpose: ""fine-tune""
            }).then(fr=>{
                console.log(""fr"",fr);
                }）

The following problems occur：
 TypeError: Received null for ""file[fd]""; to pass null in FormData, you must use the string 'null'
To Reproduce
            const openai = new OpenAI(openaiConf)
            openai.files.create({
                file: fs.createReadStream('C:\\Users\\18222\\Desktop\\workspace\\AfflatusBox\\1.jsonl'),
                purpose: ""fine-tune""
            }).then(fr=>{
                console.log(""fr"",fr);
                }）

The following problems occur：
 TypeError: Received null for ""file[fd]""; to pass null in FormData, you must use the string 'null'
Code snippets
No response
OS
windows11
Node version
18.17.0
Library version
4.3.0
 The text was updated successfully, but these errors were encountered: 
👍2
ToldFable and abriejenny reacted with thumbs up emoji
All reactions
👍2 reactions"
"https://github.com/openai/openai-node/issues/283","throw Error when set baseUrl undefined.","2023-09-07T19:55:22Z","Closed issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
create OpenAI with baseURL = undefined ,will case error:
[TypeError: Cannot read properties of undefined (reading 'endsWith')]
To Reproduce
create
  const baseURL = OPENAI_PROXY_URL ? OPENAI_PROXY_URL : undefined;

  return new OpenAI({
    apiKey: !userApiKey ? OPENAI_API_KEY : userApiKey,
    baseURL: OPENAI_PROXY_URL ? OPENAI_PROXY_URL : undefined;
  });

Code snippets
the problem is this line: https://github.com/openai/openai-node/blob/master/src/core.ts#L404
OS
macOS 13
Node version
18
Library version
4.4.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/282","Can not change baseURL dynamically","2023-09-04T15:22:13Z","Closed as not planned issue","No label","Confirm this is a feature request for the Node library and not the underlying OpenAI API.
 This is a feature request for the Node library
Describe the feature or improvement you're requesting
How to change openAI baseURL dynamically.
I just using openai.baseURL chagne it, but takes no effect.

openaiContainer.instance = new OpenAI({
  apiKey: ""sk354t3gdgtrh"",
  baseURL: baseServerPath['pp'],
  dangerouslyAllowBrowser: true,
});

export function updateOpenAIBaseURL(newBaseURL: string) {
  baseURL = newBaseURL;
  openaiContainer.instance  = new OpenAI({
    apiKey: ""sk354t3gdgtrhjuyt7"",
    baseURL: baseURL,
    dangerouslyAllowBrowser: true,
  })
  console.log(""[updateOpenAIBaseURL] "", newBaseURL)

}

export async function updateOpenAIServerName(serverName: string) {
  if (serverName in baseServerPath) {
    baseURL = baseServerPath[serverName];
    updateOpenAIBaseURL(baseURL);
  } else {
    console.log(""can not found server in presets. "", serverName)
  }
}

export  function initializeOpenAI(serverName: string | null)  {
  if (serverName) {
    updateOpenAIServerName(serverName)
  } else {
    updateOpenAIServerName(""pp"");
  }
  console.log(""!! set new openai serverName: "", serverName)
}

export async function POST(req: Request) {
  const json = await req.json();
  const { messages, previewToken } = json;

  console.log(""openai instance: "", openaiContainer.instance);

  const res = await openaiContainer.instance!.chat.completions.create({
    model: ""gpt-3.5-turbo"",
    messages,
    temperature: 0.7,
    stream: true,
  });
 ...
}

every time POST printed are old , but clearly logged URL changed, why
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/280","I use the official website method to use gpt-3.5-turbo，report an error ""This is not a chat model and thus not supported in the v1/chat/completions endpoint. Did you mean to use v1/completions""","2023-09-02T19:57:48Z","Closed as not planned issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
I use the official website method to use gpt-3.5-turbo，report an error ""This is not a chat model and thus not supported in the v1/chat/completions endpoint. Did you mean to use v1/completions""
code is：
 curl https://api.openai.com/v1/chat/completions
 -H ""Content-Type: application/json"" 
 -H ""Authorization: Bearer $OPENAI_API_KEY"" 
 -d '{
 ""model"": ""gpt-3.5-turbo"",
 ""messages"": [{""role"": ""user"", ""content"": ""Say this is a test!""}],
 ""temperature"": 0.7
 }'
$OPENAI_API_KEY is replace my API key
To Reproduce
1.in your terminal, copy code:
 curl https://api.openai.com/v1/chat/completions
 -H ""Content-Type: application/json"" 
 -H ""Authorization: Bearer $OPENAI_API_KEY"" 
 -d '{
 ""model"": ""gpt-3.5-turbo"",
 ""messages"": [{""role"": ""user"", ""content"": ""Say this is a test!""}],
 ""temperature"": 0.7
 }'
$OPENAI_API_KEY is replace your API key
2.terminal report en error :
 {
 ""error"": {
 ""message"": ""This is not a chat model and thus not supported in the v1/chat/completions endpoint. Did you mean to use v1/completions?"",
 ""type"": ""invalid_request_error"",
 ""param"": ""model"",
 ""code"": null
 }
 }
Code snippets
curl https://api.openai.com/v1/chat/completions \
  -H ""Content-Type: application/json"" \
  -H ""Authorization: Bearer $OPENAI_API_KEY"" \
  -d '{     ""model"": ""gpt-3.5-turbo"",     ""messages"": [{""role"": ""user"", ""content"": ""Say this is a test!""}],     ""temperature"": 0.7   }'

$OPENAI_API_KEY is replace your API key
OS
macOS
Node version
Node 18
Library version
terminal
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/279","MaxListenersExceededWarning: Possible EventEmitter memory leak detected.","2023-09-07T19:57:20Z","Closed as not planned issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
when i creaing a completions with stream, I got this warning.
 what should I do?
To Reproduce
const stream: Stream<ChatCompletionChunk> =
      await this.openAI.chat.completions.create(
        {
          model: 'gpt-3.5-turbo-0613',
          max_tokens: 3000,
          stream: true,
          temperature: 1,
          user: chatRequestDto.identity,
          messages: [
            {
              role: 'user',
              content: chatRequestDto.prompt,
            },
          ],
        },
        { stream: true },
      );
    const readableStream = new Readable();
    readableStream._read = async () => {
      for await (const part of stream) {
        console.log('finish_reason:', part.choices[0].finish_reason);
        console.log('part:', part.choices[0].delta);
        if (part.choices[0].finish_reason === 'stop') {
          // stream end
          readableStream.push(null);
          break;
        }
       ...
        readableStream.push(JSON.stringify(part));
      }
    };
    return readableStream;
the warning stack trace below:
(node:1700) MaxListenersExceededWarning: Possible EventEmitter memory leak detected. 11 error listeners added to [PassThrough]. Use emitter.setMaxListeners() to increase limit
(Use `node --trace-warnings ...` to show where the warning was created)
MaxListenersExceededWarning: Possible EventEmitter memory leak detected. 11 error listeners added to [PassThrough]. Use emitter.setMaxListeners() to increase limit
    at _addListener (node:events:588:17)
    at PassThrough.addListener (node:events:606:10)
    at PassThrough.Readable.on (node:internal/streams/readable:887:35)
    at eos (node:internal/streams/end-of-stream:199:12)
    at createAsyncIterator (node:internal/streams/readable:1114:19)
    at createAsyncIterator.next (<anonymous>)
    at Stream.iterMessages (/develop/node_modules/openai/src/streaming.ts:26:16)
    at iterMessages.next (<anonymous>)
    at Stream.[Symbol.asyncIterator] (/develop/node_modules/openai/src/streaming.ts:40:16)
Code snippets
No response
OS
macOS
Node version
Node v16.4.2
Library version
openai v3.0.1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/278","Add reverse proxy","2023-09-02T20:41:23Z","Closed issue","No label","Confirm this is a feature request for the Node library and not the underlying OpenAI API.
 This is a feature request for the Node library
Describe the feature or improvement you're requesting
I set up a reverse proxy for openai. But this lib can only access official api address. Could you plan to add custom url?
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/271","Chat completion request streaming","2023-08-31T09:05:05Z","Open issue","enhancement","Confirm this is a feature request for the Node library and not the underlying OpenAI API.
 This is a feature request for the Node library
Describe the feature or improvement you're requesting
It would be great to have an additional overload for chat.completions.create that can be used to stream requests.
In the most basic form it could be something like:
create(
    body: ReadableStream<string>,
    options?: Core.RequestOptions,): APIPromise<Stream<ChatCompletionChunk>>;
However, passing the completion parameters in the stream may be cumbersome – and, since the point of streaming only makes sense when having big message contexts, a better option could be to use the stream only for the messages:
create(
    body: Omit<CompletionCreateParamsStreaming, ""messages"">,
    messages: ReadableStream<string>,
    options?: Core.RequestOptions,): APIPromise<Stream<ChatCompletionChunk>>;
Usage example (of second approach)
// getMessageHistory makes a call to a database and returns a `ReadableStream<string>`// The content of the stream is a JSON array of `CreateChatCompletionRequestMessage`const stream = await getMessageHistory();

const params = {
  model: 'gpt-3.5-turbo',
  stream: true}

const res = await openai.chat.completions.create(params, stream);
Additional context
This feature would be particularly useful in Edge environments.
 The text was updated successfully, but these errors were encountered: 
👍1
jonschlinkert reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/openai-node/issues/270","Im not sure why but I keep getting this error.","2023-08-31T15:35:35Z","Closed issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
I keep getting this error, but I have already consulted the documentation to ensure that my code is correct. I have the most up-to-date version of openai installed through npm.

To Reproduce
I am using expo react native. The dataURI is the path to the audio recording saved in m4a format.
Code snippets
No response
OS
pop os linux
Node version
Node v18.16.0
Library version
openai ^4.3.1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/267","Typescript version below 4.5 not supported","2023-08-30T19:51:34Z","Closed issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
Hey there,
as we tried to update to the 4.x version of openai-node we got build issues that looked like this
[6:00:31 PM] Building project '/Users/derN3rd/git/openai-playground/tsconfig.json'...

node_modules/openai/core.d.ts:3:15 - error TS1005: ',' expected.

3 import { type Agent } from 'openai/_shims/agent';
                ~~~~~

node_modules/openai/core.d.ts:4:15 - error TS1005: ',' expected.

4 import { type RequestInfo, type RequestInit, type Response, type HeadersInit } from 'openai/_shims/fetch';
                ~~~~~~~~~~~

node_modules/openai/core.d.ts:4:33 - error TS1005: ',' expected.

4 import { type RequestInfo, type RequestInit, type Response, type HeadersInit } from 'openai/_shims/fetch';
                                  ~~~~~~~~~~~

node_modules/openai/core.d.ts:4:51 - error TS1005: ',' expected.

4 import { type RequestInfo, type RequestInit, type Response, type HeadersInit } from 'openai/_shims/fetch';
                                                    ~~~~~~~~

node_modules/openai/core.d.ts:4:66 - error TS1005: ',' expected.

4 import { type RequestInfo, type RequestInit, type Response, type HeadersInit } from 'openai/_shims/fetch';
                                                                   ~~~~~~~~~~~

node_modules/openai/core.d.ts:5:15 - error TS1005: ',' expected.

5 export { type Response };
                ~~~~~~~~

node_modules/openai/core.d.ts:10:8 - error TS1005: ',' expected.

10   type Uploadable,
          ~~~~~~~~~~

node_modules/openai/resources/audio/transcriptions.d.ts:4:15 - error TS1005: ',' expected.

4 import { type Uploadable } from 'openai/core';
                ~~~~~~~~~~

node_modules/openai/resources/audio/translations.d.ts:4:15 - error TS1005: ',' expected.

4 import { type Uploadable } from 'openai/core';
                ~~~~~~~~~~

node_modules/openai/resources/files.d.ts:4:15 - error TS1005: ',' expected.

4 import { type Uploadable } from 'openai/core';
                ~~~~~~~~~~

node_modules/openai/resources/images.d.ts:4:15 - error TS1005: ',' expected.

4 import { type Uploadable } from 'openai/core';
                ~~~~~~~~~~

node_modules/openai/uploads.d.ts:1:15 - error TS1005: ',' expected.

1 import { type RequestOptions } from './core.js';
                ~~~~~~~~~~~~~~

node_modules/openai/uploads.d.ts:2:15 - error TS1005: ',' expected.

2 import { type Readable } from 'openai/_shims/node-readable';
                ~~~~~~~~

node_modules/openai/uploads.d.ts:3:15 - error TS1005: ',' expected.

3 import { type BodyInit } from 'openai/_shims/fetch';
                ~~~~~~~~

node_modules/openai/uploads.d.ts:4:25 - error TS1005: ',' expected.

4 import { FormData, type Blob, type FilePropertyBag } from 'openai/_shims/formdata';
                          ~~~~

node_modules/openai/uploads.d.ts:4:36 - error TS1005: ',' expected.

4 import { FormData, type Blob, type FilePropertyBag } from 'openai/_shims/formdata';
                                     ~~~~~~~~~~~~~~~

node_modules/openai/uploads.d.ts:6:15 - error TS1005: ',' expected.

6 import { type FsReadStream } from 'openai/_shims/node-readable';


After trying out different node versions, we figured out that the imports in these files are only supported since Typescript 4.5.
 Maybe this should be added to the Requirements in the readme.
This can be closed, just wanted to put it here in case someone has the same issue.
Thanks in advance
To Reproduce
Install Typescript 4.4
install the openai module
use some example code
try to build with tsc
Error appears
Code snippets
No response
OS
macOS
Node version
Node v16.17.0
Library version
openai 4.3.1
 The text was updated successfully, but these errors were encountered: 
👍2
bencmbrook and dgoemans reacted with thumbs up emoji
All reactions
👍2 reactions"
"https://github.com/openai/openai-node/issues/265","Function call not respecting json schema","2023-08-29T18:36:59Z","Closed as not planned issue","bug,openai api","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
Function calling doesn't always respect JSON schema, and outputs invalid JSON.
 In this particular example it misses the array brackets and commas between the objects.
 Sometimes it doesn't respect the schema at all and answers in natural language as it would via the chatgpt web interface.
To Reproduce
const schema = {
  type: 'object',
  properties: {
    title: {
      type: 'string',
    },
    author: {
      type: 'string',
    },
    description: {
      type: 'string',
    },
  },
  required: ['title', 'author', 'description'],};

const response = await ai.chat.completions.create({
    model: 'gpt-3.5-turbo-0613',
    max_tokens: 2000,
    temperature: 0,
    messages: [
      {
        role: 'system',
        content:
          'You are a helpful assistant. You are very knowledgeable about books and literature. Your job is to help people find books to read. Your answers are in the same language as the users input. Give at least 5 book recommendations.',
      },
      {
        role: 'user',
        content:
          ""Search for books similar to Suzanne Collins' 'The Hunger Games', that combine adventure with emotional depth"",
      },
    ],
    functions: [
      {
        name: 'print',
        description: 'Prints an array with book titles',
        parameters: schema,
      },
    ],
  });
Returns the following invalid json argument (it's missing square brackets for the array and commas between the objects)
{
  ""id"": ""chatcmpl-7r8DvjLBoAnTEc41VDe1UJwci1uNk"",
  ""object"": ""chat.completion"",
  ""created"": 1692897351,
  ""model"": ""gpt-3.5-turbo-0613"",
  ""choices"": [
    {
      ""index"": 0,
      ""message"": {
        ""role"": ""assistant"",
        ""content"": null,
        ""function_call"": {
          ""name"": ""print"",
          ""arguments"": ""{\n  \""title\"": \""Divergent\"",\n  \""author\"": \""Veronica Roth\"",\n  \""description\"": \""In a dystopian Chicago, society is divided into five factions based on personality traits. Beatrice Prior must navigate her way through initiation and uncover a dangerous secret that could threaten her world.\""\n},\n{\n  \""title\"": \""The Maze Runner\"",\n  \""author\"": \""James Dashner\"",\n  \""description\"": \""Thomas wakes up in a maze with no memory of who he is or how he got there. Along with a group of other boys, he must find a way to escape the deadly maze and uncover the truth about their existence.\""\n},\n{\n  \""title\"": \""The Giver\"",\n  \""author\"": \""Lois Lowry\"",\n  \""description\"": \""In a seemingly perfect society, Jonas is chosen to be the Receiver of Memories and discovers the dark secrets behind his community's utopian facade.\""\n},\n{\n  \""title\"": \""The Selection\"",\n  \""author\"": \""Kiera Cass\"",\n  \""description\"": \""In a future where society is divided into castes, America Singer is chosen to compete in a televised competition to win the heart of Prince Maxon and become the next queen.\""\n},\n{\n  \""title\"": \""Legend\"",\n  \""author\"": \""Marie Lu\"",\n  \""description\"": \""In a future where the United States is divided into two warring factions, June, a prodigy from the elite class, and Day, a wanted criminal from the slums, form an unlikely alliance to uncover the truth about their government.\""\n}""
        }
      },
      ""finish_reason"": ""function_call""
    }
  ],
  ""usage"": {
    ""prompt_tokens"": 118,
    ""completion_tokens"": 321,
    ""total_tokens"": 439
  }
}
SyntaxError: Unexpected token , in JSON at position 284
    at JSON.parse (<anonymous>)

Code snippets
No response
OS
macOS / linux server (node:lts-alpine)
Node version
Node v16
Library version
v3.3.0 - v4
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/264","No listener for endpoint: /chat/completions","2023-08-29T18:36:01Z","Closed issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
Hi there, I am trying to use openai under a custom proxy that perfectly works in python
import base64import osimport openaiopenai.api_type = ""azure""openai.api_base = ""https://test.capi.foo.com/z/baa/s/openai-proxy/v1/dev""openai.api_version = ""2023-05-15""

client_key = ""asddsaads""client_secret = ""asddasdas""

id_secret_bytes = client_key + ':' + client_secretencoded_u = base64.b64encode(id_secret_bytes.encode()).decode()

openai.api_key = encoded_u
This works, if I try to use the same API_KEY and js
import ""dotenv/config"";

config({ path: "".env.local"" });

import OpenAI from ""openai"";import { OpenAIStream, StreamingTextResponse } from ""ai"";import { config } from ""dotenv"";

const apiKey = process.env.OPENAI_API_KEY;

const openai = new OpenAI({
  apiKey,
  baseURL: process.env.OPENAI_API_BASE,
  defaultQuery: {
    ""api-version"": process.env.OPENAI_API_VERSION,
    ""api-type"": ""azure"",
    engine: ""gpt-35-turbo"",
  },
  defaultHeaders: { ""api-key"": apiKey },});

// Extract the `messages` from the body of the requestconst messages = [
  {
    role: ""assistant"",
    content: ""You are an helpful AI assistant"",
  },
  {
    role: ""user"",
    content: ""Hi"",
  },];

// Ask OpenAI for a streaming chat completion given the promptconst response = await openai.chat.completions.create({
  model: ""gpt-35-turbo"",
  stream: false,
  messages,});console.log(response);
Got
 return new NotFoundError(status, error, message, headers);
             ^

NotFoundError: No listener for endpoint: /chat/completions
    at APIError.generate (file:///Users/FRANCESCO.ZUPPICHINI/Documents/ai-callcenter/node_modules/.pnpm/openai@4.3.1/node_modules/openai/error.mjs:40:14)
    at OpenAI.makeStatusError (file:///Users/FRANCESCO.ZUPPICHINI/Documents/ai-callcenter/node_modules/.pnpm/openai@4.3.1/node_modules/openai/core.mjs:263:21)
    at OpenAI.makeRequest (file:///Users/FRANCESCO.ZUPPICHINI/Documents/ai-callcenter/node_modules/.pnpm/openai@4.3.1/node_modules/openai/core.mjs:303:24)
    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)
    at async file:///Users/FRANCESCO.ZUPPICHINI/Documents/ai-callcenter/openai_test.mjs:35:18 {
  status: 404,
  headers: {
    connection: 'keep-alive',
    'content-encoding': 'gzip',
    'content-type': 'text/plain',
    date: 'Tue, 29 Aug 2023 12:47:04 GMT',
    'set-cookie': 'visid_incap_2400092=KVo4zJAxTxa6rYkESpXuCMjo7WQAAAAAQUIPAAAAAACxdwEKlhJWCX84EuNd5v87; expires=Tue, 27 Aug 2024 22:52:20 GMT; HttpOnly; path=/; Domain=.capi.zurich.com, nlbi_2400092=rS+SYnhz6SBgo0oSGFnVPwAAAADI7QuUEJlvqfawu0B7wAl0; path=/; Domain=.capi.zurich.com, incap_ses_184_2400092=D0pXK1qNkDgXDMPCwLONAsjo7WQAAAAAxqnHEBN3OkZhRmjzZDs+BQ==; path=/; Domain=.capi.zurich.com',
    'strict-transport-security': 'max-age=31536000; includeSubdomains;',
    'transfer-encoding': 'chunked',
    'x-cdn': 'Imperva',
    'x-iinfo': '13-133608846-133608863 NNYN CT(48 45 0) RT(1693313224096 39) q(0 0 1 -1) r(1 1) U6'
  },
  error: undefined,
  code: undefined,
  param: undefined,
  type: undefined
}

Node.js v20.3.0
Thanks a lot
To Reproduce
Impossible since you don't have access to our endpoints :(
Code snippets
No response
OS
maxOS
Node version
Node v20.3.0
Library version
latest
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/261","Library Warning About 'encoding' Module Not Existing Without Listing it in 'peerDependencies'","2023-08-28T18:58:38Z","Closed issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
I'm using a rather simple NextJS app to have an API layer to communicate with the OpenAI API using the openai npm package. After upgrading to the latest version to start using the built-in streaming capability (thanks for that BTW!) I'm seeing warnings about encoding not being able to resolve. I'm unsure if this is a problem with the OpenAI package not listing encoding as a peerDependency or a regular dependency, or maybe it's a problem with node-fetch not doing so. Either way, I ask that you kindly try to see if this is resolvable by adding it to your library as a peerDependency.
- warn ./node_modules/.pnpm/node-fetch@2.7.0/node_modules/node-fetch/lib/index.js
Module not found: Can't resolve 'encoding' in '/Users/giladsher/repos/zoomin-gpt-api/node_modules/.pnpm/node-fetch@2.7.0/node_modules/node-fetch/lib'

If I install the encoding package the warning goes away as expected, but this might be easily solvable without a manual installation and dependency management.
To Reproduce
In a NextJS app create a route.ts file that gets a prompt. Use the code snippet below to generate the completion and return it to the user as a stream. (I use the https://www.npmjs.com/package/ai package to get some easier-to-use abstractions).
openAI.chat.completions.create(
        {
          model: 'gpt-4',
          messages: [{ role: 'user', content: prompt }],
          temperature: 0,
          top_p: 1,
          n: 1,
          frequency_penalty: 0.0,
          presence_penalty: 0.0,
          stream: true,
        },
      );
See that you get the aforementioned warning about encoding module not being able to resolve
Code snippets
openAI.chat.completions.create(
        {
          model: 'gpt-4',
          messages: [{ role: 'user', content: prompt }],
          temperature: 0,
          top_p: 1,
          n: 1,
          frequency_penalty: 0.0,
          presence_penalty: 0.0,
          stream: true,
        },
      );
OS
macOS
Node version
v18.17.1
Library version
v4.3.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/258","Azure API endpoint support with openai ^4.3.0","2023-08-28T18:51:57Z","Closed issue","No label","Confirm this is a feature request for the Node library and not the underlying OpenAI API.
 This is a feature request for the Node library
Describe the feature or improvement you're requesting
I just ran into a 404 Resource not found error.
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/257","When streaming response of usage is missing","2023-08-27T18:21:14Z","Closed issue","bug,openai api","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
This is the response i should be getting where usage is part of the response.
{
  ""id"": ""chatcmpl-123"",
  ""object"": ""chat.completion"",
  ""created"": 1677652288,
  ""model"": ""gpt-3.5-turbo-0613"",
  ""choices"": [{
    ""index"": 0,
    ""message"": {
      ""role"": ""assistant"",
      ""content"": ""\n\nHello there, how may I assist you today?"",
    },
    ""finish_reason"": ""stop""
  }],
  ""usage"": {
    ""prompt_tokens"": 9,
    ""completion_tokens"": 12,
    ""total_tokens"": 21
  }
}


When i am doing streams, i do not get any indicator of usage. either part of it, which i need to calculate, or a way i track it.
Use case:
we are building a tool, and to calculate costs we are looking at total tokens per resource generated for internal tracking.
To Reproduce
Make a request for createCompletion
use streams
check response
Code snippets
const res = await openai.chat.completions.create(
            {
                model: model,
                messages,
                max_tokens: tokens,
                temperature: 0.2,
                stream: stream,
            },
            { responseType: stream ? 'stream' : 'json' }
        )
        if (stream) {
            for await (const part of res) {
                let msg = part.choices[0]?.delta?.content || ''
            }
        }


### OS

mac

### Node version

18.6

### Library version

4

 The text was updated successfully, but these errors were encountered: 
👍1
MiniSuperDev reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/openai-node/issues/251","Docs say to use stream.controller.abort() but controller is a private property","2023-08-25T14:02:05Z","Closed issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
Docs say to use stream.controller.abort() but controller is a private property of Stream.
Docs:
If you need to cancel a stream, you can break from the loop or call stream.controller.abort().

Library code:


To Reproduce
Install the node library.
Create a stream with the library. (import { Stream } from 'openai/streaming';)
Call stream.controller.abort()
You will see an IDE error telling you that controller is a private property. This is also reflected in both the interface and the implementation shown in the screenshots.
Code snippets
No response
OS
macOS
Node version
v20.1.0
Library version
openai v4.2.0
 The text was updated successfully, but these errors were encountered: 
👍1
starknt reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/openai-node/issues/250","Missing types Stream","2023-08-25T11:28:42Z","Closed issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
Hey,
the library is not exposing the type Stream which is used when specifying the param stream: true.
 It's mostly alright when using the response directly but when we want to pass the response around in a strongly typed codebase it's a bit cumbersome to write Awaited<ReturnType<typeof openai.chat.completions.create>>
I suppose it's more relevant for @ Stainless
To Reproduce
export async function aiRewrite(text: string): 
  Promise<Awaited<ReturnType<typeof openai.chat.completions.create>>> {
  const res = await openai.chat.completions.create({ ... });
  return res;}
Code snippets
I understood, after I made the fix, that the codebase is generated, but here is what a fix could look like
bodinsamuel/openai-node@
master...bodinsamuel-patch-1#diff-a2a171449d862fe29692ce031981047d7ab755ae7f84c707aef80701b3ea0c80
OS
macOs
Node version
Library version
4.2.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/249","users get unfixable (and inaccurate) type errors when using go-to-source for openai library code","2023-09-03T17:12:38Z","Closed issue","bug","Confirm this is a library issue and not an underlying OpenAI API issue
 This is an issue with the library
Describe the bug
simply adding the new openai package to my project causes all sorts of type errors when I cmd+click on into the OpenAI source code from my project
To Reproduce
See above.
OS
macOS
Node version
n/a
Library version
4.2.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/246","Support for buffer data in audio.transcriptions.create","2023-08-24T01:55:39Z","Closed issue","No label","Confirm this is a feature request for the Node library and not the underlying OpenAI API.
 This is a feature request for the Node library
Describe the feature or improvement you're requesting
Currently you must pass a fs.ReadStream to the audio.transcriptions.create call. Rather than:
 const transcription = await openai.audio.transcriptions.create({ file: fs.createReadStream(""audio.mp3""), model: ""whisper-1"", });
it would be helpful to be able to pass the content as a buffer directly, such as:
 const transcription = await openai.audio.transcriptions.create({ content: [buffer], encoding: 'flac|mp3|mp4|mpeg|mpga|m4a|ogg|wav|webm', model: ""whisper-1"", });
This would facilitate audio content transfer, not requiring it to exist as a file in the filesystem.
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/243","fetch is not exported from 'openai/_shims/fetch'","2023-09-03T17:11:46Z","Closed issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
I am getting the following error when trying to instantiate my OpenAI client.
- error ./node_modules/openai/core.mjs
Attempted import error: 'fetch' is not exported from 'openai/_shims/fetch' (imported as 'fetch').

To Reproduce
Instantiate openAI with the following:
import OpenAI from ""openai"";

export const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
});

Code snippets
The OpenAI definition in my package.json is ""openai"": ""^4.2.0""
My tsconfig.json file:
{
  ""compilerOptions"": {
    ""target"": ""es5"",
    ""lib"": [""dom"", ""dom.iterable"", ""esnext""],
    ""allowJs"": true,
    ""skipLibCheck"": true,
    ""strict"": true,
    ""forceConsistentCasingInFileNames"": true,
    ""noEmit"": true,
    ""esModuleInterop"": true,
    ""module"": ""esnext"",
    ""moduleResolution"": ""node"",
    ""resolveJsonModule"": true,
    ""isolatedModules"": true,
    ""jsx"": ""preserve"",
    ""incremental"": true,
    ""plugins"": [
      {
        ""name"": ""next""
      }
    ],
    ""paths"": {
      ""@/*"": [""./src/*""]
    }
  },
  ""include"": [""next-env.d.ts"", ""**/*.ts"", ""**/*.tsx"", "".next/types/**/*.ts""],
  ""exclude"": [""node_modules""]
}



### OS

macOS

### Node version

v20.2.0

### Library version

4.2.0

 The text was updated successfully, but these errors were encountered: 
👍2
suryasanchez and joschan21 reacted with thumbs up emoji
All reactions
👍2 reactions"
"https://github.com/openai/openai-node/issues/233","Missing status, headers in streaming response API","2023-08-22T21:58:41Z","Closed issue","No label","Confirm this is a feature request for the Node library and not the underlying OpenAI API.
 This is a feature request for the Node library
Describe the feature or improvement you're requesting
When using the streaming API, especially when debugging Azure OpenAI's support, it's helpful to be able to inspect the response status and headers. The Node 4 library forces users into a choice: streaming API with better performance, or the non-streaming API and a full response object.
This isn't ideal for debugging issues with the endpoints.
Additional context
See: #182 (comment)
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/232","chat.completions.create return null on browser","2023-08-25T20:23:46Z","Closed issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
While developing an web app on localhost
 Works like a charm with 3.6 but after upgrading to v4 and adapating the code
chat.completions.create return always null with dangerouslyAllowBrowser: true
Works well in back with the same code
To Reproduce
Use demo code in browser
Add dangerouslyAllowBrowser to initiator option
Run and get error
Code snippets
No response
OS
macOS
Node version
Node v18.16
Library version
openai v4
 The text was updated successfully, but these errors were encountered: 
👍3
piotrek-k, madwiki, and JSmithOner reacted with thumbs up emoji
All reactions
👍3 reactions"
"https://github.com/openai/openai-node/issues/231","GPT Function Documentation","2023-08-22T03:22:07Z","Closed issue","No label","Confirm this is a feature request for the Node library and not the underlying OpenAI API.
 This is a feature request for the Node library
Describe the feature or improvement you're requesting
With the new v4, can we please have a documented example for functions and type safe responses?
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/229","V4.0.1 TS Example Error","2023-08-20T08:32:09Z","Closed issue","bug","Confirm this is a Node library issue and not an underlying OpenAI API issue
 This is an issue with the Node library
Describe the bug
不能将类型“ChatCompletion | Stream”分配给类型“ChatCompletion”。
 类型“Stream”缺少类型“ChatCompletion”中的以下属性: id, choices, created, model, object

To Reproduce
find readme example
use it with ts
find error in vsc
Code snippets
import OpenAI from 'openai';

const openai = new OpenAI({
  apiKey: 'my api key', // defaults to process.env[""OPENAI_API_KEY""]});

async function main() {
  const params: OpenAI.Chat.CompletionCreateParams = {
    messages: [{ role: 'user', content: 'Say this is a test' }],
    model: 'gpt-3.5-turbo',
  };
  const completion:OpenAI.Chat.Completions.ChatCompletion = await openai.chat.completions.create(params);}

main();
OS
win10
Node version
node 18
Library version
v4.0.1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/227","V4 Type definitions not working with IntelliJ IDEA","2023-08-29T18:41:40Z","Closed issue","bug","Describe the bug
I appreciate the experiment of using nested namespacing, however in my case, it mainly becomes a nuisance.
For example, CompletionCreateParams, it can be for a chat or non-chat completion, and, it can be an interface or namespace, so that makes getting information on the fields pretty much inconvenient. For example, I cannot control-click to see what the type should contain:
bandicam.2023-08-19.11-34-27-849.mp4
In the video above, I'm trying to get to the definition of the type I'm trying to use, the chat completion request params. When I control-click, the first option takes me to the interface, but of non-chat completion.
The second option takes me to the namespace, which has the child types. I have to scroll up to get to the actual type that I want.
This way, I'm demonstrating that for me, at least, the ""nested typing system that have an identifier as possibly both a type or a parent interface"" has benefits:
looks kinda nice
disadvantages:
I cannot use it
To Reproduce
See video above, using IntelliJ IDEA. With vs code, it seems to be better.
Code snippets
No response
OS
windows
Node version
v18.17.0
Library version
^4.0.0
 The text was updated successfully, but these errors were encountered: 
👍1
Tzesar reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/openai-node/issues/226","NextJS navigator is not defined - azure v4 stream true","2023-08-21T00:45:58Z","Closed issue","bug","Describe the bug
using last v4 server side, there is an error when calling for example chat.create
here 
openai-node/src/core.ts
 Line 806 in 3ec43ee
	if(!navigator||typeofnavigator==='undefined'){
you can see code:
 if (!navigator || typeof navigator === 'undefined') {
it is checking if it is undefined ""after"" trying to use it with !navigator
To Reproduce
calling chat.completions.create with stream true. but using it on Azure from this sample
openai-node/examples/azure.ts
 Line 5 in 3ec43ee
	// The name of your Azure OpenAI Resource.
Code snippets
No response
OS
macOS
Node version
node 18
Library version
v4.0.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/225","Does v4.0.0 support Node16?","2023-08-18T23:16:06Z","Closed issue","bug","Describe the bug
Hello, nice library!
 In the README Requirements section of v4.0.0 we can see that Node16 LTS is supported but the package.json file requires ""@types/node"": ""^18.11.18"" (link here).
I haven't found problems running v4.0.0 in Node v16.20.1 so far but I'm wondering which one is correct, the requirements or the package.json file.
To Reproduce
I haven't found problems running v4.0.0 in Node v16.20.1.
Code snippets
No response
OS
macOS
Node version
Node v16.20.1
Library version
openai 4.0.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/224","Temperature=2 randomly throws error","2023-08-18T23:15:55Z","Closed issue","openai api","Describe the bug
When I set temperature=2, it throws error for 90% of times
request body:
data: '{""model"":""gpt-3.5-turbo"",""n"":1,""messages"":[{""role"":""system"",""content"":""You are an ai assistant in car marketplace website. \\n\\nHere are some instructions:\\n\\n1. If user asks to contact with human or callback, you should ask for his name and phone number or email. And reply that someone will call him as soon as possible\\n\\nYou are an ai assistant""},{""role"":""user"",""content"":""123""}],""functions"":[{""name"":""showImage"",""description"":""You should use this function to display image to user"",""parameters"":{""type"":""object"",""properties"":{""url"":{""type"":""string"",""description"":""Url of the image you want to show""}},""required"":[""url""]}}],""temperature"":2,""max_tokens"":295,""top_p"":1,""frequency_penalty"":0,""presence_penalty"":0,""stop"":[]}'
error message:
error: {
      message: 'The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID bb87b2d4944c548a8e12fada9d306040 in your email.)',
      type: 'server_error',
      param: null,
      code: null
    }

To Reproduce
Call .createChatCompletion method with the parameters from issue description
See the error
Code snippets
No response
OS
linux
Node version
Node v16.x
Library version
openai v3.3.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/223","Moderation: missing new categories","2023-08-19T08:12:02Z","Closed issue","bug","Describe the bug
Typing does not include the latest category additions as listed in the docs such as:
    | ""self-harm/intent""
    | ""self-harm/instructions""
    | ""harassment""
    | ""harassment/threatening""
See 
openai-node/src/resources/moderations.ts
 Lines 41 to 76 in 3ec43ee
	exportinterfaceCategories{
	/**
	 * Whether the content was flagged as 'hate'.
	 */
	hate: boolean;
	
	/**
	 * Whether the content was flagged as 'hate/threatening'.
	 */
	'hate/threatening': boolean;
	
	/**
	 * Whether the content was flagged as 'self-harm'.
	 */
	'self-harm': boolean;
	
	/**
	 * Whether the content was flagged as 'sexual'.
	 */
	sexual: boolean;
	
	/**
	 * Whether the content was flagged as 'sexual/minors'.
	 */
	'sexual/minors': boolean;
	
	/**
	 * Whether the content was flagged as 'violence'.
	 */
	violence: boolean;
	
	/**
	 * Whether the content was flagged as 'violence/graphic'.
	 */
	'violence/graphic': boolean;
	}
To Reproduce
import oai from ""openai""

const someFlag: keyof oai.Moderation.Categories = ""harassment"" // type error
Code snippets
No response
OS
macOS
Node version
Node v18.16.0
Library version
openai 4.0.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/218","Package size in v4 is much larger than v3 (no tree shaking)","2023-08-16T18:47:26Z","Closed as not planned issue","bug","Describe the bug
How to import only the completions functionality?
To Reproduce
Import any function and observe file size
Code snippets
No response
OS
Linux
Node version
16
Library version
4.0.0-beta.11
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/216","v4 SDK - missing ""node_modules/openai/dist/index.mjs""?","2023-08-09T03:46:56Z","Closed issue","No label","Describe the bug
Trying to work out the new v4 beta SDK (love the new features!) - but constantly get an error during initialization for ""missing node_modules/openai/dist/index.mjs""
 I can't even see this file anywhere in the v4 SDK
 Am I not doing things correctly? Any pointers very welcome.
To Reproduce
install new v4 SDK - from a zip file download
Code snippets
No response
OS
macOS
Node version
Node 18.17.0
Library version
openai SDK v4 beta
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/215","Type errors on ^4.0.0-beta.7","2023-08-05T16:48:50Z","Closed issue","bug","Describe the bug
81906@DESKTOP-608QNA0 MINGW64 ~/Documents/slackbot-gpt3 (master)
 $ npm run dev
slack-gpt3@1.0.0 dev
 nodemon
[nodemon] 3.0.1
 [nodemon] to restart at any time, enter rs
 [nodemon] watching path(s): src***
 [nodemon] watching extensions: ts
 [nodemon] starting npm start
slack-gpt3@1.0.0 start
 npm run build && node --no-warnings dist/app.js
slack-gpt3@1.0.0 build
 tsc -p .
node_modules/openai/src/core.ts:140:66 - error TS2345: Argument of type 'number | undefined' is not assignable to parameter of type 'number'.
 Type 'undefined' is not assignable to type 'number'.
140 if ('timeout' in options) validatePositiveInteger('timeout', options.timeout);
 ~~~~~~~~~~~~~~~
node_modules/openai/src/uploads.ts:234:22 - error TS2769: No overload matches this call.
 Overload 1 of 3, '(name: string, value: string | Blob): void', gave the following error.
 Argument of type 'string | number | boolean' is not assignable to parameter of type 'string | Blob'.
 Type 'number' is not assignable to type 'string | Blob'.
 Overload 2 of 3, '(name: string, value: string): void', gave the following error.
 Argument of type 'string | number | boolean' is not assignable to parameter of type 'string'.
 Type 'number' is not assignable to type 'string'.
 Overload 3 of 3, '(name: string, blobValue: Blob, filename?: string | undefined): void', gave the following error.
 Argument of type 'string | number | boolean' is not assignable to parameter of type 'Blob'.
 Type 'string' is not assignable to type 'Blob'.
234 form.append(key, value);
 ~~~~~
node_modules/openai/src/uploads.ts:237:22 - error TS2769: No overload matches this call.
 Overload 1 of 3, '(name: string, value: string | Blob): void', gave the following error.
 Argument of type 'FileLike' is not assignable to parameter of type 'string | Blob'.
 Type 'FileLike' is missing the following properties from type 'Blob': arrayBuffer, stream, prototype
 Overload 2 of 3, '(name: string, value: string): void', gave the following error.
 Argument of type 'FileLike' is not assignable to parameter of type 'string'.
 Overload 3 of 3, '(name: string, blobValue: Blob, filename?: string | undefined): void', gave the following error.
 Argument of type 'FileLike' is not assignable to parameter of type 'Blob'.
237 form.append(key, file);
 ~~~~
Found 3 errors in 2 files.
Errors Files
 1 node_modules/openai/src/core.ts:140
 2 node_modules/openai/src/uploads.ts:234
 [nodemon] app crashed - waiting for file changes before starting...
To Reproduce
{
  ""name"": ""slack-gpt3"",
  ""version"": ""1.0.0"",
  ""description"": ""Q is ChatGPT for Slack powered by OpenAI's GPT-3.5 and GPT-4 to generate responses to messages when it is mentioned by @Q."",
  ""main"": ""dist/app.js"",
  ""scripts"": {
    ""build"": ""tsc -p ."",
    ""build:watch"": ""tsc -w -p ."",
    ""start"": ""npm run build && node --no-warnings dist/app.js"",
    ""dev"": ""nodemon""
  },
  ""license"": ""MIT"",
  ""dependencies"": {
    ""@aws-sdk/client-lambda"": ""^3.363.0"",
    ""@pinecone-database/pinecone"": ""^0.1.6"",
    ""@slack/bolt"": ""^3.13.1"",
    ""@supabase/supabase-js"": ""^2.26.0"",
    ""axios"": ""^1.4.0"",
    ""cheerio"": ""^1.0.0-rc.12"",
    ""dotenv"": ""^16.3.1"",
    ""fs"": ""^0.0.1-security"",
    ""gpt-3-encoder"": ""^1.1.4"",
    ""ignore"": ""^5.2.4"",
    ""langchain"": ""^0.0.118"",
    ""openai"": ""^4.0.0-beta.7"",
    ""pdf-parse"": ""^1.1.1"",
    ""peggy"": ""^3.0.2"",
    ""puppeteer"": ""^19.8.3"",
    ""stripe"": ""^12.12.0""
  },
  ""devDependencies"": {
    ""@types/node"": ""^20.4.1"",
    ""@types/pdf-parse"": ""^1.1.1"",
    ""nodemon"": ""^3.0.1"",
    ""ts-node"": ""^10.9.1"",
    ""typescript"": ""^5.1.6""
  }
}
{
  ""compilerOptions"": {
    // Main options
    ""target"": ""ES2020"", // or higher
    ""module"": ""nodenext"",
    ""moduleResolution"": ""node"",

    // Other options
    ""allowJs"": true,
    ""allowSyntheticDefaultImports"": true,
    ""esModuleInterop"": true,
    ""outDir"": ""dist"",
    ""resolveJsonModule"": true,
    ""rootDir"": ""src"",
    ""skipLibCheck"": true,
    ""strict"": true,
    ""sourceMap"": true
  },
  ""include"": [""src/**/*""]
}

Code snippets
No response
OS
Windows
Node version
v18.12.1
Library version
^4.0.0-beta.7
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/214","(v4-B7) ChatCompletion object missing streaming properties","2023-08-03T10:43:08Z","Closed as not planned issue","No label","Describe the bug
This is only an issue when stream is set to true in params.
const stream: OpenAI.Chat.ChatCompletion = await openai.chat.completions.create(params);

Type 'Stream<ChatCompletionChunk> & { responseHeaders: Headers; }' is missing the following properties from type 'ChatCompletion': id, choices, created, model, objectts(2739)
const stream: OpenAI.Chat.Completions.ChatCompletion


To Reproduce
Create a new TS Node Server
 Implement the doc examples for V4 explicitly declaring types on all variables.
Code snippets
No response
OS
Windows 11
Node version
Node 18.12.1
Library version
v4.0.0-beta.7
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/213","(v4-B7) Configuration is not a constructor","2023-08-03T10:35:51Z","Closed as not planned issue","No label","Describe the bug
v4 Beta 7 (v4-B7)
This is not a bug, but a breaking change that will cause upgrades to break.
(Would you prefer me to add these as Suggestions instead of bugs?)
Using existing configuration object returns:
 TypeError: Configuration is not a constructor
To Reproduce
Create a new Node.js app.
Create a configuration based initializer and run the call to OpenAI
Code snippets
const { Configuration, OpenAIApi } = require(""openai"");require('dotenv').config();

const configuration = new Configuration({
    apiKey: process.env.OPEN_AI_KEY,});

const openai = new OpenAIApi(configuration);


### OS

Windows 11

### Node version

Node 18.12.1

### Library version

v4.0.0-beta.7

 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/212","Documentation Error","2023-08-02T17:04:11Z","Closed issue","No label","Describe the bug
Please look at the response information in the official documentation.
https://platform.openai.com/docs/api-reference/chat/create?lang=node.js
The createChatCompletion response looks nothing like this at all.
{
  ""id"": ""chatcmpl-123"",
  ""object"": ""chat.completion"",
  ""created"": 1677652288,
  ""choices"": [{
    ""index"": 0,
    ""message"": {
      ""role"": ""assistant"",
      ""content"": ""\n\nHello there, how may I assist you today?"",
    },
    ""finish_reason"": ""stop""
  }],
  ""usage"": {
    ""prompt_tokens"": 9,
    ""completion_tokens"": 12,
    ""total_tokens"": 21
  }
}


To Reproduce
Go to the docs and read it.
https://platform.openai.com/docs/api-reference/chat/create?lang=node.js
Code snippets
N/A
OS
Windows 11
Node version
Node 18.12.1
Library version
v3.3.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/211","Documentation Error (which can cause dev issues)","2023-08-02T17:03:41Z","Closed issue","No label","Describe the bug
Go to the documenation on this page:
https://platform.openai.com/docs/api-reference/chat/create?lang=node.js
Select the model and the language to be Node.
You will see in the instructions the following line:
  messages: [{""role"": ""system"", ""content"": ""You are a helpful assistant.""}, {role: ""user"", content: ""Hello world""}],

clearly in the first object, the property role is wrapped in """" as well as the property content. But not in the second object
To Reproduce
Go to the documentation
Code snippets
N/A
OS
Windows 11
Node version
Node 18.12.1
Library version
v3.3.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/210","Documentation Error (which can cause dev issues)","2023-08-02T16:50:15Z","Closed issue","bug","Describe the bug
Go to the documenation on this page:
https://platform.openai.com/docs/api-reference/chat/create?lang=node.js
Select the model and the language to be Node.
You will see in the instructions the following line:
  messages: [{""role"": ""system"", ""content"": ""You are a helpful assistant.""}, {role: ""user"", content: ""Hello world""}],

clearly in the first object, the property role is wrapped in """" as well as the property content. But not in the second object
To Reproduce
Go to the documentation
Code snippets
N/A
OS
Windows 11
Node version
Node 18.12.1
Library version
v3.3.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/209","Status 400 - Axios Bug","2023-08-02T16:45:08Z","Closed issue","bug","Describe the bug
Trying to run a simple test completion using the documentation, and the solution fails with an Axios bug.
I have replicated my settings using Postman and can verify that data is returned as expected.
The error in code shows me no obvious details of what is causing the error in your library.
To Reproduce
Create a new Node project and use the details as shown here:
https://platform.openai.com/docs/api-reference/chat/create?lang=node.js
Run it and you will have a 400 response returned.
Code snippets
const result = await openai.createChatCompletion({
            model: ""gpt-3.5-turbo"",
            messages: [
                {role: ""system"", content: ""You are a helpful assistant""}, 
                {role: ""user"", content: {promptValue}}
            ]
        }, { responseType: 'stream' });
OS
Windows 11
Node version
Node 18.12.1
Library version
v3.3.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/208","""warning"":""This model version is deprecated. Migrate before January 4, 2024","2023-08-03T02:59:05Z","Closed issue","fixed in v4","Describe the bug
Im using v3.3.0 attempting to make a call with text-davinci-003
 const result = await openai.createCompletion({
            model: ""text-davinci-003"",
            prompt: ` ${promptValue} ${prompt}`,
            max_tokens: 2048,
            temperature: 0,
            top_p: 1,
            n: 1,
            stream: true,
        }, { responseType: 'stream' });

I get this sort of return: {""warning"":""This model version is deprecated. Migrate before January 4, 2024 to avoid disruption of service. Learn more https://platform.openai.com/docs/deprecations""......
I go to my account and try then use - gpt-3.5-turbo-instruct for the model, and promptly get a response of Model does not exist.
The information provided in the blog posts, says this is a drop in replacement. I'm kinda stuck, and I don't believe an API call should return a warning as the object and a 200 response.
To Reproduce
Create a Node application
Add the code to call the API
Make the call and watch the result
Code snippets
try {
        const result = await openai.createCompletion({
            model: ""text-davinci-003"",
            prompt: ` ${promptValue} ${prompt}`,
            max_tokens: 2048,
            temperature: 0,
            top_p: 1,
            n: 1,
            stream: true,
        }, { responseType: 'stream' });
        
        result.data.on('data', data => {
            console.log(data);
            const lines = data.toString().split('\n').filter(line => line.trim() !== '');
            for (const line of lines) {
                const message = line.replace(/^data: /, '');
                if (message === '[DONE]') {
                    res.end();
                    return; // Stream finished
                }
                try {
                    const parsed = JSON.parse(message);
                    console.log(parsed.choices[0].text);
                    let responseData = parsed.choices[0].text;
                    res.write(`data: ${responseData}\n\n`);
                } catch(error) {
                    console.error('Could not JSON parse stream message', message, error);
                }
            }
        });



        
    } catch (error) {
        if (error.response?.status) {
            console.error(error.response.status, error.message);
            error.response.data.on('data', data => {
                const message = data.toString();
                try {
                    const parsed = JSON.parse(message);
                    console.error('An error occurred during OpenAI request: ', parsed);
                } catch(error) {
                    console.error('An error occurred during OpenAI request: ', message);
                }
            });
        } else {
            console.error('An error occurred during OpenAI request', error);
        }
    }




### OS

Windows 11

### Node version

Node 18.12.1

### Library version

v3.3.0

 The text was updated successfully, but these errors were encountered: 
👍3
and-zverev, kevinccbsg, and diegoasua reacted with thumbs up emoji👀3
and-zverev, kevinccbsg, and diegoasua reacted with eyes emoji
All reactions
👍3 reactions
👀3 reactions"
"https://github.com/openai/openai-node/issues/207","Request failed with status code 404 using gpt-4-32k","2023-07-31T01:07:11Z","Closed as not planned issue","No label","Describe the bug
I would like to use the gpt-4-32k model to create a chat completion, but I always receive the following error message.
Request failed with status code 404
Is this model not supported? I read in the api documentation that it should work, right?
To Reproduce
createChatCompletion using the gpt-4-32k model
Code snippets
this.openai.createChatCompletion({
  model: 'gpt-4-32k',
  ..
});

OS
macOS
Node version
nodejs16.x
Library version
3.3.3
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/206","Open AI API 401 error code: incorrect api key","2023-07-31T01:10:04Z","Closed as not planned issue","No label","bug is only in browsers.
Here is my code with correct request structure. If i post it by Postman App then 200 code and response from GPT, if i post from localhost:3000, then the error(
cache of the browser i did clean and also tried in other browser, the same(
To Reproduce
firstly from redux state thubk function takes the text with structure {role: string, content: string} and pushes in array with the 1st element of system message. then axios post
Code snippets
No response
OS
Windows 10
Node version
17 4 0
Library version
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/205","Axios is now 1.0.0 please update it :)","2023-07-30T00:33:21Z","Closed issue","No label","Describe the feature or improvement you're requesting
Please support version 1.x.x of axios not to force people using the latest version to install Axios twice.
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/204","Possibility to use action : ""variant"" and variant_purpose : ""comparison_implicit""","2023-09-03T15:04:13Z","Closed as not planned issue","openai api","Describe the feature or improvement you're requesting
I try to regenerate response, like in official Chat.
 If I use regenerate in Chat, I can see this in request:
{
    ""action"": ""variant"",
    ""messages"": [
        {
            ""id"": ""a4c9ab9a-99a0-4ea7-aa69-5e80c0b50120"",
            ""author"": {
                ""role"": ""user"",
                ""metadata"": {}
            },
            ""create_time"": 1689775584.464652,
            ""content"": {
                ""content_type"": ""text"",
                ""parts"": [
                    ""Ceci est un test""
                ]
            },
            ""status"": ""finished_successfully"",
            ""weight"": 1,
            ""metadata"": {
                ""timestamp_"": ""absolute""
            },
            ""recipient"": ""all""
        }
    ],
    ""conversation_id"": ""72704a41-7444-4b66-b592-ec2eaa225570"",
    ""parent_message_id"": ""66b4468a-4f56-4f8b-aa7e-78446b6540fc"",
    ""model"": ""text-davinci-002-render-sha"",
    ""timezone_offset_min"": -120,
    ""variant_purpose"": ""comparison_implicit"",
    ""history_and_training_disabled"": false,
    ""arkose_token"": null
}
2 fields are interesting:
variant_purpose
action
It is currently not possible to use them in lib. I try to ask to ChatGTP himself. He suggest me this:
{
...,
  action: {
      action_type: 'variant',
      variant_purpose: 'comparison_implicit',
    },}
But if I precise my current openai version, he respond me this:
I apologize for any confusion. You are correct that in version 4.0.0-beta.6 of the OpenAI SDK, the ""action"" parameter is not available in the chat.completions.create function. I apologize for the misinformation.
Currently, the ""action"" and ""variant_purpose"" parameters are not directly supported in the chat.completions.create function. These parameters are specific to the GPT-3 chat interface on the OpenAI website.
If you want to control the generation of the response with similar parameters, you can use the available parameters in the SDK such as ""temperature"" and ""max_tokens"" to adjust the randomness and length of the generated response.
You can also modify the content of the prompt to influence the generation of the response by adding specific instructions or constraints. Experiment with different prompts to achieve the desired results.
I encourage you to refer to the official OpenAI SDK documentation for up-to-date information on the available options: https://github.com/openai/openai-node
This feature can be very useful to use. Actually, I renew manually the request.
Additional context
openai version: 4.0.0-beta.6
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/203","Library is not compatible with nextjs","2023-07-27T14:43:35Z","Closed issue","question","Describe the bug
I found several issue when i try to build a nextjs application using openai-node :
Typescript compiler option isolatedModules is set to true => Cannot use 'export import' on a type or type-only namespace when 'isolatedModules' is enabled.ts(1269)
use of ""#"" to define a private property not working
To Reproduce
create a sample application with : npx create-next-app@latest
 install openai-node
 write a simple chatCompletion in a page component
 Launch ""npm run build""
You will see errors on ""checking validity of types"" step.
Code snippets
No response
OS
macOs
Node version
16.8.x and more
Library version
4.00 and more
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/202","OpenAI Node v4 improperly polyfills fetch","2023-07-19T22:38:14Z","Closed issue","bug","Describe the bug
Using this library, openai@4.0.0-beta.5 (and confirmed on a few other beta versions) improperly polyfills fetch with node-fetch, which has different semantics than the fetch in Node v18 and above.
This is one of the causes of this issue:
StreamingTextResponse not compatible with Remix action function vercel/ai#199
The type error looks like this:
TypeError: responseBodyStream.pipeThrough is not a function
    at AIStream (file:///app/node_modules/ai/dist/index.mjs:135:29)
    at Module.OpenAIStream (file:///app/node_modules/ai/dist/index.mjs:172:18)
    at renderCompletion (file:///app/index.js:19:34)
    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)
    at async file:///app/index.js:33:3

And it's due to node-fetch not behaving idiomatically.
To Reproduce
Proof of concept repo
Clone https://github.com/AaronFriel/openai-vercel-ai-bug
 Run node ./index.js, you should see:
$ node index.js
Catching 💥
TypeError: responseBodyStream.pipeThrough is not a function
    at AIStream (file:///app/node_modules/ai/dist/index.mjs:135:29)
    at Module.OpenAIStream (file:///app/node_modules/ai/dist/index.mjs:172:18)
    at renderCompletion (file:///app/index.js:19:34)
    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)
    at async file:///app/index.js:33:3
💥 caught!
Should the OpenAI library polyfill fetch, or should it use the global fetch on Node v18 and above?


The OpenAI library should use the global fetch on Node v18 and above, rather than polyfilling fetch.
On Next.js
1. Configure a next.js app
Create an app or copy the app dir here: https://github.com/vercel-labs/ai/blob/main/examples/next-openai
2. Create a server route handler for OpenAI
In a route handler, instantiate an OpenAI client and return a stream. Do not set runtime = 'edge' - we want to see how this behaves on Node.js on the server.
(Modified from https://github.com/vercel-labs/ai/blob/main/examples/next-openai/app/api/chat/route.ts)
// ./app/api/chat/route.tsimport { Configuration, OpenAIApi } from 'openai-edge'import { OpenAIStream, StreamingTextResponse } from 'ai'

// Create an OpenAI API client (that's edge friendly!)const config = new Configuration({
  apiKey: process.env.OPENAI_API_KEY})const openai = new OpenAIApi(config)

// IMPORTANT! DO NOT SET THE RUNTIME// export const runtime = 'edge'

export async function POST(req: Request) {
  // Extract the `prompt` from the body of the request
  const { messages } = await req.json()

  // Ask OpenAI for a streaming chat completion given the prompt
  const response = await openai.createChatCompletion({
    model: 'gpt-3.5-turbo',
    stream: true,
    messages: messages.map((message: any) => ({
      content: message.content,
      role: message.role
    }))
  })

  // Convert the response into a friendly text-stream
  const stream = OpenAIStream(response)
  // Respond with the stream
  return new StreamingTextResponse(stream)}
3. Observe a TypeError on using the route
TypeError: responseBodyStream.pipeThrough is not a function
    at AIStream (file:///app/node_modules/ai/dist/index.mjs:135:29)
    at Module.OpenAIStream (file:///app/node_modules/ai/dist/index.mjs:172:18)
    at renderCompletion (file:///app/index.js:19:34)
    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)
    at async file:///app/index.js:33:3

Code snippets
import * as ai from 'ai';import { OpenAI } from 'openai';

import * as fs from 'fs/promises';

async function renderCompletion(client, content) {
  const res = await client.chat.completions.create({
    model: 'gpt-3.5-turbo',
    stream: true,
    messages: [
      {
        role: 'system',
        content:
          'You are an expert software engineer, with deep Node.js and web development experience. You are succinct, brief, and to the point.',
      },
      {
        role: 'user',
        content,
      },
    ],
  });

  for await (const message of ai.OpenAIStream(res.response)) {
    process.stdout.write(message);
  }
  process.stdout.write('\n');}

try {
  // This API client uses the node-fetch polyfill:
  const brokenClient = new OpenAI({
    apiKey: process.env.OPENAI_API_KEY,
  });

  console.log('Catching 💥');
  await renderCompletion(brokenClient, 'PING');} catch (error) {
  console.log(error);
  console.log(`💥 caught!`);

  // This API client uses the built-in fetch support in Node v18:
  const workingClient = new OpenAI({
    apiKey: process.env.OPENAI_API_KEY,
    fetch: globalThis.fetch,
  });

  const currentSourceCode = await fs.readFile('./index.js', 'utf-8');
  const question = `Should the OpenAI library polyfill fetch, or should it use the global fetch on Node v18 and above?`;
  const prompt = `\`\`\`javascript${currentSourceCode}\`\`\`That code threw this error, as a result of the OpenAI API client named \`brokenClient\`.The \`workingClient\` however works.\`\`\`${error}\`\`\`${question}`;
  console.log(`${question}\n\n`);
  await renderCompletion(workingClient, prompt);}
OS
Linux
Node version
v18.16.1
Library version
openai v4.0.0-beta.5
 The text was updated successfully, but these errors were encountered: 
👍1
gsppe reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/openai-node/issues/201","A defect occurs when using openai.images.edit","2023-07-18T15:11:03Z","Closed issue","bug","Describe the bug
 e: APIConnectionTimeoutError at OpenAI.request (/Users/123456/Desktop/project/api-chat-node/node_modules/openai/core.js:265:49) at process.processTicksAndRejections (node:internal/process/task_queues:95:5) at async editImage (/Users/123456/Desktop/project/api-chat-node/api/editImage/index.js:57:16) at async /Users/123456/Desktop/project/api-chat-node/app.js:22:22 { status: undefined, headers: undefined, error: undefined, code: undefined, param: undefined, type: undefined }
To Reproduce
 try{ const __data=await openai.images.edit({ image:fs.createReadStream('api/editImage/dog.png') , prompt: ""Make it a little HD"", n: 1, size: ""256x256"", }); console.log(54,{__data}) return {data:__data.data} } catch(e){ console.log(26,{e}) return {e} }，
 If I use the above code, there will be an error：
	26 { e: APIConnectionTimeoutError at OpenAI.request (/Users/123456/Desktop/project/api-chat-node/node_modules/openai/core.js:265:49) at process.processTicksAndRejections (node:internal/process/task_queues:95:5) at async editImage (/Users/123456/Desktop/project/api-chat-node/api/editImage/index.js:57:16) at async /Users/123456/Desktop/project/api-chat-node/app.js:22:22 { status: undefined, headers: undefined, error: undefined, code: undefined, param: undefined, type: undefined } } 
 The basis is that I can't be sure if ""image"" is certain, ""dog.png"" is real
Code snippets
No response
OS
macOS
Node version
v18.12.1
Library version
4.0.0-beta.4
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/200","Function Calling Response Returns ""{"" for arguments","2023-07-16T20:07:43Z","Closed as not planned issue","openai api","Describe the bug
It appears that the function calling response is quite unreliable, as it does not return a complete arguments object.
 Here is an example of the JSON Schema for a function that I'm trying to get it to call.
export const getCoinPrice: ChatCompletionFunctions = {
  name: ""get_coin_price"",
  description: ""Get the current price for any cryptocurrency"",
  parameters: {
    type: ""object"",
    properties: {
      coin_symbol: {
        type: ""string"",
        description: ""The symbol of the asset to get the price for. Eg: BTC"",
      },
    },
    required: [""coin_symbol""],
  },};
This is then passed to the chat request like so:
 const chatRequest: CreateChatCompletionRequest = {
      model: ""gpt-3.5-turbo-0613"",
      temperature: 0.5,
      n: 1,
      stop: ""\n"",
      messages: messages,
      function_call: ""auto"",
      functions: functions,
    };

    const response = await this.openai.createChatCompletion(chatRequest);
The Contents of the message response will look like so:
{""role"":""user"",""content"":""what is the current price of bitcoin""},{""role"":""assistant"",""content"":"""",""function_call"":{""name"":""get_coin_price"",""arguments"":""{""}
Arguments should be of type object but I couldn't get it to return anything else than ""{""
To Reproduce
Add Function Calling to your chat request
Create a short JSON schema for your function and print the response including the function_call
Code snippets
No response
OS
Windows 11
Node version
Node v18.6.0
Library version
openai v3.3.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/199","Issue with ReactNative iOS","2023-07-18T15:07:03Z","Closed issue","bug","Describe the bug
Using v4 on ReactNative I am getting 404 from CloudFlare on ReactNative iOS. Same code works in standard node. Haven't checked with v3 yet. Other external requests work fine.
Same code in both environments, code copied from sample.
Log outputs compared:
# In ReactNative metro EAS
 LOG  OpenAI:DEBUG:request https://api.openai.com/v1/completions/ 
{""body"": {""model"": ""text-davinci-003"", ""prompt"": ""Say this is a test""}, 
""method"": ""post"", 
""path"": ""/completions"", 
""stream"": false
} 
{""Accept"": ""application/json"", ""Authorization"": ""Bearer KEY"", ""Content-Length"": ""67"", ""Content-Type"": ""application/json"", ""User-Agent"": ""OpenAI/JS 4.0.0-beta.4"", ""X-Stainless-Arch"": ""unknown"", ""X-Stainless-Lang"": ""js"", ""X-Stainless-OS"": ""Unknown"", ""X-Stainless-Package-Version"": ""4.0.0-beta.4"", ""X-Stainless-Runtime"": ""unknown"", ""X-Stainless-Runtime-Version"": ""unknown""}

 LOG  OpenAI:DEBUG:response 404 https://api.openai.com/v1/completions/ 
{""alt-svc"": ""h3=\"":443\""; ma=86400"", ""cf-cache-status"": ""DYNAMIC"", ""cf-ray"": ""7e76b784bf5abc79-HNL"", ""content-encoding"": ""br"", ""content-type"": ""application/json; charset=utf-8"", ""date"": ""Sun, 16 Jul 2023 02:11:51 GMT"", ""server"": ""cloudflare"", ""strict-transport-security"": ""max-age=15724800; includeSubDomains"", ""vary"": ""Origin"", ""x-request-id"": ""ab048a808995968bd0ffc3d382a343b6""} undefined

# In node
OpenAI:DEBUG:request https://api.openai.com/v1/completions {
  method: 'post',
  path: '/completions',
  body: { prompt: 'Say this is a test', model: 'text-davinci-003' },
  stream: false
} {
  'Content-Length': '67',
  Accept: 'application/json',
  'Content-Type': 'application/json',
  'User-Agent': 'OpenAI/JS 4.0.0-beta.4',
  'X-Stainless-Lang': 'js',
  'X-Stainless-Package-Version': '4.0.0-beta.4',
  'X-Stainless-OS': 'MacOS',
  'X-Stainless-Arch': 'arm64',
  'X-Stainless-Runtime': 'node',
  'X-Stainless-Runtime-Version': 'v20.4.0',
  Authorization: 'Bearer KEY'
}
OpenAI:DEBUG:response 200 https://api.openai.com/v1/completions {
  'access-control-allow-origin': '*',
  'alt-svc': 'h3="":443""; ma=86400',
  'cache-control': 'no-cache, must-revalidate',
  'cf-cache-status': 'DYNAMIC',
  'cf-ray': '7e76be379a1df4c4-HNL',
  connection: 'keep-alive',
  'content-encoding': 'gzip',
  'content-type': 'application/json',
  date: 'Sun, 16 Jul 2023 02:16:26 GMT',
  'openai-model': 'text-davinci-003',
  'openai-organization': 'user-7xl7qvwhwytyzbntikxlr1rt',
  'openai-processing-ms': '436',
  'openai-version': '2020-10-01',
  server: 'cloudflare',
  'strict-transport-security': 'max-age=15724800; includeSubDomains',
  'transfer-encoding': 'chunked',
  'x-ratelimit-limit-requests': '3000',
  'x-ratelimit-limit-tokens': '250000',
  'x-ratelimit-remaining-requests': '2999',
  'x-ratelimit-remaining-tokens': '249983',
  'x-ratelimit-reset-requests': '20ms',
  'x-ratelimit-reset-tokens': '3ms',
  'x-request-id': '87ae3b32149c02bc73d209309d8ef16e'
} {
  id: 'cmpl-7clb7ooA2bLuTFrycZc1JaLKLr6jE',
  object: 'text_completion',
  created: 1689473785,
  model: 'text-davinci-003',
  choices: [
    {
      text: '\n\nThis is indeed a test.',
      index: 0,
      logprobs: null,
      finish_reason: 'stop'
    }
  ],
  usage: { prompt_tokens: 5, completion_tokens: 8, total_tokens: 13 }
}

To Reproduce
async function debug() {

    try {
        const client = new OpenAI({
            apiKey: process.env['OPENAI_API_KEY']
        })
        console.log(client.baseURL)
        // Non-streaming:
        const result = await client.completions.create({
            prompt: 'Say this is a test',
            model: 'text-davinci-003',
        },
        );
        console.log(result.choices[0]!.text);
    }
    catch (e) {
        console.log(e.stack)
    }
}

Code snippets
No response
OS
macOS
Node version
Node v16.x
Library version
v4b4
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/198","Passing ""gpt-4"" as model returns 404 not found","2023-07-16T20:26:07Z","Closed issue","openai api","Describe the bug
I have some code that previously passed in the ""gpt-3.5-turbo"" model string. This code worked fine. Now with the general release of GPT 4 API, I wanted to test it by updating this string in place. It returns 404 no found when passing ""gpt-4"".
I am using the latest release of this library and am totally confounded.
Other details that may be relevant - though I have a paid account, my usage has been rounded down to no charge the past 2 months, so perhaps I do not qualify as a paid account yet? In either case, the error is not a good one. It does not communicate the actual failure reason.
To Reproduce
Any call the chatCompletions with GPT-4 as the model
Code snippets
const { Configuration, OpenAIApi } = require(""openai""); (async function(){ const configuration = new Configuration({ apiKey: process.env.OPENAI_API_KEY, }); const openai = new OpenAIApi(configuration); const chatResponse = await openai.createChatCompletion({ model: ""gpt-4"", messages: [{role: ""user"", content: ""how are you?""}], }); console.log(chatResponse) })() 
OS
macOS
Node version
Node v16
Library version
v3.0.1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/196","Lack of global proxy in langchain lib call.","2023-07-14T09:56:26Z","Closed as not planned issue","No label","Sorry. just a mistake to send issue to this project.
 The text was updated successfully, but these errors were encountered: 
👍1
rattrayalex reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/openai-node/issues/195","ChatCompletionRequestMessage doesn't allow null content","2023-07-13T21:59:37Z","Closed issue","bug,fixed in v4","Describe the bug
The docs indicate that null is an expected value for message ""content"":
However, the type definition for OpenAPI.ChatCompletionRequestMessage only allows string or undefined.
When passing message that utilize ""function_call"" to createChatCompletion(), I have to specify the message content as null. If I simply omit the ""content"" field or set it to undefined, I get a 400 ""Bad Request"" error.
To avoid this 400 error I specify the content as null, producing a type error:
Similarly, the type for OpenAI.ChatCompletionResponseMessage doesn't reflect the fact that a response's ""content"" can contain a null value.
To Reproduce
Specify a message such as:
{
  role: ""assistant"",
  content: null,
  function_call: {
    name: ""..."",
    arguments: ""...""
  }}
TypeScript Playground reproduction:
https://www.typescriptlang.org/play?#code/JYWwDg9gTgLgBAKjgQwM5wPJgKYDsCCAknAGZQQhwDkEOuywVcA9M3DAJ46oBccAzADohABgBQYgMYRcqeCGypUyAOaK+WPEUEBhABbIYOimAA22GMBkAlbAEcAropgBZRcrUBtALpwAvHA+EgpKqoqCYA6oegAUAN5icHDk5nwARGiowHLIuDBpADSJcNJ5eDB8uA6mpkVJJA64kpYyAPqSyDV8CUlJ9ArpIBytDU0tuIXFSchQKg4KebxwaYKracUAvmIbAJRAA
Code snippets
No response
OS
macOS
Node version
Node v18.16.0
Library version
openai v3.3.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/194","stream claims it was aborted even though it was not","2023-07-18T00:53:39Z","Closed issue","bug","Describe the bug
When using 4.0 beta.4, when I do a stream chat completion, after the for await loop the AbortController behaves as if it was aborted, even though I am quite certain it was not.
 This is signal.reason:
 aborted DOMException [AbortError]: This operation was aborted
 at new DOMException (node:internal/per_context/domexception:53:5)
 at AbortController.abort (node:internal/abort_controller:342:18)
 at Stream.[Symbol.asyncIterator] (/app/node_modules/openai/src/streaming.ts:69:12)
 at processTicksAndRejections (node:internal/process/task_queues:95:5)
 at streamChatCompletion (/app/src/streamChatCompletion.ts:39:36)
Is it possible that finishing the stream is handled as if it was aborted?
To Reproduce
Stream a chat completion, check the result of the corresponding abortcontroller.signal
Code snippets
No response
OS
Manjaro
Node version
node 18
Library version
v4 beta.4
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/192","Invalid character in header content [""Authorization""]","2023-07-09T10:15:49Z","Closed issue","bug","Describe the bug
normal yesterday！This error occurred today：
 message: ""Invalid character in header content [""Authorization""]""
To Reproduce
const configuration = new OpenAI.Configuration({
    apiKey: GPT_APIKEY
});

const response = await openai.createCompletion({
 model: ""text-davinci-003"",
 prompt: SS,
 temperature: 1.0,
 max_tokens: 2100,
 top_p: 1.0,
 frequency_penalty: 0.3,
 presence_penalty: 0.3,
 best_of: 1
 });
Code snippets
const configuration = new OpenAI.Configuration({
        apiKey: GPT_APIKEY
    });
       

 const response = await openai.createCompletion({
            model: ""text-davinci-003"",
            prompt: SS,
            temperature: 1.0,
            max_tokens: 2100,
            top_p: 1.0,
            frequency_penalty: 0.3,
            presence_penalty: 0.3,
            best_of: 1
        });
OS
centos7.9
Node version
nodev14
Library version
v3.3.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/190","Integrate ESLint and Prettier in TypeScript Library Project","2023-07-08T01:25:37Z","Closed issue","fixed in v4","Describe the feature or improvement you're requesting
As a part of our continuous effort to improve code quality and consistency, we should integrate ESLint and Prettier into our TypeScript library project.
ESLint is a popular JavaScript linter, and when used with TypeScript, it can significantly help us catch common mistakes in the code before they make it to production. Furthermore, ESLint can be customized to follow our own coding standards or to comply with popular style guides.
Prettier, on the other hand, is an opinionated code formatter. Integrating it into our project can help maintain consistent formatting across our codebase, leading to better readability and maintainability.
Combining the two could enhance our workflow and code quality, preventing bugs and making our code easier to understand.
To implement this feature, we'll need to:
Install and configure ESLint with TypeScript support.
Set up rules based on our coding standards or a popular style guide.
Install and configure Prettier.
Ensure that Prettier and ESLint play well together using eslint-config-prettier or a similar tool.
Add scripts in the package.json file to allow easy linting and code formatting.
(Optional) Set up pre-commit hooks using a tool like husky to enforce the linting and formatting rules before committing.
Please consider this proposal and let me know if there are any concerns or questions.
Additional context
Having these tools integrated into our project not only improves our code quality but also offers a more consistent developer experience. This way, we can prevent many potential issues before they happen and maintain a more consistent, readable, and clean codebase. Additionally, this can be an excellent learning opportunity for new developers, allowing them to become familiar with best practices in TypeScript development.
 The text was updated successfully, but these errors were encountered: 
👍1
ImBIOS reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/openai-node/issues/188","Functions not working with streaming","2023-07-03T20:31:03Z","Closed issue","bug","Describe the bug
I have streaming working well with this library.
I am trying to add a call to a function, but the response does not include the required arguments.
The model recognizes that it wants to use the function, and the part.choices[0].finish_reason contains the value of ""function_call""
But the function name nor the argument is included in the response. Here is what is in the part.choices:
 choices: [ { index: 0, delta: {}, finish_reason: 'function_call' } ]
I was expecting to see the function name and the args in delta.
To Reproduce
Create a function
Define the function as part of the chat.completions.create command
Set stream: true on the command
Loop through the chunks
When the function is requested by the model, the name and args are not included in the chunk.
Code snippets
No response
OS
Linux
Node version
Node v18.12.1
Library version
openai-node 4.0.0-beta.1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/186","prompt don't work in imageEdit model","2023-07-08T19:29:20Z","Closed issue","bug,fixed in v4","Describe the bug
I wanted to Build edit image app then i send prompt value in backend then show error prompt value is null , but i debug and see my value is arrive in Backend but in a model don't work. After i write my prompt value in mode like [prompt : "" add dragon and blue Background ] after agin show error prompt value is null
To Reproduce
i fetch this error
Error editing image: RequiredError: Required parameter prompt was null or undefined when calling createImageEdit.
 at exports.assertParamExists (C:\Users\USER\Desktop\Image-genarate\server\node_modules\openai\dist\common.js:39:15)
 at Object. (C:\Users\USER\Desktop\Image-genarate\server\node_modules\openai\dist\api.js:387:22)
 at Generator.next ()
 at C:\Users\USER\Desktop\Image-genarate\server\node_modules\openai\dist\api.js:21:71
 at new Promise ()
 at __awaiter (C:\Users\USER\Desktop\Image-genarate\server\node_modules\openai\dist\api.js:17:12)
 at Object.createImageEdit (C:\Users\USER\Desktop\Image-genarate\server\node_modules\openai\dist\api.js:383:96)
 at Object. (C:\Users\USER\Desktop\Image-genarate\server\node_modules\openai\dist\api.js:1148:75)
 at Generator.next ()
 at C:\Users\USER\Desktop\Image-genarate\server\node_modules\openai\dist\api.js:21:71 {
 field: 'prompt'
 }
Code snippets
My prompt is define but get error prompt is null 
const { Configuration, OpenAIApi } = require(""openai"");const fs = require('fs');const path = require('path');const multer = require('multer');

const configuration = new Configuration({
  apiKey: ""sk-HU05dPhCJTppTrhNSVHfT3BlbkFJFXHCEWIoYBE7GN9fmi"" 
});

const openai = new OpenAIApi(configuration);

// Configure multer for file uploadconst storage = multer.diskStorage({
  destination: path.join(__dirname, 'uploads'),
  filename: (req, file, cb) => {
    cb(null, file.originalname);
  },});

const upload = multer({ storage }).fields([{ name: 'file1' }, { name: 'file2' }]);

exports.editImage = async (req, res) => {
  try {
    await upload(req, res, async (error) => {
      if (error) {
        console.error('Error uploading files:', error);
        return res.status(400).json({ error: 'File upload failed' });
      }
      const { size,  } = req.body;
      // Validate the prompt field
      // if (!prompts) {
      //   return res.status(400).json({ error: 'Prompt field is required' });
      // }

      const file1 = req.files['file1'][0];
      const file2 = req.files['file2'][0];

      const file1Path = path.join(__dirname, 'uploads', file1.filename);
      const file2Path = path.join(__dirname, 'uploads', file2.filename);

      let imageUrl;
      try {
        const response = await openai.createImageEdit({
          image:fs.createReadStream(file1Path),
          mask:fs.createReadStream(file2Path),
          prompt:""add dragon and blue background"",
          n:1,
          size: size
        });

        imageUrl = response.data.output.url;
      } catch (error) {
        console.error('Error editing image:', error);
        return res.status(500).json({ error: 'Image editing failed' });
      } finally {
        // Delete the uploaded files
        fs.unlinkSync(file1Path);
        fs.unlinkSync(file2Path);
      }

      res.json({ imageUrl });
    });
  } catch (error) {
    console.error('Error handling file upload:', error);
    res.status(500).json({ error: 'File upload failed' });
  }};
OS
windows 10
Node version
Node v18.12.0
Library version
""openai"": ""^3.3.0""
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/185","Function Parameter being array does not work","2023-07-10T00:13:09Z","Closed issue","bug,fixed in v4","Describe the bug
I can not use type: 'array', items: 'string' for example to supply arrays as parameters.
Is that a problem with the API itself? It conforms with JSON schema as far as I know
To Reproduce
{ name: 'channelIds', type: 'array', description: 'An array of channel IDs', items: { ""type"": ""string"" }, required: true }
 Fetch a function call chat API call
Code snippets
No response
OS
Linux
Node version
v18
Library version
latest
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/184","""function_call"" parameter is not available","2023-06-19T23:27:32Z","Closed issue","bug","Describe the bug
""function_call"" parameter is not available via node package even though it is in the documentation. ""functions"" parameter seems to work though.
To Reproduce
Try to add ""function_call"" parameter to createChatCompletion request.
Code snippets
openai.createChatCompletion({
      model: 'gpt-3.5-turbo-0613',
      messages: [],
      function_call: true,
      functions: ai_functions,
    });
OS
macOS
Node version
Node v16.19.1
Library version
openai v3.2.1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/183","what to put as ChatCompletionRequestMessageFunctionCall arguments?","2023-06-18T08:04:54Z","Closed issue","No label","Describe the feature or improvement you're requesting
Doesn't look like it support function description now? Or am I missing something?
export interface ChatCompletionRequestMessageFunctionCall {
    /**
     * The name of the function to call.
     * @type {string}
     * @memberof ChatCompletionRequestMessageFunctionCall
     */
    'name'?: string;
    /**
     * The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function.
     * @type {string}
     * @memberof ChatCompletionRequestMessageFunctionCall
     */
    'arguments'?: string;
}

Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/181","Is 'functions' parameter not supported?","2023-07-04T22:08:00Z","Closed issue","bug","Describe the bug
I'm trying to use the functions parameter to help force JSON response. Is this not supported?
To Reproduce
Call createChatCompletion with 'functions' array.
Code snippets
try {
  response = await openai.createChatCompletion({
    model, messages, functions 
  });} catch (error) {
  if (error.response) {
    console.log(error.response.status);
    console.log(error.response.data);
  } else {
    console.log(error.message);
  }}
Results in the error:
    {
      error: {
        message: 'Unrecognized request argument supplied: functions',
        type: 'invalid_request_error',
        param: null,
        code: null
      }
    }



### OS

macOS

### Node version

18.7.0

### Library version

3.3.0

 The text was updated successfully, but these errors were encountered: 
👍4
felipebutcher, hsyndeniz, anurakhan, and rorymalcolm reacted with thumbs up emoji
All reactions
👍4 reactions"
"https://github.com/openai/openai-node/issues/180","node type","2023-07-10T00:14:43Z","Closed issue","bug,fixed in v4","Describe the bug
Error message Cannot find name 'require'. Do you need to install type definitions for node? Try 'npm i --save-dev @types/node'.
To Reproduce
Code snippets
May be solved by having the ""node"" type in the types attribute of tsconfig.json.
{
  ""compilerOptions"": {
    ""declaration"": true,
    ""target"": ""es6"",
    ""module"": ""commonjs"",
    ""noImplicitAny"": true,
    ""outDir"": ""dist"",
    ""rootDir"": ""."",
    ""types"": [ ""node"" ],
    ""typeRoots"": [
      ""node_modules/@types""
    ]
  },
  ""exclude"": [
    ""dist"",
    ""node_modules""
  ]}
OS
Windows 10
Node version
Node v18.12.1
Library version
openai 3.3.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/179","Upgrading from 3.2.1 to 3.3.0 introduces breaking changes.","2023-07-08T19:41:38Z","Closed issue","bug,fixed in v4","Describe the bug
I believe that changing the content field to being optional is a breaking change. I'm using TypeScript so it's only breaking my build, but this is going to cause a ton of projects to start failing at runtime if the API is indeed leaving it out now.
To Reproduce
This is pretty straightforward, you can observe the problem directly in the type.
Code snippets
// Version 3.2.1

export interface ChatCompletionResponseMessage {
    /**     * The role of the author of this message.     * @type {string}     * @memberof ChatCompletionResponseMessage     */
    'role': ChatCompletionResponseMessageRoleEnum;
    /**     * The contents of the message     * @type {string}     * @memberof ChatCompletionResponseMessage     */
    'content': string;}
// Version 3.3.0

export interface ChatCompletionResponseMessage {
    /**     * The role of the author of this message.     * @type {string}     * @memberof ChatCompletionResponseMessage     */
    'role': ChatCompletionResponseMessageRoleEnum;
    /**     * The contents of the message.     * @type {string}     * @memberof ChatCompletionResponseMessage     */
    'content'?: string;
    /**     *     * @type {ChatCompletionRequestMessageFunctionCall}     * @memberof ChatCompletionResponseMessage     */
    'function_call'?: ChatCompletionRequestMessageFunctionCall;}
OS
N/A
Node version
N/A
Library version
openai v3.3.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/174","”completion.data.choices[0].text“, Cannot read properties of undefined (reading '0')","2023-06-15T02:09:27Z","Closed issue","bug","Describe the bug
I'm running openai-quickstart-node repo, when i request openai's service, i cannot get the rignt response.
 my code is same with openai-quickstart-node, why my ""completion.data"" is an empty string ?
To Reproduce
git clone https://github.com/openai/openai-quickstart-node.git
 npm install
 cp .env.example .env, and then fill with my apiKey
 npm run dev
open http://localhost:3000/, click ""Generage name""
Code snippets
No response
OS
macOS
Node version
16.18.1
Library version
openai 3.2.1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/173","Adding support for gpt-4 models","2023-06-05T05:24:16Z","Closed issue","No label","Describe the feature or improvement you're requesting
According to api only supports up to gpt-3.5-turbo models:
Any plans to support the rest:
https://platform.openai.com/docs/models/model-endpoint-compatibility
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/172","Pass word_level_timestamps option into whisper API call","2023-09-03T15:30:16Z","Closed as not planned issue","openai api","Describe the feature or improvement you're requesting
When I use the Whisper model directly via the openai/whisper python package (https://github.com/openai/whisper) I am able to pass the word_timestamps into the transcription call. See the below snippet:
whisper_model = whisper.load_model(""large-v2"").to(device)

transcript_res = whisper_model.transcribe(
  audio=f""{filename}.mp3"",
  word_timestamps=True,
  language=""en""
)

However, when I make an API call, I'm not able to do this. I find this a very useful feature, as by default, the timestamps in the response are rounded to the nearest second, whereas when passing in word_timestamps=True they're much more accurate.
Could the word_timestamps parameter be added to the API?
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
👀1
oli5679 reacted with eyes emoji
All reactions
👀1 reaction"
"https://github.com/openai/openai-node/issues/170","In this way, the content is not complete","2023-07-10T00:34:12Z","Closed issue","No label","Describe the bug
To Reproduce
![image](https://github.com/openai/openai-node/assets/112919246/5fe022f7-3c0c-4402-b8e3-730e03f8a80a)

Code snippets
No response
OS
win11
Node version
16.20.0
Library version
3.2.1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/169","Docs: match the quickstart and api-reference","2023-09-03T15:58:53Z","Closed issue","documentation","Describe the bug
Hello, awesome library! I noticed that on the /speech-to-text/quickstart docs there is only curl and python as options where on the /api-reference/audio docs the node option exists. The python code in the two docs looks identical. I wasn't sure what to file this under so feel free to change it from a bug to something else.
https://platform.openai.com/docs/guides/speech-to-text/quickstart?lang=python
https://platform.openai.com/docs/api-reference/audio/create?lang=node
To Reproduce
The quickstart does not have the node option
The api-reference does have node as an option
Code snippets
No response
OS
macOs
Node version
node v18.7.0
Library version
openai v3.2.1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/168","Does this library distinguish 429 - Too many requests from 429 - Too many tokens? [question]","2024-07-08T18:52:21Z","Closed issue","enhancement,openai api,question","Describe the bug
Sorry, not a bug - just a question!
The OpenAI docs stipulate that they enforce rate limits in 2 ways: by request, and by
 -- source here
I'm wondering if this library distinguishes by the two. I don't think it does, because here is an error log I ahve for the 429:
stackError: Request failed with status code 429
    at createError (/usr/src/app/node_modules/axios/lib/core/createError.js:16:15)
    at settle (/usr/src/app/node_modules/axios/lib/core/settle.js:17:12)
    at IncomingMessage.handleStreamEnd (/usr/src/app/node_modules/axios/lib/adapters/http.js:322:11)
    at IncomingMessage.emit (events.js:412:35)
    at IncomingMessage.emit (domain.js:475:12)
    at endReadableNT (internal/streams/readable.js:1333:12)
    at processTicksAndRejections (internal/process/task_queues.js:82:21)message[Request failed with status code 429](javascript:void(0);)

Upon confirmation from a maintainer that it doesn't, I will open a feature request requesting this differentiation. Thank you!
P.S. I'd request a 3rd option for issue submission, a new Question one, in addition to the current Bug and Feature request options.
To Reproduce
N/A
Code snippets
N/A
OS
mac Ventura (13.0.1)
Node version
16.16
Library version
3.0.0
 The text was updated successfully, but these errors were encountered: 
👍3
nikfakel, sanman1k98, and ezzcodeezzlife reacted with thumbs up emoji
All reactions
👍3 reactions"
"https://github.com/openai/openai-node/issues/167","ERR_FR_MAX_BODY_LENGTH_EXCEEDED when calling createTranscription","2023-07-05T19:16:48Z","Closed issue","bug,fixed in v4","Describe the bug
ERR_FR_MAX_BODY_LENGTH_EXCEEDED when calling createTranscription
To Reproduce
use large file (within the limit of 25 MB), in my case the file size is: 19.4 MB
Code snippets
async function generateJson() {const resp = await openai.createTranscription(
  fs.createReadStream(""large_file.mp3""), // less than 25 MB
  ""whisper-1"",
  '',
  'vtt',
  0,
  'fr');
OS
Windows 7
Node version
Node v14.17.6
Library version
openai v3.1.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/166","localVarFormParams.getHeaders is not a function","2023-12-28T18:10:06Z","Closed issue","bug","Describe the bug
When I am using OpenAI node SDK to createTranscription,
I am sending it the file like this,
const mediaBlob = await fetch(url)
 .then(response => response.blob());
const audio = new File(
 [mediaBlob],
 'demo.mp4',
 { type: 'video/mp4' },
 );
const resp = await openai.createTranscription(audio, 'whisper-1'); // even audio.text(), stream() etc doesn't work
It's throwing me localVarFormParams.getHeaders is not a function
To Reproduce
Try to send an audio file for transcription using openai.createTranscription
Try to send a file
Error thrown - localVarFormParams.getHeaders is not a function
Code snippets
no response
OS
macOS
Node version
Node v16.16.2
Library version
openai v3.2.1
 The text was updated successfully, but these errors were encountered: 
👍1
HamadTheIronside reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/openai-node/issues/165","Occasionally getting 429 Too many requests error","2023-07-16T20:33:41Z","Closed issue","openai api","Describe the bug
I have an app that makes 8 text completion requests to OpenAI using the GPT3.5 turbo in the back-end. The requests are chained together and all together I'm using around 5000 tokens. I have a pay as you go account (older than 48 hours) which according to the documentation about rate limits means that I can make 3500 requests per minute or do 90000 tokens per minute. Now considering the limitations, I believe I don't really hit any rate limits here but somehow I get a 429 Too many requests error back on some occasions. Currently the app doesn't have any active users than me.
To Reproduce
It happens randomly, so it's hard to say how to reproduce this issue other than chaining multiple createChatCompletions back to back.
Code snippets
A request looks like this:

function openAIRequest(prompt: string) {
   const response = await OpenAI.createChatCompletion({
			model: 'gpt-3.5-turbo',
			temperature: 0.8,
			max_tokens: 2000,
			messages: [
				{
					role: ""user"",
					content: prompt
				}
			]
		});

   return response;} 
And it's used like so in the backend:
async function makeRequest() {
   const response1 = await openAIRequest1(prompt)
   const response2 = await openAIRequest2(prompt)
   ...etc
}



### OS

macOs

### Node version

Node v16.14.2

### Library version

""openai"": ""^3.2.1"",

 The text was updated successfully, but these errors were encountered: 
👍5
yaizudamashii, nikfakel, horbel, IsTheJack, and pixelass reacted with thumbs up emoji
All reactions
👍5 reactions"
"https://github.com/openai/openai-node/issues/163","createChatCompletion how to get multiple results?","2023-05-09T06:52:22Z","Closed issue","bug","Describe the bug
When I ask ""Tell me 3 Jokes"" in the Playground (Chat), it will print me 3 results. However, when using the node API, the response only returns a single item and I see no way of getting more than that. Is this intended or a bug? Or am I thinking totally wrong?
I also tried the ""n"" parameter, which actually gives me several results, but they are much worse compared to the Playground (they are too similar)
To Reproduce
createChatCompletion({
  model: ""gpt-3.5-turbo-0301"",
  messages: [
    {
      role: ""system"",
      content: ""Tell me 3 jokes"",
    },
  ],
}

Code snippets
No response
OS
Windows
Node version
v18.12.1
Library version
openai 3.2.1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/162","getting 400 without error message","2023-07-10T00:37:41Z","Closed issue","fixed in v4,question","Describe the bug
i am getting 400 without error message..
To Reproduce
fetch the latest update.
use the code snippets provided
Code snippets
export class ChatGPTServiceImplemnt implements ChatGPTService {
  async sendMessage(message:string): Promise<string|undefined> {
    try {
      const respons = await openai.createChatCompletion({
        model: ""gpt-3.5-turbo"",
        messages: [{ role: ""user"", content: message }],
        temperature: 0,
        top_p: 1.0,
        n: 1,
        frequency_penalty: 0.0,
        presence_penalty: 0.0,
        stop: [""#"", "";""],
      });
      return respons.data.choices[0].message?.content;
    } catch (error) {
      console.log(`this is the error from this ${error}`);
      throw error;
    }
  }}
OS
macOs
Node version
v18.12.1
Library version
3.2.1
 The text was updated successfully, but these errors were encountered: 
👍2
bypina and eqqe reacted with thumbs up emoji
All reactions
👍2 reactions"
"https://github.com/openai/openai-node/issues/161","Enhancing Error Messages for Gated Models","2023-09-03T15:56:54Z","Closed as not planned issue","openai api","Describe the feature or improvement you're requesting
When a user does not have access to alpha models or other gated models, provide a more expressive error message. For example, I do not have access to gpt-4-32k. When invoking chat completion, I get this error:
Error in OpenAI API call: { error: { message: 'The model: gpt-4-32k does not exist', type: 'invalid_request_error', param: null, code: 'model_not_found' }
Alternative suggestion: a bool that checks if the model is valid, and another bool that checks if the user has access. This allows you throw a new error if the model is valid and the user does not have access, e.g.:
Error in OpenAI API call: { error: { message: 'The model: gpt-4-32k is not enabled on your account', type: 'invalid_request_error', param: null, code: 'model_not_found' }
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/160","429 error","2023-05-06T14:52:27Z","Closed issue","bug","Describe the bug
Hello everyone. Recently, I have been experiencing an issue where my requests to OpenAI API are crashing with error code 429. The error description indicates that I am sending requests to the API too frequently, but this is not the case. I am sending only one request with the 'stream' option enabled, and it has been working fine until recently. Can you please help me figure out what the issue might be?
To Reproduce
send a request to OpenAI with this config https://i.imgur.com/nPtlkxp.png
occurred error https://i.imgur.com/AYOyu1m.png
Code snippets
//My express.js controller

const { Configuration, OpenAIApi } = require(""openai"");const configuration = new Configuration({
  apiKey: process.env.OPENAI_API_KEY,});const openai = new OpenAIApi(configuration);

class OpenAIController {
  processedQuestions = new Set();
  messages = [];

  async getAnswer(req, res, next) {
    const { text } = req.query;
    const self = this;
    this.messages.push({ role: ""user"", content: text });
    this.messages.push({ role: ""assistant"", content: """" });
    try {
      this.processedQuestions.add(text);
      res.setHeader(""Cache-Control"", ""no-cache"");
      res.setHeader(""Content-Type"", ""text/event-stream"");
      res.setHeader(""Access-Control-Allow-Origin"", ""*"");
      res.setHeader(""Connection"", ""keep-alive"");
      res.flushHeaders(); // flush the headers to establish SSE with client

      const response = await openai.createChatCompletion(
        {
          model: ""gpt-3.5-turbo"",
          messages: self.messages,
          stream: true,
        },
        {
          responseType: ""stream"",
        }
      );

      const stream = response.data;

      stream.on(""data"", (chunk) => {
        // Messages in the event stream are separated by a pair of newline characters.
        const payloads = chunk.toString().split(""\n\n"");
        for (const payload of payloads) {
          if (payload.includes(""[DONE]"")) {
            return;
          }
          if (payload.startsWith(""data:"")) {
            const data = payload.replaceAll(/(\n)?^data:\s*/g, """");
            try {
              const delta = JSON.parse(data.trim());
              const content = delta.choices[0].delta?.content;
              if (content) {
                this.messages[this.messages.length - 1].content += content;
                res.write(`data: ${content.replace(""\n"", ""__"")}\n\n`);
              }
            } catch (error) {
              console.log(`Error with JSON.parse and ${payload}.\n${error}`);
            }
          }
        }
      });

      stream.on(""end"", () => {
        res.end();
        console.log(""Stream done"");
      });
      stream.on(""error"", (e) => console.error(e));
    } catch (err) {
      console.error(""err: "", err);
      next(err);
    }
  }}

module.exports = OpenAIController;
OS
Windows
Node version
16.14.2
Library version
latest
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/159","ADA model","2023-05-04T13:16:30Z","Closed issue","bug","Describe the bug
How do you use the ""ada"" model? He answers me random things, is this normal?
prompt: ""say hello""
 response: ', ""Are-you-there?""), the name is typically run through Andrew
Does anyone care to explain to me? I wanted to try the cheapest model of openai to do simple tasks
thanks guys!
To Reproduce
const configuration = new Configuration({
 apiKey: process.env.OPENAI_API_KEY,
 });
 const openai = new OpenAIApi(configuration);
const completion = await openai.createCompletion({
 model: ""ada"",
 prompt: ""say 'hello'"",
 });
Code snippets
No response
OS
macos
Node version
node 16
Library version
last version
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/155","[Whisper] verbose_json responseFormat should return correct type","2023-07-10T01:02:14Z","Closed issue","No label","Describe the bug
When I use responseTypeverbose_json, I get a typescript error. The request returns segments, which is not present in the response types
To Reproduce
const { data: { segments } } = await this.openAiClient.createTranslation( // TypeScript throws an error here
  readStream as unknown as File, // voice file
  'whisper-1', // model
  undefined, // prompt
  'verbose_json', // reponseType. This means another response type should used
  0.2, // temperature);
Code snippets
No response
OS
macOs
Node version
Node v18.14.0
Library version
openai v3.2.1
 The text was updated successfully, but these errors were encountered: 
👍1
ffrenchm reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/openai-node/issues/154","Error: Request failed with status code 400","2023-07-10T00:36:28Z","Closed issue","bug,fixed in v4","Describe the bug



Too much content will cause an error, It's not clear to me why that's true
To Reproduce
Too much content
Code snippets
No response
OS
windows
Node version
Node V18.x
Library version
openai v3.2.1
 The text was updated successfully, but these errors were encountered: 
👍1
schneiderfelipe reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/openai-node/issues/153","Synchronous completion APi","2023-07-10T01:03:25Z","Closed as not planned issue","No label","Describe the feature or improvement you're requesting
Experimental integration of OpenAI into an existing node.js-based rules engine but rule clauses can't be executed in an unpredictable order so async invocation is impossible. Any possibility of a sync API?
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
👎4
amirasyraf, LukasGirsch, HyperClockUp, and skaraudio reacted with thumbs down emoji
All reactions
👎4 reactions"
"https://github.com/openai/openai-node/issues/152","getting a 404","2023-07-10T01:07:14Z","Closed as not planned issue","No label","Describe the bug
	const res = await this.ai.createCompletion({
				model: ""gpt-3.5-turbo"",
				prompt: aiPrompt,
				temperature: 0,
				max_tokens: 4000 - aiPrompt.length,
				top_p: 1,
				frequency_penalty: 0,
				presence_penalty: 0,
				stop: [""\""\""\""""],
			});

why do i get a 404?
To Reproduce
	""openai"": ""^3.1.0"",

Code snippets
const res = await this.ai.createCompletion({
				model: ""gpt-3.5-turbo"",
				prompt: aiPrompt,
				temperature: 0,
				max_tokens: 4000 - aiPrompt.length,
				top_p: 1,
				frequency_penalty: 0,
				presence_penalty: 0,
				stop: [""\""\""\""""],
			});


### OS

linux

### Node version

18

### Library version

3.1.0

 The text was updated successfully, but these errors were encountered: 
👍2
FANMixco and bypina reacted with thumbs up emoji
All reactions
👍2 reactions"
"https://github.com/openai/openai-node/issues/150","[Whisper] Prompting frequently yields hallucinations","2023-08-18T23:17:43Z","Closed issue","bug,openai api","Describe the bug
I’m using whisper through NextJS. I’m calling the API directly, given that the openai-node package doesn’t have great support for the whisper API ([Whisper] cannot call createTranscription function from Node.js due to File API · Issue #77 · openai/openai-node · GitHub).
I’m calling it like so:
import fetch, { FormData, File } from 'node-fetch';

const handler = (req, res) => {
  const { data } = await supabase.storage
    .from('audio-transcripts')
    .download(`${interviewId}.mp3`);
  
  const formData = new FormData();
  const file = new File([data], `${interviewId}.mp3`);
  formData.append('file', file);
  formData.append('model', 'whisper-1');
  formData.append('prompt', prompt);
  formData.append('response_format', 'json');
  formData.append('temperature', '0.01');
  formData.append('language', 'en');
  
  const response = await fetch(
    'https://api.openai.com/v1/audio/transcriptions',
    {
      method: 'POST',
      body: formData,
      headers: {
        Authorization: `Bearer ${process.env.OPENAI_API_KEY}`,
      },
    }
  );

  return res.status(200).json({ text: response.text });
}

I’m running into a lot of issues using the prompt. It is usually a two sentence prompt that has (1) a high level overview of the transcript (it is a call about xyz), and (2) some proper nouns and competitor names that I want to make sure a spelt correctly. This follows the example provided in OpenAI’s docs. An example of the type of prompt I'm using is: This transcript is about Bayern Munich and various soccer teams. Common competitors in the league include Real Madrid, Barcelona, Manchester City, Liverpool, Paris Saint-Germain, Juventus, Chelsea, Borussia Dortmund, and AC Milan.
80% of the time I use the prompt, however, I get garbage, hallucinated, output. It ends up on a loop, repeating the same thing (eg. a competitor’s name, or a url made up from one of the competitor’s names). An example of the output I'm seeing: In the past, the league has been a place of competition for the players. The league has been a place of competition for the players. The league has been a place of competition for the players. The league has been a place of competition for the players. The league has been a place of competition for the players. The league has been a place of competition for the players....
Am I calling it incorrectly?
What is happening, and how can I fix it? I’d like to ideally use the prompt to make sure spelling of competitor names is correct.
Note, this sometimes works better when calling the API directly from the terminal with the prompt.
To Reproduce
Download an audio file, and input a prompt like the one I shared. Example of the output I see:
Code snippets
No response
OS
macOS
Node version
v19.9.0
Library version
3.2.1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/149","how to save dialog context?","2023-09-03T15:59:36Z","Closed issue","enhancement","Describe the feature or improvement you're requesting
Hello. Which option is responsible for saving the context of the dialog? For example i ask: What is the biggest city in the world?
 it says: 'Tokio', I ask: How many people live there? and OpenAI doesnt remember my previous answer and cant give me a correct answer.
 const response = await openai.createChatCompletion( { model: ""gpt-3.5-turbo"", messages: [{role: 'user', content: text_argement}], stream: true, }, { responseType: ""stream"", } );
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/148","Timeout issue on localhost: connect ETIMEDOUT","2023-07-10T00:41:12Z","Closed issue","openai api","Describe the bug
I'm doing a project with NextJS and I'm using this lib to connect with openai ChatGPT API.
Until yesterday I was able to make my requests to:
createModeration
createChatCompletion
But today almost 90% of the time my requests are failing with a timeout erro on localhost (it's working on staging and production):
Error: connect ETIMEDOUT 104.18.6.192:443
    at TCPConnectWrap.afterConnect [as oncomplete] (node:net:1157:16) {
  errno: -60,
  code: 'ETIMEDOUT',
  syscall: 'connect',
  address: '104.18.6.192',
  port: 443,

I saw some dudes saying that this error usually happens for people that uses VPN or are in China. I'm not using a VPN and I don't live in China too.
Any idea what could be the cause of it ?
To Reproduce
Make a call to createModeration or createChatCompletion on a NextJS server running on localhost
Code snippets
const moderationResponse = await openai.createModeration({
  input: message,});

const completionResponse = await openai.createChatCompletion({
  model: 'gpt-3.5-turbo',
  messages,
  temperature: 0,
  max_tokens: MAX_TOKENS,
  user: userID,});
OS
macOS
Node version
Node 16.14.0
Library version
openai 3.2.1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/147","Escape symbols for code snippets","2023-07-10T01:10:03Z","Closed issue","question","Describe the feature or improvement you're requesting
How can I ensure that a response containing JavaScript code is always escaped? I am receiving a response from Open in the form of a text stream which contains both strings and pieces of JavaScript code. The problem is that sometimes the JavaScript code is not properly escaped with triple tilde characters. Is there any special option for that?
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/146","a ip 199.59.148.202 about api.openai.com not work","2023-04-21T02:30:52Z","Closed issue","bug","Describe the bug
199.59.148.202 now not work, ping failed
 is it a ip about domain api.openai.com?
To Reproduce
Code snippets
No response
OS
macOS
Node version
node
Library version
openai 3.2.1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/145","404 error in after running the code","2023-07-10T00:52:20Z","Closed issue","No label","Describe the bug
Error: Request failed with status code 404
 at createError (C:\Users\totid\Desktop\Sidehustle\Stock Foto Club\Js splitter bot\node_modules\openai\node_modules\axios\lib\core\createError.js:16:15)
 at settle (C:\Users\totid\Desktop\Sidehustle\Stock Foto Club\Js splitter bot\node_modules\openai\node_modules\axios\lib\core\settle.js:17:12)
 at IncomingMessage.handleStreamEnd (C:\Users\totid\Desktop\Sidehustle\Stock Foto Club\Js splitter bot\node_modules\openai\node_modules\axios\lib\adapters\http.js:322:11)
 at IncomingMessage.emit (node:events:525:35)
 at endReadableNT (node:internal/streams/readable:1359:12)
 at process.processTicksAndRejections (node:internal/process/task_queues:82:21)
To Reproduce
Just run the code in command line with> node script.js
Code snippets
(async () => {
    
    const { Configuration, OpenAIApi } = require(""openai"");
    const configuration = new Configuration({
      apiKey: 'API',
    });
    
    const openai = new OpenAIApi(configuration);

    const response = await openai.createCompletion({
      model: ""gpt-3.5-turbo"",
      prompt: ""Say this is a test"",
      messages: [{role: 'user', content: '翻译：大海'}],
      max_tokens: 100,
      temperature: 0,
    });
    console.log(response);
  })();
OS
windows 11
Node version
v18.15.0
Library version
3.2.1
 The text was updated successfully, but these errors were encountered: 
👍1
YueHua46 reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/openai-node/issues/144","Plugin support","2023-07-10T01:19:52Z","Closed issue","No label","Describe the feature or improvement you're requesting
I would like to create a plugin to include data from a third part api from my node server using this lib.
 I'm not able to join discord for information, neither getting answer from openai support regarding this matter.
Is this possible to do? If not, is it something you're looking into?
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
👍1
narthur reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/openai-node/issues/142","UnhandledPromiseRejectionWarning originating from OpenAI library","2023-04-14T21:39:16Z","Closed issue","bug","Describe the bug
I'm using the Langchain library in conjunction with the OpenAI API to create a conversational agent.
Everything is working fine locally, but when I deploy to a render.com server, when I make a request, I get an error that orginates with the OpenAI api. Here is the call stack:
Apr 14 04:09:03 PM  (node:94) UnhandledPromiseRejectionWarning: ReferenceError: Headers is not defined
Apr 14 04:09:03 PM      at createRequest (file:///opt/render/project/src/node_modules/langchain/dist/util/axios-fetch-adapter.js:234:21)
Apr 14 04:09:03 PM      at fetchAdapter (file:///opt/render/project/src/node_modules/langchain/dist/util/axios-fetch-adapter.js:164:21)
Apr 14 04:09:03 PM      at dispatchRequest (/opt/render/project/src/node_modules/axios/lib/core/dispatchRequest.js:58:10)
Apr 14 04:09:03 PM      at Axios.request (/opt/render/project/src/node_modules/axios/lib/core/Axios.js:108:15)
Apr 14 04:09:03 PM      at Function.wrap [as request] (/opt/render/project/src/node_modules/axios/lib/helpers/bind.js:9:15)
Apr 14 04:09:03 PM      at /opt/render/project/src/node_modules/openai/dist/common.js:149:22
Apr 14 04:09:03 PM      at /opt/render/project/src/node_modules/openai/dist/api.js:1738:133
Apr 14 04:09:03 PM  (Use `node --trace-warnings ...` to show where the warning was created)
Apr 14 04:09:03 PM  (node:94) UnhandledPromiseRejectionWarning: Unhandled promise rejection. This error originated either by throwing inside of an async function without a catch block, or by rejecting a promise which was not handled with .catch(). To terminate the node process on unhandled promise rejection, use the CLI flag `--unhandled-rejections=strict` (see https://nodejs.org/api/cli.html#cli_unhandled_rejections_mode). (rejection id: 13)
Apr 14 04:09:03 PM  (node:94) [DEP0018] DeprecationWarning: Unhandled promise rejections are deprecated. In the future, promise rejections that are not handled will terminate the Node.js process with a non-zero exit code.

To Reproduce
I'm using the ConversationChain class from Langchain that makes a call to the OpenAI api.
I apologize if this is not the appropriate place to report this bug, but the call stack goes back to the OpenAI api, so it seemed like it might be a good place to look for a solution.
Code snippets
response = await chain.call({
  userInput: userInput});
OS
macOS (locally) and render.com when i deploy remotely
Node version
Node v18.12.1
Library version
3.2.1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/141","Check quota","2023-09-03T15:36:44Z","Closed as not planned issue","enhancement,openai api","Describe the feature or improvement you're requesting
Is there an api to get result the same as we can see in the https://platform.openai.com/account/usage ?
This can be used to
inform user quota in the UI
check connectivity issue
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
👀3
francisrod01, kapik, and iaseth reacted with eyes emoji
All reactions
👀3 reactions"
"https://github.com/openai/openai-node/issues/139","Make BASE_PATH user changeable to enable local models","2023-07-10T01:09:06Z","Closed issue","fixed in v4","Describe the feature or improvement you're requesting
I would like to use llama-cpp-python which offers a drop-in replacement for the OpenAI API to run my local models with the AgentGPT package further downstream.
 To achieve this I would need a way to change BASE_PATH to my local api.
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
👍1
sschueller reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/openai-node/issues/137","Streaming not working on client side React","2023-07-10T01:24:18Z","Closed issue","bug","Describe the bug
Having an issue where using the key responseType with a value of ""stream"" gives me a warning that there is no stream for this XML request.
To Reproduce
const response = await openai.createChatCompletion(
      {
        model: ""gpt-3.5-turbo"",
        messages,
        stream: true,
      },
      {
        responseType: ""stream"",
      }
    );

Add this chunk of code to your React app and run it on the browser after a button click (with some array of messages).
Code snippets
const sendChatQuery = async (messages: ChatMessage[]) => {
    let answer = """";
    for await (const token of streamChatCompletion(messages)) {
      answer += token;
      console.log(token);
    }
    console.log(""answer finished:"", answer);
  };

  async function* streamChatCompletion(messages: ChatMessage[]) {
    const response = await openai.createChatCompletion(
      {
        model: ""gpt-3.5-turbo"",
        messages,
        stream: true,
      },
      {
        responseType: ""stream"",
      }
    );

    // @ts-ignore
    for await (const chunk of response.data) {
      const lines = chunk
        .toString(""utf8"")
        .split(""\n"")
        .filter((line: string) => line.trim().startsWith(""data: ""));

      for (const line of lines) {
        const message = line.replace(/^data: /, """");
        if (message === ""[DONE]"") {
          return;
        }

        const json = JSON.parse(message);
        const token = json.choices[0].delta.content;
        if (token) {
          yield token;
        }
      }
    }
  }
OS
Ventura
Node version
18.15
Library version
3.2.1
 The text was updated successfully, but these errors were encountered: 
👍1
eagor reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/openai-node/issues/136","404 when making createCompletion","2023-04-26T15:50:27Z","Closed issue","bug","Describe the bug
Some days ago createCompletion request started to return 404
Error: Request failed with status code 404
  request: {
    headers: {
      Accept: 'application/json, text/plain, */*',
      'Content-Type': 'application/json',
      'User-Agent': 'OpenAI/NodeJS/3.1.0',
      Authorization: 'Bearer ******',
      'Content-Length': 247
    },
    method: 'post',
    data: '{""model"":""code-davinci-002"",""prompt"":""### PostgresSQL tables, with their properties:\\n#\\n# string(string)\\n#\\n###string\\nprettify SELECT"",""temperature"":0,""max_tokens"":256,""top_p"":1,""n"":1,""frequency_penalty"":0,""presence_penalty"":0,""stop"":[""#"","";""]}',
    url: 'https://api.openai.com/v1/completions'
  },
  response: {
    status: 404,
    statusText: 'Not Found',
    headers: {
      date: 'Thu, 06 Apr 2023 17:06:35 GMT',
      'content-type': 'application/json; charset=utf-8',
      'content-length': '190',
      connection: 'close',
      vary: 'Origin',
      'x-request-id': '622966fb6783fe3d61c1906253b71879',
      'strict-transport-security': 'max-age=15724800; includeSubDomains',
      'cf-cache-status': 'DYNAMIC',
      server: 'cloudflare',
      'cf-ray': '7b3b9f4c4ed730e8-FRA',
      'alt-svc': 'h3="":443""; ma=86400, h3-29="":443""; ma=86400'
    },
  },
}

To Reproduce
Run createCompletion
Code snippets
const response = await new OpenAIApi(
        new Configuration({
          apiKey: this.configService.get('OPENAI_API_KEY'),
        }),
      ).createCompletion({
        model: 'code-davinci-002',
        prompt: this.openAiAPIService.convertDTOToPrompt(body),
        temperature: 0,
        max_tokens: this.configService.get('OPENAI_MAX_TOKENS'),
        top_p: 1.0,
        n: 1,
        frequency_penalty: 0.0,
        presence_penalty: 0.0,
        stop: ['#', ';'],
      });


### OS

macOs

### Node version

Node v14.17.4

### Library version

openai v3.1.0

 The text was updated successfully, but these errors were encountered: 
👍1
zeropaper reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/openai-node/issues/135","{""message"":""invalid csrf token""}","2023-07-10T01:34:53Z","Closed issue","No label","Describe the bug
请求网址: https://ai.cone.cainiao-inc.com/api/retrieveFineTune
 请求方法: POST
 状态代码: 403
 远程地址: 59.82.112.179:443
 引荐来源网址政策: strict-origin-when-cross-origin
{""message"":""invalid csrf token""}
To Reproduce
请求网址: https://ai.cone.cainiao-inc.com/api/retrieveFineTune
 请求方法: POST
 状态代码: 403
 远程地址: 59.82.112.179:443
 引荐来源网址政策: strict-origin-when-cross-origin
{""message"":""invalid csrf token""}
Code snippets
No response
OS
macos
Node version
16.14
Library version
""openai"": ""^3.2.1"",

 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/134","createChatCompletion seems to ignore the abort signal","2023-07-10T00:11:20Z","Closed issue","bug,fixed in v4","Describe the bug
Sending an 'abort' signal to the createChatCompletion does not raise an error nor stop the completion.
It makes me believe that this discussion on the openai community is true https://community.openai.com/t/cancelling-openai-apis-request/99754, but I would like to verify it isn't a bug in this library.
To Reproduce
Here's my code
import { Configuration, OpenAIApi } from ""openai"";

const configuration = new Configuration({
  apiKey: process.env.OPENAI_API_KEY,});const openai = new OpenAIApi(configuration);

// instantiate the abortControllerconst abortController = new AbortController();const abortSignal = abortController.signal;

// send the request, passing in the abortController's signalconst response = await openai.createChatCompletion(
  {
    model: ""gpt-3.5-turbo"",
    messages: [
      {
        role: ""user"",
        content: ""write a 1000-word essay about George Washington"",
      },
    ],
    stream: true,
  },
  {
    // This is an Axios request config object
    responseType: ""stream"",
    signal: abortSignal,
  });

// abort the request in 1 secondconst abort = () => {
  abortController.abort();};setTimeout(abort, 1000);

// read the response dataconst dataStream = response.data as unknown as AsyncIterable<Buffer>;for await (const chunk of dataStream) {
  if (abortSignal.aborted) {
    console.log(""aborted"");
  }

  const lines = chunk
    .toString(""utf8"")
    .split(""\n"")
    .filter((line) => line.trim().startsWith(""data: ""));

  for (const line of lines) {
    const message = line.replace(""data: "", """");
    if (message === ""[DONE]"") {
      console.log(""done"");
    }

    const json = JSON.parse(message);
    const token = json.choices[0]?.delta?.content;
    if (token != null) {
      console.log(`token: ${token}`);
    }
  }}
Expectation: I should see output like this, and then an error should be raised:
token: George
token:  Washington
token:  is
token:  perhaps
token:  the
token:  most
token:  significant
token:  figure
token:  in
token:  American
token:  history
token: .
aborted

Actual: I see output like this that never stops:
token: George
token:  Washington
token:  is
token:  perhaps
token:  the
token:  most
token:  significant
token:  figure
token:  in
token:  American
token:  history
token: .
aborted
token:  He
aborted
token:  was
aborted
token:  a
aborted
token:  man
aborted
token:  who
aborted
token:  rose
aborted
token:  from
aborted
token:  humble
aborted
token:  beginnings
aborted
token:  to
aborted
token:  become
aborted
token:  the
aborted
token:  first
aborted
token:  President
aborted
token:  of
aborted
token:  the
aborted
token:  United
aborted
token:  States
aborted
token:  of
aborted
token:  America
aborted
token: ,
aborted
token:  and
aborted
token:  he
aborted
token:  did
aborted
token:  so
aborted
token:  with
aborted
token:  a
aborted
token:  level
... (and so on)

Code snippets
No response
OS
macOS
Node version
v19.8.1
Library version
openai v3.2.1
 The text was updated successfully, but these errors were encountered: 
👍7
numeralz, jgeggatt, jongmoon, goa, snlamm, zeiteisen, and wong2 reacted with thumbs up emoji
All reactions
👍7 reactions"
"https://github.com/openai/openai-node/issues/133","Completion API is not able to read the data from the pdf file uploaded in google drive","2023-07-08T19:52:44Z","Closed as not planned issue","bug,invalid","Describe the bug
I am using the createCompletion API to get the summary of the file (pdf file) that is uploaded to my google drive and it is giving me the wrong summary.
To Reproduce
In your nodejs application install the openai npm package (npm i openai)
then configure your openai key
copy the code snippet that I have given in your controller
run the controller
you will see that the actual document and the summary given the chatGPT Completion API is not accurate
Code snippets
const documentUrl = ""https://drive.google.com/file/d/1SceiyOSxPrLd14egDAQCTSqbi3_dIrOG/view?usp=sharing"";

const response = await this.openai.createCompletion({
    model: ""text-davinci-003"",
    prompt: `Please summarize the following document ${documentUrl}`,
    max_tokens: 256,
    temperature: 1,});
OS
Window 11
Node version
Node v18.15.0
Library version
openai 3.2.1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/132","Error 404 while using createChatCompletion","2023-04-21T09:55:37Z","Closed issue","bug","Describe the bug
Error
{
    ""error"": {
        ""message"": ""Invalid URL (POST /v1/chat/completions)"",
        ""type"": ""invalid_request_error"",
        ""param"": null,
        ""code"": null
    }
}

To Reproduce
Use the Below Snippet, I Facing an issue while calling API from the browser.
Code snippets
const configuration = new Configuration({
    apiKey,
  })
  const openai = new OpenAIApi(configuration)

const completion = await openai.createChatCompletion({
      model: engine,
      messages: [{ role: 'user', content: prompt }],
    })
OS
Window 10
Node version
v18.14.2
Library version
^3.2.1
 The text was updated successfully, but these errors were encountered: 
👀1
kevinohara80 reacted with eyes emoji
All reactions
👀1 reaction"
"https://github.com/openai/openai-node/issues/131","createImage function no longer working","2023-07-08T19:43:51Z","Closed as not planned issue","bug","Describe the bug
The createImage function was working fine in my Node.js web app until the last couple of days. Now I get an error when trying to execute it.
To Reproduce
Call createImage function
See error
Error:
Error: Request failed with status code 400
 at createError (C:\Web dev\practiceReact\ImageGeneratorAI\server\node_modules\openai\node_modules\axios\lib\core\createError.js:16:15)
 at settle (C:\Web dev\practiceReact\ImageGeneratorAI\server\node_modules\openai\node_modules\axios\lib\core\settle.js:17:12)
 at IncomingMessage.handleStreamEnd (C:\Web dev\practiceReact\ImageGeneratorAI\server\node_modules\openai\node_modules\axios\lib\adapters\http.js:322:11)
 at IncomingMessage.emit (node:events:525:35)
 at endReadableNT (node:internal/streams/readable:1359:12)
 at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
 config: {
 transitional: {
 silentJSONParsing: true,
 forcedJSONParsing: true,
 clarifyTimeoutError: false
 },
 adapter: [Function: httpAdapter],
 transformRequest: [ [Function: transformRequest] ],
 transformResponse: [ [Function: transformResponse] ],
 timeout: 0,
 xsrfCookieName: 'XSRF-TOKEN',
 xsrfHeaderName: 'X-XSRF-TOKEN',
 maxContentLength: -1,
 maxBodyLength: -1,
 validateStatus: [Function: validateStatus],
 headers: {
 Accept: 'application/json, text/plain, /',
 'Content-Type': 'application/json',
 'User-Agent': 'OpenAI/NodeJS/3.1.0',
 Authorization: 'Bearer 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',
 'Content-Length': 128
 },
 method: 'post',
 data: '{""prompt"":""A centered explosion of colorful powder on a black background"",""n"":1,""size"":""1024x1024"",""response_format"":""b64_json""}',
 url: 'https://api.openai.com/v1/images/generations'
 },
 request: <ref *1> ClientRequest {
 _events: [Object: null prototype] {
 abort: [Function (anonymous)],
 aborted: [Function (anonymous)],
 connect: [Function (anonymous)],
 error: [Function (anonymous)],
 socket: [Function (anonymous)],
 timeout: [Function (anonymous)],
 finish: [Function: requestOnFinish]
 },
 _eventsCount: 7,
 _maxListeners: undefined,
 outputData: [],
 outputSize: 0,
 writable: true,
 destroyed: false,
 _last: true,
 chunkedEncoding: false,
 shouldKeepAlive: false,
 maxRequestsOnConnectionReached: false,
 _defaultKeepAlive: true,
 useChunkedEncodingByDefault: true,
 sendDate: false,
 _removedConnection: false,
 _removedContLen: false,
 _removedTE: false,
 strictContentLength: false,
 _contentLength: 128,
 _hasBody: true,
 _trailer: '',
 finished: true,
 _headerSent: true,
 _closed: false,
 socket: TLSSocket {
 _tlsOptions: [Object],
 _secureEstablished: true,
 _securePending: false,
 _newSessionPending: false,
 _controlReleased: true,
 secureConnecting: false,
 _SNICallback: null,
 servername: 'api.openai.com',
 alpnProtocol: false,
 authorized: true,
 authorizationError: null,
 encrypted: true,
 _events: [Object: null prototype],
 _eventsCount: 10,
 connecting: false,
 _hadError: false,
 _parent: null,
 _host: 'api.openai.com',
 _closeAfterHandlingError: false,
 _readableState: [ReadableState],
 _maxListeners: undefined,
 _writableState: [WritableState],
 allowHalfOpen: false,
 _sockname: null,
 _pendingData: null,
 _pendingEncoding: '',
 server: undefined,
 _server: null,
 ssl: [TLSWrap],
 _requestCert: true,
 _rejectUnauthorized: true,
 parser: null,
 _httpMessage: [Circular *1],
 [Symbol(res)]: [TLSWrap],
 [Symbol(verified)]: true,
 [Symbol(pendingSession)]: null,
 [Symbol(async_id_symbol)]: 1427,
 [Symbol(kHandle)]: [TLSWrap],
 [Symbol(lastWriteQueueSize)]: 0,
 [Symbol(timeout)]: null,
 [Symbol(kBuffer)]: null,
 [Symbol(kBufferCb)]: null,
 [Symbol(kBufferGen)]: null,
 [Symbol(kCapture)]: false,
 [Symbol(kSetNoDelay)]: false,
 [Symbol(kSetKeepAlive)]: true,
 [Symbol(kSetKeepAliveInitialDelay)]: 60,
 [Symbol(kBytesRead)]: 0,
 [Symbol(kBytesWritten)]: 0,
 [Symbol(connect-options)]: [Object]
 },
 _header: 'POST /v1/images/generations HTTP/1.1\r\n' +
 'Accept: application/json, text/plain, /\r\n' +
 'Content-Type: application/json\r\n' +
 'User-Agent: OpenAI/NodeJS/3.1.0\r\n' +
 'Authorization: Bearer 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\r\n' +
 'Content-Length: 128\r\n' +
 'Host: api.openai.com\r\n' +
 'Connection: close\r\n' +
 '\r\n',
 _keepAliveTimeout: 0,
 _onPendingData: [Function: nop],
 agent: Agent {
 _events: [Object: null prototype],
 _eventsCount: 2,
 _maxListeners: undefined,
 defaultPort: 443,
 protocol: 'https:',
 options: [Object: null prototype],
 requests: [Object: null prototype] {},
 sockets: [Object: null prototype],
 freeSockets: [Object: null prototype] {},
 keepAliveMsecs: 1000,
 keepAlive: false,
 maxSockets: Infinity,
 maxFreeSockets: 256,
 scheduling: 'lifo',
 maxTotalSockets: Infinity,
 totalSocketCount: 1,
 maxCachedSessions: 100,
 _sessionCache: [Object],
 [Symbol(kCapture)]: false
 },
 socketPath: undefined,
 method: 'POST',
 maxHeaderSize: undefined,
 insecureHTTPParser: undefined,
 joinDuplicateHeaders: undefined,
 path: '/v1/images/generations',
 _ended: true,
 res: IncomingMessage {
 _readableState: [ReadableState],
 _events: [Object: null prototype],
 _eventsCount: 4,
 _maxListeners: undefined,
 socket: [TLSSocket],
 httpVersionMajor: 1,
 httpVersionMinor: 1,
 httpVersion: '1.1',
 complete: true,
 rawHeaders: [Array],
 rawTrailers: [],
 joinDuplicateHeaders: undefined,
 aborted: false,
 upgrade: false,
 url: '',
 method: null,
 statusCode: 400,
 statusMessage: 'Bad Request',
 client: [TLSSocket],
 _consuming: false,
 _dumped: false,
 req: [Circular *1],
 responseUrl: 'https://api.openai.com/v1/images/generations',
 redirects: [],
 [Symbol(kCapture)]: false,
 [Symbol(kHeaders)]: [Object],
 [Symbol(kHeadersCount)]: 28,
 [Symbol(kTrailers)]: null,
 [Symbol(kTrailersCount)]: 0
 },
 aborted: false,
 timeoutCb: null,
 upgradeOrConnect: false,
 parser: null,
 maxHeadersCount: null,
 reusedSocket: false,
 host: 'api.openai.com',
 protocol: 'https:',
 _redirectable: Writable {
 _writableState: [WritableState],
 _events: [Object: null prototype],
 _eventsCount: 3,
 _maxListeners: undefined,
 _options: [Object],
 _ended: true,
 _ending: true,
 _redirectCount: 0,
 _redirects: [],
 _requestBodyLength: 128,
 _requestBodyBuffers: [],
 _onNativeResponse: [Function (anonymous)],
 _currentRequest: [Circular 1],
 _currentUrl: 'https://api.openai.com/v1/images/generations',
 [Symbol(kCapture)]: false
 },
 [Symbol(kCapture)]: false,
 [Symbol(kBytesWritten)]: 0,
 [Symbol(kEndCalled)]: true,
 [Symbol(kNeedDrain)]: false,
 [Symbol(corked)]: 0,
 [Symbol(kOutHeaders)]: [Object: null prototype] {
 accept: [Array],
 'content-type': [Array],
 'user-agent': [Array],
 authorization: [Array],
 'content-length': [Array],
 host: [Array]
 },
 [Symbol(errored)]: null,
 [Symbol(kUniqueHeaders)]: null
 },
 response: {
 status: 400,
 statusText: 'Bad Request',
 headers: {
 date: 'Tue, 04 Apr 2023 22:32:41 GMT',
 'content-type': 'application/json',
 'content-length': '172',
 connection: 'close',
 'access-control-allow-origin': '',
 'openai-version': '2020-10-01',
 'openai-organization': 'user-hohs5y1guudtogkf6k2glor8',
 'x-request-id': '86461dbb395a0e7c0855a07197923f09',
 'openai-processing-ms': '96',
 'strict-transport-security': 'max-age=15724800; includeSubDomains',
 'cf-cache-status': 'DYNAMIC',
 server: 'cloudflare',
 'cf-ray': '7b2d023b8a7a4911-LHR',
 'alt-svc': 'h3="":443""; ma=86400, h3-29="":443""; ma=86400'
 },
 config: {
 transitional: [Object],
 adapter: [Function: httpAdapter],
 transformRequest: [Array],
 transformResponse: [Array],
 timeout: 0,
 xsrfCookieName: 'XSRF-TOKEN',
 xsrfHeaderName: 'X-XSRF-TOKEN',
 maxContentLength: -1,
 maxBodyLength: -1,
 validateStatus: [Function: validateStatus],
 headers: [Object],
 method: 'post',
 data: '{""prompt"":""A centered explosion of colorful powder on a black background"",""n"":1,""size"":""1024x1024"",""response_format"":""b64_json""}',
 url: 'https://api.openai.com/v1/images/generations'
 },
 request: <ref *1> ClientRequest {
 _events: [Object: null prototype],
 _eventsCount: 7,
 _maxListeners: undefined,
 outputData: [],
 outputSize: 0,
 writable: true,
 destroyed: false,
 _last: true,
 chunkedEncoding: false,
 shouldKeepAlive: false,
 maxRequestsOnConnectionReached: false,
 _defaultKeepAlive: true,
 useChunkedEncodingByDefault: true,
 sendDate: false,
 _removedConnection: false,
 _removedContLen: false,
 _removedTE: false,
 strictContentLength: false,
 _contentLength: 128,
 _hasBody: true,
 _trailer: '',
 finished: true,
 _headerSent: true,
 _closed: false,
 socket: [TLSSocket],
 _header: 'POST /v1/images/generations HTTP/1.1\r\n' +
 'Accept: application/json, text/plain, /\r\n' +
 'Content-Type: application/json\r\n' +
 'User-Agent: OpenAI/NodeJS/3.1.0\r\n' +
 'Authorization: Bearer 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\r\n' +
 'Content-Length: 128\r\n' +
 'Host: api.openai.com\r\n' +
 'Connection: close\r\n' +
 '\r\n',
 _keepAliveTimeout: 0,
 _onPendingData: [Function: nop],
 agent: [Agent],
 socketPath: undefined,
 method: 'POST',
 maxHeaderSize: undefined,
 insecureHTTPParser: undefined,
 joinDuplicateHeaders: undefined,
 path: '/v1/images/generations',
 _ended: true,
 res: [IncomingMessage],
 aborted: false,
 timeoutCb: null,
 upgradeOrConnect: false,
 parser: null,
 maxHeadersCount: null,
 reusedSocket: false,
 host: 'api.openai.com',
 protocol: 'https:',
 _redirectable: [Writable],
 [Symbol(kCapture)]: false,
 [Symbol(kBytesWritten)]: 0,
 [Symbol(kEndCalled)]: true,
 [Symbol(kNeedDrain)]: false,
 [Symbol(corked)]: 0,
 [Symbol(kOutHeaders)]: [Object: null prototype],
 [Symbol(errored)]: null,
 [Symbol(kUniqueHeaders)]: null
 },
 data: { error: [Object] }
 },
 isAxiosError: true,
 toJSON: [Function: toJSON]
 }
The function in the server which executes the createImage function:
import * as dotenv from 'dotenv'import { Configuration, OpenAIApi } from 'openai'


dotenv.config()

const configuration = new Configuration({
    apiKey: process.env.OPENAI_API_KEY})

const openai = new OpenAIApi(configuration)

export const CreateImage = async(req, res) =>{
    try
    {
        const prompt = req.body.prompt



        //Doesn't work
        const aiResponse = await openai.createImage({  
            prompt,
            n:1,
            size: '1024x1024',
            response_format: 'b64_json'
        })

        const image = aiResponse.data.data[0].b64_json

        res.status(200).json({ photo: image })
    }
    catch (err)
    {   
        console.log(err)
        res.status(500).send(err?.response.data.error.message)
    }}
OS
Windows 10
Node version
Node v18.14.1
Library version
openai v3.1.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/130","Pr Word segments for whisper","2023-09-03T15:32:18Z","Closed as not planned issue","openai api","Describe the feature or improvement you're requesting
Would it be possible or considered to extended the whisper API to be able to output segments pr word when using audio to text? It would create a lot of flexibilty!
Would love to see that feature
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
👍1
krowek reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/openai-node/issues/129","createImageEdit is not working","2023-07-10T01:25:06Z","Closed issue","bug,fixed in v4","Describe the bug
So I'm new to openai, I'm using version 3.2.1, aside the wrong argument ordering in docs, the method openai.createImageEdit is not working
  const openaiRes = await openai.createImageEdit(image, prompt, image, n, size)

error:
TypeError: j.getHeaders is not a function

any workaround for this issue? I couldn't find any solution
 also I'm using Supabase and Deno, in case it is related
To Reproduce
call createImageEdit
raises the error
Code snippets
const configuration = new Configuration({
   apiKey: Deno.env.get('OPENAI_API_KEY')})const openai = new OpenAIApi(configuration)const openaiRes = await openai.createImageEdit(image, prompt, image, n, size)
OS
Linux - Debian
Node version
None - Deno
Library version
openai 3.2.1
 The text was updated successfully, but these errors were encountered: 
👀1
dawadam reacted with eyes emoji
All reactions
👀1 reaction"
"https://github.com/openai/openai-node/issues/127","createTranscription File type does not exist in node","2023-07-08T19:30:39Z","Closed issue","bug,fixed in v4","Describe the bug
createTranscription's interface is defined as: createTranscription(file: File, ....)
However, File does not exist in node.js. This is a browser only class.
What is expected here? A return value from fs.readFileSync?
To Reproduce
const file = fs.readFileSync(audioOutputFilePath)

const transcript = await openai.createTranscription(new File([file]), ...) // File does not exist

Code snippets
No response
OS
macOS
Node version
Node v14.19.0
Library version
openai 3.2.1
 The text was updated successfully, but these errors were encountered: 
👍10
jonathanlal, ajsharp, joshkay, ItaiZabanEmpathy, Arrow7000, kiki-le-singe, dosstx, altarrok, zaunermax, and roblframpton reacted with thumbs up emoji
All reactions
👍10 reactions"
"https://github.com/openai/openai-node/issues/126","A complete node repository documentation is required","2023-07-10T01:29:43Z","Closed issue","No label","Describe the feature or improvement you're requesting
The current README of this repository does not fully illustrate all its usages, and the official documentation only provides API reference. I need a user manual for this Node.js project repository. When I try to call the v3+ version, an error occurs, indicating that my access path is incorrect. I cannot find a solution through the README, but I did find an answer in the issues section. However, I would prefer to directly locate the corresponding API call through documentation.
PLEASE!!!!!!
目前这个仓库的readme不能体现所有的用法，官方提供的文档只是接口文档。我需要一份这个nodejs项目仓库的使用说明文档。我现在调用v3+版本的时候，会报错，提示我的访问路径不对。我没办法通过readme来找到解决方法，不过从issue中找到了回答。然而我期望通过文档能直接找到对应的api调用。
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/125","CreateChatCompletion Request throws: TypeError: Converting circular structure to JSON","2023-03-31T03:38:48Z","Closed issue","bug","Describe the bug
I am making a simple request to the createChatCompletion endpoint using the open ai node sdk. My request is very simple but I am getting this error every time the request is sent:
""TypeError: Converting circular structure to JSON\n --> starting at object with constructor 'ClientRequest' | property 'socket' -> object with constructor 'TLSSocket' --- property '_httpMessage' closes the circle at JSON.stringify () at summarizationModel (/Users/leopaz/dev/outlit/Core/outlit/apps/functions/lib/openai.js:68:49) at processTicksAndRejections (node:internal/process/task_queues:96:5)\n at async onSendPrompt (/Users/leopaz/dev/outlit/Core/outlit/apps/functions/lib/openai.js:97:25)""
To Reproduce
const chatCompletionRequest: CreateChatCompletionRequest = {
            model: 'gpt-4',
            messages: [
                {
                    role: 'user',
                    // content: JSON.stringify(prompt),
                    content: 'hi',
                }
            ],
            max_tokens: 100,
            temperature: 0.7,
            stream: false,
        };

        functions.logger.log(""ABOUT TO SEND CHAT COMPLETION"", prompt);

        const chatCompletion = await openai.createChatCompletion(chatCompletionRequest);

Code snippets
const chatCompletionRequest: CreateChatCompletionRequest = {
            model: 'gpt-4',
            messages: [
                {
                    role: 'user',
                    // content: JSON.stringify(prompt),
                    content: 'hi',
                }
            ],
            max_tokens: 100,
            temperature: 0.7,
            stream: false,
        };

        functions.logger.log(""ABOUT TO SEND CHAT COMPLETION"", prompt);

        const chatCompletion = await openai.createChatCompletion(chatCompletionRequest);  
OS
macOS
Node version
16.18.0
Library version
3.2.1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/124","Can't specify max tokens in createChatCompletion","2023-07-10T00:48:39Z","Closed issue","bug","Describe the bug
Title pretty much says it all. The api docs specify a max_tokens value:
https://platform.openai.com/docs/api-reference/chat/create
However, when I use the api to create a request:
const completion = await openai.createChatCompletion({
	model: ""gpt-4"",
	messages: [
		{role: 'system', content: prompt},
		...msg
	],
	max_tokens: 300
	top_p: 1,
	frequency_penalty: 1,
	presence_penalty: 0.6,
	temperature: 0.5,
})

max_tokens doesn't seem to be specified in the input type for the function.
To Reproduce
Get the latest openai-node package (3.2.0)
Write out a createChatCompletion request
Try to use the max_tokens param in the request
Code snippets
No response
OS
Ventura 13.2
Node version
Node v18.12.0
Library version
openai 3.2.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/122","How to turn on networking","2023-07-10T01:28:34Z","Closed as not planned issue","No label","Describe the bug
my model is not connected during use
To Reproduce
my model is not connected during use
Code snippets
No response
OS
windows
Node version
node18.14.0
Library version
openai 3.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/121","GPT4 is now consistently cutting off its responses. It hasn't done this for me before. It also won't allow me to regenerate.","2023-07-10T01:27:09Z","Closed issue","No label","Describe the bug
Chat gpt-4 is now cutting off it's responses, requiring prompts to continue, sometimes needing this twice.
To Reproduce
Just enter a statement requiring a relatively long response on gpt4
Code snippets
No response
OS
Windows
Node version
Node v16.14.2
Library version
openai v3.0.1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/120","If the argument of mask image is empty in createImageEdit(), the API will result in error","2023-07-10T01:32:02Z","Closed issue","bug,fixed in v4","Describe the bug
according to the api.ts
 the function createImageEdit is defined as
createImageEdit: async (image: File, prompt: string, mask?: File, n?: number, size?: string, responseFormat?: string, user?: string, options: AxiosRequestConfig = {}): 

It is documented as
* @param {File} [mask] An additional image whose fully transparent areas (e.g. where alpha is zero) indicate where `image` should be edited. Must be a valid PNG file, less than 4MB, and have the same dimensions as `image`.

Found problem,
In the current document, it is referred to the version 3.1.0.
In the document, it is described to be optional.
In 3.2.1, if it is being left out, error will be prompted.
To Reproduce
call createImageEdit
leave the mask argument blank with empty string ''
submit to run and will see the error.
Code snippets
const completion = await openai.createImageEdit(
                    readStream, // File stream for the original image
                    ""A cute baby sea otter wearing a beret"",
                    '', // leave back for testing, if using a mask, will be fine
                    2,
                    ""256x256""
                  );

The test is conducted with original and mask image.
OS
macOS 12.6.3
Node version
v16.17.1
Library version
openai 3.2.1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/119","When trying to use gpt-4 as model, its return 404","2023-03-29T00:47:41Z","Closed issue","bug","Describe the bug
I am implementing an API in Node to call the GPT-4 chat completion feature, however, it is returning a 404 error both when calling directly through the API and using the library. Is this model not yet available for use?
To Reproduce
Just use the method of chat completion with GPT-4 as model
Code snippets
No response
OS
Windows
Node version
18.15.0
Library version
openai v3.2.1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/118","Support for new GPT4 models","2023-07-10T01:07:49Z","Closed issue","No label","Describe the feature or improvement you're requesting
`const configuration = new Configuration({
 apiKey: OPENAI_API_KEY,
 });
 const openai = new OpenAIApi(configuration);
function createCompletion(questionText) {
 const request = openai
 .createCompletion({
 model: ""gpt-3.5-turbo"",
 prompt: questionText,
 max_tokens: 1200,
 temperature: 0.7,
 })
 return request.then((response) => {
 return response.data
 })
 }`
When I set the model to new model ""gpt-3.5-turbo"", I got 404
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/117","When using createEdit() I get a 404 error with the message: ""The model: text-davinci-edit-001 does not exist""","2023-04-16T04:44:01Z","Closed issue","bug","Describe the bug
When making any request via the Node.js API method createEdit() I get a 404 response from the library with the following payload as the error:
{
    message: 'The model: `text-davinci-edit-001` does not exist',
    type: 'invalid_request_error',
    param: null,
    code: 'model_not_found'
}

It seems like something has changed on the Platform/API side since this was working about 3 days ago.
To Reproduce
Make any request using createEdit() and deconstruct the error, and shown in the code snippet below...
Code snippets
try {
    const response = await openai.createEdit({
        model: ""text-davinci-edit-001"",
        input: ""my dog is 3 years old"",
        instruction: ""reword this description to be about my cat instead of my dog"",
        temperature: 0.7,
        top_p: 1,
    });

    const text = response.data.choices[0].text;

    console.log(text);}catch (error) {
    console.log(error.response.data.error);}
OS
macOS
Node version
Node v18.13.0
Library version
openai v3.2.1
 The text was updated successfully, but these errors were encountered: 
👍1
medv reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/openai-node/issues/116","When accessing the createChatCompletion API method using a proxy,the status shows as 403","2023-07-10T01:27:20Z","Closed issue","bug","Describe the bug
when i run this code
(async () => {
    
    const { Configuration, OpenAIApi } = require(""openai"");
    const configuration = new Configuration({
      apiKey: 'openai_api_key',
    });
    
    const openai = new OpenAIApi(configuration);

    const response = await openai.createCompletion({
      model: ""gpt-3.5-turbo"",
      prompt: ""Say this is a test"",
      messages: [{role: 'user', content: '翻译：大海'}],
      max_tokens: 100,
      temperature: 0,
    });
    console.log(response);
  })();

and error:
data: {
      error: {
        message: 'The OpenAI API can only be accessed over HTTPS. You should access https://api.openai.com rather than the current URL.',
        type: 'invalid_request_error'
      }

but my current URL is ""https://api.openai.com/v1/completions""
To Reproduce
1, (node openai3.2.1) use openai.createChatCompletion method with proxy, connect error, status=403
 2, (python openai 0.27.2) use openai.ChatCompletion.create method with same proxy, connect successful, and get correct data.
Code snippets
No response
OS
macOS
Node version
Node v19.8.1
Library version
openai v3.1.2
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/115","When accessing the createChatCompletion API method using a proxy, the status shows as 200, but the ""data"" field is empty ("""").","2023-03-24T06:40:07Z","Closed issue","bug","Describe the bug
As shown in the figure, I must use a proxy to use the createChatCompletion API method properly, otherwise a 400 error will occur. After using the proxy, I received the returned data, which showed a successful 200 status, but with an empty ""data"" field. I am concerned that this may be due to proxy issues or problems with my API key, so I used the same proxy, API key, and ChatCompletion.create API method in Python, and was able to retrieve complete and correct returned data.
To Reproduce
1, (node openai3.2.1) direct use openai.createChatCompletion method without proxy, but connect error status=400
 2, (node openai3.2.1) use openai.createChatCompletion method with proxy, connect successful, status=200, but data=""""
 3, (python openai 0.27.2) use openai.ChatCompletion.create method with same proxy, connect successful, and get correct data.
Code snippets
No response
OS
win10
Node version
node v16
Library version
openai v3.2.1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/113","chatgpt-3.5-turbo automatically changes to chatgpt-3.5-turbo-0301","2023-07-10T00:51:04Z","Closed as not planned issue","No label","Describe the bug
I was simply calling the ""chatgpt-3.5-turbo"" model for chatCompletion
Error:
I noticed the model returned from response was chatgpt-3.5-turbo-0301. Is this normal?
Here's the code snippet that I used to call

Here's the response

To Reproduce
Call chatCompletionAPI
const completion = await openai.createChatCompletion({ model: ""gpt-3.5-turbo"", messages: messages, });
Check response
`const data = completion?.data
if(data && data.choices) {
  console.log('data', data)
  const choices = data.choices
  console.log('choices', choices)
} else {
  console.log('no data? completion', completion)
}`

Code snippets
No response
OS
macOS
Node version
node v16.15.1
Library version
""openai"": ""^3.2.1"",
 The text was updated successfully, but these errors were encountered: 
👍3
ohnosharks, medv, and lgh06 reacted with thumbs up emoji
All reactions
👍3 reactions"
"https://github.com/openai/openai-node/issues/112","Add edge folder","2023-07-10T01:42:09Z","Closed issue","fixed in v4","Describe the feature or improvement you're requesting
Instead of having other libraries like https://github.com/dan-kwiat/openai-edge , OpenAI's node library should support edge out of the box. like so: import { Configuration, OpenAIApi } from ""openai/edge"";
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
👍3
thomkrupa, tan-t, and abhagsain reacted with thumbs up emoji
All reactions
👍3 reactions"
"https://github.com/openai/openai-node/issues/111","Improve error for incorrect model name","2023-07-10T01:44:08Z","Closed issue","bug","Describe the bug
If using an incorrect model name, e.g. gpt-3-turbo instead of gpt-3.5-turbo, it returns 404. Would it be possible to catch these routes to return a response with ""unsupported model name requested"". And/or use the model names instead of string for the typescript definition instead of string to catch this?
To Reproduce
openai.createChatCompletion({
        model: 'gpt-3-turbo',
        messages,})
Code snippets
No response
OS
macOS
Node version
v18.12.1
Library version
v3.2.1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/110","Timeout on extremely simple query","2023-07-10T01:42:43Z","Closed issue","openai api","Describe the bug
I've run into a set of issues with the latency of OpenAI's API.
Specifically, every once in a while a request takes an extremely long time. For example, the request below took 145 seconds.
One potential cause is I did this query three times in a row. Maybe I was throttled?
To Reproduce
Run the same request three times in a row, the third or fourth time, it will take an extremely long time (> 50 seconds)
Code snippets
I initially tried: 

const response = await openai.createChatCompletion({
        model: 'gpt-3.5-turbo',
        messages: [
            { ""role"": ""system"", ""content"": 'this is a test'},
        ],
        temperature: 0,
        max_tokens: 200
    });
but I also tried the raw HTTP request
const response = await axios.post(
    'https://api.openai.com/v1/chat/completions',
    {
        model: 'gpt-3.5-turbo',
        messages: [
            { ""role"": ""system"", ""content"": 'say this is a test' },
            // { ""role"": ""user"", ""content"": user_string },
        ],
        temperature: 0,
        max_tokens: 200
    },
    {
        headers: {
            'Content-Type': 'application/json',
            'Authorization': 'Bearer ' + process.env['OPEN_AI']
        }
    }
).catch((e) => {console.log(e);});

OS
macOS
Node version
Node v14.20.0
Library version
3.2.1
 The text was updated successfully, but these errors were encountered: 
👍1
Olovorr reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/openai-node/issues/109","[Whisper] Support OGG file extension","2023-07-30T00:32:32Z","Closed issue","openai api","Describe the feature or improvement you're requesting
Dear OpenAI Team,
I am writing to request the addition of OGG file format support in the Whisper model. As you know, OGG is a popular open-source multimedia container format that is widely used for streaming, storing, and transmitting digital multimedia content such as audio and video.
Currently, the Whisper model supports only a limited number of audio file formats, such as WAV and MP3. However, many users, including myself, prefer to use OGG format due to its superior compression, quality, and open-source nature.
Therefore, I would like to request that the OpenAI team considers adding OGG file format support to the Whisper model. This would allow users to process and generate high-quality audio content in OGG format, which is important for many applications such as music production, podcasting, and voiceover work.
I believe that adding support for OGG file format in the Whisper model would be a valuable addition to the platform, and would help to expand the range of options available to users. Thank you for your consideration, and I look forward to hearing your response.
Sincerely,
 Ido
Additional context
Specifically opus codecs
 The text was updated successfully, but these errors were encountered: 
👍26
maxpain, lifeitech, dan19, TheFerryn, XogZ3, jesusmarcos, frandorr, Stvad, krambox, Fidelius-lp, and 16 more reacted with thumbs up emoji
All reactions
👍26 reactions"
"https://github.com/openai/openai-node/issues/108","Issue with ""createAnswer"" method (maybe deprecated?)","2023-07-10T01:44:45Z","Closed issue","bug","Describe the bug
When attempting to use the createAnwer method, it always fails with Error: Request failed with status code 404
{
 ""data"": {
      ""error"": {
        ""message"": ""Invalid URL (POST /v1/answers)"",
        ""type"": ""invalid_request_error"",
        ""param"": null,
        ""code"": null
      }
    }
}
To Reproduce
Create an index.mjs with the following code:
import { Configuration, OpenAIApi } from ""openai"";

const configuration = new Configuration({
  apiKey: process.env.OPENAI_API_KEY,});

const openai = new OpenAIApi(configuration);

const completion = await openai
  .createAnswer({
    model: ""text-davinci-003"",
    question: ""Is this a test?""
  })
  .then(({ data }) => data);

console.log("" completion : "", completion);
Run it using node:
OPENAI_API_KEY=sk-<your-key>  node index.mjs

Code snippets
No response
OS
macOs
Node version
v18.11.0
Library version
^3.1.0
 The text was updated successfully, but these errors were encountered: 
👍2
mbukeRepo and floscher reacted with thumbs up emoji
All reactions
👍2 reactions"
"https://github.com/openai/openai-node/issues/107","Issue with using 'stream' option in TypeScript with CreateChatCompletionResponse","2023-07-08T19:45:24Z","Closed issue","bug,fixed in v4","Describe the bug
I have been exploring how to use the 'stream' option in TypeScript for discussion #18. While trying to implement it, I found that the CreateChatCompletionResponse does not have a property 'on', which is required to stream the data.
To overcome this issue, I converted the CreateChatCompletionResponse to Readable type, and it worked perfectly. However, I believe this is a problem that needs to be addressed as it is not intuitive for TypeScript users.
To Reproduce
Use the createChatCompletion method to create a chat completion.
Add the 'stream' property and set it to 'true' to enable streaming.
Add the 'responseType' property to the axios config object and set it to 'stream' to specify the data format.
Try to access the 'on' property in the response data, which is not available in TypeScript.
Code snippets
const completion = await openai.createChatCompletion(
  {
    model: 'gpt-3.5-turbo',
    messages: [
      {
        role: 'user',
        content: 'Hello, how are you?',
      },
    ],
    stream: true,
  },
  { responseType: 'stream' });

completion.data.on('data', (data: Buffer) => {
  console.log(data.toString());});
OS
Windows 11 22621.1413
Node version
Node v18.8.0
Library version
openai v3.2.1
 The text was updated successfully, but these errors were encountered: 
👍12
mbukeRepo, UncaughtCursor, ChangQing666, SunghyunKwon, hjKangIB, sandinmyjoints, hannahlaek-wt, EmilBengtsson, 243f6a8885a308d313198a2e037, zhangsi929, and 2 more reacted with thumbs up emoji
All reactions
👍12 reactions"
"https://github.com/openai/openai-node/issues/105","Missing Authorization header in requests","2023-07-10T01:47:08Z","Closed issue","bug","Describe the bug
When running createCompletion, the error printed mentions that the request sent to openai does not have the Authorization header, even though i passed the apiKey to the client.
data: {
      error: {
        message: ""You didn't provide an API key. You need to provide your API key in an Authorization header using Bearer auth (i.e. Authorization: Bearer YOUR_KEY), or as the password field (with blank username) if you're accessing the API from your browser and are prompted for a username and password. You can obtain an API key from https://platform.openai.com/account/api-keys."",
        type: 'invalid_request_error',
        param: null,
        code: null
      }
    }

To Reproduce
Run the code below with your own API key
See the error
Code snippets
import { OpenAIApi } from ""openai"";

const client = new OpenAIApi({ apiKey: 'the_api_key_just_generated' })

await client.createCompletion({
  prompt: 'what is yellow and is waiting',
  max_tokens: 10,})


### OS

macOS

### Node version

18

### Library version

3.2.1

 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/104","Limit For The Embeddings Return","2023-09-03T15:59:27Z","Closed issue","openai api","Describe the feature or improvement you're requesting
When using the current API to get the embeddings for text it seems to return an arrays of over 1500 embeddings and no matter How long I tried to search there seems to be no parameter to limit the number of embeddings gotten
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
👍1
snapdeus reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/openai-node/issues/103","Error with openAI API: Cannot read property 'split' of undefined","2023-03-12T12:19:19Z","Closed issue","bug","Describe the bug
Getting error ""Error with openAI API: Cannot read property 'split' of undefined"" when trying to use openai.createChatCompletion
To Reproduce
Just a simple call using the createChatCompletion
Code snippets
const response = await openai.createChatCompletion({
		model: ""gpt-3.5-turbo"",
		messages: [{ ""role"": ""user"", ""content"": ""Hello!"" }]
	});
OS
macOs
Node version
Node v14.7.0
Library version
openai v3.2.1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/102","OpenAI in browser","2023-07-10T00:49:59Z","Closed issue","fixed in v4","Describe the feature or improvement you're requesting
I have been using huggingFaceJS and inputting my token through a input text field. Can the same tecnique be used with openAI Javascript, and if anyone has done it can you send me a link or an example to load a basic object detection code.
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
👍1
martin12333 reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/openai-node/issues/101","Azure deployment support","2023-03-10T10:17:01Z","Closed issue","No label","Describe the feature or improvement you're requesting
Hello there,
Is there a plan or a date of release for Azure support as in python library?
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/100","Define ""model"" as string literal union type","2023-04-16T07:12:38Z","Closed issue","No label","Describe the feature or improvement you're requesting
I am currently trying to post multiple requests using both chat completion and text completion at a same time using Promise.race like this.
const { data } = await Promise.race([
    openai.createChatCompletion({
      model: ""gpt-3.5-turbo"",
      messages: [
        {
          role: ""system"",
          content: prompt,
        },
      ],
      temperature: 0.9,
      n: 1,
    }),
    openai.createCompletion({
      model: ""text-davinci-003"",
      prompt,
      temperature: 0.9,
      n: 1,
      max_tokens: 2000,
    }),
  ]);
Both CreateChatCompletionResponse | CreateCompletionResponse has same key model, but I can't narrow type of response like this.
if (data.model === ""gpt-3.5-turbo"") {
  const { message } = data.choices[0]; // error!}
but, if type of model is defined as string literal union type like below, it will resolve the error. (so that it can be distinguished using discreminated union)
type ChatCompletionModel = ""gpt-3.5-turbo"" | ""gpt-3.5-turbo-0301"";interface CreateChatCompletionResponse = {
  model: ChatCompletionModel;
  /* ... */}

if (data.model === ""gpt-3.5-turbo"") {
   /* now the type checker knows the type of `data` is `CreateChatCompletionResponse` */}
I know the type of model should be string because of the fine-tuned models, but if possible, could you please define model type? I can resolve this issue by using custom type guards, but I think it would be nice to have more narrow type of model.
thank you!
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/99","SSL Errors, axios","2023-04-08T08:51:54Z","Closed issue","bug","Describe the bug
This error is constantly occurs when using createChatCompletion() method:
To Reproduce
openai.createChatCompletion({})
 .catch((error) => {})
Code snippets
No response
OS
Windows
Node version
Node v18.14.0
Library version
openai v3.2.1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/98","node report buffer error in createChatCompletion","2023-03-09T11:12:20Z","Closed issue","bug","Sorry, my mistake , please ignore this issuue
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/97","nodes","2023-03-08T17:23:41Z","Closed issue","bug","No description provided.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/96","Support for longer video/audio files in Whisper model","2023-09-03T15:30:35Z","Closed as not planned issue","enhancement,openai api","Describe the feature or improvement you're requesting
Currently, the Whisper model only supports video files that are up to 30 seconds long and audio files that are up to 25 MB in size.
This limitation can be a challenge for users who want to process longer video/audio files.
To overcome this limitation, I propose implementing a feature that allows the user to split their video/audio files into smaller segments and send multiple requests to the Whisper API to process each segment.
This will enable users to process longer video/audio files while still taking advantage of the Whisper model's powerful capabilities.
I would like to request the OpenAI team to consider implementing this feature or suggest any alternative solutions to handle longer videos/audio files using the Whisper model.
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
👍7
software-trizzey, Toby4200, caleblawrence, seriousidea, nosepoke, andrewk-dd, and JimmyLv reacted with thumbs up emoji
All reactions
👍7 reactions"
"https://github.com/openai/openai-node/issues/95","Arguments of createImageEdit() are in the wrong order compared to documentation","2023-07-10T01:19:14Z","Closed issue","bug,documentation,fixed in v4","Describe the bug
https://platform.openai.com/docs/api-reference/images/create-edit
According to the documentation, the arguments of OpenAIApi.createImageEdit are supposed to be in this order:
image,
mask,
prompt,
...

but the reality is that the method expects them (and only works if they are) in this order instead:
image
prompt
mask

Either the documentation is incorrect, or it is a bug in the library. You decide, actually. The documented order makes way more sense, so I'd suggest you treat it as a bug in the library, although then you'll break backward compatibility.
Or, it would be actually pretty easy to heuristically detect what argument is what and to support both orders. Theoretically it's possible to have a prompt like ""path/to/image.png"" and an image file whose filename is ""A painting of a rabbit doing drugs"", but I think it's more useful to support the use case of inverted arguments for BC than the one of crazy filenames and filename-like prompts.
To Reproduce
Run the code snippet.
Code snippets
const { Configuration, OpenAIApi } = require(""openai"");const configuration = new Configuration({
  apiKey: process.env.OPENAI_API_KEY,});const openai = new OpenAIApi(configuration);const response = await openai.createImageEdit(
  fs.createReadStream(""otter.png""),
  fs.createReadStream(""mask.png""),
  ""A cute baby sea otter wearing a beret"",
  2,
  ""1024x1024"");
OS
linux
Node version
irrelevant
Library version
3.2.1
 The text was updated successfully, but these errors were encountered: 
👍8
damianstasik, st3phhays, MuLoo, gpassero, ArsHinMQ, ShortBeard, francescozaia, and SimonBurmer reacted with thumbs up emoji❤️3
ShortBeard, gezzergarcia, and Ivannnnn reacted with heart emoji
All reactions
👍8 reactions
❤️3 reactions"
"https://github.com/openai/openai-node/issues/93","createTranscription() doesn't work as expected in NodeJS","2023-07-10T01:46:35Z","Closed issue","bug","Describe the bug
Hi!
I tried to follow the documentation when I was writing a transcription script using NodeJS. And I wanted to get a response in .srt format. But it returns an error. I tried to use the argument response_format as well as responseFormat(). But that didn't work. Also, there is only one way to communicate with OpenAI API:
 const resp = await openai.createTranscription(
        fs.createReadStream('audio.mp3'),
        ""whisper-1""
    );

But anyway, it doesn't work if I would like to specify the output file format.
To Reproduce
Run the function (one of them)
Get Required parameter model was null or undefined error
Code snippets
async function getTranscription() {
  const resp = await openai.createTranscription(
        fs.createReadStream('audio.mp3'),
        ""whisper-1"", 
        responseFormat('srt')
    );
    console.log(resp.data);
 }
const options = {
 file:  fs.createReadStream('audio.mp3'),
 model: ""whisper-1"",
 response_format: ""srt""
};

async function getTranscription() {
   const resp = await openai.createTranscription(options);
   console.log(resp.data);
}



### OS

macOS

### Node version

Node v.16.13.0

### Library version

openai v.3.2.1

 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/92","connect ETIMEDOUT - API's are not working","2023-07-10T00:47:34Z","Closed issue","bug","Describe the bug
Below attach code is working fine in curl, python but the same set of code is not working in Node JS (tried different versions) as well.
Below error -
Uncaught Error: connect ETIMEDOUT 64:ff9b::3498:60fc:443
 at TCPConnectWrap.afterConnect [as oncomplete] (node:net:1494:16)
 at TCPConnectWrap.callbackTrampoline (node:internal/async_hooks:130:17) {
 errno: -4039,
 code: 'ETIMEDOUT',
 syscall: 'connect',
 address: '64:ff9b::3498:60fc',
To Reproduce
Run the program in node shell and you will get the error
Code snippets
const { Configuration, OpenAIApi } = require(""openai"");

const configuration = new Configuration({
  apiKey: process.env.OPENAI_API_KEY,});const openai = new OpenAIApi(configuration);

const completion = await openai.createCompletion({
  model: ""text-davinci-003"",
  prompt: ""Hello world"",});console.log(completion.data.choices[0].text);
OS
macOS
Node version
Node v18.14.2
Library version
openai@3.0.0 / openai@3.2.1 / openai@3.2.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/91","Whisper fine tuning?","2023-07-10T01:50:13Z","Closed issue","openai api","Describe the feature or improvement you're requesting
Hi, this is a question. :) Will the Whisper API ever have fine tuning capabilities?
And will all whisper improvements be published on the public repo?
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/90","[Whisper] Support URLs for createTranscription call","2023-07-10T01:53:42Z","Closed issue","fixed in v4","Describe the feature or improvement you're requesting
Right now the only way to provide input to createTranscription method is to have the file on disk and create a ReadStream for that. It's a very common use case where the files are stored somewhere (cloud hosting most of the cases) and easiest way is to provide a signed URL (GCP case) to the audio file to be transcribed.
 When I'm providing a URL as a parameter for the audio file path I'm getting a ""400 - Bad Request"" back.
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
👍11
thomasmol, Seanitzel, daniilpogosyan, davidkathoh, cohogain, wacioc, atsixian, jeffkloy, jaehwlee, CGurg, and filandra reacted with thumbs up emoji
All reactions
👍11 reactions"
"https://github.com/openai/openai-node/issues/89","createChatCompletion() takes a long time to process.","2023-07-10T01:22:52Z","Closed issue","openai api","Describe the bug
As described in the title, the method takes a while to load. This is a big problem because, in Vercel, the timeout limit for any call is 5 seconds. And in Netlify, the limit is 10 seconds. But most often the call takes more than 10 seconds to respond. As a result, my website is not working after refactoring the site to use the new gpt-3.5-trubo model. (It works fine with davinci)
Basically, my website works on localhost but not when I deploy it to any service. Am I missing something? Is there a way to reduce the time?
To Reproduce
 const completion = await openai.createChatCompletion({
       model: ""gpt-3.5-turbo"",
       messages: [
           {
             role: 'user',
             content: ""Write a blog on artificial intelligence"",
           }
       ],
      });
This takes more than 10 seconds to complete.
Code snippets
No response
OS
Windows 11
Node version
v18.12.1
Library version
3.2.1
 The text was updated successfully, but these errors were encountered: 
👍7
Seanitzel, luoxuhai, siddiqss, eumemic, raphaelmerx, ikramhasan, and dafu-wu reacted with thumbs up emoji
All reactions
👍7 reactions"
"https://github.com/openai/openai-node/issues/88","Please recommend a reliable agent in mainland China","2023-07-10T01:54:34Z","Closed as not planned issue","No label","Describe the feature or improvement you're requesting
Please recommend a reliable agent in mainland China
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/87","Type CreateChatCompletionResponse doesn't seem to be exported correctly","2023-07-10T01:48:29Z","Closed issue","bug,fixed in v4","Describe the bug
On 3.2.1, can't seem to find the CreateChatCompletionResponse type definition, even though I see it in the definitions file:
https://github.com/openai/openai-node/blob/master/dist/api.d.ts#L345
No exported member detected when trying to export it in my TS project:

To Reproduce
Try to import type CreateChatCompletionResponse from openai
Cannot find type declaration
Code snippets
No response
OS
macOS
Node version
Node v16.17.0
Library version
openai v3.2.1
 The text was updated successfully, but these errors were encountered: 
👍2
topher-benjamin and shivan2418 reacted with thumbs up emoji
All reactions
👍2 reactions"
"https://github.com/openai/openai-node/issues/86","UI bug when prompting the model to ""continue""","2023-03-04T01:15:07Z","Closed issue","bug","Describe the bug
I ask for code in the prompt. It starts writing it, but the code written is too long. So, when I prompt it to 'continue', the response is a continuation of the code snippet, however it is recognised by the UI/front-end as a text, so it is written without the CSS for ""code""... rather it's shown as text. So, the next part where text is shown, it's written in a code-block design.
To Reproduce
Ask the model to generate any log piece of code, such that one response will not be enough.
Code snippets
No response
OS
macos
Node version
Node v16
Library version
Chatgpt
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/85","Plz add 'proxy' option for configuration","2023-07-10T01:41:37Z","Closed issue","fixed in v4","Describe the feature or improvement you're requesting
I want to break through some firewalls to access the openai api, but I don't want to affect the access speed of other services. In addition to customizing proxy rules for openai domain names, the best way is to support the configuration of request proxies, which will facilitate the overall configuration.
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/84","openai-node v3.2.1 createImageEdit() endpoint stopped working","2023-07-10T01:37:04Z","Closed issue","bug","Describe the bug
I've been using openai-node v3.1.0 and openai.createImageEdit() worked well.
I updated to the v3.2.1 to support the createChatCompletion() method, however my createImageEdit() is not working anymore. I get Error: Request failed with status code 400. If i downgrade to 3.1.0 is starts working back. Has createImageEdit() been changed in the latest update, so my call does not satisfy the new api? will add a code snippet wtih my method which works in v3.1.0 but doesn't in v3.2.1
To Reproduce
update from v3.1.0 to v.3.2.1
call createImageEdit (implementation provided in snippets section)
Code snippets
app.post('/inpaint', async (req, res) => {
    const base64Image = req.body.image.split(';base64,').pop();
    const path = `img-${uuidv4()}.png`;
    fs.writeFile(path, base64Image, {encoding: 'base64'}, async function(err) {
        const response = await openai.createImageEdit(
            // We send the same image as a base and as a mask
            fs.createReadStream(path),
            fs.createReadStream(path),
            req.body.prompt,
            1,
            '1024x1024'
        );
        fs.unlink(path, () => {
            console.log(`Deleted file ${path}`)
        });

        // Download & proxy image to mitigate CORS
        const imgPath = `static/${uuidv4()}.png`;
        await axios({
            method: 'get',
            url: response.data.data[0].url,
            responseType: 'arraybuffer',
        }).then(response => {
            fs.writeFileSync(imgPath, response.data);
        });

        res.json(`/${imgPath}`)
    });})
OS
macOS
Node version
v18.7.0
Library version
3.2.1
 The text was updated successfully, but these errors were encountered: 
👍3
mattisssa, kouohhashi, and zero41120 reacted with thumbs up emoji
All reactions
👍3 reactions"
"https://github.com/openai/openai-node/issues/83","No TS type for streamed data — uses message instead of delta","2023-07-08T19:43:14Z","Closed issue","bug,fixed in v4","Describe the bug
Streamed data still uses CreateChatCompletionResponseChoicesInner type which still contains message instead of delta.
To Reproduce
Reference the CreateChatCompletionResponseChoicesInner TS type
Code snippets
No response
OS
macOS v13.1 (22C65)
Node version
node v18.12.1
Library version
openai v3.2.1
 The text was updated successfully, but these errors were encountered: 
👍3
NarHakobyan, wong-codaio, and sciencefidelity reacted with thumbs up emoji
All reactions
👍3 reactions"
"https://github.com/openai/openai-node/issues/82","Incorrect TS typing for finish_reason","2023-07-13T19:53:04Z","Closed issue","bug,fixed in v4","Describe the bug
The typing for finish_reason sets its type to string | undefined but its actual type used is string | null.
To Reproduce
Reference a CreateChatCompletionResponse object and access the finish_reason property to inspect its type.
Code snippets
No response
OS
macOS v13.1 (22C65)
Node version
node v18.12.1
Library version
openai v3.2.1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/81","createChatCompletion method returning undefined in the choices array","2023-07-10T00:37:03Z","Closed as not planned issue","invalid","Describe the bug
I'm using node.js with the official opean ai lib and when I tried to send a request, I got nothing, the choices array is empty.
It's working fine with davinci model, just this new one model: ""gpt-3.5-turbo"", isn't working.
To Reproduce
Code snippets
No response
OS
windows
Node version
v10.19.0
Library version
3.2.1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/79","[Whisper] Update docs to better explain use of prompt and temperature for transcription and translation","2023-07-10T01:36:43Z","Closed issue","documentation","Describe the bug
The docs do not clearly explain the use of prompt and temperature when it comes to the transcription and translation endpoints.
Include a sample file with various prompt values to clarify.
To Reproduce
Read docs
Code snippets
No response
OS
Windows 11
Node version
Node v16.16.0
Library version
openai v3.2.0
 The text was updated successfully, but these errors were encountered: 
👍2
nt294 and spnq reacted with thumbs up emoji
All reactions
👍2 reactions"
"https://github.com/openai/openai-node/issues/77","[Whisper] cannot call createTranscription function from Node.js due to File API","2023-07-08T19:42:32Z","Closed issue","bug,fixed in v4","Describe the bug
Cannot call createTranscription function like below:
...
const audio = await fs.readFile('path/to/audio.mp4');
// Compile Error at the first argument
const response = await openai.createTranscription(audio, 'whisper-1');

This is because createTranscription interface asks me for File API, which is mainly for Browser API.
public createTranscription(file: File, model: string, prompt?: string, responseFormat?: string, temperature?: number, language?: string, options?: AxiosRequestConfig) {
  return OpenAIApiFp(this.configuration).createTranscription(file, model, prompt, responseFormat, temperature, language, options).then((request) => request(this.axios, this.basePath));
}

How can I use this function from Node.js?
 Thanks!
Node.js version: v18.14.2
MacOS Monterey

To Reproduce
...
const audio = await fs.readFile('path/to/audio.mp4');
// Compile Error at the first argument
const response = await openai.createTranscription(audio, 'whisper-1');

Code snippets
No response
OS
MacOS
Node version
Node v18.14.2
Library version
openai v3.2.1
 The text was updated successfully, but these errors were encountered: 
👍17
snettah, aetherplex, KyleTryon, ctb248, IsaacKnowles, jamescash, Maftalion, glebmachine, samuelsit, dosstx, and 7 more reacted with thumbs up emoji
All reactions
👍17 reactions"
"https://github.com/openai/openai-node/issues/76","Add support for error types.","2023-07-08T19:45:43Z","Closed issue","fixed in v4","Describe the feature or improvement you're requesting
For error handling, it would be great if error types were defined in the library, like in the python version (https://platform.openai.com/docs/guides/error-codes/python-library-error-types). It would be easier to detect rate limit errors, perform exponential backoff only for specific errors, etc.
 The text was updated successfully, but these errors were encountered: 
👍15
ruskakimov, NanoCode012, benilyxdd, holdenmatt, mrondin1, kanoi-y, Epimodev, niklasnikanti, rossmartin, Zakinator123, and 5 more reacted with thumbs up emoji
All reactions
👍15 reactions"
"https://github.com/openai/openai-node/issues/75","localVarFormParams.getHeaders is not a function","2023-07-10T00:38:51Z","Closed issue","bug,fixed in v4","Describe the bug
When I am using OpenAI node SDK to createTranscription,
I am sending it the file like this,
  const mediaBlob = await fetch(url)
    .then(response => response.blob());

const audio = new File(
    [mediaBlob],
    'demo.mp4',
    { type: 'video/mp4' },
  );

const resp = await openai.createTranscription(audio, 'whisper-1'); // even audio.text(), stream() etc doesn't work

It's throwing me localVarFormParams.getHeaders is not a function
To Reproduce
Try to send an audio file for transcription using openai.createTranscription
Try to send a file
Error thrown - localVarFormParams.getHeaders is not a function
Code snippets
No response
OS
macOS
Node version
Node v16.16.0
Library version
openai v3.2.1
 The text was updated successfully, but these errors were encountered: 
👍19
miroslavzeman, yermie, bkamapantula, sidmohanty11, marcosaurelioo, Seanitzel, k10wl, skovbaech, JoshSnider, JeffreyArts, and 9 more reacted with thumbs up emoji
All reactions
👍19 reactions"
"https://github.com/openai/openai-node/issues/74","TS error: Property 'delta' does not exist on type 'CreateChatCompletionResponseChoicesInner'. ts(2339)","2023-07-08T19:45:09Z","Closed issue","bug,fixed in v4","Describe the bug
Property 'delta' does not exist on type 'CreateChatCompletionResponseChoicesInner'.ts(2339)

To Reproduce
When streaming, try accessing completion.data.choices?.[0].delta instead of completion.data.choices?.[0].message
Code snippets
No response
OS
macOS v13.1 (22C65)
Node version
node v18.12.1
Library version
openai v3.2.1
 The text was updated successfully, but these errors were encountered: 
👍7
NarHakobyan, chanmathew, jasonsilvers, experteam-Campus, LuccaBuffara, dobeerman, and jklinect reacted with thumbs up emoji
All reactions
👍7 reactions"
"https://github.com/openai/openai-node/issues/73","[Chat]: Request failed with status code 401","2023-03-02T05:39:57Z","Closed issue","bug","Describe the bug
Request failed with status code 401!
I'm pretty sure my OPENAI_API_KEY is correct.
To Reproduce
fetch('https://api.openai.com/v1/chat/completions', {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
          'Authoriaztion': 'Bearer sk-xxxxx-xxx-xx-xxxxxx-'
        },
        body: {
          model: 'gpt-3.5-turbo',
          messages: [{ ""role"": ""user"", ""content"": ""Hello!"" }],
        }
      }).then(res => res.json().then(json => console.log(json)));

error: {
    message: ""You didn't provide an API key. You need to provide your API key in an Authorization header using Bearer auth (i.e. Authorization: Bearer YOUR_KEY), or as the password field (with blank username) if you're accessing the API from your browser and are prompted for a username and password. You can obtain an API key from https://platform.openai.com/account/api-keys."",
    type: 'invalid_request_error',
    param: null,
    code: null
  }

Code snippets
No response
OS
windows10
Node version
v18.13.0
Library version
3.2.1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/72","Latest release has typescript issues","2023-07-08T19:44:56Z","Closed issue","bug,fixed in v4","Describe the bug
After the new GPT-3.5 update, building my typescript app now failed.
 I get errors:
node_modules/openai/api.ts:3819:38 - error TS2304: Cannot find name 'File'.

3819     public createTranscription(file: File, model: string, prompt?: string, responseFormat?: string, temperature?: number, options?: AxiosRequestConfig) {
                                          ~~~~

node_modules/openai/api.ts:3835:36 - error TS2304: Cannot find name 'File'.

3835     public createTranslation(file: File, model: string, prompt?: string, responseFormat?: string, temperature?: number, options?: AxiosRequestConfig) {

To Reproduce
npm i openai@latest
 npm run build
Code snippets
No response
OS
windows/linux
Node version
18
Library version
3.2.0
 The text was updated successfully, but these errors were encountered: 
👍7
jboolean, davemeyer, jaimefps, nikwen, dsernst, roblframpton, and brookmg reacted with thumbs up emoji
All reactions
👍7 reactions"
"https://github.com/openai/openai-node/issues/71","I see the project was updated a few hours ago, does this support ""gpt-3.5-turbo""","2023-07-10T01:43:20Z","Closed issue","No label","Describe the feature or improvement you're requesting
I tried the new ""gpt-3.5-turbo"" with my previous install and I get an 404 error on return. Has this been updated for the new ""gpt-3.5-turbo"" model?
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
👍11
1dolinski, DanNicolau, kevinNgws, dytra, Sain-Tech, 1g0rrr, nem035, ElProfessorFR, FrenchMajesty, AnandChowdhary, and murderteeth reacted with thumbs up emoji
All reactions
👍11 reactions"
"https://github.com/openai/openai-node/issues/70","max_tokens not defined as a valid parameter in CreateChatCompletionRequest","2023-03-01T22:03:57Z","Closed issue","bug","Describe the bug
It's perfectly fine to pass max_tokens to a chat completion request.
It's documented, and I even tried it myself.
Despite that, the types are not defined in the interface for CreateChatCompletionRequest
To Reproduce
Perform a createChatCompletion request.
Pass the max_tokens parameter to your request.
You should see the warning:
Object literal may only specify known properties, and 'max_tokens' does not exist in type 'CreateChatCompletionRequest'. 
Code snippets
No response
OS
macOS
Node version
v16.17.0
Library version
openai v3.2.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/68","[gpt-3.5-turbo] Getting ""Invalid status code: Cannot read properties of undefined (reading 'create')""","2023-03-01T20:36:33Z","Closed issue","bug","Describe the bug
I'm getting ""Invalid status code: Cannot read properties of undefined (reading 'create')"" on the next Node JS code:
To Reproduce
const { Configuration, OpenAIApi } = require(""openai"");
 const openai = new OpenAIApi(configuration);
const completion = await openai.ChatCompletion.create(
 model=""gpt-3.5-turbo"",
 messages=[
 {""role"": ""system"", ""content"": ""You are a helpful assistant.""},
 {""role"": ""user"", ""content"": ""How are you?""},
 ],
 max_tokens=1000,
 );
Code snippets
No response
OS
Windows 11
Node version
Node.js v18.12.1
Library version
3.2.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/67","3.2.0 does not work with newest models","2023-03-01T20:03:33Z","Closed issue","bug","Describe the bug
I just upgraded to 3.2.0, but the API throws an error for me when being used with the newest models.
To Reproduce
import { Configuration, OpenAIApi }  from  ""openai"";

const configuration = new Configuration({
  apiKey: process.env.OPENAI_API_KEY,});const openai = new OpenAIApi(configuration);

const completion = await openai.createCompletion({
  model: ""gpt-3.5-turbo"",
  prompt: ""Hello world"",});console.log(completion.data.choices[0].text);
Code snippets
This is (part) of the error message I receive:

data: {
      error: {
        message: 'Invalid URL (POST /v1/completions)',
        type: 'invalid_request_error',
        param: null,
        code: null
      }
    }


### OS

macOS

### Node version

Node.js v19.4.0

### Library version

openai v3.2.0

 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/66","gpt-3.5-turbo model example","2023-03-01T19:12:40Z","Closed issue","No label","Describe the feature or improvement you're requesting
Hey guys, Amazing work on the quick update for the node library with ChatGPT api.
 Since the website's playground still does not have a gpt-3.5-turbo model option, neither is there any documentation on node. Could you please add how to use gpt-3.5-turbo model with a few examples.
Thanks
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
👍2
Frohrer and asp3 reacted with thumbs up emoji
All reactions
👍2 reactions"
"https://github.com/openai/openai-node/issues/65","Unnecessary required isJsonMime property","2023-07-10T01:47:27Z","Closed issue","bug,fixed in v4","Describe the bug
BaseAPI::constructor requires an instance of Configuration.
Configuration class requires setting an unnecessary isJsonMime property.
To Reproduce
As per the docs, to create a new instance of OpenAIApi, this should suffice:
const configuration = new Configuration({
  apiKey: process.env.OPENAI_API_KEY,});
When I do that, TypeScript complains because I must implement isJsonMime(mime: string): boolean;.
The error goes away if I write it as such:
const configuration = new Configuration({
  apiKey: process.env.OPENAI_API_KEY,
  isJsonMime: () => true,});
Code snippets
No response
OS
Linux (Ubuntu) 22.10
Node version
v18.7.0
Library version
3.1.0
 The text was updated successfully, but these errors were encountered: 
👍2
conrc and lezsakdomi reacted with thumbs up emoji
All reactions
👍2 reactions"
"https://github.com/openai/openai-node/issues/64","API question: how to make a uuid for topic, and can keep post request(ask questions) in this uuid topic?","2023-07-10T01:32:40Z","Closed issue","documentation,enhancement","Describe the feature or improvement you're requesting
such as Create completion get 1st response,
 and then I need keep continue ask question based on this opened completion.
In case I need start two topic/task/completion at one time, and can keep asking each other... like one task for help me doing office doc, and other one for doing excel... at the same time in two completion, so i need uuid for completion
 thus, during next post, i wanna bring this uuid to make sure GPT in the right track.
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
👍1
jackson-sandland reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/openai-node/issues/63","Open AI Bug Reveals Previous User Response Instead of Mine","2023-07-10T01:20:35Z","Closed issue","bug","Describe the bug
When I ask Rewrite the previous response Or Show a Previous Response instead of My answer, it shows a different user response instead of my prompts every time.


To Reproduce
Rewrite the previous response
Code snippets
No response
OS
Windows 10
Node version
Node Unknown
Library version
ChatGPT Feb 13 Version
 The text was updated successfully, but these errors were encountered: 
👎2
sathoro and kitfit-dave reacted with thumbs down emoji
All reactions
👎2 reactions"
"https://github.com/openai/openai-node/issues/62","Why is the result of my request response always incomplete?","2023-07-10T02:02:47Z","Closed issue","bug","Describe the bug
Is there a maximum length set for the output result?
To Reproduce
i just try to run the demo and find that the result is always incomplete
Code snippets
No response
OS
windows
Node version
node v14.16.0
Library version
openai v3.1.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/61","Conversation and Parent Message Id Feature Request","2023-09-03T15:31:22Z","Closed as not planned issue","enhancement","The conversation_id and parent_id parameters can be used to keep track of the context of a conversation between a user and an AI.
The conversation_id parameter is a unique identifier for a specific conversation, and it can be generated by the client. This allows the API to keep track of all messages in a specific conversation and return relevant responses.
The parent_id parameter is used to identify the previous message in the conversation and provides context for the current message. By including the parent_id in the API request, you can indicate to the API which message should be considered the previous message in the conversation, and the API can use this information to generate a more contextually appropriate response.
async function sendMessage(conversation_id, parent_id, message) {
  const response = await axios.post('https://api.openai.com/v1/engines/davinci/completions', {
    prompt: message,
    max_tokens: 100,
    temperature: 0.5,
    n: 1,
    conversation_id,
    parent_id
  }, {
    headers: {
      'Content-Type': 'application/json',
      'Authorization': 'Bearer <API_KEY>'
    }
  });

  return response.data.choices[0].text;}
please add support them to returned response from api and send request api.
 The text was updated successfully, but these errors were encountered: 
👍20
Sayene, leducgiachoang, almerindo, ccpu, iosifnicolae2, vamonke, qhkm, mwarger, Ligengxin96, derogab, and 10 more reacted with thumbs up emoji🎉4
leducgiachoang, almerindo, simpledev0042, and jung-han reacted with hooray emoji❤️6
leducgiachoang, almerindo, mariodian, Ligengxin96, simpledev0042, and jung-han reacted with heart emoji
All reactions
👍20 reactions
🎉4 reactions
❤️6 reactions"
"https://github.com/openai/openai-node/issues/60","openai.createEmbedding() bug: cannot pass in array for ""input"" param","2023-07-09T16:42:10Z","Closed issue","bug,fixed in v4","Describe the bug
When I pass in an array of strings for the ""input"" param as specified here, the connection times out and eventually I get an error 500 with the error message: ""Error: read ECONNRESET"".
Passing in a non-array string for ""input"" works fine.
I am using Next.js using their serverless endpoints testing on localhost.
To Reproduce
Reproduction info in description
Code snippets
try {
    const promptEmbeddingsRes = await openAi.createEmbedding({
      model: 'text-embedding-ada-002',
      input: ['test', 'test', 'test'], 
    })
    console.log('prompt embeddings', promptEmbeddingRes.data.data)
    return res.status(200).json(promptEmbeddingsRes.data.data)
  } catch (e) {
    console.error('Error creating embedding: ', e)
    return res.status(500).send(""Error creating embedding: "" + e)
  }
OS
macOS
Node version
v16
Library version
3.1
 The text was updated successfully, but these errors were encountered: 
👍1
HenryKeen reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/openai-node/issues/59","The completion data is always empty","2023-02-10T08:33:29Z","Closed issue","bug","Describe the bug
I tested code on Mac Ventuna, node v14.21.1, but I cannot get the expected response data.
To Reproduce
RESPONSE:
 {
  status: 200,
  statusText: 'Connection established',
  headers: {},
  config: {
    transitional: {
      silentJSONParsing: true,
      forcedJSONParsing: true,
      clarifyTimeoutError: false
    },
    adapter: [Function: httpAdapter],
    transformRequest: [ [Function: transformRequest] ],
    transformResponse: [ [Function: transformResponse] ],
    timeout: 0,
    xsrfCookieName: 'XSRF-TOKEN',
    xsrfHeaderName: 'X-XSRF-TOKEN',
    maxContentLength: -1,
    maxBodyLength: -1,
    validateStatus: [Function: validateStatus],
    headers: {
      Accept: 'application/json, text/plain, */*',
      'Content-Type': 'application/json',
      'User-Agent': 'OpenAI/NodeJS/3.1.0',
      Authorization: 'Bearer omited',
      'Content-Length': 135,
      host: 'api.openai.com'
    },
    method: 'post',
    data: '{""model"":""text-davinci-003"",""prompt"":""say test"",""max_tokens"":1024,""top_p"":1,""frequency_penalty"":0,""presence_penalty"":0}',
    url: 'https://api.openai.com/v1/completions'
  },
  request: <ref *1> ClientRequest {
    _events: [Object: null prototype] {
      abort: [Function (anonymous)],
      aborted: [Function (anonymous)],
      connect: [Function (anonymous)],
      error: [Function (anonymous)],
      socket: [Function (anonymous)],
      timeout: [Function (anonymous)],
      prefinish: [Function: requestOnPrefinish]
    },
    _eventsCount: 7,
    _maxListeners: undefined,
    outputData: [],
    outputSize: 0,
    writable: true,
    destroyed: false,
    _last: true,
    chunkedEncoding: false,
    shouldKeepAlive: false,
    _defaultKeepAlive: true,
    useChunkedEncodingByDefault: true,
    sendDate: false,
    _removedConnection: false,
    _removedContLen: false,
    _removedTE: false,
    _contentLength: null,
    _hasBody: true,
    _trailer: '',
    finished: true,
    _headerSent: true,
    socket: Socket {
      connecting: false,
      _hadError: false,
      _parent: null,
      _host: null,
      _readableState: [ReadableState],
      _events: [Object: null prototype],
      _eventsCount: 9,
      _maxListeners: undefined,
      _writableState: [WritableState],
      allowHalfOpen: false,
      _sockname: null,
      _pendingData: null,
      _pendingEncoding: '',
      server: null,
      _server: null,
      parser: null,
      _httpMessage: [Circular *1],
      write: [Function: writeAfterFIN],
      [Symbol(async_id_symbol)]: 2,
      [Symbol(kHandle)]: null,
      [Symbol(kSetNoDelay)]: false,
      [Symbol(lastWriteQueueSize)]: 0,
      [Symbol(timeout)]: null,
      [Symbol(kBuffer)]: null,
      [Symbol(kBufferCb)]: null,
      [Symbol(kBufferGen)]: null,
      [Symbol(kCapture)]: false,
      [Symbol(kBytesRead)]: 39,
      [Symbol(kBytesWritten)]: 435,
      [Symbol(RequestTimeout)]: undefined
    },
    _header: 'POST https://api.openai.com/v1/completions HTTP/1.1\r\n' +
      'Accept: application/json, text/plain, */*\r\n' +
      'Content-Type: application/json\r\n' +
      'User-Agent: OpenAI/NodeJS/3.1.0\r\n' +
      'Authorization: Bearer omited\r\n' +
      'Content-Length: 135\r\n' +
      'host: api.openai.com\r\n' +
      'Connection: close\r\n' +
      '\r\n',
    _keepAliveTimeout: 0,
    _onPendingData: [Function: noopPendingOutput],
    agent: Agent {
      _events: [Object: null prototype],
      _eventsCount: 2,
      _maxListeners: undefined,
      defaultPort: 80,
      protocol: 'http:',
      options: [Object],
      requests: {},
      sockets: [Object],
      freeSockets: {},
      keepAliveMsecs: 1000,
      keepAlive: false,
      maxSockets: Infinity,
      maxFreeSockets: 256,
      scheduling: 'lifo',
      maxTotalSockets: Infinity,
      totalSocketCount: 1,
      [Symbol(kCapture)]: false
    },
    socketPath: undefined,
    method: 'POST',
    maxHeaderSize: undefined,
    insecureHTTPParser: undefined,
    path: 'https://api.openai.com/v1/completions',
    _ended: true,
    res: IncomingMessage {
      _readableState: [ReadableState],
      _events: [Object: null prototype],
      _eventsCount: 4,
      _maxListeners: undefined,
      socket: [Socket],
      httpVersionMajor: 1,
      httpVersionMinor: 1,
      httpVersion: '1.1',
      complete: true,
      headers: {},
      rawHeaders: [],
      trailers: {},
      rawTrailers: [],
      aborted: false,
      upgrade: false,
      url: '',
      method: null,
      statusCode: 200,
      statusMessage: 'Connection established',
      client: [Socket],
      _consuming: true,
      _dumped: false,
      req: [Circular *1],
      responseUrl: 'https://api.openai.com/v1/completions',
      redirects: [],
      [Symbol(kCapture)]: false,
      [Symbol(RequestTimeout)]: undefined
    },
    aborted: false,
    timeoutCb: null,
    upgradeOrConnect: false,
    parser: null,
    maxHeadersCount: null,
    reusedSocket: false,
    host: '127.0.0.1',
    protocol: 'http:',
    _redirectable: Writable {
      _writableState: [WritableState],
      _events: [Object: null prototype],
      _eventsCount: 3,
      _maxListeners: undefined,
      _options: [Object],
      _ended: true,
      _ending: true,
      _redirectCount: 0,
      _redirects: [],
      _requestBodyLength: 135,
      _requestBodyBuffers: [],
      _onNativeResponse: [Function (anonymous)],
      _currentRequest: [Circular *1],
      _currentUrl: 'https://api.openai.com/v1/completions',
      [Symbol(kCapture)]: false
    },
    [Symbol(kCapture)]: false,
    [Symbol(kNeedDrain)]: false,
    [Symbol(corked)]: 0,
    [Symbol(kOutHeaders)]: [Object: null prototype] {
      accept: [Array],
      'content-type': [Array],
      'user-agent': [Array],
      authorization: [Array],
      'content-length': [Array],
      host: [Array]
    }
  },
  data: ''
}

^^^^the data is always empty
Code snippets
const { Configuration, OpenAIApi } = require(""openai"");

const configuration = new Configuration({
  apiKey: '....omited',});const openai = new OpenAIApi(configuration)

async function main () {
  const response = await openai.createCompletion({
    model: ""text-davinci-003"",
    prompt: 'say test',
    max_tokens: 7,
    top_p: 1,
    frequency_penalty: 0,
    presence_penalty: 0,
  })
  console.log('GPT RESPONSE：', response)
  console.log(response.data)}main()
OS
macOS
Node version
Node v14.21.1
Library version
openai v3.1.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/58","You requested a model that is not compatible with this engine","2023-07-10T01:46:04Z","Closed issue","bug","Describe the bug
You requested a model that is not compatible with this engine
To Reproduce
i meet a error: ""You requested a model that is not compatible with this engine"" yesterday, but it can run normally before
Code snippets
const response = await openai.createCompletion({
      model: 'text-davinci-003',
      prompt: prompt,
      temperature: 0.7, 
      top_p: 1,
      max_tokens: 4000,
      frequency_penalty: 0.0,
      presence_penalty: 0.6,
      stop: [' Human:', ' AI:'],
    })
OS
macos
Node version
node v18
Library version
openai 3.1.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/57","Is there a way to detect source code in ChatGPT responses ?","2023-07-10T02:03:14Z","Closed issue","openai api","Describe the feature or improvement you're requesting
Hello,
Is there a way to detect source code in ChatGPT responses ?
Example:
Prompt: how to write hello world using c#
Reposnse:
{
    ""id"": ""cmpl-xxx"",
    ""object"": ""text_completion"",
    ""created"": 1675398039,
    ""model"": ""text-davinci-003"",
    ""choices"": [
        {
            ""text"": ""\n\nThis code below will print the phrase \""Hello world\"" to the screen using the C# language.:\n\nusing System;\n\nnamespace HelloWorld\n{\n    class Program\n    {\n        static void Main(string[] args)\n        {\n            Console.WriteLine(\""Hello World!\"");\n        }\n    }\n}"",
            ""index"": 0,
            ""logprobs"": null,
            ""finish_reason"": ""stop""
        }
    ],
    ""usage"": {
        ""prompt_tokens"": 123,
        ""completion_tokens"": 123,
        ""total_tokens"": 123
    }
}
I need to parse and format the source code in `text', like that:
=================================================
This code below will print the phrase ""Hello world"" to the screen using the C# language:
using System;

public class HelloWorld 
{ 
    public static void Main() 
    { 
        Console.WriteLine(""Hello World!""); 
    } 
}
=================================================
 Thanks in advanced !
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/56","CreateModeration gives 400","2023-02-01T09:56:39Z","Closed issue","bug","Describe the bug
createModeration is giving 400 for all models/input variations.
To Reproduce
  const openai = new OpenAIApi(
    new Configuration({
      apiKey: process.env.OPEN_AI_SECRET,
    })
  );

  // gives 400 without clues
  const moderation = await openai.createModeration({
    model: ""text-davinci-003"",
    input: ""This is a very nice text"",
  });
OS
macOs
Node version
v18.12.1
Library version
3.1.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/55","回答出问题了，代码太旧了","2023-02-04T18:40:20Z","Closed issue","bug","Describe the bug
To Reproduce
Code snippets
No response
OS
windows
Node version
Node v16
Library version
openai^3.1.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/54","prompt history","2023-02-04T18:42:39Z","Closed issue","No label","Describe the feature or improvement you're requesting
Maybe it exists and I am not finding how, I need to make multiple calls while maintaining history, for example:
1- What is the size of the earth?
 2- And of the moon?
Does this functionality exist or would it be a feature?
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/53","support Microsoft Azure OpenAI service endpoints","2023-07-10T01:06:21Z","Closed issue","No label","Describe the feature or improvement you're requesting
Update the API configuration to support Azure openai endpoints as well.
In order to use the Python OpenAI library with Microsoft Azure endpoints, we need to set the api_type, api_base and api_version in addition to the api_key. The api_type must be set to 'azure' and the others correspond to the properties of your endpoint. In addition, the deployment name must be passed as the engine parameter.
python
import openai
 openai.api_type = ""azure""
 openai.api_key = ""...""
 openai.api_base = ""https://example-endpoint.openai.azure.com""
 openai.api_version = ""2022-12-01""
create a completion
completion = openai.Completion.create(engine=""deployment-name"", prompt=""Hello world"")
print the completion
print(completion.choices[0].text)
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
👍44
jmeviler, gberman-coursera, aymenfurter, jose-workpath, jtvcodes, chech0x, sheikhshack, keijiinoue, koudaiii, hellion22, and 34 more reacted with thumbs up emoji👀3
delgermurun, fragno, and cypherfunc reacted with eyes emoji
All reactions
👍44 reactions
👀3 reactions"
"https://github.com/openai/openai-node/issues/52","Max prompt token not work when using text-davinci-003","2023-01-26T10:55:45Z","Closed issue","bug","Describe the bug
Hi team, I'm using openai pkg with model text-davinci-003 in my code.
I tried to use createCompletion to get a response back, and all work well if I just make prompt length blew 2000.
 But if I use a prompt which more than 2000 tokens, it will return 400 error.
I can see the document told us it could accept 4000 tokens.
https://beta.openai.com/docs/models/gpt-3
SO is it a bug will be fixed in the future?
Calculate the tokens: https://beta.openai.com/tokenizer
Node version: v18.12.0
Openai package version: 3.1.0
model: text-davinci-003
Here is the code with params:
await openAIAgent.createCompletion({
  model: ""text-davinci-003"",
  prompt: prompt,
  temperature: 0.3,
  max_tokens: 2048,
  top_p: 1.0,
  frequency_penalty: 0.8,
  presence_penalty: 0.0,
})

To Reproduce
Use createCompletion, the prompt should more than 2000 tokens, maybe directly use a prompt between 3000 and 4000
 The params same as follow code:
await openAIAgent.createCompletion({
  model: ""text-davinci-003"",
  prompt: prompt,
  temperature: 0.3,
  max_tokens: 2048,
  top_p: 1.0,
  frequency_penalty: 0.8,
  presence_penalty: 0.0,
})

Code snippets
No response
OS
macOS
Node version
Node v18.12.0
Library version
openai v3.1.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/51","Create model through API","2023-02-04T18:41:42Z","Closed issue","No label","Describe the feature or improvement you're requesting
Is it possible to create and train a model through the api or only train?
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
👍1
staminna reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/openai-node/issues/50","hi why i can't access openAI","2023-01-21T18:59:10Z","Closed issue","No label","Describe the feature or improvement you're requesting
HI i use VPN to access chatGPT why you block some countrys ???
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/49","It is posibble to save chat just like chatGPT does?","2023-07-10T01:26:40Z","Closed issue","duplicate","Describe the feature or improvement you're requesting
I like to use the api to ask question, but you know the chatGPT will save the chat conversation, next time you ask a question in the chat, it will answer based on the chat conversations. But this api of the openai library using seems did not save the chat.
here is the test:


Here is the chatGPT did.

Additional context
I dont know why it did not save that chat as chatGPT does, because its free account or something else?
 I can pay for the api that can save chat and then response just like chatGPT did. I need help. Thanks.
 The text was updated successfully, but these errors were encountered: 
👍1
vitor-harmonize reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/openai-node/issues/48","Cannot find name File.","2023-07-10T01:28:17Z","Closed issue","bug,fixed in v4","Describe the bug
Imported the openai module and I'm getting an error for an undefined type File.
To Reproduce
Run npm install openai.
Create a new NodeJS server-side project.

Code snippets
No response
OS
Windows 10
Node version
v16.6.1
Library version
v3.1.0
 The text was updated successfully, but these errors were encountered: 
👍3
Jarzka, avigpt, and alzin reacted with thumbs up emoji
All reactions
👍3 reactions"
"https://github.com/openai/openai-node/issues/47","replace axios with fetch","2023-07-08T19:46:19Z","Closed issue","fixed in v4","Describe the feature or improvement you're requesting
Axios is not compatible with other runtimes (for example Edge).
Significant reduction in size
Fetch support for all runtimes (browser, node, edge, deno, workers)
The fetch lib could optionally be passed as a dependency
ref: https://developer.mozilla.org/en-US/docs/Web/API/Fetch_API
Additional context
When running current version with axios on the Edge I got this error:
An error occurred during OpenAI request [TypeError: adapter is not a function]

 The text was updated successfully, but these errors were encountered: 
👍20
avindra, thomasgauvin, pspeter3, jankaifer, rawnly, ItsWendell, redcpp, jimmywarting, mmkal, polRk, and 10 more reacted with thumbs up emoji
All reactions
👍20 reactions"
"https://github.com/openai/openai-node/issues/46","CreateCompletion fails with prompts > 478 characters","2023-01-10T20:17:35Z","Closed issue","bug","Describe the bug
openai.createCompletion({}) throws an error with message ""Request failed with status code 400"" with the following call:
const response = await openai.createCompletion({ model: ""text-davinci-003"", prompt: p, max_tokens, temperature });
 Where
 p = ""Devin: Hello, how can I help you? you: What can you do for me Devin: I can help you with any questions you may have about our products or services. I can also provide you with information about our company and answer any other questions you may have. you: Okay tell me about your company Devin: Sure! Our company is a leading provider of innovative technology solutions. We specialize in developing custom software and hardware solutions for businesses of all sizes. We have alto""
 max_tokens = 4000
 temperature = 0.0
My configuration is configured correctly, as all calls with prompt < 478 characters works, but once I get past this character limit, it starts to fail every time.
To Reproduce
call
const response = await openai.createCompletion({ model: ""text-davinci-003"", prompt: p, max_tokens, temperature });
 with p = any string longer than 478 characters. Use example string above.
Code snippets
Error response given back to me:
`{""message"":""Request failed with status code 400"",""name"":""Error"",""stack"":""Error: Request failed with status code 400\n    at createError (node_modules/axios/lib/core/createError.js:16:15)\n    at settle (node_modules/axios/lib/core/settle.js:17:12)\n    at IncomingMessage.handleStreamEnd (node_modules/axios/lib/adapters/http.js:322:11)\n    at IncomingMessage.emit (node:events:539:35)\n    at endReadableNT (node:internal/streams/readable:1345:12)\n    at processTicksAndRejections (node:internal/process/task_queues:83:21)"",""config"":{""transitional"":{""silentJSONParsing"":true,""forcedJSONParsing"":true,""clarifyTimeoutError"":false},""transformRequest"":[null],""transformResponse"":[null],""timeout"":0,""xsrfCookieName"":""XSRF-TOKEN"",""xsrfHeaderName"":""X-XSRF-TOKEN"",""maxContentLength"":-1,""maxBodyLength"":-1,""headers"":{""Accept"":""application/json, text/plain, */*"",""Content-Type"":""application/json"",""User-Agent"":""OpenAI/NodeJS/3.1.0"",""Authorization"":""Bearer sk-***"",""Content-Length"":553},""method"":""post"",""data"":""{\""model\"":\""text-davinci-003\"",\""prompt\"":\""Devin: Hello, how can I help you? you: What can you do for me Devin: I can help you with any questions you may have about our products or services. I can also provide you with information about our company and answer any other questions you may have. you: Okay tell me about your company Devin: Sure! Our company is a leading provider of innovative technology solutions. We specialize in developing custom software and hardware solutions for businesses of all sizes. We have alto\"",\""max_tokens\"":4000,\""temperature\"":0}"",""url"":""https://api.openai.com/v1/completions""},""status"":400}`

The above was printed using JSON.stringify FYI
OS
macos
Node version
node 16
Library version
3.1.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/44","Search Option for previous conversations, groupings for similar chats","2023-02-04T18:43:34Z","Closed issue","No label","Describe the feature or improvement you're requesting
It would be amazingly helpful if when exceeding 5 saved conversations, having 6 and up would grant a search bar in the chat selection. This way if you need to pull text or ideas from previous chats, you can simply search not just titles, but keywords. For example, you might ask in a previous conversation about planning out a goal. Maybe after talking and problem-solving keeping your resolutions you ask for a book recommendation or source to learn more from. A week later you are on amazon and you think maybe you want that book the ai recommended. So now the previous chat which was titled ""new years resolution goals"" will come up when typing in new Year, book/books, or the title of the book such as atomic habits. To take this idea further you could ask to group conversations, maybe you have three priorities for the year beyond general goals and plans, one working out, the second one diet, and the third general goals/habits. You could then group these into one titled New Year. Perhaps the ai would do this finding similar context between threads but even a manual option would be nice.
Additional context
In using the ai so much for general questions, having new ideas, or simply keeping a branching idea separate from the previous topic, my prior chat selection box has the need a lot of scrolling when pulling out prior topics. This would greatly help out when recalling a record of past conversations.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/43","Ability to copy without having to request new format","2023-02-04T18:43:55Z","Closed issue","No label","Describe the feature or improvement you're requesting
The ability to copy was something I found lacking at first, but as I played around a bit more I remembered that this thing can do basically anything, so I told it to put the text it generated into a copy-abel format. Usually, this works and if not I can say put it into code format. It would however be more convenient to have a copy button right there at the top right corner, below the prompt you entered; this way you would not have the same thing twice if you needed, for example, a record of notes. It would also mean fewer things to do for the ai as you would not need to request different formats to do one thing. I am aware I could highlight and copy manually, but sometimes text can get lengthy when fleshing out ideas.
Additional context
This is mostly used for lengthy text generation, and because for the time being you occasionally have to tell the ai to continue what it was typing, it would greatly increase efficiency when moving stuff into a notes folder on EverNote. This is not something that is totally undoable, but adding this feature which the code format already has would make things a bit more complete and natural for creative and productive work.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/42","Information of updates upon request","2023-02-04T18:44:37Z","Closed issue","No label","Describe the feature or improvement you're requesting
It would be nice for the ability to ask open ai what new features it has.
Additional context
It can already tell you what the ai its self is capable of, but it seems oblivious to any new changes in updates. It would be nice if upon request it said new bug fixes, features added, and current version model. This would help to know if any issues or quality of life improvements have changed without having to play around to see what works. I know this information is probably already listed somewhere, but It would be nice for even just a link to the information upon the request to the ai.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/41","Chat bot stopping in the middle of generating text","2023-02-04T18:45:14Z","Closed issue","bug","Describe the bug
I have been using this chatbot to keep track of schedules for a yearly calendar and for keeping/organize notes. When making a calender or really anything that requires a decent amount of typing, the chatbot will stop mid-sentence. This is fixable when you say ""continue"" or ""you stopped halfway"" etc. but it is very tedious to keep typing this halfway through a length of text. This is especially annoying when you tell it to put it into a copyable format, and thus the stop results in having to copy multiple things, and editing a few lines so it is coherent where I paste the text into.organizing
To Reproduce
Tell it to write out every day, each week, or monthly benchmarked calendar of the year. It will probably say that it is pointless to write out each individual day, might do the same for every week, but for months it can.
with the calendar it has outlined, tell it to add benchmark dates like all key holidays, maybe put in a few birthdays of people you know, and just make an overbooked sort of calendar.
If it manages to do that without stopping halfway, tell it to put it into a copyable format, or keep adding dates, and make the time slots more defined. Eventually, it will stop halfway through a sentence or forget its typical ending of ""I hope this information provided fulfills your request""
Tell it to continue until it gives the rest of the code, it's nothing system-breaking, but very annoying to do each time you want to revise a huge selection of text or add one too many days to your filled calendar.
Code snippets
No response
OS
chrome
Node version
December 15th version
Library version
v3.0.1 is what was filled in as a example of this text box, but I could not currently find it.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/40","Light/dark mode screen rapidly flashing back and forth","2023-02-04T18:45:18Z","Closed issue","bug","Describe the bug
I was using open chat to make a few different calendars for this year, in the process I had multiple tabs of a new chat open, this way I was able to have one for the entire year, and one for the day-to-day routine, when I got to make the routine, however, I decided to toggle the screen into dark mode as it has been getting late, then upon doing so, the screen rapidly started flashing between the two as i forgot to close the other tabs, this happened for each tab. I closed the first 3 and the problem still seems to persist.
To Reproduce
open a new chat in multiple tabs, two should work but I had 3 to 4 open working on multiple projects.
have the first tabs in light mode, then toggle the third onto dark mode. It was fast to occur, and the button itself should be flashing making it hard to click in an effort to fix
enjoy your broken open chat screen as it annoyingly flashes while you try to be productive.
try to close the first two tabs and see if you can fix the third, it won't but you can try, you can even try closing the third. I am not sure how long it will last but after 30 seconds of trying anything I could think of I decided to report this bug
opening open chat is now taking ages to load, probably because the flickering has not yet stopped
after finally loading the flickering has slowed, but still persists consistently at least on this device.
Code snippets
No response
OS
chrome
Node version
December 15th version
Library version
v3.0.1 is what was filled in as a example of this text box, but I could not currently find it.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/39","More embedding examples for Nodejs","2023-09-03T15:05:55Z","Closed as not planned issue","documentation","OpenAI has some great embedding examples in the OpenAI Cookbook & API docs for Python, but none for Nodejs.
Would be awesome if you could add some for different use cases like clustering, regression, anomaly detection, visualization, search, context relevance, information retrieval
Thanks 🚀
 The text was updated successfully, but these errors were encountered: 
👍30
yassinebridi, ng-plentisoft, platform-kit, grimmer0125, gfortaine, saravmajestic, luismartinezs, trae410, lucasbemol, SinanAkkoyun, and 20 more reacted with thumbs up emoji
All reactions
👍30 reactions"
"https://github.com/openai/openai-node/issues/38","Help me to createImageVariation Fromhttps://url","2023-07-10T03:29:35Z","Closed issue","fixed in v4","Describe the feature or improvement you're requesting
#Working Below Code.
 let readStream = fs.createReadStream(""image.png"");
 response = await openai.createImageVariation(readStream, 1, ""1024x1024"");
#Not Working Below Code.
 let readStream = https.get(""https://storage.googleapis.com/inceptivestudio/1672042338704.png"", (stream) => {
 return stream;
 });
 response = await openai.createImageVariation(readStream, 1, ""1024x1024"");
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/36","Response (completion) is always empty","2022-12-23T11:25:44Z","Closed issue","bug","Describe the bug
I am trying to get the time complexity for some source code, and the response always comes back null. The call to OpenAI works, however.
I am doing this through a Firebase Callable Cloud function, and when I log the response, this is an example of what I typically get:
completion.data.choices: [{""text"":"""",""index"":0,""logprobs"":null,""finish_reason"":""stop""}]
Any idea what's happening here?
Code snippets
const { Configuration, OpenAIApi } = require(""openai"");const key =  'xxxxxxxxxxxx';const configuration = new Configuration({ apiKey: key });const openai = new OpenAIApi(configuration);

exports.getTimeComplexity = functions.https.onCall(async (data, context) => {
    const selection = data.selection;
    if (selection.length === 0) {
        throw new functions.https.HttpsError('invalid-argument', '[getTimeComplexity] Selection must be > 0 characters long');
    }
    if (!context.auth) {
        throw new functions.https.HttpsError('failed-precondition', '[getTimeComplexity] The function must be called while authenticated.');
    }

    // Get time complexity
    openai.createCompletion({
        model: ""text-davinci-003"",
        prompt: selection,
        temperature: 0,
        max_tokens: 64,
        top_p: 1.0,
        frequency_penalty: 0.0,
        presence_penalty: 0.0,
        stop: [""\n""],
    }).then((completion) => {
        const timeComplexity = completion.data.choices[0].text;
        console.log(`[getTimeComplexity] Time Complexity ✅: ${timeComplexity}`);
        return { 'success': true, 'complexity': timeComplexity };
    });});
OS
macOS v12.6
Node version
Node v16
Library version
openai v3.1.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/35","Can the axios dependency please be bumped to a current release of axios?","2023-07-08T19:44:17Z","Closed issue","bug,fixed in v4","Describe the bug
Currently it uses axios ^0.26.0 while we are at axios 1.2.1
It's easy to mitigate, but feels really wrong to use such an old version which has a totally different type interface.
To Reproduce
Simply install the openai package and try and pass a current version of axios into the openai instance constructor
Code snippets
No response
OS
osx
Node version
node 19
Library version
openai 3.1.0
 The text was updated successfully, but these errors were encountered: 
👍6
drob, trentearl, avindra, bads77, grimmer0125, and isaced reacted with thumbs up emoji
All reactions
👍6 reactions"
"https://github.com/openai/openai-node/issues/34","createImageVariation gives unclear error when input image is not square","2023-07-10T03:30:41Z","Closed issue","bug,fixed in v4","Describe the bug
Using the function createImageVariation with a non square image results in the following error:
(node:58341) UnhandledPromiseRejectionWarning: Error: Request failed with status code 400
    at createError (/Users/kenny.lindahl/Dev/test/open-ai-gpt/node_modules/axios/lib/core/createError.js:16:15)
    at settle (/Users/kenny.lindahl/Dev/test/open-ai-gpt/node_modules/axios/lib/core/settle.js:17:12)
    at IncomingMessage.handleStreamEnd (/Users/kenny.lindahl/Dev/test/open-ai-gpt/node_modules/axios/lib/adapters/http.js:322:11)
    at IncomingMessage.emit (events.js:387:35)
    at endReadableNT (internal/streams/readable.js:1317:12)
    at processTicksAndRejections (internal/process/task_queues.js:82:21)

Solution:
Option 1:
 The client should know that the image is not square and throw an error and not call the API.
Option 2:
 Alternatively it should change the image size (add margin, not stretch the image content) so it can be sent to the API with a successful response.
To Reproduce
Complete node program that reproduces the issue:
const { Configuration, OpenAIApi } = require(""openai"");
const fs = require(""fs"");

const configuration = new Configuration({
  apiKey: process.env.OPENAI_API_KEY,
});
const openai = new OpenAIApi(configuration);

(async () => {
  const response = await openai.createImageVariation(
    fs.createReadStream(__dirname + ""/images/non-square-image.png""),
    2,
    ""1024x1024""
  );

  console.log(""-------------------"");
  console.log(response);
})();

Code snippets
No response
OS
macOS Monterey: 12.5.1 (21G83)
Node version
v14.17.3
Library version
3.1.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/33","createCompletionFromModel is missing","2022-12-24T18:40:15Z","Closed issue","bug","Describe the bug
According to the fine-tuning docs on OpenAi, there should be a createCompletionFromModel function in your API:
const response = await openai.createCompletionFromModel({
  model: FINE_TUNED_MODEL
  prompt: YOUR_PROMPT,});
There is even a post in the forums that says it was included in versions 2.0.2:
But I'm getting errors saying that it's not part of the import. Is that function deprecated? How do we create a completion using a fine-tuning model?
To Reproduce
I forked and cloned the repo to search for createCompletionFromModel to make sure I wasn't missing something, but it came up empty.
Code snippets
No response
OS
mac)S
Node version
Node 16
Library version
latest
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/32","code writing","2023-02-04T18:47:41Z","Closed issue","bug","Describe the bug
I don't get back all the code from the api.
write me a function in javascript that makes 10 parallel fetch requests simultaneously for 100 iterations
the code is cut off.
To Reproduce
const { Configuration, OpenAIApi } = require(""openai"");

const argv = require('minimist')(process.argv.slice(2));
console.log(argv.help);
const configuration = new Configuration({
  apiKey: 'xxx',
});
const openai = new OpenAIApi(configuration);

(async () => {

  const response = await openai.createCompletion({
    model: ""code-davinci-002"",
    prompt: `/* javascript: ${argv.help}  */`,
    temperature: 0,
    max_tokens: 256,
    top_p: 1,
    frequency_penalty: 0,
    presence_penalty: 0,
  });

  response.data.choices.map(c => console.log(c.text));
})();

$ node code.js --help ""write me a function that makes 10 parallel fetch requests simultaneously for 100 iterations""
Code snippets
No response
OS
Linux
Node version
19
Library version
3.0.1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/31","all models except davinci 2 + not working...","2023-01-07T07:20:29Z","Closed issue","bug","Describe the bug
all models except davinci 2 + not working...
I get axios errors when trying to use models such as ada / babbage / etc EXCEPT davinci 2+ -> these models are not working; throws error.
To Reproduce
Fetch using redux.
Error snip:
response: {
 status: 404,
 statusText: 'Not Found',
 headers: {
 date: 'Fri, 02 Dec 2022 00:49:31 GMT',
 'content-type': 'application/json; charset=utf-8',
 'content-length': '158',
 connection: 'close',
 vary: 'Origin',
 },
 config: {
 transitional: [Object],
 adapter: [Function: httpAdapter],
 transformRequest: [Array],
 transformResponse: [Array],
 timeout: 0,
 xsrfCookieName: 'XSRF-TOKEN',
 xsrfHeaderName: 'X-XSRF-TOKEN',
 maxContentLength: -1,
 maxBodyLength: -1,
 validateStatus: [Function: validateStatus],
 headers: [Object],
 method: 'post',
Code snippets
import { OpenAIApi, Configuration } from 'openai';import { Ratelimit } from '@upstash/ratelimit';import { Redis } from '@upstash/redis';import getIP from '../../../utils/get-ip';

const configuration = new Configuration({
    apiKey: process.env.OPENAI_API_KEY,});const openai = new OpenAIApi(configuration);

const redis = new Redis({
    url: process.env.UPSTASH_REST_API_DOMAIN,
    token: process.env.UPSTASH_REST_API_TOKEN,});

const ratelimit = new Ratelimit({
    redis: redis,
    limiter: Ratelimit.slidingWindow(3, '1 d'),});

export default async function response(req, res) {
    const ip = getIP(req);
    const result = await ratelimit.limit(ip);
    res.setHeader('X-RateLimit-Limit', result.limit);
    res.setHeader('X-RateLimit-Remaining', result.remaining);

    if (req.method !== 'POST') {
        res.status(405).json({ error: 'Method not allowed' });
        return;
    }

    if (!req.body.projectId) {
        res.status(400).json({ error: 'Missing projectId' });
        return;
    }

    if (!result.success) {
        res.status(429).json({
            error: 'You have reached your daily limit of 3 free completions. Try again tomorrow or upgrade your plan in account settings to continue using services regularly.',
        });
        return;
    }

    const completion = await openai.createCompletion({
        model: 'text-davinci-002',
        prompt: req.body.prompt,
        temperature: 0.6,
        max_tokens: 2000,
        presence_penalty: 0.5,
        // frequency_penalty: 0.5,
    });

    try {
        const completeModeration = await openai.createModeration({
            input: completion.data.choices[0].text,
            model: 'text-moderation-latest',
        });
        const moderationRes = completeModeration.data.results[0].flagged;
        if (moderationRes === false) {
            res.status(200).json({ response: completion.data.choices[0].text });
        } else {
            res.status(500).json({
                error: 'Sorry. The output has been flagged for inappropriate content. Please try again.',
            });
        }
    } catch (error) {
        res.status(500).json({ error: error.message });
    }}
OS
mac
Node version
v18.12.0
Library version
3.1.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/30","Internal use of axios causes error inside Cloudflare Workers","2023-07-10T00:15:39Z","Closed issue","bug,fixed in v4","Describe the bug
I am trying to use the client inside a Cloudflare Worker and I get an error as follows:
TypeError: adapter is not a function
    at dispatchRequest (index.js:35781:14)
    at Axios.request (index.js:36049:19)
    at Function.wrap [as request] (index.js:34878:20)

Seems to be a common problem as the way Axios checks for XHR breaks in CF workers which is a reduced node environment:
https://community.cloudflare.com/t/typeerror-e-adapter-s-adapter-is-not-a-function/166469/2
Recommendation is to use fetch instead.
To Reproduce
Try to use API in a cloudflare worker
Code snippets
No response
OS
Windows 10
Node version
Node v16
Library version
openai v3.1.0
 The text was updated successfully, but these errors were encountered: 
👍2
lucassarcanjo and yousefamar reacted with thumbs up emoji
All reactions
👍2 reactions"
"https://github.com/openai/openai-node/issues/28","Getting more than a dozen errors in api.d.ts","2023-07-10T01:33:58Z","Closed issue","bug,fixed in v4","Describe the bug
Updated from v2.0.5 to 3.0.0 of the package and got 16 errors in node_modules/openai/dist/api.d.ts:
 [{
 ""resource"": ""PROJECT_PATH/node_modules/openai/dist/api.d.ts"",
 ""owner"": ""typescript"",
 ""code"": ""2304"",
 ""severity"": 8,
 ""message"": ""Cannot find name 'File'."",
 ""source"": ""ts"",
 ""startLineNumber"": 1666,
 ""startColumn"": 24,
 ""endLineNumber"": 1666,
 ""endColumn"": 24
 }]
To Reproduce
Simply npm install openai on any typescript project.
Code snippets
""devDependencies"": {
    ""@types/glob"": ""^7.2.0"",
    ""@types/mocha"": ""^9.1.1"",
    ""@types/node"": ""14.x"",
    ""@types/vscode"": ""^1.67.0"",
    ""@typescript-eslint/eslint-plugin"": ""^5.21.0"",
    ""@typescript-eslint/parser"": ""^5.21.0"",
    ""@vscode/test-electron"": ""^2.1.3"",
    ""eslint"": ""^8.14.0"",
    ""glob"": ""^8.0.1"",
    ""mocha"": ""^9.2.2"",
    ""typescript"": ""^4.6.4""
  },
  ""dependencies"": {
    ""openai"": ""^3.0.0""
  }
OS
macOS
Node version
Node v16.13.1
Library version
3.0.0
 The text was updated successfully, but these errors were encountered: 
👍2
jaimefps and nikwen reacted with thumbs up emoji
All reactions
👍2 reactions"
"https://github.com/openai/openai-node/issues/27","openai.createImage doesn't surface errors","2023-07-10T01:29:03Z","Closed issue","bug,fixed in v4","Describe the bug
If I try calling the OpenAI API with openai.createImage and my request is malformed, I just get a generic ""request failed with status code 400"". If I make the request with cURL instead, I can see the reason my request failed (invalid_request_error, rejected due to safety system).
Possible to surface these errors to the client?
To Reproduce
Try to create an image from the Node client with the text ""Elon Musk crying laughing emoji""
Observe the error message
Do the same via cURL
Observe the verbose error message
Code snippets
No response
OS
macOS
Node version
16
Library version
3.1.0
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/26","Add type definitions for TS support","2022-11-04T20:20:56Z","Closed issue","No label","Describe the feature or improvement you're requesting
Right now, migrating to Typescript or creating a d.ts file manually are few of the options.
 If the API is going to be minimal in the long term, then simple index.d.ts created manually should be enough.
Additional context
No response
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/25","Mismatch between createFile(file: File) and createReadStream in docs","2023-07-07T04:24:44Z","Closed issue","bug,fixed in v4","Describe the bug
The typings were updated such that the signature is createFile(file: File), but the docs example shows a ReadStream being provided.
File is not available in Node. What is meant to be done here? Is this a typo, should be File | ReadStream?
To Reproduce
Try to pass a ReadStream to createFile(), see type error.
Code snippets
No response
OS
N/A
Node version
latest
Library version
latest
 The text was updated successfully, but these errors were encountered: 
👍4
danrasmuson, sam-goodwin, beytarovski, and ilyess-hs reacted with thumbs up emoji
All reactions
👍4 reactions"
"https://github.com/openai/openai-node/issues/24","The result is completely wrong for unknow reason","2023-07-10T03:31:27Z","Closed issue","No label","Describe the bug
Use the config below, get the unrelated content in the choices array. And put same prompt on Open AI Play ground, it returns right content (""The tl;dr version of this would be to simply say that the article is about the importance of choosing the right words when communicating, and that the wrong words can easily lead to misunderstanding."").
if I changed prompt: 'Tl;dr, summarize in one paragraph without bullet:\n in one paragraph without bullet.\n', it works fine. Try other content works fine. Just like there's some cache. However I tried to run from AWS lambda and local, both same wrong result.
config: {
 model: 'text-davinci-002',
 prompt: 'Tl;dr, summarize in one paragraph without bullet:\nsummarize in one paragraph without bullet.\n',
 temperature: 0.5,
 max_tokens: 320,
 best_of: 1,
 frequency_penalty: 0,
 presence_penalty: 0,
 logprobs: 0,
 top_p: 1
 },
choices: [
 {
 text: '\nThe article discusses the pros and cons of taking a gap year, or a year off between high school and college. The pros include gaining life experience, taking time to figure out what you want to study, and having the opportunity to travel. The cons include falling behind your peers academically, feeling out of place when you return to school, and struggling to find a job after graduation. Ultimately, the decision to take a gap year is a personal one and depends on what you hope to gain from the experience.',
 index: 0,
 logprobs: [Object],
 finish_reason: 'stop'
 }
 ],
To Reproduce
Simple use the same config above. It keeps happening to me, always same result.
Code snippets
No response
OS
Windows
Node version
Node v16
Library version
v3.0.1
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/23","TypeError: adapter is not a function","2022-10-24T04:04:43Z","Closed issue","question","When making a request,
 I get TypeError: adapter is not a function
Assuming issue with axios
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/22","npm run dev not working","2022-10-08T07:46:04Z","Closed issue","No label","No description provided.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/20","Missing Async in most functions","2022-10-20T02:23:38Z","Closed issue","No label","All of your examples use await, however you did not set any of your functions as async (at least in version 3.0.0).
so instead of :
 const val = await blahblah()
you have to do :
 const val = blahblah();
 val.then((data)=>{
 console.log(data);
 });
is this intentional?
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/19","createModeration doesn't exist","2022-10-20T02:15:24Z","Closed issue","No label","Can't use the new moderation endpoint with this library and I'd rather use it via the library than making the request myself.
https://beta.openai.com/docs/guides/moderation/overview
 The text was updated successfully, but these errors were encountered: 
👍15
ezzcodeezzlife, jacobpedd, Josh-McFarlin, sashabaranov, bensalilijames, Lisennk, princechauhan1992, timothypratley, WesWeCan, progrmoiz, and 5 more reacted with thumbs up emoji
All reactions
👍15 reactions"
"https://github.com/openai/openai-node/issues/18","How to use stream: true?","2023-07-08T19:50:52Z","Closed issue","fixed in v4","I'm a bit lost as to how to actually use stream: true in this library.
Example incorrect syntax:
const res = await openai.createCompletion({
  model: ""text-davinci-002"",
  prompt: ""Say this is a test"",
  max_tokens: 6,
  temperature: 0,
  stream: true,});

res.onmessage = (event) => {
  console.log(event.data);}
 The text was updated successfully, but these errors were encountered: 
👍52
mattgabor, Awendel, runvnc, vagoel, AnimeAllstar, drob, 3chospirits, raoulcapello, gfortaine, ctjlewis, and 42 more reacted with thumbs up emoji
All reactions
👍52 reactions"
"https://github.com/openai/openai-node/issues/17","Usage type for Completion Requests is missing","2022-10-20T03:08:05Z","Closed issue","No label","Description
According to the docs a response to a completion request should have a usage property that allows to see how many tokens have been used for the request + response. Manually checking the response of openai.createCompletion also shows that the usage property exists in response.data:
const response = await openai.createCompletion({
    model: 'text-davinci-002',
    prompt: `<someprompt>`,
  });

console.log(response.data.usage)
However, the CreateCompletionResponse type does not include usage and thus Typescript is throwing an error when trying to access usage in an openai completion response.
Expected Behavior
openai should have a type definition for usage in CreateCompletionResponse that allows to see & access the used tokens in a request.
 The text was updated successfully, but these errors were encountered: 
👍4
ezzcodeezzlife, DeveloperHarris, alanpog, and Dezainer reacted with thumbs up emoji
All reactions
👍4 reactions"
"https://github.com/openai/openai-node/issues/16","Insert completion","2022-10-20T03:53:17Z","Closed issue","No label","Is it possible to use the insert functionality?
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/15","code-davinci-002","2022-10-20T04:07:50Z","Closed issue","No label","Im using the example code from the playground:
const { Configuration, OpenAIApi } = require(""openai"");

const configuration = new Configuration({
  apiKey: process.env.OPENAI_API_KEY,
});
const openai = new OpenAIApi(configuration);

const response = await openai.createCompletion(""code-davinci-002"", {
  prompt: ""##### Translate this function  from Python into Haskell\n### Python\n    \n    def predict_proba(X: Iterable[str]):\n        return np.array([predict_one_probas(tweet) for tweet in X])\n    \n### Haskell"",
  temperature: 0,
  max_tokens: 54,
  top_p: 1,
  frequency_penalty: 0,
  presence_penalty: 0,
  stop: [""###""],
});

This returns a 404 error. Is codex not available as api?
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/14","Deleting Fine-tune model results in a 404","2022-10-20T03:57:14Z","Closed issue","No label","Using the call
const response = await openai.listFineTunes();
To get the list of my fine tunes. Then from that list, I'm using the fine_tuned_model field and passing that to:
await openai.deleteModel(model as string);
I receive a 404 error back that says;
{
  error: {
    message: 'That model does not exist',
    type: 'invalid_request_error',
    param: 'model',
    code: null
  }}
The url looks like this:
https://api.openai.com/v1/models/curie%3Aft-personal-2022-05-02-16-11-13

Following the steps in the documentation here: https://beta.openai.com/docs/api-reference/fine-tunes/delete-model
Thanks!
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/13","Provide a user identifer","2022-05-02T00:14:52Z","Closed issue","No label","OpenAi Safety best practices:
To help with monitoring for possible misuse, developers serving multiple end-users should pass an additional user parameter to OpenAI with each API call, in which user is a unique ID representing a particular end-user.
With the Python Client you can pass the additional ""user"" argument:
response = openai.Completion.create(
  engine=""davinci"",
  prompt=""This is a test"",
  max_tokens=5,
  user=""1""
)

Is this also a feature in this node client?
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/12","Providing a user identifer","2022-05-01T23:49:35Z","Closed issue","No label","No description provided.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/11","429 error despite zero API usage","2022-04-30T17:40:08Z","Closed issue","No label","im using the nodejs example from the docs. Inserted my API key.
const { Configuration, OpenAIApi } = require(""openai"");const configuration = new Configuration({
  apiKey: ""MY-KEY"",});

async function getCompletion () {
  const openai = new OpenAIApi(configuration);
  const response = await openai.createCompletion(""text-curie-001"", {
    prompt: ""Say this is a test"",
    max_tokens: 5
  })
  .catch(err => {
    console.log(err);
  });
  console.log(response);}

getCompletion();
returns:
 response: {
    status: 429,
    statusText: 'Too Many Requests',
    headers: {
      date: 'Thu, 28 Apr 2022 12:51:34 GMT',
      'content-type': 'application/json; charset=utf-8',
      'content-length': '205',
      connection: 'close',
      vary: 'Origin',
      'x-request-id': 'xxxx',
      'strict-transport-security': 'max-age=15724800; includeSubDomains'
    },
    config: {
      transitional: [Object],
      adapter: [Function: httpAdapter],
      transformRequest: [Array],
      transformResponse: [Array],
      timeout: 0,
      xsrfCookieName: 'XSRF-TOKEN',
      xsrfHeaderName: 'X-XSRF-TOKEN',
      maxContentLength: -1,
      maxBodyLength: -1,
      validateStatus: [Function: validateStatus],
      headers: [Object],
      method: 'post',
      data: '{""prompt"":""Say this is a test"",""max_tokens"":5}',
      url: 'https://api.openai.com/v1/engines/text-curie-001/completions'
    },
    request: <ref *1> ClientRequest {
      _events: [Object: null prototype],
      _eventsCount: 7,
      _maxListeners: undefined,
      outputData: [],
      outputSize: 0,
      writable: true,
      destroyed: false,
      _last: true,
      chunkedEncoding: false,
      shouldKeepAlive: false,
      _defaultKeepAlive: true,
      useChunkedEncodingByDefault: true,
      sendDate: false,
      _removedConnection: false,
      _removedContLen: false,
      _removedTE: false,
      _contentLength: null,
      _hasBody: true,
      _trailer: '',
      finished: true,
      _headerSent: true,
      socket: [TLSSocket],
      _header: 'POST /v1/engines/text-curie-001/completions HTTP/1.1\r\n' +
        'Accept: application/json, text/plain, */*\r\n' +
        'Content-Type: application/json\r\n' +
        'User-Agent: OpenAI/NodeJS/2.0.5\r\n' +
        'Authorization: Bearer MY-KEY\n' +
        'Content-Length: 46\r\n' +
        'Host: api.openai.com\r\n' +
        'Connection: close\r\n' +
        '\r\n',
      _keepAliveTimeout: 0,
      _onPendingData: [Function: noopPendingOutput],
      agent: [Agent],
      socketPath: undefined,
      method: 'POST',
      maxHeaderSize: undefined,
      insecureHTTPParser: undefined,
      path: '/v1/engines/text-curie-001/completions',
      _ended: true,
      res: [IncomingMessage],
      aborted: false,
      timeoutCb: null,
      upgradeOrConnect: false,
      parser: null,
      maxHeadersCount: null,
      reusedSocket: false,
      host: 'api.openai.com',
      protocol: 'https:',
      _redirectable: [Writable],
      [Symbol(kCapture)]: false,
      [Symbol(kNeedDrain)]: false,
      [Symbol(corked)]: 0,
      [Symbol(kOutHeaders)]: [Object: null prototype]
    },
    data: { error: [Object] }
  },
  isAxiosError: true,
  toJSON: [Function: toJSON]
}

The request returns a 429 Status. I have zero usage on my account and i only make a single request each time.
 I tried all engines.
Is this a known problem? Since this problem has little reltation to this node repo i will close this issue as soon as possible.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/10","Classification betas not leading to expected results","2022-10-20T03:59:07Z","Closed issue","No label","What happens:
 Passing classification_betas, e.g. classification_betas: [1, 0.5] leads to equal f-beta values in the resulting CSV file, although precision and recall differ. The columns are named correctly in the resulting file, e.g. classification/f0.5 etc, but the cell values always equal f-1, not the β of the respective column.
What I expected:
 Differing f-values, as the parameter weighs precision higher (1x, 2x, ... times as much) than recall.
Maybe in interpreted the docs wrongly though, and this parameter is supposed to be used differently.
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/9","Edit request giving 404","2022-05-10T21:13:13Z","Closed issue","No label","Hi openai,
I'm currently using completion API and am attempting to use the edit API now as well.
Code:
const result = await openAI.createCompletion('text-davinci-002', {
    prompt: `${content}\n\nTl;dr`,
    temperature: 0.7,
    max_tokens: 60,
    top_p: 1.0,
    frequency_penalty: 0.0,
    presence_penalty: 0.0,
  })

const result = await openAI.createEdit('text-davinci-002', {
    input: content,
    instruction: 'Rewrite this more simply',
  })

The first request has been working for months and still is, but the second returns this
(node:5013) UnhandledPromiseRejectionWarning: Error: Request failed with status code 404
    at createError (/Users/zfoster/gravity/node_modules/openai/node_modules/axios/lib/core/createError.js:16:15)
    at settle (/Users/zfoster/gravity/node_modules/openai/node_modules/axios/lib/core/settle.js:17:12)
    at IncomingMessage.handleStreamEnd (/Users/zfoster/gravity/node_modules/openai/node_modules/axios/lib/adapters/http.js:322:11)
    at IncomingMessage.emit (events.js:412:35)
    at IncomingMessage.emit (domain.js:470:12)
    at endReadableNT (internal/streams/readable.js:1317:12)
    at processTicksAndRejections (internal/process/task_queues.js:82:21)

Let me know if I need to make any changes and if there's an existing example for doing something like a ""simplification rewrite"". Basically trying to summarize and rewrite the text in more simple words with these two operations.
Thanks!
 The text was updated successfully, but these errors were encountered: 
👍1
bayramn reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/openai-node/issues/7","CreateCompletionRequest Types","2023-07-10T03:33:31Z","Closed issue","fixed in v4","As part of a 'Pre-launch Review' we've been instructed to provide a user id as part of our completion requests:
Pass a uniqueID for every user w/ each API call (both for Completion & the Content Filter) e.g. user= $uniqueID. This 'user' param can be passed in the request body along with other params such as prompt, max_tokens etc.
However, the CreateCompletionRequest interface does not have an optional user property.
Let me know if I'm missing anything or if anything else is required on my end.
 The text was updated successfully, but these errors were encountered: 
👍1
GoulartNogueira reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/openai-node/issues/6","Refused to set unsafe header ""User-Agent""","2022-10-20T04:04:08Z","Closed issue","fixed in v4","Getting this error when trying to run the following code:
Refused to set unsafe header ""User-Agent""
const configuration = new Configuration({
    apiKey: process.env.OPENAI_API_KEY,
});
const openai = new OpenAIApi(configuration);
const response = await openai.createCompletion(""code-davinci-001"", {
  prompt: filePart,
  temperature: 0.1,
  max_tokens: 2000,
  top_p: 1,
  frequency_penalty: 0,
  presence_penalty: 0.5,
});

 The text was updated successfully, but these errors were encountered: 
👍4
DiegoVenancioVieira, briansunter, nick-cjyx9, and mavsin reacted with thumbs up emoji
All reactions
👍4 reactions"
"https://github.com/openai/openai-node/issues/5","Can't upload file","2022-02-04T20:32:58Z","Closed issue","No label","I'm trying to upload a file that can then be used to create a fine-tune. It's been passed through the CLI validator so I know it's correct, but I keep getting the following error from Axios:
data: {
      error: {
        message: 'The browser (or proxy) sent a request that this server could not understand.',
        type: 'server_error',
        param: null,
        code: null
      }
    }
Here's how I'm trying to upload the file;
const configuration = new Configuration({
    apiKey: process.env.OPENAI_API_KEY,
  });
  const openai = new OpenAIApi(configuration);

await openai.createFile(`${uploadFilename}.jsonl`, ""fine-tune"");
Am I doing this right? I can't seem to see what the problem could be.
 The text was updated successfully, but these errors were encountered: 
👍1
beytarovski reacted with thumbs up emoji
All reactions
👍1 reaction"
"https://github.com/openai/openai-node/issues/4","test","2022-01-28T17:12:00Z","Closed issue","No label","test
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/3","Input sanitizing causes 400 BAD_REQUEST","2022-01-28T18:03:12Z","Closed issue","No label","I was using the openai.createClassification method and started getting 400 BAD REQUEST when I introduced the input string (attached to the bottom) as one of the examples for labeling.
I believe there is some sanitizing that fails in some area.
href=\""https:// becomes href=\\""https:// which is not valid JSON for payload.
Here is the JSON which seems to be entirely valid
Here is the raw request which was created by the method and seems to be invalid JSON.
Error: Parse error on line 11:
...rom <a href=\\""				https: //github.com/
----------------------^
Expecting 'EOF', '}', ':', ',', ']', got 'undefined'

Here is my code
 The text was updated successfully, but these errors were encountered: 
All reactions"
"https://github.com/openai/openai-node/issues/2","Use createCompletion() with fine-tune models","2022-01-27T14:10:00Z","Closed issue","No label","Currently createCompletion() function requires an engine id as first parameter. When I pass the model id instead i get 404 error which makes sense if is waiting an engine.
 Is it posible to use a fine tune model?
 The text was updated successfully, but these errors were encountered: 
All reactions"
